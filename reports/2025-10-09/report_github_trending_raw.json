{
  "generated_at": "2025-10-09T02:06:04Z",
  "site": "github-trending",
  "num_articles": 31,
  "articles": [
    {
      "url": "https://github.com/Stremio/stremio-web",
      "title": "Stremio/stremio-web",
      "date": null,
      "executive_summary": [
        "Stremio - Freedom to Stream",
        "---",
        "Stremio - Freedom to Stream\nStremio is a modern media center that's a one-stop solution for your video entertainment. You discover, watch and organize video content from easy to install addons.\nBuild\nPrerequisites\nNode.js 12 or higher\npnpm\n10 or higher\nInstall dependencies\npnpm install\nStart development server\npnpm start\nProduction build\npnpm run build\nRun with Docker\ndocker build -t stremio-web\n.\ndocker run -p 8080:8080 stremio-web\nScreenshots\nBoard\nDiscover\nMeta Details\nLicense\nStremio is copyright 2017-2023 Smart code and available under GPLv2 license. See the\nLICENSE\nfile in the project for more information.",
        "今日の獲得スター数: 1,799",
        "累積スター数: 5,573"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/Stremio/stremio-web"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/aandrew-me/ytDownloader",
      "title": "aandrew-me/ytDownloader",
      "date": null,
      "executive_summary": [
        "Desktop App for downloading Videos and Audios from hundreds of sites",
        "---",
        "ytDownloader\nA modern GUI video and audio downloader supporting\nhundreds of sites\nFeatures 🚀\n✅ Supports hundreds of sites including Youtube, Facebook, Instagram, Tiktok, Twitter and so on.\n✅ Multiple themes\n✅ Video Compressor with Hardware Acceleration\n✅ Advanced options like Range Selection, Subtitles\n✅ Download playlists\n✅ Available on Linux, Windows & macOS\n✅ Fast download speeds\n✅ And of-course no trackers or ads\nScreenshots\nInstallation\nWindows 🪟\nTraditional way\nDownload and install the exe or msi file. Exe file lets you choose custom download location, msi file doesn't ask for location. Windows defender may show a popup saying\nWindows Protected Your PC\n. Just click on\nMore info\nand click on\nRun Anyway\nChocolatey\nApp can be installed from\nChocolatey\nusing the following command\nchoco install ytdownloader\nScoop\nApp can be installed with\nScoop\nusing the following command\nscoop install https://raw.githubusercontent.com/aandrew-me/ytDownloader/main/ytdownloader.json\nWinget\nApp can be installed with\nWinget\nusing the following command\nwinget install aandrew-me.ytDownloader\nLinux 🐧\nLinux has several options available - Flatpak, AppImage and Snap.\nFlatpak is recommended. For arm processors, download from flathub.\nAppImage\nAppImage\nformat is supported on most Linux distros and has Auto-Update support.\nIt just needs to be executed after downloading. See more about\nAppImages here\n.\nAppImageLauncher\nis recommended for integrating AppImages.\nFlatpak\nflatpak install flathub io.github.aandrew_me.ytdn\nSnapcraft\nsudo snap install ytdownloader\nmacOS 🍎\nSince the app is not signed, when you will try to open the app, macOS will not allow you to open it.\nYou need to open terminal and execute:\nsudo xattr -r -d com.apple.quarantine /Applications/YTDownloader.app\nYou will also need to install\nyt-dlp\nwith\nhomebrew\nbrew install yt-dlp\nInternationalization (Localization) 🌍\nTranslations into other languages would be highly appreciated. If you want to help translating the app to other languages, you can join from\nhere\n. Open a new issue and that language will be added to Crowdin. Please don't make pull requests with json files, instead use Crowdin.\n✅ Available languages\nName\nStatus\nArabic\n✔️\nEnglish\n✔️\nSimplified Chinese\n✔️\nFinnish\n✔️\nFrench\n✔️\nGerman\n✔️\nGreek\n✔️\nHungarian\n✔️\nItalian\n✔️\nJapanese\n✔️\nPersian\n✔️\nPolish\n✔️\nPortuguese (Brazil)\n✔️\nRussian\n✔️\nSpanish\n✔️\nTurkish\n✔️\nUkrainian\n✔️\nVietnamese\n✔️\nThanks to\nnxjosephofficial\n,\nLINUX-SAUNA\n,\nProxycon\n,\nalbanobattistella\n,\nTheBlueQuasar\n,\nMrQuerter\n,\nKotoWhiskas\n,\nAndré\n,\nhaggen88\n,\nXfedeX\n,\nJok3r\n,\nTitouanReal\n,\nsoredake\n,\nyoi\n,\nHowlingWerewolf\n,\nKum\n,\nMohammed Bakry\n,\nHuang Bingfeng\nand others for helping.\nUsed technologies\nyt-dlp\nElectron\nffmpeg\nnodeJS\nflaticon\nFor building or running from source code\nNodejs\n(along with npm) needs to be installed.\nRequired commands to get started.\ngit clone https://github.com/aandrew-me/ytDownloader.git\ncd ytDownloader\nnpm i\nTo run with\nElectron\n:\nnpm start\nYou need to download ffmpeg and put it in the root directory of the project. If you don't need to build for arm processor, you can download ffmpeg by executing any of the files - linux.sh / mac.sh / windows.sh depending on the platform. Otherwise you need to download ffmpeg from\nhere\nfor windows/linux and from\nhere\nfor mac (not tested)\nTo build for Linux (It will create packages as specified in package.json). The builds are stored in\nrelease\nfolder.\nnpm run linux\nTo build for Windows\nnpm run windows\nTo build for macOS\nnpm run mac\nIf you only want to build for one format, you can do\nnpx electron-builder -l appimage\nIt will just create a linux appimage build.",
        "今日の獲得スター数: 612",
        "累積スター数: 3,894"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/aandrew-me/ytDownloader"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/FlowiseAI/Flowise",
      "title": "FlowiseAI/Flowise",
      "date": null,
      "executive_summary": [
        "Build AI Agents, Visually",
        "---",
        "English |\n繁體中文\n|\n简体中文\n|\n日本語\n|\n한국어\nBuild AI Agents, Visually\n📚 Table of Contents\n⚡ Quick Start\n🐳 Docker\n👨‍💻 Developers\n🌱 Env Variables\n📖 Documentation\n🌐 Self Host\n☁️ Flowise Cloud\n🙋 Support\n🙌 Contributing\n📄 License\n⚡Quick Start\nDownload and Install\nNodeJS\n>= 18.15.0\nInstall Flowise\nnpm install -g flowise\nStart Flowise\nnpx flowise start\nOpen\nhttp://localhost:3000\n🐳 Docker\nDocker Compose\nClone the Flowise project\nGo to\ndocker\nfolder at the root of the project\nCopy\n.env.example\nfile, paste it into the same location, and rename to\n.env\nfile\ndocker compose up -d\nOpen\nhttp://localhost:3000\nYou can bring the containers down by\ndocker compose stop\nDocker Image\nBuild the image locally:\ndocker build --no-cache -t flowise\n.\nRun image:\ndocker run -d --name flowise -p 3000:3000 flowise\nStop image:\ndocker stop flowise\n👨‍💻 Developers\nFlowise has 3 different modules in a single mono repository.\nserver\n: Node backend to serve API logics\nui\n: React frontend\ncomponents\n: Third-party nodes integrations\napi-documentation\n: Auto-generated swagger-ui API docs from express\nPrerequisite\nInstall\nPNPM\nnpm i -g pnpm\nSetup\nClone the repository:\ngit clone https://github.com/FlowiseAI/Flowise.git\nGo into repository folder:\ncd\nFlowise\nInstall all dependencies of all modules:\npnpm install\nBuild all the code:\npnpm build\nExit code 134 (JavaScript heap out of memory)\nIf you get this error when running the above `build` script, try increasing the Node.js heap size and run the script again:\n#\nmacOS / Linux / Git Bash\nexport\nNODE_OPTIONS=\n\"\n--max-old-space-size=4096\n\"\n#\nWindows PowerShell\n$env\n:NODE_OPTIONS=\n\"\n--max-old-space-size=4096\n\"\n#\nWindows CMD\nset\nNODE_OPTIONS=--max-old-space-size=4096\nThen run:\npnpm build\nStart the app:\npnpm start\nYou can now access the app on\nhttp://localhost:3000\nFor development build:\nCreate\n.env\nfile and specify the\nVITE_PORT\n(refer to\n.env.example\n) in\npackages/ui\nCreate\n.env\nfile and specify the\nPORT\n(refer to\n.env.example\n) in\npackages/server\nRun:\npnpm dev\nAny code changes will reload the app automatically on\nhttp://localhost:8080\n🌱 Env Variables\nFlowise supports different environment variables to configure your instance. You can specify the following variables in the\n.env\nfile inside\npackages/server\nfolder. Read\nmore\n📖 Documentation\nYou can view the Flowise Docs\nhere\n🌐 Self Host\nDeploy Flowise self-hosted in your existing infrastructure, we support various\ndeployments\nAWS\nAzure\nDigital Ocean\nGCP\nAlibaba Cloud\nOthers\nRailway\nRender\nHuggingFace Spaces\nElestio\nSealos\nRepoCloud\n☁️ Flowise Cloud\nGet Started with\nFlowise Cloud\n.\n🙋 Support\nFeel free to ask any questions, raise problems, and request new features in\nDiscussion\n.\n🙌 Contributing\nThanks go to these awesome contributors\nSee\nContributing Guide\n. Reach out to us at\nDiscord\nif you have any questions or issues.\n📄 License\nSource code in this repository is made available under the\nApache License Version 2.0\n.",
        "今日の獲得スター数: 366",
        "累積スター数: 44,832"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/FlowiseAI/Flowise"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/BeehiveInnovations/zen-mcp-server",
      "title": "BeehiveInnovations/zen-mcp-server",
      "date": null,
      "executive_summary": [
        "The power of Claude Code / GeminiCLI / CodexCLI + [Gemini / OpenAI / OpenRouter / Azure / Grok / Ollama / Custom Model / All Of The Above] working as one.",
        "---",
        "Zen MCP: Many Workflows. One Context.\nZen_CLink_web.mp4\n👉\nWatch more examples\nYour CLI + Multiple Models = Your AI Dev Team\nUse the 🤖 CLI you love:\nClaude Code\n·\nGemini CLI\n·\nCodex CLI\n·\nQwen Code CLI\n·\nCursor\n·\nand more\nWith multiple models within a single prompt:\nGemini · OpenAI · Anthropic · Grok · Azure · Ollama · OpenRouter · DIAL · On-Device Model\n🆕 Now with CLI-to-CLI Bridge\nThe new\nclink\n(CLI + Link) tool connects external AI CLIs directly into your workflow:\nConnect external CLIs\nlike\nGemini CLI\n,\nCodex CLI\n, and\nClaude Code\ndirectly into your workflow\nCLI Subagents\n- Launch isolated CLI instances from\nwithin\nyour current CLI! Claude Code can spawn Codex subagents, Codex can spawn Gemini CLI subagents, etc. Offload heavy tasks (code reviews, bug hunting) to fresh contexts while your main session's context window remains unpolluted. Each subagent returns only final results.\nContext Isolation\n- Run separate investigations without polluting your primary workspace\nRole Specialization\n- Spawn\nplanner\n,\ncodereviewer\n, or custom role agents with specialized system prompts\nFull CLI Capabilities\n- Web search, file inspection, MCP tool access, latest documentation lookups\nSeamless Continuity\n- Sub-CLIs participate as first-class members with full conversation context between tools\n#\nCodex spawns Codex subagent for isolated code review in fresh context\nclink with codex codereviewer to audit auth module\nfor\nsecurity issues\n#\nSubagent reviews in isolation, returns final report without cluttering your context as codex reads each file and walks the directory structure\n#\nConsensus from different AI models → Implementation handoff with full context preservation between tools\nUse consensus with gpt-5 and gemini-pro to decide: dark mode or offline support next\nContinue with clink gemini - implement the recommended feature\n#\nGemini receives full debate context and starts coding immediately\n👉\nLearn more about clink\nWhy Zen MCP?\nWhy rely on one AI model when you can orchestrate them all?\nA Model Context Protocol server that supercharges tools like\nClaude Code\n,\nCodex CLI\n, and IDE clients such\nas\nCursor\nor the\nClaude Dev VS Code extension\n.\nZen MCP connects your favorite AI tool\nto multiple AI models\nfor enhanced code analysis, problem-solving, and collaborative development.\nTrue AI Collaboration with Conversation Continuity\nZen supports\nconversation threading\nso your CLI can\ndiscuss ideas with multiple AI models, exchange reasoning, get second opinions, and even run collaborative debates between models\nto help you reach deeper insights and better solutions.\nYour CLI always stays in control but gets perspectives from the best AI for each subtask. Context carries forward seamlessly across tools and models, enabling complex workflows like: code reviews with multiple models → automated planning → implementation → pre-commit validation.\nYou're in control.\nYour CLI of choice orchestrates the AI team, but you decide the workflow. Craft powerful prompts that bring in Gemini Pro, GPT 5, Flash, or local offline models exactly when needed.\nReasons to Use Zen MCP\nA typical workflow with Claude Code as an example:\nMulti-Model Orchestration\n- Claude coordinates with Gemini Pro, O3, GPT-5, and 50+ other models to get the best analysis for each task\nContext Revival Magic\n- Even after Claude's context resets, continue conversations seamlessly by having other models \"remind\" Claude of the discussion\nGuided Workflows\n- Enforces systematic investigation phases that prevent rushed analysis and ensure thorough code examination\nExtended Context Windows\n- Break Claude's limits by delegating to Gemini (1M tokens) or O3 (200K tokens) for massive codebases\nTrue Conversation Continuity\n- Full context flows across tools and models - Gemini remembers what O3 said 10 steps ago\nModel-Specific Strengths\n- Extended thinking with Gemini Pro, blazing speed with Flash, strong reasoning with O3, privacy with local Ollama\nProfessional Code Reviews\n- Multi-pass analysis with severity levels, actionable feedback, and consensus from multiple AI experts\nSmart Debugging Assistant\n- Systematic root cause analysis with hypothesis tracking and confidence levels\nAutomatic Model Selection\n- Claude intelligently picks the right model for each subtask (or you can specify)\nVision Capabilities\n- Analyze screenshots, diagrams, and visual content with vision-enabled models\nLocal Model Support\n- Run Llama, Mistral, or other models locally for complete privacy and zero API costs\nBypass MCP Token Limits\n- Automatically works around MCP's 25K limit for large prompts and responses\nThe Killer Feature:\nWhen Claude's context resets, just ask to \"continue with O3\" - the other model's response magically revives Claude's understanding without re-ingesting documents!\nExample: Multi-Model Code Review Workflow\nPerform a codereview using gemini pro and o3 and use planner to generate a detailed plan, implement the fixes and do a final precommit check by continuing from the previous codereview\nThis triggers a\ncodereview\nworkflow where Claude walks the code, looking for all kinds of issues\nAfter multiple passes, collects relevant code and makes note of issues along the way\nMaintains a\nconfidence\nlevel between\nexploring\n,\nlow\n,\nmedium\n,\nhigh\nand\ncertain\nto track how confidently it's been able to find and identify issues\nGenerates a detailed list of critical -> low issues\nShares the relevant files, findings, etc with\nGemini Pro\nto perform a deep dive for a second\ncodereview\nComes back with a response and next does the same with o3, adding to the prompt if a new discovery comes to light\nWhen done, Claude takes in all the feedback and combines a single list of all critical -> low issues, including good patterns in your code. The final list includes new findings or revisions in case Claude misunderstood or missed something crucial and one of the other models pointed this out\nIt then uses the\nplanner\nworkflow to break the work down into simpler steps if a major refactor is required\nClaude then performs the actual work of fixing highlighted issues\nWhen done, Claude returns to Gemini Pro for a\nprecommit\nreview\nAll within a single conversation thread! Gemini Pro in step 11\nknows\nwhat was recommended by O3 in step 7! Taking that context\nand review into consideration to aid with its final pre-commit review.\nThink of it as Claude Code\nfor\nClaude Code.\nThis MCP isn't magic. It's just\nsuper-glue\n.\nRemember:\nClaude stays in full control — but\nYOU\ncall the shots.\nZen is designed to have Claude engage other models only when needed — and to follow through with meaningful back-and-forth.\nYou're\nthe one who crafts the powerful prompt that makes Claude bring in Gemini, Flash, O3 — or fly solo.\nYou're the guide. The prompter. The puppeteer.\nYou are the AI -\nActually Intelligent\n.\nRecommended AI Stack\nFor Claude Code Users\nFor best results when using\nClaude Code\n:\nSonnet 4.5\n- All agentic work and orchestration\nGemini 2.5 Pro\nOR\nGPT-5-Pro\n- Deep thinking, additional code reviews, debugging and validations, pre-commit analysis\nFor Codex Users\nFor best results when using\nCodex CLI\n:\nGPT-5 Codex Medium\n- All agentic work and orchestration\nGemini 2.5 Pro\nOR\nGPT-5-Pro\n- Deep thinking, additional code reviews, debugging and validations, pre-commit analysis\nQuick Start (5 minutes)\nPrerequisites:\nPython 3.10+, Git,\nuv installed\n1. Get API Keys\n(choose one or more):\nOpenRouter\n- Access multiple models with one API\nGemini\n- Google's latest models\nOpenAI\n- O3, GPT-5 series\nAzure OpenAI\n- Enterprise deployments of GPT-4o, GPT-4.1, GPT-5 family\nX.AI\n- Grok models\nDIAL\n- Vendor-agnostic model access\nOllama\n- Local models (free)\n2. Install\n(choose one):\nOption A: Clone and Automatic Setup\n(recommended)\ngit clone https://github.com/BeehiveInnovations/zen-mcp-server.git\ncd\nzen-mcp-server\n#\nHandles everything: setup, config, API keys from system environment.\n#\nAuto-configures Claude Desktop, Claude Code, Gemini CLI, Codex CLI, Qwen CLI\n#\nEnable / disable additional settings in .env\n./run-server.sh\nOption B: Instant Setup with\nuvx\n// Add to ~/.claude/settings.json or .mcp.json\n// Don't forget to add your API keys under env\n{\n\"mcpServers\"\n: {\n\"zen\"\n: {\n\"command\"\n:\n\"\nbash\n\"\n,\n\"args\"\n: [\n\"\n-c\n\"\n,\n\"\nfor p in $(which uvx 2>/dev/null) $HOME/.local/bin/uvx /opt/homebrew/bin/uvx /usr/local/bin/uvx uvx; do [ -x\n\\\"\n$p\n\\\"\n] && exec\n\\\"\n$p\n\\\"\n--from git+https://github.com/BeehiveInnovations/zen-mcp-server.git zen-mcp-server; done; echo 'uvx not found' >&2; exit 1\n\"\n],\n\"env\"\n: {\n\"PATH\"\n:\n\"\n/usr/local/bin:/usr/bin:/bin:/opt/homebrew/bin:~/.local/bin\n\"\n,\n\"GEMINI_API_KEY\"\n:\n\"\nyour-key-here\n\"\n,\n\"DISABLED_TOOLS\"\n:\n\"\nanalyze,refactor,testgen,secaudit,docgen,tracer\n\"\n,\n\"DEFAULT_MODEL\"\n:\n\"\nauto\n\"\n}\n    }\n  }\n}\n3. Start Using!\n\"Use zen to analyze this code for security issues with gemini pro\"\n\"Debug this error with o3 and then get flash to suggest optimizations\"\n\"Plan the migration strategy with zen, get consensus from multiple models\"\n\"clink with cli_name=\\\"gemini\\\" role=\\\"planner\\\" to draft a phased rollout plan\"\n👉\nComplete Setup Guide\nwith detailed installation, configuration for Gemini / Codex / Qwen, and troubleshooting\n👉\nCursor & VS Code Setup\nfor IDE integration instructions\n📺\nWatch tools in action\nto see real-world examples\nProvider Configuration\nZen activates any provider that has credentials in your\n.env\n. See\n.env.example\nfor deeper customization.\nCore Tools\nNote:\nEach tool comes with its own multi-step workflow, parameters, and descriptions that consume valuable context window space even when not in use. To optimize performance, some tools are disabled by default. See\nTool Configuration\nbelow to enable them.\nCollaboration & Planning\n(Enabled by default)\nclink\n- Bridge requests to external AI CLIs (Gemini planner, codereviewer, etc.)\nchat\n- Brainstorm ideas, get second opinions, validate approaches. With capable models (GPT-5 Pro, Gemini 2.5 Pro), generates complete code / implementation\nthinkdeep\n- Extended reasoning, edge case analysis, alternative perspectives\nplanner\n- Break down complex projects into structured, actionable plans\nconsensus\n- Get expert opinions from multiple AI models with stance steering\nCode Analysis & Quality\ndebug\n- Systematic investigation and root cause analysis\nprecommit\n- Validate changes before committing, prevent regressions\ncodereview\n- Professional reviews with severity levels and actionable feedback\nanalyze\n(disabled by default -\nenable\n)\n- Understand architecture, patterns, dependencies across entire codebases\nDevelopment Tools\n(Disabled by default -\nenable\n)\nrefactor\n- Intelligent code refactoring with decomposition focus\ntestgen\n- Comprehensive test generation with edge cases\nsecaudit\n- Security audits with OWASP Top 10 analysis\ndocgen\n- Generate documentation with complexity analysis\nUtilities\napilookup\n- Forces current-year API/SDK documentation lookups in a sub-process (saves tokens within the current context window), prevents outdated training data responses\nchallenge\n- Prevent \"You're absolutely right!\" responses with critical analysis\ntracer\n(disabled by default -\nenable\n)\n- Static analysis prompts for call-flow mapping\n👉 Tool Configuration\nDefault Configuration\nTo optimize context window usage, only essential tools are enabled by default:\nEnabled by default:\nchat\n,\nthinkdeep\n,\nplanner\n,\nconsensus\n- Core collaboration tools\ncodereview\n,\nprecommit\n,\ndebug\n- Essential code quality tools\napilookup\n- Rapid API/SDK information lookup\nchallenge\n- Critical thinking utility\nDisabled by default:\nanalyze\n,\nrefactor\n,\ntestgen\n,\nsecaudit\n,\ndocgen\n,\ntracer\nEnabling Additional Tools\nTo enable additional tools, remove them from the\nDISABLED_TOOLS\nlist:\nOption 1: Edit your .env file\n#\nDefault configuration (from .env.example)\nDISABLED_TOOLS=analyze,refactor,testgen,secaudit,docgen,tracer\n#\nTo enable specific tools, remove them from the list\n#\nExample: Enable analyze tool\nDISABLED_TOOLS=refactor,testgen,secaudit,docgen,tracer\n#\nTo enable ALL tools\nDISABLED_TOOLS=\nOption 2: Configure in MCP settings\n// In ~/.claude/settings.json or .mcp.json\n{\n\"mcpServers\"\n: {\n\"zen\"\n: {\n\"env\"\n: {\n// Tool configuration\n\"DISABLED_TOOLS\"\n:\n\"\nrefactor,testgen,secaudit,docgen,tracer\n\"\n,\n\"DEFAULT_MODEL\"\n:\n\"\npro\n\"\n,\n\"DEFAULT_THINKING_MODE_THINKDEEP\"\n:\n\"\nhigh\n\"\n,\n// API configuration\n\"GEMINI_API_KEY\"\n:\n\"\nyour-gemini-key\n\"\n,\n\"OPENAI_API_KEY\"\n:\n\"\nyour-openai-key\n\"\n,\n\"OPENROUTER_API_KEY\"\n:\n\"\nyour-openrouter-key\n\"\n,\n// Logging and performance\n\"LOG_LEVEL\"\n:\n\"\nINFO\n\"\n,\n\"CONVERSATION_TIMEOUT_HOURS\"\n:\n\"\n6\n\"\n,\n\"MAX_CONVERSATION_TURNS\"\n:\n\"\n50\n\"\n}\n    }\n  }\n}\nOption 3: Enable all tools\n// Remove or empty the DISABLED_TOOLS to enable everything\n{\n\"mcpServers\"\n: {\n\"zen\"\n: {\n\"env\"\n: {\n\"DISABLED_TOOLS\"\n:\n\"\n\"\n}\n    }\n  }\n}\nNote:\nEssential tools (\nversion\n,\nlistmodels\n) cannot be disabled\nAfter changing tool configuration, restart your Claude session for changes to take effect\nEach tool adds to context window usage, so only enable what you need\n📺 Watch Tools In Action\nChat Tool\n- Collaborative decision making and multi-turn conversations\nPicking Redis vs Memcached:\nChat.Redis.or.Memcached_web.webm\nMulti-turn conversation with continuation:\nChat.With.Gemini_web.webm\nConsensus Tool\n- Multi-model debate and decision making\nMulti-model consensus debate:\nZen.Debate_web.webm\nPreCommit Tool\n- Comprehensive change validation\nPre-commit validation workflow:\nAPI Lookup Tool\n- Current vs outdated API documentation\nWithout Zen - outdated APIs:\nAPI_without_zen_web.mp4\nWith Zen - current APIs:\nAPI_with_zen.mp4\nChallenge Tool\n- Critical thinking vs reflexive agreement\nWithout Zen:\nWith Zen:\nKey Features\nAI Orchestration\nAuto model selection\n- Claude picks the right AI for each task\nMulti-model workflows\n- Chain different models in single conversations\nConversation continuity\n- Context preserved across tools and models\nContext revival\n- Continue conversations even after context resets\nModel Support\nMultiple providers\n- Gemini, OpenAI, Azure, X.AI, OpenRouter, DIAL, Ollama\nLatest models\n- GPT-5, Gemini 2.5 Pro, O3, Grok-4, local Llama\nThinking modes\n- Control reasoning depth vs cost\nVision support\n- Analyze images, diagrams, screenshots\nDeveloper Experience\nGuided workflows\n- Systematic investigation prevents rushed analysis\nSmart file handling\n- Auto-expand directories, manage token limits\nWeb search integration\n- Access current documentation and best practices\nLarge prompt support\n- Bypass MCP's 25K token limit\nExample Workflows\nMulti-model Code Review:\n\"Perform a codereview using gemini pro and o3, then use planner to create a fix strategy\"\n→ Claude reviews code systematically → Consults Gemini Pro → Gets O3's perspective → Creates unified action plan\nCollaborative Debugging:\n\"Debug this race condition with max thinking mode, then validate the fix with precommit\"\n→ Deep investigation → Expert analysis → Solution implementation → Pre-commit validation\nArchitecture Planning:\n\"Plan our microservices migration, get consensus from pro and o3 on the approach\"\n→ Structured planning → Multiple expert opinions → Consensus building → Implementation roadmap\n👉\nAdvanced Usage Guide\nfor complex workflows, model configuration, and power-user features\nQuick Links\n📖 Documentation\nDocs Overview\n- High-level map of major guides\nGetting Started\n- Complete setup guide\nTools Reference\n- All tools with examples\nAdvanced Usage\n- Power user features\nConfiguration\n- Environment variables, restrictions\nAdding Providers\n- Provider-specific setup (OpenAI, Azure, custom gateways)\nModel Ranking Guide\n- How intelligence scores drive auto-mode suggestions\n🔧 Setup & Support\nWSL Setup\n- Windows users\nTroubleshooting\n- Common issues\nContributing\n- Code standards, PR process\nLicense\nApache 2.0 License - see\nLICENSE\nfile for details.\nAcknowledgments\nBuilt with the power of\nMulti-Model AI\ncollaboration 🤝\nA\nctual\nI\nntelligence by real Humans\nMCP (Model Context Protocol)\nCodex CLI\nClaude Code\nGemini\nOpenAI\nAzure OpenAI\nStar History",
        "今日の獲得スター数: 329",
        "累積スター数: 8,339"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/BeehiveInnovations/zen-mcp-server"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/trycua/cua",
      "title": "trycua/cua",
      "date": null,
      "executive_summary": [
        "Open-source infrastructure for Computer-Use Agents. Sandboxes, SDKs, and benchmarks to train and evaluate AI agents that can control full desktops (macOS, Linux, Windows).",
        "---",
        "We’re hosting the\nComputer-Use Agents SOTA Challenge\nat\nHack the North\nand online!\nTrack A (On-site @ UWaterloo)\n: Reserved for participants accepted to Hack the North. 🏆 Prize:\nYC interview guaranteed\n.\nTrack B (Remote)\n: Open to everyone worldwide. 🏆 Prize:\nCash award\n.\n👉 Sign up here:\ntrycua.com/hackathon\ncua\n(\"koo-ah\") is Docker for\nComputer-Use Agents\n- it enables AI agents to control full operating systems in virtual containers and deploy them locally or to the cloud.\nvibe-photoshop.mp4\nWith the Computer SDK, you can:\nautomate Windows, Linux, and macOS VMs with a consistent,\npyautogui-like API\ncreate & manage VMs\nlocally\nor using\ncua cloud\nWith the Agent SDK, you can:\nrun computer-use models with a\nconsistent schema\nbenchmark on OSWorld-Verified, SheetBench-V2, and more\nwith a single line of code using HUD\n(\nNotebook\n)\ncombine UI grounding models with any LLM using\ncomposed agents\nuse new UI agent models and UI grounding models from the Model Zoo below with just a model string (e.g.,\nComputerAgent(model=\"openai/computer-use-preview\")\n)\nuse API or local inference by changing a prefix (e.g.,\nopenai/\n,\nopenrouter/\n,\nollama/\n,\nhuggingface-local/\n,\nmlx/\n,\netc.\n)\nCUA Model Zoo 🐨\nAll-in-one CUAs\nUI Grounding Models\nUI Planning Models\nanthropic/claude-sonnet-4-5-20250929\nhuggingface-local/xlangai/OpenCUA-{7B,32B}\nany all-in-one CUA\nopenai/computer-use-preview\nhuggingface-local/HelloKKMe/GTA1-{7B,32B,72B}\nany VLM (using liteLLM, requires\ntools\nparameter)\nopenrouter/z-ai/glm-4.5v\nhuggingface-local/Hcompany/Holo1.5-{3B,7B,72B}\nany LLM (using liteLLM, requires\nmoondream3+\nprefix )\nhuggingface-local/OpenGVLab/InternVL3_5-{1B,2B,4B,8B,...}\nany all-in-one CUA\nhuggingface-local/ByteDance-Seed/UI-TARS-1.5-7B\nmoondream3+{ui planning}\n(supports text-only models)\nomniparser+{ui planning}\n{ui grounding}+{ui planning}\nhuman/human\n→\nHuman-in-the-Loop\nMissing a model?\nRaise a feature request\nor\ncontribute\n!\nQuick Start\nGet started with a Computer-Use Agent UI\nGet started with the Computer-Use Agent CLI\nGet started with the Python SDKs\nUsage (\nDocs\n)\npip install cua-agent[all]\nfrom\nagent\nimport\nComputerAgent\nagent\n=\nComputerAgent\n(\nmodel\n=\n\"anthropic/claude-3-5-sonnet-20241022\"\n,\ntools\n=\n[\ncomputer\n],\nmax_trajectory_budget\n=\n5.0\n)\nmessages\n=\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Take a screenshot and tell me what you see\"\n}]\nasync\nfor\nresult\nin\nagent\n.\nrun\n(\nmessages\n):\nfor\nitem\nin\nresult\n[\n\"output\"\n]:\nif\nitem\n[\n\"type\"\n]\n==\n\"message\"\n:\nprint\n(\nitem\n[\n\"content\"\n][\n0\n][\n\"text\"\n])\nOutput format (OpenAI Agent Responses Format):\n{\n\"output\"\n: [\n# user input\n{\n\"role\"\n:\n\"\nuser\n\"\n,\n\"content\"\n:\n\"\ngo to trycua on gh\n\"\n},\n# first agent turn adds the model output to the history\n{\n\"summary\"\n: [\n            {\n\"text\"\n:\n\"\nSearching Firefox for Trycua GitHub\n\"\n,\n\"type\"\n:\n\"\nsummary_text\n\"\n}\n        ],\n\"type\"\n:\n\"\nreasoning\n\"\n},\n    {\n\"action\"\n: {\n\"text\"\n:\n\"\nTrycua GitHub\n\"\n,\n\"type\"\n:\n\"\ntype\n\"\n},\n\"call_id\"\n:\n\"\ncall_QI6OsYkXxl6Ww1KvyJc4LKKq\n\"\n,\n\"status\"\n:\n\"\ncompleted\n\"\n,\n\"type\"\n:\n\"\ncomputer_call\n\"\n},\n# second agent turn adds the computer output to the history\n{\n\"type\"\n:\n\"\ncomputer_call_output\n\"\n,\n\"call_id\"\n:\n\"\ncall_QI6OsYkXxl6Ww1KvyJc4LKKq\n\"\n,\n\"output\"\n: {\n\"type\"\n:\n\"\ninput_image\n\"\n,\n\"image_url\"\n:\n\"\ndata:image/png;base64,...\n\"\n}\n    },\n# final agent turn adds the agent output text to the history\n{\n\"type\"\n:\n\"\nmessage\n\"\n,\n\"role\"\n:\n\"\nassistant\n\"\n,\n\"content\"\n: [\n          {\n\"text\"\n:\n\"\nSuccess! The Trycua GitHub page has been opened.\n\"\n,\n\"type\"\n:\n\"\noutput_text\n\"\n}\n        ]\n    }\n  ],\n\"usage\"\n: {\n\"prompt_tokens\"\n:\n150\n,\n\"completion_tokens\"\n:\n75\n,\n\"total_tokens\"\n:\n225\n,\n\"response_cost\"\n:\n0.01\n,\n  }\n}\nComputer (\nDocs\n)\npip install cua-computer[all]\nfrom\ncomputer\nimport\nComputer\nasync\nwith\nComputer\n(\nos_type\n=\n\"linux\"\n,\nprovider_type\n=\n\"cloud\"\n,\nname\n=\n\"your-container-name\"\n,\napi_key\n=\n\"your-api-key\"\n)\nas\ncomputer\n:\n# Take screenshot\nscreenshot\n=\nawait\ncomputer\n.\ninterface\n.\nscreenshot\n()\n# Click and type\nawait\ncomputer\n.\ninterface\n.\nleft_click\n(\n100\n,\n100\n)\nawait\ncomputer\n.\ninterface\n.\ntype\n(\n\"Hello!\"\n)\nResources\nHow to use the MCP Server with Claude Desktop or other MCP clients\n- One of the easiest ways to get started with Cua\nHow to use OpenAI Computer-Use, Anthropic, OmniParser, or UI-TARS for your Computer-Use Agent\nHow to use Lume CLI for managing desktops\nTraining Computer-Use Models: Collecting Human Trajectories with Cua (Part 1)\nModules\nModule\nDescription\nInstallation\nLume\nVM management for macOS/Linux using Apple's Virtualization.Framework\ncurl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh | bash\nLumier\nDocker interface for macOS and Linux VMs\ndocker pull trycua/lumier:latest\nComputer (Python)\nPython Interface for controlling virtual machines\npip install \"cua-computer[all]\"\nComputer (Typescript)\nTypescript Interface for controlling virtual machines\nnpm install @trycua/computer\nAgent\nAI agent framework for automating tasks\npip install \"cua-agent[all]\"\nMCP Server\nMCP server for using CUA with Claude Desktop\npip install cua-mcp-server\nSOM\nSelf-of-Mark library for Agent\npip install cua-som\nComputer Server\nServer component for Computer\npip install cua-computer-server\nCore (Python)\nPython Core utilities\npip install cua-core\nCore (Typescript)\nTypescript Core utilities\nnpm install @trycua/core\nCommunity\nJoin our\nDiscord community\nto discuss ideas, get assistance, or share your demos!\nLicense\nCua is open-sourced under the MIT License - see the\nLICENSE\nfile for details.\nPortions of this project, specifically components adapted from Kasm Technologies Inc., are also licensed under the MIT License. See\nlibs/kasm/LICENSE\nfor details.\nMicrosoft's OmniParser, which is used in this project, is licensed under the Creative Commons Attribution 4.0 International License (CC-BY-4.0). See the\nOmniParser LICENSE\nfor details.\nThird-Party Licenses and Optional Components\nSome optional extras for this project depend on third-party packages that are licensed under terms different from the MIT License.\nThe optional \"omni\" extra (installed via\npip install \"cua-agent[omni]\"\n) installs the\ncua-som\nmodule, which includes\nultralytics\nand is licensed under the AGPL-3.0.\nWhen you choose to install and use such optional extras, your use, modification, and distribution of those third-party components are governed by their respective licenses (e.g., AGPL-3.0 for\nultralytics\n).\nContributing\nWe welcome contributions to Cua! Please refer to our\nContributing Guidelines\nfor details.\nTrademarks\nApple, macOS, and Apple Silicon are trademarks of Apple Inc.\nUbuntu and Canonical are registered trademarks of Canonical Ltd.\nMicrosoft is a registered trademark of Microsoft Corporation.\nThis project is not affiliated with, endorsed by, or sponsored by Apple Inc., Canonical Ltd., Microsoft Corporation, or Kasm Technologies.\nStargazers\nThank you to all our supporters!",
        "今日の獲得スター数: 310",
        "累積スター数: 10,369"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/trycua/cua"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/TapXWorld/ChinaTextbook",
      "title": "TapXWorld/ChinaTextbook",
      "date": null,
      "executive_summary": [
        "所有小初高、大学PDF教材。",
        "---",
        "项目的由来\n虽然国内教育网站已提供免费资源，但大多数普通人获取信息的途径依然受限。有些人利用这一点，在某站上销售这些带有私人水印的资源。为了应对这种情况，我计划将这些资源集中并开源，以促进义务教育的普及和消除地区间的教育贫困。\n还有一个最重要的原因是，希望海外华人能够让自己的孩子继续了解国内教育。\n学习数学\n希望未来出现更多不是为了考学而读书的人。\n小学数学\n一年级上册\n一年级下册\n二年级上册\n二年级下册\n三年级上册\n三年级下册\n四年级上册\n四年级下册\n五年级上册\n五年级下册\n六年级上册\n六年级下册\n初中数学\n初一上册\n初一下册\n初二上册\n初二下册\n初三上册\n初三下册\n高中数学\n目录\n大学数学\n高等数学\n线性代数\n离散数学\n概率论\n更多数学资料-(大学数学网)\n问题：如何合并被拆分的文件？\n由于 GitHub 对单个文件的上传有最大限制，超过 100MB 的文件会被拒绝上传，超过 50MB 的文件上传时会收到警告。因此，文件大小超过 50MB 的文件会被拆分成每个 35MB 的多个文件。\n示例\n文件被拆分的示例：\n义务教育教科书 · 数学一年级上册.pdf.1\n义务教育教科书 · 数学一年级上册.pdf.2\n解决办法\n要合并这些被拆分的文件，您只需执行以下步骤(其他操作系统同理)：\n将合并程序\nmergePDFs-windows-amd64.exe\n下载到包含 PDF 文件的文件夹中。\n确保\nmergePDFs-windows-amd64.exe\n和被拆分的 PDF 文件在同一目录下。\n双击\nmergePDFs-windows-amd64.exe\n程序即可自动完成文件合并。\n下载方式\n您可以通过以下链接，下载文件合并程序：\n下载文件合并程序\n文件和程序示例\nmergePDFs-windows-amd64.exe\n义务教育教科书 · 数学一年级上册.pdf.1\n义务教育教科书 · 数学一年级上册.pdf.2\n重新下载\n如果您位于内地，并且网络不错，想重新下载，您可以使用\ntchMaterial-parser\n项目（鼓励开源），进行重新下载。\n如果您位于国外，和内地网络通信速度较慢，建议使用本存储库进行签出。\n教材捐献\n如果这个项目帮助您免费获取教育资源，请考虑支持我们推广开放教育的努力！您的捐献将帮助我们维护和扩展这个资源库。\n加入我们的 Telegram 社区，获取最新动态并分享您的想法：\nhttps://t.me/+1V6WjEq8WEM4MDM1\n支持我\n如果您觉得这个项目对您有帮助，您可以扫描以下二维码进行捐赠：",
        "今日の獲得スター数: 304",
        "累積スター数: 51,626"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/TapXWorld/ChinaTextbook"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/openai/openai-agents-python",
      "title": "openai/openai-agents-python",
      "date": null,
      "executive_summary": [
        "A lightweight, powerful framework for multi-agent workflows",
        "---",
        "OpenAI Agents SDK\nThe OpenAI Agents SDK is a lightweight yet powerful framework for building multi-agent workflows. It is provider-agnostic, supporting the OpenAI Responses and Chat Completions APIs, as well as 100+ other LLMs.\nNote\nLooking for the JavaScript/TypeScript version? Check out\nAgents SDK JS/TS\n.\nCore concepts:\nAgents\n: LLMs configured with instructions, tools, guardrails, and handoffs\nHandoffs\n: A specialized tool call used by the Agents SDK for transferring control between agents\nGuardrails\n: Configurable safety checks for input and output validation\nSessions\n: Automatic conversation history management across agent runs\nTracing\n: Built-in tracking of agent runs, allowing you to view, debug and optimize your workflows\nExplore the\nexamples\ndirectory to see the SDK in action, and read our\ndocumentation\nfor more details.\nGet started\nTo get started, set up your Python environment (Python 3.9 or newer required), and then install OpenAI Agents SDK package.\nvenv\npython -m venv .venv\nsource\n.venv/bin/activate\n#\nOn Windows: .venv\\Scripts\\activate\npip install openai-agents\nFor voice support, install with the optional\nvoice\ngroup:\npip install 'openai-agents[voice]'\n.\nFor Redis session support, install with the optional\nredis\ngroup:\npip install 'openai-agents[redis]'\n.\nuv\nIf you're familiar with\nuv\n, using the tool would be even similar:\nuv init\nuv add openai-agents\nFor voice support, install with the optional\nvoice\ngroup:\nuv add 'openai-agents[voice]'\n.\nFor Redis session support, install with the optional\nredis\ngroup:\nuv add 'openai-agents[redis]'\n.\nHello world example\nfrom\nagents\nimport\nAgent\n,\nRunner\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n,\ninstructions\n=\n\"You are a helpful assistant\"\n)\nresult\n=\nRunner\n.\nrun_sync\n(\nagent\n,\n\"Write a haiku about recursion in programming.\"\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# Code within the code,\n# Functions calling themselves,\n# Infinite loop's dance.\n(\nIf running this, ensure you set the\nOPENAI_API_KEY\nenvironment variable\n)\n(\nFor Jupyter notebook users, see\nhello_world_jupyter.ipynb\n)\nHandoffs example\nfrom\nagents\nimport\nAgent\n,\nRunner\nimport\nasyncio\nspanish_agent\n=\nAgent\n(\nname\n=\n\"Spanish agent\"\n,\ninstructions\n=\n\"You only speak Spanish.\"\n,\n)\nenglish_agent\n=\nAgent\n(\nname\n=\n\"English agent\"\n,\ninstructions\n=\n\"You only speak English\"\n,\n)\ntriage_agent\n=\nAgent\n(\nname\n=\n\"Triage agent\"\n,\ninstructions\n=\n\"Handoff to the appropriate agent based on the language of the request.\"\n,\nhandoffs\n=\n[\nspanish_agent\n,\nenglish_agent\n],\n)\nasync\ndef\nmain\n():\nresult\n=\nawait\nRunner\n.\nrun\n(\ntriage_agent\n,\ninput\n=\n\"Hola, ¿cómo estás?\"\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# ¡Hola! Estoy bien, gracias por preguntar. ¿Y tú, cómo estás?\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nFunctions example\nimport\nasyncio\nfrom\nagents\nimport\nAgent\n,\nRunner\n,\nfunction_tool\n@\nfunction_tool\ndef\nget_weather\n(\ncity\n:\nstr\n)\n->\nstr\n:\nreturn\nf\"The weather in\n{\ncity\n}\nis sunny.\"\nagent\n=\nAgent\n(\nname\n=\n\"Hello world\"\n,\ninstructions\n=\n\"You are a helpful agent.\"\n,\ntools\n=\n[\nget_weather\n],\n)\nasync\ndef\nmain\n():\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\ninput\n=\n\"What's the weather in Tokyo?\"\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# The weather in Tokyo is sunny.\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nThe agent loop\nWhen you call\nRunner.run()\n, we run a loop until we get a final output.\nWe call the LLM, using the model and settings on the agent, and the message history.\nThe LLM returns a response, which may include tool calls.\nIf the response has a final output (see below for more on this), we return it and end the loop.\nIf the response has a handoff, we set the agent to the new agent and go back to step 1.\nWe process the tool calls (if any) and append the tool responses messages. Then we go to step 1.\nThere is a\nmax_turns\nparameter that you can use to limit the number of times the loop executes.\nFinal output\nFinal output is the last thing the agent produces in the loop.\nIf you set an\noutput_type\non the agent, the final output is when the LLM returns something of that type. We use\nstructured outputs\nfor this.\nIf there's no\noutput_type\n(i.e. plain text responses), then the first LLM response without any tool calls or handoffs is considered as the final output.\nAs a result, the mental model for the agent loop is:\nIf the current agent has an\noutput_type\n, the loop runs until the agent produces structured output matching that type.\nIf the current agent does not have an\noutput_type\n, the loop runs until the current agent produces a message without any tool calls/handoffs.\nCommon agent patterns\nThe Agents SDK is designed to be highly flexible, allowing you to model a wide range of LLM workflows including deterministic flows, iterative loops, and more. See examples in\nexamples/agent_patterns\n.\nTracing\nThe Agents SDK automatically traces your agent runs, making it easy to track and debug the behavior of your agents. Tracing is extensible by design, supporting custom spans and a wide variety of external destinations, including\nLogfire\n,\nAgentOps\n,\nBraintrust\n,\nScorecard\n, and\nKeywords AI\n. For more details about how to customize or disable tracing, see\nTracing\n, which also includes a larger list of\nexternal tracing processors\n.\nLong running agents & human-in-the-loop\nYou can use the Agents SDK\nTemporal\nintegration to run durable, long-running workflows, including human-in-the-loop tasks. View a demo of Temporal and the Agents SDK working in action to complete long-running tasks\nin this video\n, and\nview docs here\n.\nSessions\nThe Agents SDK provides built-in session memory to automatically maintain conversation history across multiple agent runs, eliminating the need to manually handle\n.to_input_list()\nbetween turns.\nQuick start\nfrom\nagents\nimport\nAgent\n,\nRunner\n,\nSQLiteSession\n# Create agent\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n,\ninstructions\n=\n\"Reply very concisely.\"\n,\n)\n# Create a session instance\nsession\n=\nSQLiteSession\n(\n\"conversation_123\"\n)\n# First turn\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"What city is the Golden Gate Bridge in?\"\n,\nsession\n=\nsession\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# \"San Francisco\"\n# Second turn - agent automatically remembers previous context\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"What state is it in?\"\n,\nsession\n=\nsession\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# \"California\"\n# Also works with synchronous runner\nresult\n=\nRunner\n.\nrun_sync\n(\nagent\n,\n\"What's the population?\"\n,\nsession\n=\nsession\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# \"Approximately 39 million\"\nSession options\nNo memory\n(default): No session memory when session parameter is omitted\nsession: Session = DatabaseSession(...)\n: Use a Session instance to manage conversation history\nfrom\nagents\nimport\nAgent\n,\nRunner\n,\nSQLiteSession\n# SQLite - file-based or in-memory database\nsession\n=\nSQLiteSession\n(\n\"user_123\"\n,\n\"conversations.db\"\n)\n# Redis - for scalable, distributed deployments\n# from agents.extensions.memory import RedisSession\n# session = RedisSession.from_url(\"user_123\", url=\"redis://localhost:6379/0\")\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n)\n# Different session IDs maintain separate conversation histories\nresult1\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"Hello\"\n,\nsession\n=\nsession\n)\nresult2\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"Hello\"\n,\nsession\n=\nSQLiteSession\n(\n\"user_456\"\n,\n\"conversations.db\"\n)\n)\nCustom session implementations\nYou can implement your own session memory by creating a class that follows the\nSession\nprotocol:\nfrom\nagents\n.\nmemory\nimport\nSession\nfrom\ntyping\nimport\nList\nclass\nMyCustomSession\n:\n\"\"\"Custom session implementation following the Session protocol.\"\"\"\ndef\n__init__\n(\nself\n,\nsession_id\n:\nstr\n):\nself\n.\nsession_id\n=\nsession_id\n# Your initialization here\nasync\ndef\nget_items\n(\nself\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n)\n->\nList\n[\ndict\n]:\n# Retrieve conversation history for the session\npass\nasync\ndef\nadd_items\n(\nself\n,\nitems\n:\nList\n[\ndict\n])\n->\nNone\n:\n# Store new items for the session\npass\nasync\ndef\npop_item\n(\nself\n)\n->\ndict\n|\nNone\n:\n# Remove and return the most recent item from the session\npass\nasync\ndef\nclear_session\n(\nself\n)\n->\nNone\n:\n# Clear all items for the session\npass\n# Use your custom session\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n)\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"Hello\"\n,\nsession\n=\nMyCustomSession\n(\n\"my_session\"\n)\n)\nDevelopment (only needed if you need to edit the SDK/examples)\nEnsure you have\nuv\ninstalled.\nuv --version\nInstall dependencies\nmake sync\n(After making changes) lint/test\nmake check # run tests linter and typechecker\nOr to run them individually:\nmake tests  # run tests\nmake mypy   # run typechecker\nmake lint   # run linter\nmake format-check # run style checker\nAcknowledgements\nWe'd like to acknowledge the excellent work of the open-source community, especially:\nPydantic\n(data validation) and\nPydanticAI\n(advanced agent framework)\nLiteLLM\n(unified interface for 100+ LLMs)\nMkDocs\nGriffe\nuv\nand\nruff\nWe're committed to continuing to build the Agents SDK as an open source framework so others in the community can expand on our approach.",
        "今日の獲得スター数: 276",
        "累積スター数: 15,755"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/openai/openai-agents-python"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/Infisical/infisical",
      "title": "Infisical/infisical",
      "date": null,
      "executive_summary": [
        "Infisical is the open-source platform for secrets management, PKI, and SSH access.",
        "---",
        "The open-source secret management platform\n: Sync secrets/configs across your team/infrastructure and prevent secret leaks.\nSlack\n|\nInfisical Cloud\n|\nSelf-Hosting\n|\nDocs\n|\nWebsite\n|\nHiring (Remote/SF)\nIntroduction\nInfisical\nis the open source secret management platform that teams use to centralize their application configuration and secrets like API keys and database credentials as well as manage their internal PKI.\nWe're on a mission to make security tooling more accessible to everyone, not just security teams, and that means redesigning the entire developer experience from ground up.\nFeatures\nSecrets Management:\nDashboard\n: Manage secrets across projects and environments (e.g. development, production, etc.) through a user-friendly interface.\nNative Integrations\n: Sync secrets to platforms like\nGitHub\n,\nVercel\n,\nAWS\n, and use tools like\nTerraform\n,\nAnsible\n, and more.\nSecret versioning\nand\nPoint-in-Time Recovery\n: Keep track of every secret and project state; roll back when needed.\nSecret Rotation\n: Rotate secrets at regular intervals for services like\nPostgreSQL\n,\nMySQL\n,\nAWS IAM\n, and more.\nDynamic Secrets\n: Generate ephemeral secrets on-demand for services like\nPostgreSQL\n,\nMySQL\n,\nRabbitMQ\n, and more.\nSecret Scanning and Leak Prevention\n: Prevent secrets from leaking to git.\nInfisical Kubernetes Operator\n: Deliver secrets to your Kubernetes workloads and automatically reload deployments.\nInfisical Agent\n: Inject secrets into applications without modifying any code logic.\nInfisical (Internal) PKI:\nPrivate Certificate Authority\n: Create CA hierarchies, configure\ncertificate templates\nfor policy enforcement, and start issuing X.509 certificates.\nCertificate Management\n: Manage the certificate lifecycle from\nissuance\nto\nrevocation\nwith support for CRL.\nAlerting\n: Configure alerting for expiring CA and end-entity certificates.\nInfisical PKI Issuer for Kubernetes\n: Deliver TLS certificates to your Kubernetes workloads with automatic renewal.\nEnrollment over Secure Transport\n: Enroll and manage certificates via EST protocol.\nInfisical Key Management System (KMS):\nCryptographic Keys\n: Centrally manage keys across projects through a user-friendly interface or via the API.\nEncrypt and Decrypt Data\n: Use symmetric keys to encrypt and decrypt data.\nInfisical SSH\nSigned SSH Certificates\n: Issue ephemeral SSH credentials for secure, short-lived, and centralized access to infrastructure.\nGeneral Platform:\nAuthentication Methods\n: Authenticate machine identities with Infisical using a cloud-native or platform agnostic authentication method (\nKubernetes Auth\n,\nGCP Auth\n,\nAzure Auth\n,\nAWS Auth\n,\nOIDC Auth\n,\nUniversal Auth\n).\nAccess Controls\n: Define advanced authorization controls for users and machine identities with\nRBAC\n,\nadditional privileges\n,\ntemporary access\n,\naccess requests\n,\napproval workflows\n, and more.\nAudit logs\n: Track every action taken on the platform.\nSelf-hosting\n: Deploy Infisical on-prem or cloud with ease; keep data on your own infrastructure.\nInfisical SDK\n: Interact with Infisical via client SDKs (\nNode\n,\nPython\n,\nGo\n,\nRuby\n,\nJava\n,\n.NET\n)\nInfisical CLI\n: Interact with Infisical via CLI; useful for injecting secrets into local development and CI/CD pipelines.\nInfisical API\n: Interact with Infisical via API.\nGetting started\nCheck out the\nQuickstart Guides\nUse Infisical Cloud\nDeploy Infisical on premise\nThe fastest and most reliable way to\nget started with Infisical is signing up\nfor free to\nInfisical Cloud\n.\nView all\ndeployment options\nRun Infisical locally\nTo set up and run Infisical locally, make sure you have Git and Docker installed on your system. Then run the command for your system:\nLinux/macOS:\ngit clone https://github.com/Infisical/infisical && cd \"$(basename $_ .git)\" && cp .env.example .env && docker compose -f docker-compose.prod.yml up\nWindows Command Prompt:\ngit clone https://github.com/Infisical/infisical && cd infisical && copy .env.example .env && docker compose -f docker-compose.prod.yml up\nCreate an account at\nhttp://localhost:80\nScan and prevent secret leaks\nOn top managing secrets with Infisical, you can also\nscan for over 140+ secret types\nin your files, directories and git repositories.\nTo scan your full git history, run:\ninfisical scan --verbose\nInstall pre commit hook to scan each commit before you push to your repository\ninfisical scan install --pre-commit-hook\nLearn about Infisical's code scanning feature\nhere\nOpen-source vs. paid\nThis repo available under the\nMIT expat license\n, with the exception of the\nee\ndirectory which will contain premium enterprise features requiring a Infisical license.\nIf you are interested in managed Infisical Cloud of self-hosted Enterprise Offering, take a look at\nour website\nor\nbook a meeting with us\n.\nSecurity\nPlease do not file GitHub issues or post on our public forum for security vulnerabilities, as they are public!\nInfisical takes security issues very seriously. If you have any concerns about Infisical or believe you have uncovered a vulnerability, please get in touch via the e-mail address\nsecurity@infisical.com\n. In the message, try to provide a description of the issue and ideally a way of reproducing it. The security team will get back to you as soon as possible.\nNote that this security address should be used only for undisclosed vulnerabilities. Please report any security problems to us before disclosing it publicly.\nContributing\nWhether it's big or small, we love contributions. Check out our guide to see how to\nget started\n.\nNot sure where to get started? You can:\nJoin our\nSlack\n, and ask us any questions there.\nWe are hiring!\nIf you're reading this, there is a strong chance you like the products we created.\nYou might also make a great addition to our team. We're growing fast and would love for you to\njoin us\n.",
        "今日の獲得スター数: 266",
        "累積スター数: 22,291"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/Infisical/infisical"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/openemr/openemr",
      "title": "openemr/openemr",
      "date": null,
      "executive_summary": [
        "The most popular open source electronic health records and medical practice management solution.",
        "---",
        "OpenEMR\nOpenEMR\nis a Free and Open Source electronic health records and medical practice management application. It features fully integrated electronic health records, practice management, scheduling, electronic billing, internationalization, free support, a vibrant community, and a whole lot more. It runs on Windows, Linux, Mac OS X, and many other platforms.\nContributing\nOpenEMR is a leader in healthcare open source software and comprises a large and diverse community of software developers, medical providers and educators with a very healthy mix of both volunteers and professionals.\nJoin us and learn how to start contributing today!\nAlready comfortable with git? Check out\nCONTRIBUTING.md\nfor quick setup instructions and requirements for contributing to OpenEMR by resolving a bug or adding an awesome feature 😊.\nSupport\nCommunity and Professional support can be found\nhere\n.\nExtensive documentation and forums can be found on the\nOpenEMR website\nthat can help you to become more familiar about the project 📖.\nReporting Issues and Bugs\nReport these on the\nIssue Tracker\n. If you are unsure if it is an issue/bug, then always feel free to use the\nForum\nand\nChat\nto discuss about the issue 🪲.\nReporting Security Vulnerabilities\nCheck out\nSECURITY.md\nAPI\nCheck out\nAPI_README.md\nDocker\nCheck out\nDOCKER_README.md\nFHIR\nCheck out\nFHIR_README.md\nFor Developers\nIf using OpenEMR directly from the code repository, then the following commands will build OpenEMR (Node.js version 22.* is required) :\ncomposer install --no-dev\nnpm install\nnpm run build\ncomposer dump-autoload -o\nContributors\nThis project exists thanks to all the people who have contributed.\n[Contribute]\n.\nSponsors\nThanks to our\nONC Certification Major Sponsors\n!\nLicense\nGNU GPL",
        "今日の獲得スター数: 219",
        "累積スター数: 4,208"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/openemr/openemr"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/openai/codex",
      "title": "openai/codex",
      "date": null,
      "executive_summary": [
        "Lightweight coding agent that runs in your terminal",
        "---",
        "npm i -g @openai/codex\nor\nbrew install codex\nCodex CLI\nis a coding agent from OpenAI that runs locally on your computer.\nIf you want Codex in your code editor (VS Code, Cursor, Windsurf),\ninstall in your IDE\nIf you are looking for the\ncloud-based agent\nfrom OpenAI,\nCodex Web\n, go to\nchatgpt.com/codex\nQuickstart\nInstalling and running Codex CLI\nInstall globally with your preferred package manager. If you use npm:\nnpm install -g @openai/codex\nAlternatively, if you use Homebrew:\nbrew install codex\nThen simply run\ncodex\nto get started:\ncodex\nYou can also go to the\nlatest GitHub Release\nand download the appropriate binary for your platform.\nEach GitHub Release contains many executables, but in practice, you likely want one of these:\nmacOS\nApple Silicon/arm64:\ncodex-aarch64-apple-darwin.tar.gz\nx86_64 (older Mac hardware):\ncodex-x86_64-apple-darwin.tar.gz\nLinux\nx86_64:\ncodex-x86_64-unknown-linux-musl.tar.gz\narm64:\ncodex-aarch64-unknown-linux-musl.tar.gz\nEach archive contains a single entry with the platform baked into the name (e.g.,\ncodex-x86_64-unknown-linux-musl\n), so you likely want to rename it to\ncodex\nafter extracting it.\nUsing Codex with your ChatGPT plan\nRun\ncodex\nand select\nSign in with ChatGPT\n. We recommend signing into your ChatGPT account to use Codex as part of your Plus, Pro, Team, Edu, or Enterprise plan.\nLearn more about what's included in your ChatGPT plan\n.\nYou can also use Codex with an API key, but this requires\nadditional setup\n. If you previously used an API key for usage-based billing, see the\nmigration steps\n. If you're having trouble with login, please comment on\nthis issue\n.\nModel Context Protocol (MCP)\nCodex can access MCP servers. To configure them, refer to the\nconfig docs\n.\nConfiguration\nCodex CLI supports a rich set of configuration options, with preferences stored in\n~/.codex/config.toml\n. For full configuration options, see\nConfiguration\n.\nDocs & FAQ\nGetting started\nCLI usage\nRunning with a prompt as input\nExample prompts\nMemory with AGENTS.md\nConfiguration\nSandbox & approvals\nAuthentication\nAuth methods\nLogin on a \"Headless\" machine\nAutomating Codex\nGitHub Action\nTypeScript SDK\nNon-interactive mode (\ncodex exec\n)\nAdvanced\nTracing / verbose logging\nModel Context Protocol (MCP)\nZero data retention (ZDR)\nContributing\nInstall & build\nSystem Requirements\nDotSlash\nBuild from source\nFAQ\nOpen source fund\nLicense\nThis repository is licensed under the\nApache-2.0 License\n.",
        "今日の獲得スター数: 210",
        "累積スター数: 46,520"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/openai/codex"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/dyad-sh/dyad",
      "title": "dyad-sh/dyad",
      "date": null,
      "executive_summary": [
        "Free, local, open-source AI app builder ✨ v0 / lovable / Bolt alternative 🌟 Star if you like it!",
        "---",
        "Dyad\nDyad is a local, open-source AI app builder. It's fast, private, and fully under your control — like Lovable, v0, or Bolt, but running right on your machine.\nMore info at:\nhttp://dyad.sh/\n🚀 Features\n⚡️\nLocal\n: Fast, private and no lock-in.\n🛠\nBring your own keys\n: Use your own AI API keys — no vendor lock-in.\n🖥️\nCross-platform\n: Easy to run on Mac or Windows.\n📦 Download\nNo sign-up required. Just download and go.\n👉 Download for your platform\n🤝 Community\nJoin our growing community of AI app builders on\nReddit\n:\nr/dyadbuilders\n- share your projects and get help from the community!\n🛠️ Contributing\nDyad\nis open-source (Apache 2.0 licensed).\nIf you're interested in contributing to dyad, please read our\ncontributing\ndoc.",
        "今日の獲得スター数: 201",
        "累積スター数: 15,759"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/dyad-sh/dyad"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/audacity/audacity",
      "title": "audacity/audacity",
      "date": null,
      "executive_summary": [
        "Audio Editor",
        "---",
        "Audacity\nAudacity\nis an easy-to-use, multi-track audio editor and recorder for Windows, macOS, GNU/Linux and other operating systems. More info can be found on\nhttps://www.audacityteam.org\nThis repository is currently undergoing major structural change.\nWe're currently working on Audacity 4, which means an entirely new UI and also refactorings aplenty. As such, the\nmaster\nbranch is currently not particularly friendly to new contributors. It is still possible to submit patches to Audacity 3.x; make sure you branch off\naudacity3\nif you choose to do so. Build instructions for 3.x can be found\nhere\n; build instructions for Audacity 4 can be found\nhere\n.\nYou can stay updated with our efforts on\nYouTube\n,\nDiscord\nand\nour blog\n.\nLicense\nAudacity is open source software licensed GPLv3. Most code files are GPLv2-or-later, with the notable exceptions being\n/au3/lib-src\n(which contains third party libraries), as well as VST3-related code. Documentation is licensed CC-by 3.0 unless otherwise noted. Details can be found in the\nlicense file\n.",
        "今日の獲得スター数: 144",
        "累積スター数: 15,192"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/audacity/audacity"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/is-a-dev/register",
      "title": "is-a-dev/register",
      "date": null,
      "executive_summary": [
        "Grab your own sweet-looking '.is-a.dev' subdomain.",
        "---",
        "is-a.dev\nis-a.dev\nis a service that allows developers to get a sweet-looking\n.is-a.dev\nsubdomain for their personal websites.\nAnnouncements & Status Updates\nPlease join our\nDiscord server\nfor announcements, updates & upgrades, and downtime notifications regarding the service.\nNot all of these will be posted on GitHub\n1\n, however they will always be posted in our Discord server.\nRegister\nIf you want a visual guide, check out\nthis blog post\n.\nFork\nthis repository.\nRead the documentation\n.\nIf you are applying for NS records please read\nthis\n.\nYour pull request will be reviewed and merged.\nKeep an eye on it in case changes are needed!\nAfter the pull request is merged, your DNS records should be published with-in a few minutes.\nEnjoy your new\n.is-a.dev\nsubdomain! Please consider leaving us a star ⭐️ to help support us!\nNS Records\nWhen applying for NS records, please be aware we already support a\nwide range of DNS records\n, so you likely do not need them.\nIn your PR, please explain why you need NS records, including examples, to help mitigate potential abuse. Refer to the\nFAQ\nfor guidelines on allowed usage.\nPull requests adding NS records without sufficient reasoning will be closed.\nAlso see:\nWhy are NS records restricted?\nReport Abuse\nIf you find any subdomains being used for abusive purposes, please report them by\ncreating an issue\nwith the relevant evidence.\nWe are proud to announce that we are supported by Cloudflare's\nProject Alexandria\nsponsorship program. We would not be able to operate without their help! 💖\nFootnotes\nWe only post announcements on GitHub in the case of a serious incident, which you'll see at the top of this README.\n↩",
        "今日の獲得スター数: 124",
        "累積スター数: 8,486"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/is-a-dev/register"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/firefly-iii/firefly-iii",
      "title": "firefly-iii/firefly-iii",
      "date": null,
      "executive_summary": [
        "Firefly III: a personal finances manager",
        "---",
        "Firefly III\nA free and open source personal finance manager\nExplore the documentation\nView the demo\n·\nReport a bug\n·\nRequest a feature\n·\nAsk questions\nAbout Firefly III\nPurpose\nFeatures\nWho's it for?\nThe Firefly III eco-system\nGetting Started\nContributing\nSupport the development of Firefly III\nLicense\nDo you need help, or do you want to get in touch?\nAcknowledgements\nAbout Firefly III\n\"Firefly III\" is a (self-hosted) manager for your personal finances. It can help you keep track of your expenses and income, so you can spend less and save more. Firefly III supports the use of budgets, categories and tags. Using a bunch of external tools, you can import data. It also has many neat financial reports available.\nFirefly III should give you\ninsight\ninto and\ncontrol\nover your finances. Money should be useful, not scary. You should be able to\nsee\nwhere it is going, to\nfeel\nyour expenses and to... wow, I'm going overboard with this aren't I?\nBut you get the idea: this is your money. These are your expenses. Stop them from controlling you. I built this tool because I started to dislike money. Having money, not having money, paying bills with money, you get the idea. But no more. I want to feel \"safe\", whatever my balance is. And I hope this tool can help you. I know it helps me.\nPurpose\nPersonal financial management is pretty difficult, and everybody has their own approach to it. Some people make budgets, other people limit their cashflow by throwing away their credit cards, others try to increase their current cashflow. There are tons of ways to save and earn money. Firefly III works on the principle that if you know where your money is going, you can stop it from going there.\nBy keeping track of your expenses and your income you can budget accordingly and save money. Stop living from paycheck to paycheck but give yourself the financial wiggle room you need.\nYou can read more about the purpose of Firefly III in the\ndocumentation\n.\nFeatures\nFirefly III is pretty feature packed. Some important stuff first:\nIt is completely self-hosted and isolated, and will never contact external servers until you explicitly tell it to.\nIt features a REST JSON API that covers almost every part of Firefly III.\nThe most exciting features are:\nCreate\nrecurring transactions to manage your money\n.\nRule based transaction handling\nwith the ability to create your own rules.\nThen the things that make you go \"yeah OK, makes sense\".\nA\ndouble-entry\nbookkeeping system.\nSave towards a goal using\npiggy banks\n.\nView\nincome and expense reports\n.\nAnd the things you would hope for but not expect:\n2 factor authentication for extra security 🔒.\nSupports\nany currency you want\n.\nThere is a\nDocker image\n.\nAnd to organise everything:\nClear views that should show you how you're doing.\nEasy navigation through your records.\nLots of charts because we all love them.\nMany more features are listed in the\ndocumentation\n.\nWho's it for?\nThis application is for people who want to track their finances, keep an eye on their money\nwithout having to upload their financial records to the cloud\n. You're a bit tech-savvy, you like open source software and you don't mind tinkering with (self-hosted) servers.\nThe Firefly III eco-system\nSeveral users have built pretty awesome stuff around the Firefly III API.\nCheck out these tools in the documentation\n.\nGetting Started\nThere are many ways to run Firefly III\nThere is a\ndemo site\nwith an example financial administration already present.\nYou can\ninstall it on your server\n.\nYou can\nrun it using Docker\n.\nYou can\ndeploy via Kubernetes\n.\nYou can\ninstall it using Softaculous\n.\nYou can\ninstall it using AMPPS\n.\nYou can\ninstall it on Cloudron\n.\nYou can\ninstall it on Lando\n.\nYou can\ninstall it on Yunohost\n.\nContributing\nYou can contact me at\njames@firefly-iii.org\n, you may open an issue in the\nmain repository\nor contact me through\ngitter\nand\nMastodon\n.\nOf course, there are some\ncontributing guidelines\nand a\ncode of conduct\n, which I invite you to check out.\nI can always use your help\nsquashing bugs\n, thinking about\nnew features\nor\ntranslating Firefly III\ninto other languages.\nSonarcloud\nscans the code of Firefly III. If you want to help improve Firefly III, check out the latest reports and take your pick!\nThere is also a\nsecurity policy\n.\nSupport the development of Firefly III\nIf you like Firefly III and if it helps you save lots of money, why not send me a dime for every dollar saved! 🥳\nOK that was a joke. If you feel Firefly III made your life better, please consider contributing as a sponsor. Please check out my\nPatreon\nand\nGitHub Sponsors\npage for more information. You can also\nbuy me a ☕️ coffee at ko-fi.com\n. Thank you for your consideration.\nLicense\nThis work\nis licensed\nunder the\nGNU Affero General Public License v3\n.\nDo you need help, or do you want to get in touch?\nDo you want to contact me? You can email me at\njames@firefly-iii.org\nor get in touch through one of the following support channels:\nGitHub Discussions\nfor questions and support\nGitter.im\nfor a good chat and a quick answer\nGitHub Issues\nfor bugs and issues\nMastodon\nfor news and updates\nAcknowledgements\nOver time,\nmany people have contributed to Firefly III\n. I'm grateful for their support and code contributions.\nThe Firefly III logo is made by the excellent Cherie Woo.",
        "今日の獲得スター数: 110",
        "累積スター数: 20,970"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/firefly-iii/firefly-iii"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/sharkdp/bat",
      "title": "sharkdp/bat",
      "date": null,
      "executive_summary": [
        "A cat(1) clone with wings.",
        "---",
        "A\ncat(1)\nclone with syntax highlighting and Git integration.\nKey Features\n•\nHow To Use\n•\nInstallation\n•\nCustomization\n•\nProject goals, alternatives\n[English]\n  [\n中文\n]\n  [\n日本語\n]\n  [\n한국어\n]\n  [\nРусский\n]\nSponsors\nA special\nthank you\ngoes to our biggest\nsponsors\n:\nWarp, the intelligent terminal\nAvailable on MacOS, Linux, Windows\nGraphite is the AI developer productivity platform helping\nteams on GitHub ship higher quality software, faster\nSyntax highlighting\nbat\nsupports syntax highlighting for a large number of programming and markup\nlanguages:\nGit integration\nbat\ncommunicates with\ngit\nto show modifications with respect to the index\n(see left side bar):\nShow non-printable characters\nYou can use the\n-A\n/\n--show-all\noption to show and highlight non-printable\ncharacters:\nAutomatic paging\nBy default,\nbat\npipes its own output to a pager (e.g.\nless\n) if the output is too large for one screen.\nIf you would rather\nbat\nwork like\ncat\nall the time (never page output), you can set\n--paging=never\nas an option, either on the command line or in your configuration file.\nIf you intend to alias\ncat\nto\nbat\nin your shell configuration, you can use\nalias cat='bat --paging=never'\nto preserve the default behavior.\nFile concatenation\nEven with a pager set, you can still use\nbat\nto concatenate files 😉.\nWhenever\nbat\ndetects a non-interactive terminal (i.e. when you pipe into another process or into a file),\nbat\nwill act as a drop-in replacement for\ncat\nand fall back to printing the plain file contents, regardless of the\n--pager\noption's value.\nHow to use\nDisplay a single file on the terminal\nbat README.md\nDisplay multiple files at once\nbat src/\n*\n.rs\nRead from stdin, determine the syntax automatically (note, highlighting will\nonly work if the syntax can be determined from the first line of the file,\nusually through a shebang such as\n#!/bin/sh\n)\ncurl -s https://sh.rustup.rs\n|\nbat\nRead from stdin, specify the language explicitly\nyaml2json .travis.yml\n|\njson_pp\n|\nbat -l json\nShow and highlight non-printable characters:\nbat -A /etc/hosts\nUse it as a\ncat\nreplacement:\nbat\n>\nnote.md\n#\nquickly create a new file\nbat header.md content.md footer.md\n>\ndocument.md\n\nbat -n main.rs\n#\nshow line numbers (only)\nbat f - g\n#\noutput 'f', then stdin, then 'g'.\nIntegration with other tools\nfzf\nYou can use\nbat\nas a previewer for\nfzf\n. To do this,\nuse\nbat\n's\n--color=always\noption to force colorized output. You can also use\n--line-range\noption to restrict the load times for long files:\nfzf --preview\n\"\nbat --color=always --style=numbers --line-range=:500 {}\n\"\nFor more information, see\nfzf\n's\nREADME\n.\nfind\nor\nfd\nYou can use the\n-exec\noption of\nfind\nto preview all search results with\nbat\n:\nfind … -exec bat {} +\nIf you happen to use\nfd\n, you can use the\n-X\n/\n--exec-batch\noption to do the same:\nfd … -X bat\nripgrep\nWith\nbatgrep\n,\nbat\ncan be used as the printer for\nripgrep\nsearch results.\nbatgrep needle src/\ntail -f\nbat\ncan be combined with\ntail -f\nto continuously monitor a given file with syntax highlighting.\ntail -f /var/log/pacman.log\n|\nbat --paging=never -l log\nNote that we have to switch off paging in order for this to work. We have also specified the syntax\nexplicitly (\n-l log\n), as it can not be auto-detected in this case.\ngit\nYou can combine\nbat\nwith\ngit show\nto view an older version of a given file with proper syntax\nhighlighting:\ngit show v0.6.0:src/main.rs\n|\nbat -l rs\ngit diff\nYou can combine\nbat\nwith\ngit diff\nto view lines around code changes with proper syntax\nhighlighting:\nbatdiff\n() {\n    git diff --name-only --relative --diff-filter=d -z\n|\nxargs -0 bat --diff\n}\nIf you prefer to use this as a separate tool, check out\nbatdiff\nin\nbat-extras\n.\nIf you are looking for more support for git and diff operations, check out\ndelta\n.\nxclip\nThe line numbers and Git modification markers in the output of\nbat\ncan make it hard to copy\nthe contents of a file. To prevent this, you can call\nbat\nwith the\n-p\n/\n--plain\noption or\nsimply pipe the output into\nxclip\n:\nbat main.cpp\n|\nxclip\nbat\nwill detect that the output is being redirected and print the plain file contents.\nman\nbat\ncan be used as a colorizing pager for\nman\n, by setting the\nMANPAGER\nenvironment variable:\nexport\nMANPAGER=\n\"\nsh -c 'awk '\\''{ gsub(/\\x1B\\[[0-9;]*m/,\n\\\"\\\"\n,\n\\$\n0); gsub(/.\\x08/,\n\\\"\\\"\n,\n\\$\n0); print }'\\'' | bat -p -lman'\n\"\nman 2\nselect\n(replace\nbat\nwith\nbatcat\nif you are on Debian or Ubuntu)\nIf you prefer to have this bundled in a new command, you can also use\nbatman\n.\nWarning\nThis will\nnot work\nout of the box with Mandoc's\nman\nimplementation.\nPlease either use\nbatman\n, or convert the shell script to a\nshebang executable\nand point\nMANPAGER\nto that.\nNote that the\nManpage syntax\nis developed in this repository and still needs some work.\nprettier\n/\nshfmt\n/\nrustfmt\nThe\nprettybat\nscript is a wrapper that will format code and print it with\nbat\n.\nWarp\nHighlighting\n--help\nmessages\nYou can use\nbat\nto colorize help text:\n$ cp --help | bat -plhelp\nYou can also use a wrapper around this:\n#\nin your .bashrc/.zshrc/*rc\nalias\nbathelp=\n'\nbat --plain --language=help\n'\nhelp\n() {\n\"\n$@\n\"\n--help\n2>&1\n|\nbathelp\n}\nThen you can do\n$ help cp\nor\n$ help git commit\n.\nWhen you are using\nzsh\n, you can also use global aliases to override\n-h\nand\n--help\nentirely:\nalias\n-g -- -h=\n'\n-h 2>&1 | bat --language=help --style=plain\n'\nalias\n-g -- --help=\n'\n--help 2>&1 | bat --language=help --style=plain\n'\nFor\nfish\n, you can use abbreviations:\nabbr\n-a\n--position\nanywhere --\n--help\n'\n--help | bat -plhelp\n'\nabbr\n-a\n--position\nanywhere --\n-h\n'\n-h | bat -plhelp\n'\nThis way, you can keep on using\ncp --help\n, but get colorized help pages.\nBe aware that in some cases,\n-h\nmay not be a shorthand of\n--help\n(for example with\nls\n). In cases where you need to use\n-h\nas a command argument you can prepend\n\\\nto the arguement (eg.\nls \\-h\n) to escape the aliasing defined above.\nPlease report any issues with the help syntax in\nthis repository\n.\nInstallation\nOn Ubuntu (using\napt\n)\n... and other Debian-based Linux distributions.\nbat\nis available on\nUbuntu since 20.04 (\"Focal\")\nand\nDebian since August 2021 (Debian 11 - \"Bullseye\")\n.\nIf your Ubuntu/Debian installation is new enough you can simply run:\nsudo apt install bat\nImportant\n: If you install\nbat\nthis way, please note that the executable may be installed as\nbatcat\ninstead of\nbat\n(due to\na name\nclash with another package\n). You can set up a\nbat -> batcat\nsymlink or alias to prevent any issues that may come up because of this and to be consistent with other distributions:\nmkdir -p\n~\n/.local/bin\nln -s /usr/bin/batcat\n~\n/.local/bin/bat\nan example alias for\nbatcat\nas\nbat\n:\nalias\nbat=\n\"\nbatcat\n\"\nOn Ubuntu (using most recent\n.deb\npackages)\n... and other Debian-based Linux distributions.\nIf the package has not yet been promoted to your Ubuntu/Debian installation, or you want\nthe most recent release of\nbat\n, download the latest\n.deb\npackage from the\nrelease page\nand install it via:\nsudo dpkg -i bat_0.18.3_amd64.deb\n#\nadapt version number and architecture\nOn Alpine Linux\nYou can install\nthe\nbat\npackage\nfrom the official sources, provided you have the appropriate repository enabled:\napk add bat\nOn Arch Linux\nYou can install\nthe\nbat\npackage\nfrom the official sources:\npacman -S bat\nOn Fedora\nYou can install\nthe\nbat\npackage\nfrom the official\nFedora Modular\nrepository.\ndnf install bat\nOn Gentoo Linux\nYou can install\nthe\nbat\npackage\nfrom the official sources:\nemerge sys-apps/bat\nOn FreeBSD\nYou can install a precompiled\nbat\npackage\nwith pkg:\npkg install bat\nor build it on your own from the FreeBSD ports:\ncd\n/usr/ports/textproc/bat\nmake install\nOn OpenBSD\nYou can install\nbat\npackage using\npkg_add(1)\n:\npkg_add bat\nVia nix\nYou can install\nbat\nusing the\nnix package manager\n:\nnix-env -i bat\nOn openSUSE\nYou can install\nbat\nwith zypper:\nzypper install bat\nVia snap package\nThere is currently no recommended snap package available.\nExisting packages may be available, but are not officially supported and may contain\nissues\n.\nOn macOS (or Linux) via Homebrew\nYou can install\nbat\nwith\nHomebrew\n:\nbrew install bat\nOn macOS via MacPorts\nOr install\nbat\nwith\nMacPorts\n:\nport install bat\nOn Windows\nThere are a few options to install\nbat\non Windows. Once you have installed\nbat\n,\ntake a look at the\n\"Using\nbat\non Windows\"\nsection.\nPrerequisites\nYou will need to install the\nVisual C++ Redistributable\nWith WinGet\nYou can install\nbat\nvia\nWinGet\n:\nwinget install sharkdp.bat\nWith Chocolatey\nYou can install\nbat\nvia\nChocolatey\n:\nchoco install bat\nWith Scoop\nYou can install\nbat\nvia\nscoop\n:\nscoop install bat\nFrom prebuilt binaries:\nYou can download prebuilt binaries from the\nRelease page\n,\nYou will need to install the\nVisual C++ Redistributable\npackage.\nFrom binaries\nCheck out the\nRelease page\nfor\nprebuilt versions of\nbat\nfor many different architectures. Statically-linked\nbinaries are also available: look for archives with\nmusl\nin the file name.\nFrom source\nIf you want to build\nbat\nfrom source, you need Rust 1.74.0 or\nhigher. You can then use\ncargo\nto build everything:\nFrom local source\ncargo install --path\n.\n--locked\nNote\nThe\n--path .\nabove specifies the directory of the source code and NOT where\nbat\nwill be installed.\nFor more information see the docs for\ncargo install\n.\nFrom\ncrates.io\ncargo install --locked bat\nNote that additional files like the man page or shell completion\nfiles can not be installed automatically in both these ways.\nIf installing from a local source, they will be generated by\ncargo\nand should be available in the cargo target folder under\nbuild\n.\nFurthermore, shell completions are also available by running:\nbat --completion\n<\nshell\n>\n#\nsee --help for supported shells\nCustomization\nHighlighting theme\nUse\nbat --list-themes\nto get a list of all available themes for syntax\nhighlighting. By default,\nbat\nuses\nMonokai Extended\nor\nMonokai Extended Light\nfor dark and light themes respectively. To select the\nTwoDark\ntheme, call\nbat\nwith the\n--theme=TwoDark\noption or set the\nBAT_THEME\nenvironment variable to\nTwoDark\n. Use\nexport BAT_THEME=\"TwoDark\"\nin your shell's startup file to\nmake the change permanent. Alternatively, use\nbat\n's\nconfiguration file\n.\nIf you want to preview the different themes on a custom file, you can use\nthe following command (you need\nfzf\nfor this):\nbat --list-themes\n|\nfzf --preview=\n\"\nbat --theme={} --color=always /path/to/file\n\"\nbat\nautomatically picks a fitting theme depending on your terminal's background color.\nYou can use the\n--theme-dark\n/\n--theme-light\noptions or the\nBAT_THEME_DARK\n/\nBAT_THEME_LIGHT\nenvironment variables\nto customize the themes used. This is especially useful if you frequently switch between dark and light mode.\nYou can also use a custom theme by following the\n'Adding new themes' section below\n.\n8-bit themes\nbat\nhas three themes that always use\n8-bit colors\n,\neven when truecolor support is available:\nansi\nlooks decent on any terminal. It uses 3-bit colors: black, red, green,\nyellow, blue, magenta, cyan, and white.\nbase16\nis designed for\nbase16\nterminal themes. It uses\n4-bit colors (3-bit colors plus bright variants) in accordance with the\nbase16 styling guidelines\n.\nbase16-256\nis designed for\ntinted-shell\n.\nIt replaces certain bright colors with 8-bit colors from 16 to 21.\nDo not\nuse this simply\nbecause you have a 256-color terminal but are not using tinted-shell.\nAlthough these themes are more restricted, they have three advantages over truecolor themes. They:\nEnjoy maximum compatibility. Some terminal utilities do not support more than 3-bit colors.\nAdapt to terminal theme changes. Even for already printed output.\nVisually harmonize better with other terminal software.\nOutput style\nYou can use the\n--style\noption to control the appearance of\nbat\n's output.\nYou can use\n--style=numbers,changes\n, for example, to show only Git changes\nand line numbers but no grid and no file header. Set the\nBAT_STYLE\nenvironment\nvariable to make these changes permanent or use\nbat\n's\nconfiguration file\n.\nTip\nIf you specify a default style in\nbat\n's config file, you can change which components\nare displayed during a single run of\nbat\nusing the\n--style\ncommand-line argument.\nBy prefixing a component with\n+\nor\n-\n, it can be added or removed from the current style.\nFor example, if your config contains\n--style=full,-snip\n, you can run bat with\n--style=-grid,+snip\nto remove the grid and add back the\nsnip\ncomponent.\nOr, if you want to override the styles completely, you use\n--style=numbers\nto\nonly show the line numbers.\nAdding new syntaxes / language definitions\nShould you find that a particular syntax is not available within\nbat\n, you can follow these\ninstructions to easily add new syntaxes to your current\nbat\ninstallation.\nbat\nuses the excellent\nsyntect\nlibrary for syntax highlighting.\nsyntect\ncan read any\nSublime Text\n.sublime-syntax\nfile\nand theme.\nA good resource for finding Sublime Syntax packages is\nPackage Control\n. Once you found a\nsyntax:\nCreate a folder with syntax definition files:\nmkdir -p\n\"\n$(\nbat --config-dir\n)\n/syntaxes\n\"\ncd\n\"\n$(\nbat --config-dir\n)\n/syntaxes\n\"\n#\nPut new '.sublime-syntax' language definition files\n#\nin this folder (or its subdirectories), for example:\ngit clone https://github.com/tellnobody1/sublime-purescript-syntax\nNow use the following command to parse these files into a binary cache:\nbat cache --build\nFinally, use\nbat --list-languages\nto check if the new languages are available.\nIf you ever want to go back to the default settings, call:\nbat cache --clear\nIf you think that a specific syntax should be included in\nbat\nby default, please\nconsider opening a \"syntax request\" ticket after reading the policies and\ninstructions\nhere\n:\nOpen Syntax Request\n.\nAdding new themes\nThis works very similar to how we add new syntax definitions.\nNote\nThemes are stored in\n.tmTheme\nfiles\n.\nFirst, create a folder with the new syntax highlighting themes:\nmkdir -p\n\"\n$(\nbat --config-dir\n)\n/themes\n\"\ncd\n\"\n$(\nbat --config-dir\n)\n/themes\n\"\n#\nDownload a theme in '.tmTheme' format, for example:\ngit clone https://github.com/greggb/sublime-snazzy\n#\nUpdate the binary cache\nbat cache --build\nFinally, use\nbat --list-themes\nto check if the new themes are available.\nNote\nbat\nuses the name of the\n.tmTheme\nfile for the theme's name.\nAdding or changing file type associations\nYou can add new (or change existing) file name patterns using the\n--map-syntax\ncommand line option. The option takes an argument of the form\npattern:syntax\nwhere\npattern\nis a glob pattern that is matched against the file name and\nthe absolute file path. The\nsyntax\npart is the full name of a supported language\n(use\nbat --list-languages\nfor an overview).\nNote:\nYou probably want to use this option as\nan entry in\nbat\n's configuration file\nfor persistence instead of passing it on the command line as a one-off. Generally\nyou'd just use\n-l\nif you want to manually specify a language for a file.\nExample: To use \"INI\" syntax highlighting for all files with a\n.conf\nfile extension, use\n--map-syntax=\n'\n*.conf:INI\n'\nExample: To open all files called\n.ignore\n(exact match) with the \"Git Ignore\" syntax, use:\n--map-syntax=\n'\n.ignore:Git Ignore\n'\nExample: To open all\n.conf\nfiles in subfolders of\n/etc/apache2\nwith the \"Apache Conf\"\nsyntax, use (this mapping is already built in):\n--map-syntax=\n'\n/etc/apache2/**/*.conf:Apache Conf\n'\nUsing a different pager\nbat\nuses the pager that is specified in the\nPAGER\nenvironment variable. If this variable is not\nset,\nless\nis used by default. You can also use bat's built-in pager with\n--pager=builtin\nor\nby setting the\nBAT_PAGER\nenvironment variable to \"builtin\".\nIf you want to use a different pager, you can either modify the\nPAGER\nvariable or set the\nBAT_PAGER\nenvironment variable to override what is specified in\nPAGER\n.\nNote\nIf\nPAGER\nis\nmore\nor\nmost\n,\nbat\nwill silently use\nless\ninstead to ensure support for colors.\nIf you want to pass command-line arguments to the pager, you can also set them via the\nPAGER\n/\nBAT_PAGER\nvariables:\nexport\nBAT_PAGER=\n\"\nless -RFK\n\"\nInstead of using environment variables, you can also use\nbat\n's\nconfiguration file\nto configure the pager (\n--pager\noption).\nUsing\nless\nas a pager\nWhen using\nless\nas a pager,\nbat\nwill automatically pass extra options along to\nless\nto improve the experience. Specifically,\n-R\n/\n--RAW-CONTROL-CHARS\n,\n-F\n/\n--quit-if-one-screen\n,\n-K\n/\n--quit-on-intr\nand under certain conditions,\n-X\n/\n--no-init\nand/or\n-S\n/\n--chop-long-lines\n.\nImportant\nThese options will not be added if:\nThe pager is not named\nless\n.\nThe\n--pager\nargument contains any command-line arguments (e.g.\n--pager=\"less -R\"\n).\nThe\nBAT_PAGER\nenvironment variable contains any command-line arguments (e.g.\nexport BAT_PAGER=\"less -R\"\n)\nThe\n--quit-if-one-screen\noption will not be added when:\nThe\n--paging=always\nargument is used.\nThe\nBAT_PAGING\nenvironment is set to\nalways\n.\nThe\n-R\noption is needed to interpret ANSI colors correctly.\nThe\n-F\noption instructs\nless\nto exit immediately if the output size is smaller than\nthe vertical size of the terminal. This is convenient for small files because you do not\nhave to press\nq\nto quit the pager.\nThe\n-K\noption instructs\nless\nto exit immediately when an interrupt signal is received.\nThis is useful to ensure that\nless\nquits together with\nbat\non SIGINT.\nThe\n-X\noption is needed to fix a bug with the\n--quit-if-one-screen\nfeature in versions\nof\nless\nolder than version 530. Unfortunately, it also breaks mouse-wheel support in\nless\n.\nIf you want to enable mouse-wheel scrolling on older versions of\nless\nand do not mind losing\nthe quit-if-one-screen feature, you can set the pager (via\n--pager\nor\nBAT_PAGER\n) to\nless -R\n.\nFor\nless\n530 or newer, it should work out of the box.\nThe\n-S\noption is added when\nbat\n's\n-S\n/\n--chop-long-lines\noption is used. This tells\nless\nto truncate any lines larger than the terminal width.\nIndentation\nbat\nexpands tabs to 4 spaces by itself, not relying on the pager. To change this, simply add the\n--tabs\nargument with the number of spaces you want to be displayed.\nNote\n: Defining tab stops for the pager (via the\n--pager\nargument by\nbat\n, or via the\nLESS\nenvironment variable for\nless\n) won't be taken into account because the pager will already get\nexpanded spaces instead of tabs. This behaviour is added to avoid indentation issues caused by the\nsidebar. Calling\nbat\nwith\n--tabs=0\nwill override it and let tabs be consumed by the pager.\nDark mode\nIf you make use of the dark mode feature in\nmacOS\n, you might want to configure\nbat\nto use a different\ntheme based on the OS theme. The following snippet uses the\ndefault\ntheme when in the\ndark mode\nand the\nGitHub\ntheme when in the\nlight mode\n.\nalias\ncat=\n\"\nbat --theme auto:system --theme-dark default --theme-light GitHub\n\"\nThe same dark mode feature is now available in\nGNOME\nand affects the\norg.gnome.desktop.interface color-scheme\nsetting. The following code converts the above to use said setting.\n#\n.bashrc\nsys_color_scheme_is_dark\n() {\n    condition=\n$(\ngsettings get org.gnome.desktop.interface color-scheme\n)\ncondition=\n$(\necho\n\"\n$condition\n\"\n|\ntr -d\n\"\n[:space:]'\n\"\n)\nif\n[\n$condition\n==\n\"\nprefer-dark\n\"\n]\n;\nthen\nreturn\n0\nelse\nreturn\n1\nfi\n}\nbat_alias_wrapper\n() {\n#\nget color scheme\nsys_color_scheme_is_dark\nif\n[[\n$?\n-eq\n0 ]]\n;\nthen\n#\nbat command with dark color scheme\nbat --theme=default\n\"\n$@\n\"\nelse\n#\nbat command with light color scheme\nbat --theme=GitHub\n\"\n$@\n\"\nfi\n}\nalias\ncat=\n'\nbat_alias_wrapper\n'\nConfiguration file\nbat\ncan also be customized with a configuration file. The location of the file is dependent\non your operating system. To get the default path for your system, call\nbat --config-file\nAlternatively, you can use\nBAT_CONFIG_PATH\nor\nBAT_CONFIG_DIR\nenvironment variables to point\nbat\nto a non-default location of the configuration file or the configuration directory respectively:\nexport\nBAT_CONFIG_PATH=\n\"\n/path/to/bat/bat.conf\n\"\nexport\nBAT_CONFIG_DIR=\n\"\n/path/to/bat\n\"\nA default configuration file can be created with the\n--generate-config-file\noption.\nbat --generate-config-file\nThere is also now a systemwide configuration file, which is located under\n/etc/bat/config\non\nLinux and Mac OS and\nC:\\ProgramData\\bat\\config\non windows. If the system wide configuration\nfile is present, the content of the user configuration will simply be appended to it.\nFormat\nThe configuration file is a simple list of command line arguments. Use\nbat --help\nto see a full list of possible options and values. In addition, you can add comments by prepending a line with the\n#\ncharacter.\nExample configuration file:\n#\nSet the theme to \"TwoDark\"\n--theme=\n\"\nTwoDark\n\"\n#\nShow line numbers, Git modifications and file header (but no grid)\n--style=\n\"\nnumbers,changes,header\n\"\n#\nUse italic text on the terminal (not supported on all terminals)\n--italic-text=always\n#\nUse C++ syntax for Arduino .ino files\n--map-syntax\n\"\n*.ino:C++\n\"\nUsing\nbat\non Windows\nbat\nmostly works out-of-the-box on Windows, but a few features may need extra configuration.\nPrerequisites\nYou will need to install the\nVisual C++ Redistributable\npackage.\nPaging\nWindows only includes a very limited pager in the form of\nmore\n. You can download a Windows binary\nfor\nless\nfrom its homepage\nor\nthrough\nChocolatey\n. To use it, place the binary in a directory in\nyour\nPATH\nor\ndefine an environment variable\n. The\nChocolatey package\ninstalls\nless\nautomatically.\nColors\nWindows 10 natively supports colors in both\nconhost.exe\n(Command Prompt) and PowerShell since\nv1511\n, as\nwell as in newer versions of bash. On earlier versions of Windows, you can use\nCmder\n, which includes\nConEmu\n.\nNote:\nOld versions of\nless\ndo not correctly interpret colors on Windows. To fix this, you can add the optional Unix tools to your PATH when installing Git. If you don’t have any other pagers installed, you can disable paging entirely by passing\n--paging=never\nor by setting\nBAT_PAGER\nto an empty string.\nCygwin\nbat\non Windows does not natively support Cygwin's unix-style paths (\n/cygdrive/*\n). When passed an absolute cygwin path as an argument,\nbat\nwill encounter the following error:\nThe system cannot find the path specified. (os error 3)\nThis can be solved by creating a wrapper or adding the following function to your\n.bash_profile\nfile:\nbat\n() {\nlocal\nindex\nlocal\nargs=(\n\"\n$@\n\"\n)\nfor\nindex\nin\n$(\nseq 0\n${\n#\nargs[@]}\n)\n;\ndo\ncase\n\"\n${args[index]}\n\"\nin\n-\n*\n)\ncontinue\n;;\n*\n)  [\n-e\n\"\n${args[index]}\n\"\n]\n&&\nargs[index]=\n\"\n$(\ncygpath --windows\n\"\n${args[index]}\n\"\n)\n\"\n;;\nesac\ndone\ncommand\nbat\n\"\n${args[@]}\n\"\n}\nTroubleshooting\nGarbled output\nIf an input file contains color codes or other ANSI escape sequences or control characters,\nbat\nwill have problems\nperforming syntax highlighting and text wrapping, and thus the output can become garbled.\nIf your version of\nbat\nsupports the\n--strip-ansi=auto\noption, it can be used to remove such sequences\nbefore syntax highlighting. Alternatively, you may disable both syntax highlighting and wrapping by\npassing the\n--color=never --wrap=never\noptions to\nbat\n.\nNote\nThe\nauto\noption of\n--strip-ansi\navoids removing escape sequences when the syntax is plain text.\nTerminals & colors\nbat\nhandles terminals\nwith\nand\nwithout\ntruecolor support. However, the colors in most syntax\nhighlighting themes are not optimized for 8-bit colors. It is therefore strongly recommended\nthat you use a terminal with 24-bit truecolor support (\nterminator\n,\nkonsole\n,\niTerm2\n, ...),\nor use one of the basic\n8-bit themes\ndesigned for a restricted set of colors.\nSee\nthis article\nfor more details and a full list of\nterminals with truecolor support.\nMake sure that your truecolor terminal sets the\nCOLORTERM\nvariable to either\ntruecolor\nor\n24bit\n. Otherwise,\nbat\nwill not be able to determine whether or not 24-bit escape sequences\nare supported (and fall back to 8-bit colors).\nLine numbers and grid are hardly visible\nPlease try a different theme (see\nbat --list-themes\nfor a list). The\nOneHalfDark\nand\nOneHalfLight\nthemes provide grid and line colors that are brighter.\nFile encodings\nbat\nnatively supports UTF-8 as well as UTF-16. For every other file encoding, you may need to\nconvert to UTF-8 first because the encodings can typically not be auto-detected. You can\niconv\nto do so.\nExample: if you have a PHP file in Latin-1 (ISO-8859-1) encoding, you can call:\niconv -f ISO-8859-1 -t UTF-8 my-file.php\n|\nbat\nNote: you might have to use the\n-l\n/\n--language\noption if the syntax can not be auto-detected\nby\nbat\n.\nDevelopment\n#\nRecursive clone to retrieve all submodules\ngit clone --recursive https://github.com/sharkdp/bat\n#\nBuild (debug version)\ncd\nbat\ncargo build --bins\n#\nRun unit tests and integration tests\ncargo\ntest\n#\nInstall (release version)\ncargo install --path\n.\n--locked\n#\nBuild a bat binary with modified syntaxes and themes\nbash assets/create.sh\ncargo install --path\n.\n--locked --force\nIf you want to build an application that uses\nbat\n's pretty-printing\nfeatures as a library, check out the\nthe API documentation\n.\nNote that you have to use either\nregex-onig\nor\nregex-fancy\nas a feature\nwhen you depend on\nbat\nas a library.\nContributing\nTake a look at the\nCONTRIBUTING.md\nguide.\nMaintainers\nsharkdp\neth-p\nkeith-hall\nEnselic\nSecurity vulnerabilities\nSee\nSECURITY.md\n.\nProject goals and alternatives\nbat\ntries to achieve the following goals:\nProvide beautiful, advanced syntax highlighting\nIntegrate with Git to show file modifications\nBe a drop-in replacement for (POSIX)\ncat\nOffer a user-friendly command-line interface\nThere are a lot of alternatives, if you are looking for similar programs. See\nthis document\nfor a comparison.\nLicense\nCopyright (c) 2018-2025\nbat-developers\n.\nbat\nis made available under the terms of either the MIT License or the Apache License 2.0, at your option.\nSee the\nLICENSE-APACHE\nand\nLICENSE-MIT\nfiles for license details.",
        "今日の獲得スター数: 97",
        "累積スター数: 54,887"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/sharkdp/bat"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/shadcn-ui/ui",
      "title": "shadcn-ui/ui",
      "date": null,
      "executive_summary": [
        "A set of beautifully-designed, accessible components and a code distribution platform. Works with your favorite frameworks. Open Source. Open Code.",
        "---",
        "shadcn/ui\nAccessible and customizable components that you can copy and paste into your apps. Free. Open Source.\nUse this to build your own component library\n.\nDocumentation\nVisit\nhttp://ui.shadcn.com/docs\nto view the documentation.\nContributing\nPlease read the\ncontributing guide\n.\nLicense\nLicensed under the\nMIT license\n.",
        "今日の獲得スター数: 91",
        "累積スター数: 96,851"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/shadcn-ui/ui"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/MODSetter/SurfSense",
      "title": "MODSetter/SurfSense",
      "date": null,
      "executive_summary": [
        "Open Source Alternative to NotebookLM / Perplexity, connected to external sources such as Search Engines, Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more. Join our discord: https://discord.gg/ejRNvftDp9",
        "---",
        "SurfSense\nWhile tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar, Luma and more to come.\nVideo\ntemp_demo_v7.mp4\nPodcast Sample\nelon_vs_trump_podcast.mp4\nKey Features\n💡\nIdea\n:\nHave your own highly customizable private NotebookLM and Perplexity integrated with external sources.\n📁\nMultiple File Format Uploading Support\nSave content from your own personal files\n(Documents, images, videos and supports\n50+ file extensions\n)\nto your own personal knowledge base .\n🔍\nPowerful Search\nQuickly research or find anything in your saved content .\n💬\nChat with your Saved Content\nInteract in Natural Language and get cited answers.\n📄\nCited Answers\nGet Cited answers just like Perplexity.\n🔔\nPrivacy & Local LLM Support\nWorks Flawlessly with Ollama local LLMs.\n🏠\nSelf Hostable\nOpen source and easy to deploy locally.\n🎙️ Podcasts\nBlazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)\nConvert your chat conversations into engaging audio content\nSupport for local TTS providers (Kokoro TTS)\nSupport for multiple TTS providers (OpenAI, Azure, Google Vertex AI)\n📊\nAdvanced RAG Techniques\nSupports 100+ LLM's\nSupports 6000+ Embedding Models.\nSupports all major Rerankers (Pinecode, Cohere, Flashrank etc)\nUses Hierarchical Indices (2 tiered RAG setup).\nUtilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).\nRAG as a Service API Backend.\nℹ️\nExternal Sources\nSearch Engines (Tavily, LinkUp)\nSlack\nLinear\nJira\nClickUp\nConfluence\nNotion\nGmail\nYoutube Videos\nGitHub\nDiscord\nAirtable\nGoogle Calendar\nLuma\nand more to come.....\n📄\nSupported File Extensions\nNote\n: File format support depends on your ETL service configuration. LlamaCloud supports 50+ formats, Unstructured supports 34+ core formats, and Docling (core formats, local processing, privacy-focused, no API key).\nDocuments & Text\nLlamaCloud\n:\n.pdf\n,\n.doc\n,\n.docx\n,\n.docm\n,\n.dot\n,\n.dotm\n,\n.rtf\n,\n.txt\n,\n.xml\n,\n.epub\n,\n.odt\n,\n.wpd\n,\n.pages\n,\n.key\n,\n.numbers\n,\n.602\n,\n.abw\n,\n.cgm\n,\n.cwk\n,\n.hwp\n,\n.lwp\n,\n.mw\n,\n.mcw\n,\n.pbd\n,\n.sda\n,\n.sdd\n,\n.sdp\n,\n.sdw\n,\n.sgl\n,\n.sti\n,\n.sxi\n,\n.sxw\n,\n.stw\n,\n.sxg\n,\n.uof\n,\n.uop\n,\n.uot\n,\n.vor\n,\n.wps\n,\n.zabw\nUnstructured\n:\n.doc\n,\n.docx\n,\n.odt\n,\n.rtf\n,\n.pdf\n,\n.xml\n,\n.txt\n,\n.md\n,\n.markdown\n,\n.rst\n,\n.html\n,\n.org\n,\n.epub\nDocling\n:\n.pdf\n,\n.docx\n,\n.html\n,\n.htm\n,\n.xhtml\n,\n.adoc\n,\n.asciidoc\nPresentations\nLlamaCloud\n:\n.ppt\n,\n.pptx\n,\n.pptm\n,\n.pot\n,\n.potm\n,\n.potx\n,\n.odp\n,\n.key\nUnstructured\n:\n.ppt\n,\n.pptx\nDocling\n:\n.pptx\nSpreadsheets & Data\nLlamaCloud\n:\n.xlsx\n,\n.xls\n,\n.xlsm\n,\n.xlsb\n,\n.xlw\n,\n.csv\n,\n.tsv\n,\n.ods\n,\n.fods\n,\n.numbers\n,\n.dbf\n,\n.123\n,\n.dif\n,\n.sylk\n,\n.slk\n,\n.prn\n,\n.et\n,\n.uos1\n,\n.uos2\n,\n.wk1\n,\n.wk2\n,\n.wk3\n,\n.wk4\n,\n.wks\n,\n.wq1\n,\n.wq2\n,\n.wb1\n,\n.wb2\n,\n.wb3\n,\n.qpw\n,\n.xlr\n,\n.eth\nUnstructured\n:\n.xls\n,\n.xlsx\n,\n.csv\n,\n.tsv\nDocling\n:\n.xlsx\n,\n.csv\nImages\nLlamaCloud\n:\n.jpg\n,\n.jpeg\n,\n.png\n,\n.gif\n,\n.bmp\n,\n.svg\n,\n.tiff\n,\n.webp\n,\n.html\n,\n.htm\n,\n.web\nUnstructured\n:\n.jpg\n,\n.jpeg\n,\n.png\n,\n.bmp\n,\n.tiff\n,\n.heic\nDocling\n:\n.jpg\n,\n.jpeg\n,\n.png\n,\n.bmp\n,\n.tiff\n,\n.tif\n,\n.webp\nAudio & Video\n(Always Supported)\n.mp3\n,\n.mpga\n,\n.m4a\n,\n.wav\n,\n.mp4\n,\n.mpeg\n,\n.webm\nEmail & Communication\nUnstructured\n:\n.eml\n,\n.msg\n,\n.p7s\n🔖 Cross Browser Extension\nThe SurfSense extension can be used to save any webpage you like.\nIts main usecase is to save any webpages protected beyond authentication.\nFEATURE REQUESTS AND FUTURE\nSurfSense is actively being developed.\nWhile it's not yet production-ready, you can help us speed up the process.\nJoin the\nSurfSense Discord\nand help shape the future of SurfSense!\n🚀 Roadmap\nStay up to date with our development progress and upcoming features!\nCheck out our public roadmap and contribute your ideas or feedback:\nView the Roadmap:\nSurfSense Roadmap on GitHub Projects\nHow to get started?\nInstallation Options\nSurfSense provides two installation methods:\nDocker Installation\n- The easiest way to get SurfSense up and running with all dependencies containerized.\nIncludes pgAdmin for database management through a web UI\nSupports environment variable customization via\n.env\nfile\nFlexible deployment options (full stack or core services only)\nNo need to manually edit configuration files between environments\nSee\nDocker Setup Guide\nfor detailed instructions\nFor deployment scenarios and options, see\nDeployment Guide\nManual Installation (Recommended)\n- For users who prefer more control over their setup or need to customize their deployment.\nBoth installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.\nBefore installation, make sure to complete the\nprerequisite setup steps\nincluding:\nPGVector setup\nFile Processing ETL Service\n(choose one):\nUnstructured.io API key (supports 34+ formats)\nLlamaIndex API key (enhanced parsing, supports 50+ formats)\nDocling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)\nOther required API keys\nScreenshots\nResearch Agent\nSearch Spaces\nManage Documents\nPodcast Agent\nAgent Chat\nBrowser Extension\nTech Stack\nBackEnd\nFastAPI\n: Modern, fast web framework for building APIs with Python\nPostgreSQL with pgvector\n: Database with vector search capabilities for similarity searches\nSQLAlchemy\n: SQL toolkit and ORM (Object-Relational Mapping) for database interactions\nAlembic\n: A database migrations tool for SQLAlchemy.\nFastAPI Users\n: Authentication and user management with JWT and OAuth support\nLangGraph\n: Framework for developing AI-agents.\nLangChain\n: Framework for developing AI-powered applications.\nLLM Integration\n: Integration with LLM models through LiteLLM\nRerankers\n: Advanced result ranking for improved search relevance\nHybrid Search\n: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)\nVector Embeddings\n: Document and text embeddings for semantic search\npgvector\n: PostgreSQL extension for efficient vector similarity operations\nChonkie\n: Advanced document chunking and embedding library\nUses\nAutoEmbeddings\nfor flexible embedding model selection\nLateChunker\nfor optimized document chunking based on embedding model's max sequence length\nFrontEnd\nNext.js 15.2.3\n: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.\nReact 19.0.0\n: JavaScript library for building user interfaces.\nTypeScript\n: Static type-checking for JavaScript, enhancing code quality and developer experience.\nVercel AI SDK Kit UI Stream Protocol\n: To create scalable chat UI.\nTailwind CSS 4.x\n: Utility-first CSS framework for building custom UI designs.\nShadcn\n: Headless components library.\nLucide React\n: Icon set implemented as React components.\nFramer Motion\n: Animation library for React.\nSonner\n: Toast notification library.\nGeist\n: Font family from Vercel.\nReact Hook Form\n: Form state management and validation.\nZod\n: TypeScript-first schema validation with static type inference.\n@hookform/resolvers\n: Resolvers for using validation libraries with React Hook Form.\n@tanstack/react-table\n: Headless UI for building powerful tables & datagrids.\nDevOps\nDocker\n: Container platform for consistent deployment across environments\nDocker Compose\n: Tool for defining and running multi-container Docker applications\npgAdmin\n: Web-based PostgreSQL administration tool included in Docker setup\nExtension\nManifest v3 on Plasmo\nFuture Work\nAdd More Connectors.\nPatch minor bugs.\nDocument Podcasts\nContribute\nContributions are very welcome! A contribution can be as small as a ⭐ or even finding and creating issues.\nFine-tuning the Backend is always desired.\nFor detailed contribution guidelines, please see our\nCONTRIBUTING.md\nfile.\nStar History",
        "今日の獲得スター数: 88",
        "累積スター数: 8,601"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/MODSetter/SurfSense"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/jdx/mise",
      "title": "jdx/mise",
      "date": null,
      "executive_summary": [
        "dev tools, env vars, task runner",
        "---",
        "mise-en-place\nThe front-end to your dev env\nGetting Started\n•\nDocumentation\n•\nDev Tools\n•\nEnvironments\n•\nTasks\nWhat is it?\nLike\nasdf\n(or\nnvm\nor\npyenv\nbut for any language) it manages\ndev tools\nlike node, python, cmake, terraform, and\nhundreds more\n.\nLike\ndirenv\nit manages\nenvironment variables\nfor different project directories.\nLike\nmake\nit manages\ntasks\nused to build and test projects.\nDemo\nThe following demo shows how to install and use\nmise\nto manage multiple versions of\nnode\non the same system.\nNote that calling\nwhich node\ngives us a real path to node, not a shim.\nIt also shows that you can use\nmise\nto install and many other tools such as\njq\n,\nterraform\n, or\ngo\n.\nSee\ndemo transcript\n.\nQuickstart\nInstall mise\nSee\nGetting started\nfor more options.\n$\ncurl https://mise.run\n|\nsh\n$\n~\n/.local/bin/mise --version\n2025.10.6 macos-arm64 (a1b2d3e 2025-10-08)\nHook mise into your shell (pick the right one for your shell):\n#\nnote this assumes mise is located at\n~\n/.local/bin/mise\n#\nwhich is what https://mise.run does by default\necho 'eval \"$(~/.local/bin/mise activate bash)\"' >> ~/.bashrc\necho 'eval \"$(~/.local/bin/mise activate zsh)\"' >> ~/.zshrc\necho '~/.local/bin/mise activate fish | source' >> ~/.config/fish/config.fish\necho '~/.local/bin/mise activate pwsh | Out-String | Invoke-Expression' >> ~/.config/powershell/Microsoft.PowerShell_profile.ps1\nExecute commands with specific tools\n$\nmise\nexec\nnode@22 -- node -v\nmise node@22.x.x ✓ installed\nv22.x.x\nInstall tools\n$\nmise use --global node@22 go@1\n$\nnode -v\nv22.x.x\n$\ngo version\ngo version go1.x.x macos/arm64\nSee\ndev tools\nfor more examples.\nManage environment variables\n#\nmise.toml\n[\nenv\n]\nSOME_VAR\n=\n\"\nfoo\n\"\n$\nmise\nset\nSOME_VAR=bar\n$\necho\n$SOME_VAR\nbar\nNote that\nmise\ncan also\nload\n.env\nfiles\n.\nRun tasks\n#\nmise.toml\n[\ntasks\n.\nbuild\n]\ndescription\n=\n\"\nbuild the project\n\"\nrun\n=\n\"\necho building...\n\"\n$\nmise run build\nbuilding...\nSee\ntasks\nfor more information.\nExample mise project\nHere is a combined example to give you an idea of how you can use mise to manage your a project's tools, environment, and tasks.\n#\nmise.toml\n[\ntools\n]\nterraform\n=\n\"\n1\n\"\naws-cli\n=\n\"\n2\n\"\n[\nenv\n]\nTF_WORKSPACE\n=\n\"\ndevelopment\n\"\nAWS_REGION\n=\n\"\nus-west-2\n\"\nAWS_PROFILE\n=\n\"\ndev\n\"\n[\ntasks\n.\nplan\n]\ndescription\n=\n\"\nRun terraform plan with configured workspace\n\"\nrun\n=\n\"\"\"\nterraform init\nterraform workspace select $TF_WORKSPACE\nterraform plan\n\"\"\"\n[\ntasks\n.\nvalidate\n]\ndescription\n=\n\"\nValidate AWS credentials and terraform config\n\"\nrun\n=\n\"\"\"\naws sts get-caller-identity\nterraform validate\n\"\"\"\n[\ntasks\n.\ndeploy\n]\ndescription\n=\n\"\nDeploy infrastructure after validation\n\"\ndepends\n= [\n\"\nvalidate\n\"\n,\n\"\nplan\n\"\n]\nrun\n=\n\"\nterraform apply -auto-approve\n\"\nRun it with:\nmise install # install tools specified in mise.toml\nmise run deploy\nFind more examples in the\nmise cookbook\n.\nFull Documentation\nSee\nmise.jdx.dev\nGitHub Issues & Discussions\nDue to the volume of issue submissions mise received, using GitHub Issues became unsustainable for\nthe project. Instead, mise uses GitHub Discussions which provide a more community-centric platform\nfor communication and require less management on the part of the maintainers.\nPlease note the following discussion categories, which match how issues are often used:\nAnnouncements\nIdeas\n: for feature requests, etc.\nTroubleshooting & Bug Reports\nSpecial Thanks\nWe're grateful for Cloudflare's support through\nProject Alexandria\n.\nContributors",
        "今日の獲得スター数: 77",
        "累積スター数: 20,215"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/jdx/mise"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/tursodatabase/turso",
      "title": "tursodatabase/turso",
      "date": null,
      "executive_summary": [
        "Turso is an in-process SQL database, compatible with SQLite.",
        "---",
        "Turso Database\nAn in-process SQL database, compatible with SQLite.\nAbout\nTurso Database is an in-process SQL database written in Rust, compatible with SQLite.\n⚠️\nWarning:\nThis software is in BETA. It may still contain bugs and unexpected behavior. Use caution with production data and ensure you have backups.\nFeatures and Roadmap\nSQLite compatibility\nfor SQL dialect, file formats, and the C API [see\ndocument\nfor details]\nChange data capture (CDC)\nfor real-time tracking of database changes.\nMulti-language support\nfor\nGo\nJavaScript\nJava\nPython\nRust\nWebAssembly\nAsynchronous I/O\nsupport on Linux with\nio_uring\nCross-platform\nsupport for Linux, macOS, Windows and browsers (through WebAssembly)\nVector support\nsupport including exact search and vector manipulation\nImproved schema management\nincluding extended\nALTER\nsupport and faster schema changes.\nThe database has the following experimental features:\nBEGIN CONCURRENT\nfor improved write throughput using multi-version concurrency control (MVCC).\nEncryption at rest\nfor protecting the data locally.\nIncremental computation\nusing DBSP for incremental view mainatenance and query subscriptions.\nThe following features are on our current roadmap:\nVector indexing\nfor fast approximate vector search, similar to\nlibSQL vector search\n.\nGetting Started\nPlease see the\nTurso Database Manual\nfor more information.\n💻 Command Line\nYou can install the latest `turso` release with:\ncurl --proto\n'\n=https\n'\n--tlsv1.2 -LsSf \\\n  https://github.com/tursodatabase/turso/releases/latest/download/turso_cli-installer.sh\n|\nsh\nThen launch the interactive shell:\n$ tursodb\nThis will start the Turso interactive shell where you can execute SQL statements:\nTurso\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database\nturso> CREATE TABLE users (id INT, username TEXT);\nturso> INSERT INTO users VALUES (1, 'alice');\nturso> INSERT INTO users VALUES (2, 'bob');\nturso> SELECT * FROM users;\n1|alice\n2|bob\nYou can also build and run the latest development version with:\ncargo run\nIf you like docker, we got you covered. Simply run this in the root folder:\nmake docker-cli-build\n&&\n\\\nmake docker-cli-run\n🦀 Rust\ncargo add turso\nExample usage:\nlet\ndb =\nBuilder\n::\nnew_local\n(\n\"sqlite.db\"\n)\n.\nbuild\n(\n)\n.\nawait\n?\n;\nlet\nconn = db\n.\nconnect\n(\n)\n?\n;\nlet\nres = conn\n.\nquery\n(\n\"SELECT * FROM users\"\n,\n(\n)\n)\n.\nawait\n?\n;\n✨ JavaScript\nnpm i @tursodatabase/database\nExample usage:\nimport\n{\nconnect\n}\nfrom\n'@tursodatabase/database'\n;\nconst\ndb\n=\nawait\nconnect\n(\n'sqlite.db'\n)\n;\nconst\nstmt\n=\ndb\n.\nprepare\n(\n'SELECT * FROM users'\n)\n;\nconst\nusers\n=\nstmt\n.\nall\n(\n)\n;\nconsole\n.\nlog\n(\nusers\n)\n;\n🐍 Python\nuv pip install pyturso\nExample usage:\nimport\nturso\ncon\n=\nturso\n.\nconnect\n(\n\"sqlite.db\"\n)\ncur\n=\ncon\n.\ncursor\n()\nres\n=\ncur\n.\nexecute\n(\n\"SELECT * FROM users\"\n)\nprint\n(\nres\n.\nfetchone\n())\n🦫 Go\ngo get github.com/tursodatabase/turso-go\ngo install github.com/tursodatabase/turso-go\nExample usage:\nimport\n(\n\"database/sql\"\n_\n\"github.com/tursodatabase/turso-go\"\n)\nconn\n,\n_\n=\nsql\n.\nOpen\n(\n\"turso\"\n,\n\"sqlite.db\"\n)\ndefer\nconn\n.\nClose\n()\nstmt\n,\n_\n:=\nconn\n.\nPrepare\n(\n\"select * from users\"\n)\ndefer\nstmt\n.\nClose\n()\nrows\n,\n_\n=\nstmt\n.\nQuery\n()\nfor\nrows\n.\nNext\n() {\nvar\nid\nint\nvar\nusername\nstring\n_\n:=\nrows\n.\nScan\n(\n&\nid\n,\n&\nusername\n)\nfmt\n.\nPrintf\n(\n\"User: ID: %d, Username: %s\n\\n\n\"\n,\nid\n,\nusername\n)\n}\n☕️ Java\nWe integrated Turso Database into JDBC. For detailed instructions on how to use Turso Database with java, please refer to\nthe\nREADME.md under bindings/java\n.\n🤖 MCP Server Mode\nThe Turso CLI includes a built-in\nModel Context Protocol (MCP)\nserver that allows AI assistants to interact with your databases.\nStart the MCP server with:\ntursodb your_database.db --mcp\nConfiguration\nAdd Turso to your MCP client configuration:\n{\n\"mcpServers\"\n: {\n\"turso\"\n: {\n\"command\"\n:\n\"\n/path/to/.turso/tursodb\n\"\n,\n\"args\"\n: [\n\"\n/path/to/your/database.db\n\"\n,\n\"\n--mcp\n\"\n]\n    }\n  }\n}\nAvailable Tools\nThe MCP server provides nine tools for database interaction:\nopen_database\n- Open a new database\ncurrent_database\n- Describe the current database\nlist_tables\n- List all tables in the database\ndescribe_table\n- Describe the structure of a specific table\nexecute_query\n- Execute read-only SELECT queries\ninsert_data\n- Insert new data into tables\nupdate_data\n- Update existing data in tables\ndelete_data\n- Delete data from tables\nschema_change\n- Execute schema modification statements (CREATE TABLE, ALTER TABLE, DROP TABLE)\nOnce connected, you can ask your AI assistant:\n\"Show me all tables in the database\"\n\"What's the schema for the users table?\"\n\"Find all posts with more than 100 upvotes\"\n\"Insert a new user with name 'Alice' and email '\nalice@example.com\n'\"\nMCP Clients\nClaude Code\nIf you're using\nClaude Code\n, you can easily connect to your Turso MCP server using the built-in MCP management commands:\nQuick Setup\nAdd the MCP server\nto Claude Code:\nclaude mcp add my-database -- tursodb ./path/to/your/database.db --mcp\nRestart Claude Code\nto activate the connection\nStart querying\nyour database through natural language!\nCommand Breakdown\nclaude mcp add my-database -- tursodb ./path/to/your/database.db --mcp\n#\n↑            ↑       ↑                           ↑\n#\n|            |       |                           |\n#\nName         |       Database path               MCP flag\n#\nSeparator\nmy-database\n- Choose any name for your MCP server\n--\n- Required separator between Claude options and your command\ntursodb\n- The Turso database CLI\n./path/to/your/database.db\n- Path to your SQLite database file\n--mcp\n- Enables MCP server mode\nExample Usage\n#\nFor a local project database\ncd\n/your/project\nclaude mcp add my-project-db -- tursodb ./data/app.db --mcp\n#\nFor an absolute path\nclaude mcp add analytics-db -- tursodb /Users/you/databases/analytics.db --mcp\n#\nFor a specific project (local scope)\nclaude mcp add project-db --local -- tursodb ./database.db --mcp\nManaging MCP Servers\n#\nList all configured MCP servers\nclaude mcp list\n#\nGet details about a specific server\nclaude mcp get my-database\n#\nRemove an MCP server\nclaude mcp remove my-database\nClaude Desktop\nFor Claude Desktop, add the configuration to your\nclaude_desktop_config.json\nfile:\n{\n\"mcpServers\"\n: {\n\"turso\"\n: {\n\"command\"\n:\n\"\n/path/to/.turso/tursodb\n\"\n,\n\"args\"\n: [\n\"\n./path/to/your/database.db.db\n\"\n,\n\"\n--mcp\n\"\n]\n    }\n  }\n}\nCursor\nFor Cursor, configure MCP in your settings:\nOpen Cursor settings\nNavigate to Extensions → MCP\nAdd a new server with:\nName\n:\nturso\nCommand\n:\n/path/to/.turso/tursodb\nArgs\n:\n[\"./path/to/your/database.db.db\", \"--mcp\"]\nAlternatively, you can add it to your Cursor configuration file directly.\nDirect JSON-RPC Usage\nThe MCP server runs as a single process that handles multiple JSON-RPC requests over stdin/stdout. Here's how to interact with it directly:\nExample with In-Memory Database\ncat\n<<\n'\nEOF\n' | tursodb --mcp\n{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"initialize\", \"params\": {\"protocolVersion\": \"2024-11-05\", \"capabilities\": {}, \"clientInfo\": {\"name\": \"client\", \"version\": \"1.0\"}}}\n{\"jsonrpc\": \"2.0\", \"id\": 2, \"method\": \"tools/call\", \"params\": {\"name\": \"schema_change\", \"arguments\": {\"query\": \"CREATE TABLE users (id INTEGER, name TEXT, email TEXT)\"}}}\n{\"jsonrpc\": \"2.0\", \"id\": 3, \"method\": \"tools/call\", \"params\": {\"name\": \"list_tables\", \"arguments\": {}}}\n{\"jsonrpc\": \"2.0\", \"id\": 4, \"method\": \"tools/call\", \"params\": {\"name\": \"insert_data\", \"arguments\": {\"query\": \"INSERT INTO users VALUES (1, 'Alice', 'alice@example.com')\"}}}\n{\"jsonrpc\": \"2.0\", \"id\": 5, \"method\": \"tools/call\", \"params\": {\"name\": \"execute_query\", \"arguments\": {\"query\": \"SELECT * FROM users\"}}}\nEOF\nExample with Existing Database\n#\nWorking with an existing database file\ncat\n<<\n'\nEOF\n' | tursodb mydb.db --mcp\n{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"initialize\", \"params\": {\"protocolVersion\": \"2024-11-05\", \"capabilities\": {}, \"clientInfo\": {\"name\": \"client\", \"version\": \"1.0\"}}}\n{\"jsonrpc\": \"2.0\", \"id\": 2, \"method\": \"tools/call\", \"params\": {\"name\": \"list_tables\", \"arguments\": {}}}\nEOF\nContributing\nWe'd love to have you contribute to Turso Database! Please check out the\ncontribution guide\nto get started.\nFound a data corruption bug? Get up to $1,000.00\nSQLite is loved because it is the most reliable database in the world. The next evolution of SQLite has\nto match or surpass this level of reliability. Turso is built with\nDeterministic Simulation Testing\nfrom the ground up, and is also tested by\nAntithesis\n.\nEven during Alpha, if you find a bug that leads to a data corruption and demonstrate\nhow our simulator failed to catch it, you can get up to $1,000.00. As the project matures we will\nincrease the size of the prize, and the scope of the bugs.\nMore details\nhere\n.\nYou can see an example of an awarded case on\n#2049\n.\nTurso core staff are not eligible.\nFAQ\nIs Turso Database ready for production use?\nTurso Database is currently under heavy development and is\nnot\nready for production use.\nHow is Turso Database different from Turso's libSQL?\nTurso Database is a project to build the next evolution of SQLite in Rust, with a strong open contribution focus and features like native async support, vector search, and more. The libSQL project is also an attempt to evolve SQLite in a similar direction, but through a fork rather than a rewrite.\nRewriting SQLite in Rust started as an unassuming experiment, and due to its incredible success, replaces libSQL as our intended direction. At this point, libSQL is production ready, Turso Database is not - although it is evolving rapidly. More details\nhere\n.\nPublications\nPekka Enberg, Sasu Tarkoma, Jon Crowcroft Ashwin Rao (2024). Serverless Runtime / Database Co-Design With Asynchronous I/O. In\nEdgeSys ‘24\n.\n[PDF]\nPekka Enberg, Sasu Tarkoma, and Ashwin Rao (2023). Towards Database and Serverless Runtime Co-Design. In\nCoNEXT-SW ’23\n. [\nPDF\n] [\nSlides\n]\nLicense\nThis project is licensed under the\nMIT license\n.\nContribution\nUnless you explicitly state otherwise, any contribution intentionally submitted\nfor inclusion in Turso Database by you, shall be licensed as MIT, without any additional\nterms or conditions.\nPartners\nThanks to all the partners of Turso!\nContributors\nThanks to all the contributors to Turso Database!",
        "今日の獲得スター数: 70",
        "累積スター数: 14,012"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/tursodatabase/turso"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/simular-ai/Agent-S",
      "title": "simular-ai/Agent-S",
      "date": null,
      "executive_summary": [
        "Agent S: an open agentic framework that uses computers like a human",
        "---",
        "Agent S:\n  Use Computer Like a Human\n🌐\n[S3 blog]\n📄\n[S3 Paper]\n🎥\n[S3 Video]\n🌐\n[S2 blog]\n📄\n[S2 Paper (COLM 2025)]\n🎥\n[S2 Video]\n🌐\n[S1 blog]\n📄\n[S1 Paper (ICLR 2025)]\n🎥\n[S1 Video]\nDeutsch\n|\nEspañol\n|\nfrançais\n|\n日本語\n|\n한국어\n|\nPortuguês\n|\nРусский\n|\n中文\nSkip the setup? Try Agent S in\nSimular Cloud\n🥳 Updates\n2025/10/02\n: Released Agent S3 and its\ntechnical paper\n, setting a new SOTA of\n69.9%\non OSWorld (approaching 72% human performance), with strong generalizability on WindowsAgentArena and AndroidWorld! It is also simpler, faster, and more flexible.\n2025/08/01\n: Agent S2.5 is released (gui-agents v0.2.5): simpler, better, and faster! New SOTA on\nOSWorld-Verified\n!\n2025/07/07\n: The\nAgent S2 paper\nis accepted to COLM 2025! See you in Montreal!\n2025/04/27\n: The Agent S paper won the Best Paper Award 🏆 at ICLR 2025 Agentic AI for Science Workshop!\n2025/04/01\n: Released the\nAgent S2 paper\nwith new SOTA results on OSWorld, WindowsAgentArena, and AndroidWorld!\n2025/03/12\n: Released Agent S2 along with v0.2.0 of\ngui-agents\n, the new state-of-the-art for computer use agents (CUA), outperforming OpenAI's CUA/Operator and Anthropic's Claude 3.7 Sonnet Computer-Use!\n2025/01/22\n: The\nAgent S paper\nis accepted to ICLR 2025!\n2025/01/21\n: Released v0.1.2 of\ngui-agents\nlibrary, with support for Linux and Windows!\n2024/12/05\n: Released v0.1.0 of\ngui-agents\nlibrary, allowing you to use Agent-S for Mac, OSWorld, and WindowsAgentArena with ease!\n2024/10/10\n: Released the\nAgent S paper\nand codebase!\nTable of Contents\n💡 Introduction\n🎯 Current Results\n🛠️ Installation & Setup\n🚀 Usage\n🤝 Acknowledgements\n💬 Citation\n💡 Introduction\nWelcome to\nAgent S\n, an open-source framework designed to enable autonomous interaction with computers through Agent-Computer Interface. Our mission is to build intelligent GUI agents that can learn from past experiences and perform complex tasks autonomously on your computer.\nWhether you're interested in AI, automation, or contributing to cutting-edge agent-based systems, we're excited to have you here!\n🎯 Current Results\nOn OSWorld, Agent S3 alone reaches 62.6% in the 100-step setting, already exceeding the previous state of the art of 61.4% (Claude Sonnet 4.5). With the addition of Behavior Best-of-N, performance climbs even higher to 69.9%, bringing computer-use agents to within just a few points of human-level accuracy (72%).\nAgent S3 also demonstrates strong zero-shot generalization. On WindowsAgentArena, accuracy rises from 50.2% using only Agent S3 to 56.6% by selecting from 3 rollouts. Similarly on AndroidWorld, performance improves from 68.1% to 71.6%\n🛠️ Installation & Setup\nPrerequisites\nSingle Monitor\n: Our agent is designed for single monitor screens\nSecurity\n: The agent runs Python code to control your computer - use with care\nSupported Platforms\n: Linux, Mac, and Windows\nInstallation\nTo install Agent S3 without cloning the repository, run\npip install gui-agents\nIf you would like to test Agent S3 while making changes, clone the repository and install using\npip install -e .\nDon't forget to also\nbrew install tesseract\n! Pytesseract requires this extra installation to work.\nAPI Configuration\nOption 1: Environment Variables\nAdd to your\n.bashrc\n(Linux) or\n.zshrc\n(MacOS):\nexport\nOPENAI_API_KEY=\n<\nYOUR_API_KEY\n>\nexport\nANTHROPIC_API_KEY=\n<\nYOUR_ANTHROPIC_API_KEY\n>\nexport\nHF_TOKEN=\n<\nYOUR_HF_TOKEN\n>\nOption 2: Python Script\nimport\nos\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\n\"<YOUR_API_KEY>\"\nSupported Models\nWe support Azure OpenAI, Anthropic, Gemini, Open Router, and vLLM inference. See\nmodels.md\nfor details.\nGrounding Models (Required)\nFor optimal performance, we recommend\nUI-TARS-1.5-7B\nhosted on Hugging Face Inference Endpoints or another provider. See\nHugging Face Inference Endpoints\nfor setup instructions.\n🚀 Usage\n⚡️\nRecommended Setup:\nFor the best configuration, we recommend using\nOpenAI gpt-5-2025-08-07\nas the main model, paired with\nUI-TARS-1.5-7B\nfor grounding.\nCLI\nNote, this is running Agent S3, our improved agent, without bBoN.\nRun Agent S3 with the required parameters:\nagent_s \\\n    --provider openai \\\n    --model gpt-5-2025-08-07 \\\n    --ground_provider huggingface \\\n    --ground_url http://localhost:8080 \\\n    --ground_model ui-tars-1.5-7b \\\n    --grounding_width 1920 \\\n    --grounding_height 1080\nLocal Coding Environment (Optional)\nFor tasks that require code execution (e.g., data processing, file manipulation, system automation), you can enable the local coding environment:\nagent_s \\\n    --provider openai \\\n    --model gpt-5-2025-08-07 \\\n    --ground_provider huggingface \\\n    --ground_url http://localhost:8080 \\\n    --ground_model ui-tars-1.5-7b \\\n    --grounding_width 1920 \\\n    --grounding_height 1080 \\\n    --enable_local_env\n⚠️\nWARNING\n: The local coding environment executes arbitrary Python and Bash code locally on your machine. Only use this feature in trusted environments and with trusted inputs.\nRequired Parameters\n--provider\n: Main generation model provider (e.g., openai, anthropic, etc.) - Default: \"openai\"\n--model\n: Main generation model name (e.g., gpt-5-2025-08-07) - Default: \"gpt-5-2025-08-07\"\n--ground_provider\n: The provider for the grounding model -\nRequired\n--ground_url\n: The URL of the grounding model -\nRequired\n--ground_model\n: The model name for the grounding model -\nRequired\n--grounding_width\n: Width of the output coordinate resolution from the grounding model -\nRequired\n--grounding_height\n: Height of the output coordinate resolution from the grounding model -\nRequired\nOptional Parameters\n--model_temperature\n: The temperature to fix all model calls to (necessary to set to 1.0 for models like o3 but can be left blank for other models)\nGrounding Model Dimensions\nThe grounding width and height should match the output coordinate resolution of your grounding model:\nUI-TARS-1.5-7B\n: Use\n--grounding_width 1920 --grounding_height 1080\nUI-TARS-72B\n: Use\n--grounding_width 1000 --grounding_height 1000\nOptional Parameters\n--model_url\n: Custom API URL for main generation model - Default: \"\"\n--model_api_key\n: API key for main generation model - Default: \"\"\n--ground_api_key\n: API key for grounding model endpoint - Default: \"\"\n--max_trajectory_length\n: Maximum number of image turns to keep in trajectory - Default: 8\n--enable_reflection\n: Enable reflection agent to assist the worker agent - Default: True\n--enable_local_env\n: Enable local coding environment for code execution (WARNING: Executes arbitrary code locally) - Default: False\nLocal Coding Environment Details\nThe local coding environment enables Agent S3 to execute Python and Bash code directly on your machine. This is particularly useful for:\nData Processing\n: Manipulating spreadsheets, CSV files, or databases\nFile Operations\n: Bulk file processing, content extraction, or file organization\nSystem Automation\n: Configuration changes, system setup, or automation scripts\nCode Development\n: Writing, editing, or executing code files\nText Processing\n: Document manipulation, content editing, or formatting\nWhen enabled, the agent can use the\ncall_code_agent\naction to execute code blocks for tasks that can be completed through programming rather than GUI interaction.\nRequirements:\nPython\n: The same Python interpreter used to run Agent S3 (automatically detected)\nBash\n: Available at\n/bin/bash\n(standard on macOS and Linux)\nSystem Permissions\n: The agent runs with the same permissions as the user executing it\nSecurity Considerations:\nThe local environment executes arbitrary code with the same permissions as the user running the agent\nOnly enable this feature in trusted environments\nBe cautious when the agent generates code for system-level operations\nConsider running in a sandboxed environment for untrusted tasks\nBash scripts are executed with a 30-second timeout to prevent hanging processes\ngui_agents\nSDK\nFirst, we import the necessary modules.\nAgentS3\nis the main agent class for Agent S3.\nOSWorldACI\nis our grounding agent that translates agent actions into executable python code.\nimport\npyautogui\nimport\nio\nfrom\ngui_agents\n.\ns3\n.\nagents\n.\nagent_s\nimport\nAgentS3\nfrom\ngui_agents\n.\ns3\n.\nagents\n.\ngrounding\nimport\nOSWorldACI\nfrom\ngui_agents\n.\ns3\n.\nutils\n.\nlocal_env\nimport\nLocalEnv\n# Optional: for local coding environment\n# Load in your API keys.\nfrom\ndotenv\nimport\nload_dotenv\nload_dotenv\n()\ncurrent_platform\n=\n\"linux\"\n# \"darwin\", \"windows\"\nNext, we define our engine parameters.\nengine_params\nis used for the main agent, and\nengine_params_for_grounding\nis for grounding. For\nengine_params_for_grounding\n, we support custom endpoints like HuggingFace TGI, vLLM, and Open Router.\nengine_params\n=\n{\n\"engine_type\"\n:\nprovider\n,\n\"model\"\n:\nmodel\n,\n\"base_url\"\n:\nmodel_url\n,\n# Optional\n\"api_key\"\n:\nmodel_api_key\n,\n# Optional\n\"temperature\"\n:\nmodel_temperature\n# Optional\n}\n# Load the grounding engine from a custom endpoint\nground_provider\n=\n\"<your_ground_provider>\"\nground_url\n=\n\"<your_ground_url>\"\nground_model\n=\n\"<your_ground_model>\"\nground_api_key\n=\n\"<your_ground_api_key>\"\n# Set grounding dimensions based on your model's output coordinate resolution\n# UI-TARS-1.5-7B: grounding_width=1920, grounding_height=1080\n# UI-TARS-72B: grounding_width=1000, grounding_height=1000\ngrounding_width\n=\n1920\n# Width of output coordinate resolution\ngrounding_height\n=\n1080\n# Height of output coordinate resolution\nengine_params_for_grounding\n=\n{\n\"engine_type\"\n:\nground_provider\n,\n\"model\"\n:\nground_model\n,\n\"base_url\"\n:\nground_url\n,\n\"api_key\"\n:\nground_api_key\n,\n# Optional\n\"grounding_width\"\n:\ngrounding_width\n,\n\"grounding_height\"\n:\ngrounding_height\n,\n}\nThen, we define our grounding agent and Agent S3.\n# Optional: Enable local coding environment\nenable_local_env\n=\nFalse\n# Set to True to enable local code execution\nlocal_env\n=\nLocalEnv\n()\nif\nenable_local_env\nelse\nNone\ngrounding_agent\n=\nOSWorldACI\n(\nenv\n=\nlocal_env\n,\n# Pass local_env for code execution capability\nplatform\n=\ncurrent_platform\n,\nengine_params_for_generation\n=\nengine_params\n,\nengine_params_for_grounding\n=\nengine_params_for_grounding\n,\nwidth\n=\n1920\n,\n# Optional: screen width\nheight\n=\n1080\n# Optional: screen height\n)\nagent\n=\nAgentS3\n(\nengine_params\n,\ngrounding_agent\n,\nplatform\n=\ncurrent_platform\n,\nmax_trajectory_length\n=\n8\n,\n# Optional: maximum image turns to keep\nenable_reflection\n=\nTrue\n# Optional: enable reflection agent\n)\nFinally, let's query the agent!\n# Get screenshot.\nscreenshot\n=\npyautogui\n.\nscreenshot\n()\nbuffered\n=\nio\n.\nBytesIO\n()\nscreenshot\n.\nsave\n(\nbuffered\n,\nformat\n=\n\"PNG\"\n)\nscreenshot_bytes\n=\nbuffered\n.\ngetvalue\n()\nobs\n=\n{\n\"screenshot\"\n:\nscreenshot_bytes\n,\n}\ninstruction\n=\n\"Close VS Code\"\ninfo\n,\naction\n=\nagent\n.\npredict\n(\ninstruction\n=\ninstruction\n,\nobservation\n=\nobs\n)\nexec\n(\naction\n[\n0\n])\nRefer to\ngui_agents/s3/cli_app.py\nfor more details on how the inference loop works.\nOSWorld\nTo deploy Agent S3 in OSWorld, follow the\nOSWorld Deployment instructions\n.\n💬 Citations\nIf you find this codebase useful, please cite:\n@misc{Agent-S3,\n      title={The Unreasonable Effectiveness of Scaling Agents for Computer Use}, \n      author={Gonzalo Gonzalez-Pumariega and Vincent Tu and Chih-Lun Lee and Jiachen Yang and Ang Li and Xin Eric Wang},\n      year={2025},\n      eprint={2510.02250},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2510.02250}, \n}\n\n@misc{Agent-S2,\n      title={Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents}, \n      author={Saaket Agashe and Kyle Wong and Vincent Tu and Jiachen Yang and Ang Li and Xin Eric Wang},\n      year={2025},\n      eprint={2504.00906},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2504.00906}, \n}\n\n@inproceedings{Agent-S,\n    title={{Agent S: An Open Agentic Framework that Uses Computers Like a Human}},\n    author={Saaket Agashe and Jiuzhou Han and Shuyu Gan and Jiachen Yang and Ang Li and Xin Eric Wang},\n    booktitle={International Conference on Learning Representations (ICLR)},\n    year={2025},\n    url={https://arxiv.org/abs/2410.08164}\n}\nStar History",
        "今日の獲得スター数: 70",
        "累積スター数: 6,928"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/simular-ai/Agent-S"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/open-ani/animeko",
      "title": "open-ani/animeko",
      "date": null,
      "executive_summary": [
        "集找番、追番、看番的一站式弹幕追番平台，云收藏同步 (Bangumi)，离线缓存，BitTorrent，弹幕云过滤。100% Kotlin/Compose Multiplatform",
        "---",
        "正式版\n测试版\n讨论群\nAnimeko 支持云同步观看记录 (\nBangumi\n)、多视频数据源、缓存、弹幕、以及更多功能，提供尽可能简单且舒适的追番体验。\nAnimeko 曾用名 Ani，现在也简称 Ani。\n立即下载\nplay.mp4\n主要功能\n浏览来自\nBangumi\n的番剧信息以及社区评价\n丰富的检索方式：新番时间表、标签搜索\n由 Bangumi 和 Animeko 服务端共同提供的精确新番时间表\n云同步追番进度\n省心的追番进度管理，看完视频自动更新进度\n打开 APP 立即继续观看，无需回想上次看到了哪\n聚合数据源\n聚合视频数据源，全自动选择\n还支持 BitTorrent、Jellyfin、Emby、以及自定义源\n聚合全网弹幕源（\n弹弹play\n），以及 Animeko 自己的\n弹幕服务\n离线缓存\n所有数据源都能缓存\n精美界面\n适配平板和大屏设备\n更多个性设置\n下载\nAnimeko 支持所有主流平台：Android、iOS、Windows、macOS、Linux。\n稳定版本: 每两周更新, 功能稳定\n下载稳定版本\n通常建议使用稳定版本. 如果你愿意参与测试并拥有一定的对 bug 的处理能力, 也欢迎使用测试版本更快体验新功能.\n具体版本类型可查看下方.\n测试版本: 每两天更新, 体验最新功能\n下载测试版本\n点击查看具体版本类型\nAnimeko 采用语义化版本号, 简单来说就是\n4.x.y\n的格式. 有以下几种版本类型:\n稳定版本:\n新特性发布\n: 当\nx\n更新时, 会有新特性的发布. 通常为 2 周一次.\nBug 修复\n: 当\ny\n更新时, 只会有针对前个版本的重要的 bug 修复. 这些 Bug 修复版本穿插在新特性更新的间隔中,\n时间不固定.\n在稳定版本的发布周期之间, 会发布测试版本:\nAlpha 测试版\n: 所有重大新功能都会首先发布到\nalpha\n测试通道, 客户端内可使用 \"每日构建\"\n接收更新. 这些新功能非常不稳定, 适合热情的先锋测试员!\nBeta 测试版\n: 在功能经过 alpha 测试修复重大问题后, 会进入\nbeta\n测试通道,\n在客户端内名称为 \"测试版\". 此版本仍然不稳定, 是一个平衡新功能和稳定性的选择\n技术总览\n如果你是开发者，我们总是欢迎你提交 PR 参与开发！\n以下几点可以给你一个技术上的大概了解。\nKotlin 多平台\n架构；\n使用新一代响应式 UI 框架\nCompose Multiplatform\n构建\nUI；\n内置专为 Animeko 打造的“基于\nlibtorrent\n的 BitTorrent 引擎，优化边下边播的体验；\n高性能弹幕引擎，公益弹幕服务器 + 网络弹幕源；\n适配多平台的\n视频播放器\n，Android 底层为\nExoPlayer\n，PC 底层为\nVLC\n；\n多类型数据源适配，内置\n动漫花园\n、\nMikan\n，拥有强大的自定义数据源编辑器和自动数据源选择器。\n参与开发\n欢迎你提交 PR 参与开发，\n有关项目技术细节请参考\nCONTRIBUTING\n。\nFAQ\n资源来源是什么?\n全部视频数据都来自网络, Animeko 本身不存储任何视频数据。\nAnimeko 支持两大数据源类型：BT 和在线。BT 源即为公共 BitTorrent P2P 网络，\n每个在 BT\n网络上的人都可分享自己拥有的资源供他人下载。在线源即为其他视频资源网站分享的内容。Animeko\n本身并不提供任何视频资源。\n本着互助精神，使用 BT 源时 Animeko 会自动做种 (分享数据)。\nBT 指纹为\n-AL4123-\n，其中\n4123\n为版本号\n4.12.3\n；UA 为类似\nani_libtorrent/4.12.3\n。\n弹幕来源是什么?\nAnimeko 拥有自己的公益弹幕服务器，在 Animeko 应用内发送的弹幕将会发送到弹幕服务器上。每条弹幕都会以\nBangumi\n用户名绑定以防滥用（并考虑未来增加举报和屏蔽功能）。\nAnimeko 还会从\n弹弹play\n获取关联弹幕，弹弹play还会从其他弹幕平台例如哔哩哔哩港澳台和巴哈姆特获取弹幕。\n番剧每集可拥有几十到几千条不等的弹幕量。",
        "今日の獲得スター数: 68",
        "累積スター数: 11,874"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/open-ani/animeko"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/LadybirdBrowser/ladybird",
      "title": "LadybirdBrowser/ladybird",
      "date": null,
      "executive_summary": [
        "Truly independent web browser",
        "---",
        "Ladybird\nLadybird\nis a truly independent web browser, using a novel engine based on web standards.\nImportant\nLadybird is in a pre-alpha state, and only suitable for use by developers\nFeatures\nWe aim to build a complete, usable browser for the modern web.\nLadybird uses a multi-process architecture with a main UI process, several WebContent renderer processes,\nan ImageDecoder process, and a RequestServer process.\nImage decoding and network connections are done out of process to be more robust against malicious content.\nEach tab has its own renderer process, which is sandboxed from the rest of the system.\nAt the moment, many core library support components are inherited from SerenityOS:\nLibWeb: Web rendering engine\nLibJS: JavaScript engine\nLibWasm: WebAssembly implementation\nLibCrypto/LibTLS: Cryptography primitives and Transport Layer Security\nLibHTTP: HTTP/1.1 client\nLibGfx: 2D Graphics Library, Image Decoding and Rendering\nLibUnicode: Unicode and locale support\nLibMedia: Audio and video playback\nLibCore: Event loop, OS abstraction layer\nLibIPC: Inter-process communication\nHow do I build and run this?\nSee\nbuild instructions\nfor information on how to build Ladybird.\nLadybird runs on Linux, macOS, Windows (with WSL2), and many other *Nixes.\nHow do I read the documentation?\nCode-related documentation can be found in the\ndocumentation\nfolder.\nGet in touch and participate!\nJoin\nour Discord server\nto participate in development discussion.\nPlease read\nGetting started contributing\nif you plan to contribute to Ladybird for the first time.\nBefore opening an issue, please see the\nissue policy\nand the\ndetailed issue-reporting guidelines\n.\nThe full contribution guidelines can be found in\nCONTRIBUTING.md\n.\nLicense\nLadybird is licensed under a 2-clause BSD license.",
        "今日の獲得スター数: 61",
        "累積スター数: 49,502"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/LadybirdBrowser/ladybird"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/EFForg/rayhunter",
      "title": "EFForg/rayhunter",
      "date": null,
      "executive_summary": [
        "Rust tool to detect cell site simulators on an orbic mobile hotspot",
        "---",
        "Rayhunter\nRayhunter is a project for detecting IMSI catchers, also known as cell-site simulators or stingrays. It was first designed to run on a cheap mobile hotspot called the Orbic RC400L, but thanks to community efforts can\nsupport some other devices as well\n.\nIt's also designed to be as easy to install and use as possible, regardless of your level of technical skills, and to minimize false positives.\n→  Check out the\ninstallation guide\nto get started.\n→ To learn more about the aim of the project, and about IMSI catchers in general, please check out our\nintroductory blog post\n.\n→ For discussion, help, or to join the mattermost channel and get involved with the project and community check out the\nmany ways listed here\n!\n→ To learn more about the project in general check out the\nRayhunter Book\n.\nLEGAL DISCLAIMER:\nUse this program at your own risk. We believe running this program does not currently violate any laws or regulations in the United States. However, we are not responsible for civil or criminal liability resulting from the use of this software. If you are located outside of the US please consult with an attorney in your country to help you assess the legal risks of running this program.\nGood Hunting!",
        "今日の獲得スター数: 54",
        "累積スター数: 3,053"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/EFForg/rayhunter"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/ggml-org/llama.cpp",
      "title": "ggml-org/llama.cpp",
      "date": null,
      "executive_summary": [
        "LLM inference in C/C++",
        "---",
        "llama.cpp\nManifesto\n/\nggml\n/\nops\nLLM inference in C/C++\nRecent API changes\nChangelog for\nlibllama\nAPI\nChangelog for\nllama-server\nREST API\nHot topics\nguide : running gpt-oss with llama.cpp\n[FEEDBACK] Better packaging for llama.cpp to support downstream consumers 🤗\nSupport for the\ngpt-oss\nmodel with native MXFP4 format has been added |\nPR\n|\nCollaboration with NVIDIA\n|\nComment\nHot PRs:\nAll\n|\nOpen\nMultimodal support arrived in\nllama-server\n:\n#12898\n|\ndocumentation\nVS Code extension for FIM completions:\nhttps://github.com/ggml-org/llama.vscode\nVim/Neovim plugin for FIM completions:\nhttps://github.com/ggml-org/llama.vim\nIntroducing GGUF-my-LoRA\n#10123\nHugging Face Inference Endpoints now support GGUF out of the box!\n#9669\nHugging Face GGUF editor:\ndiscussion\n|\ntool\nQuick start\nGetting started with llama.cpp is straightforward. Here are several ways to install it on your machine:\nInstall\nllama.cpp\nusing\nbrew, nix or winget\nRun with Docker - see our\nDocker documentation\nDownload pre-built binaries from the\nreleases page\nBuild from source by cloning this repository - check out\nour build guide\nOnce installed, you'll need a model to work with. Head to the\nObtaining and quantizing models\nsection to learn more.\nExample command:\n#\nUse a local model file\nllama-cli -m my_model.gguf\n#\nOr download and run a model directly from Hugging Face\nllama-cli -hf ggml-org/gemma-3-1b-it-GGUF\n#\nLaunch OpenAI-compatible API server\nllama-server -hf ggml-org/gemma-3-1b-it-GGUF\nDescription\nThe main goal of\nllama.cpp\nis to enable LLM inference with minimal setup and state-of-the-art performance on a wide\nrange of hardware - locally and in the cloud.\nPlain C/C++ implementation without any dependencies\nApple silicon is a first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks\nAVX, AVX2, AVX512 and AMX support for x86 architectures\n1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit integer quantization for faster inference and reduced memory use\nCustom CUDA kernels for running LLMs on NVIDIA GPUs (support for AMD GPUs via HIP and Moore Threads GPUs via MUSA)\nVulkan and SYCL backend support\nCPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity\nThe\nllama.cpp\nproject is the main playground for developing new features for the\nggml\nlibrary.\nModels\nTypically finetunes of the base models below are supported as well.\nInstructions for adding support for new models:\nHOWTO-add-model.md\nText-only\nLLaMA 🦙\nLLaMA 2 🦙🦙\nLLaMA 3 🦙🦙🦙\nMistral 7B\nMixtral MoE\nDBRX\nFalcon\nChinese LLaMA / Alpaca\nand\nChinese LLaMA-2 / Alpaca-2\nVigogne (French)\nBERT\nKoala\nBaichuan 1 & 2\n+\nderivations\nAquila 1 & 2\nStarcoder models\nRefact\nMPT\nBloom\nYi models\nStableLM models\nDeepseek models\nQwen models\nPLaMo-13B\nPhi models\nPhiMoE\nGPT-2\nOrion 14B\nInternLM2\nCodeShell\nGemma\nMamba\nGrok-1\nXverse\nCommand-R models\nSEA-LION\nGritLM-7B\n+\nGritLM-8x7B\nOLMo\nOLMo 2\nOLMoE\nGranite models\nGPT-NeoX\n+\nPythia\nSnowflake-Arctic MoE\nSmaug\nPoro 34B\nBitnet b1.58 models\nFlan T5\nOpen Elm models\nChatGLM3-6b\n+\nChatGLM4-9b\n+\nGLMEdge-1.5b\n+\nGLMEdge-4b\nGLM-4-0414\nSmolLM\nEXAONE-3.0-7.8B-Instruct\nFalconMamba Models\nJais\nBielik-11B-v2.3\nRWKV-6\nQRWKV-6\nGigaChat-20B-A3B\nTrillion-7B-preview\nLing models\nLFM2 models\nHunyuan models\nMultimodal\nLLaVA 1.5 models\n,\nLLaVA 1.6 models\nBakLLaVA\nObsidian\nShareGPT4V\nMobileVLM 1.7B/3B models\nYi-VL\nMini CPM\nMoondream\nBunny\nGLM-EDGE\nQwen2-VL\nLFM2-VL\nBindings\nPython:\nddh0/easy-llama\nPython:\nabetlen/llama-cpp-python\nGo:\ngo-skynet/go-llama.cpp\nNode.js:\nwithcatai/node-llama-cpp\nJS/TS (llama.cpp server client):\nlgrammel/modelfusion\nJS/TS (Programmable Prompt Engine CLI):\noffline-ai/cli\nJavaScript/Wasm (works in browser):\ntangledgroup/llama-cpp-wasm\nTypescript/Wasm (nicer API, available on npm):\nngxson/wllama\nRuby:\nyoshoku/llama_cpp.rb\nRust (more features):\nedgenai/llama_cpp-rs\nRust (nicer API):\nmdrokz/rust-llama.cpp\nRust (more direct bindings):\nutilityai/llama-cpp-rs\nRust (automated build from crates.io):\nShelbyJenkins/llm_client\nC#/.NET:\nSciSharp/LLamaSharp\nC#/VB.NET (more features - community license):\nLM-Kit.NET\nScala 3:\ndonderom/llm4s\nClojure:\nphronmophobic/llama.clj\nReact Native:\nmybigday/llama.rn\nJava:\nkherud/java-llama.cpp\nJava:\nQuasarByte/llama-cpp-jna\nZig:\ndeins/llama.cpp.zig\nFlutter/Dart:\nnetdur/llama_cpp_dart\nFlutter:\nxuegao-tzx/Fllama\nPHP (API bindings and features built on top of llama.cpp):\ndistantmagic/resonance\n(more info)\nGuile Scheme:\nguile_llama_cpp\nSwift\nsrgtuszy/llama-cpp-swift\nSwift\nShenghaiWang/SwiftLlama\nDelphi\nEmbarcadero/llama-cpp-delphi\nUIs\n(to have a project listed here, it should clearly state that it depends on\nllama.cpp\n)\nAI Sublime Text plugin\n(MIT)\ncztomsik/ava\n(MIT)\nDot\n(GPL)\neva\n(MIT)\niohub/collama\n(Apache-2.0)\njanhq/jan\n(AGPL)\njohnbean393/Sidekick\n(MIT)\nKanTV\n(Apache-2.0)\nKodiBot\n(GPL)\nllama.vim\n(MIT)\nLARS\n(AGPL)\nLlama Assistant\n(GPL)\nLLMFarm\n(MIT)\nLLMUnity\n(MIT)\nLMStudio\n(proprietary)\nLocalAI\n(MIT)\nLostRuins/koboldcpp\n(AGPL)\nMindMac\n(proprietary)\nMindWorkAI/AI-Studio\n(FSL-1.1-MIT)\nMobile-Artificial-Intelligence/maid\n(MIT)\nMozilla-Ocho/llamafile\n(Apache-2.0)\nnat/openplayground\n(MIT)\nnomic-ai/gpt4all\n(MIT)\nollama/ollama\n(MIT)\noobabooga/text-generation-webui\n(AGPL)\nPocketPal AI\n(MIT)\npsugihara/FreeChat\n(MIT)\nptsochantaris/emeltal\n(MIT)\npythops/tenere\n(AGPL)\nramalama\n(MIT)\nsemperai/amica\n(MIT)\nwithcatai/catai\n(MIT)\nAutopen\n(GPL)\nTools\nakx/ggify\n– download PyTorch models from HuggingFace Hub and convert them to GGML\nakx/ollama-dl\n– download models from the Ollama library to be used directly with llama.cpp\ncrashr/gppm\n– launch llama.cpp instances utilizing NVIDIA Tesla P40 or P100 GPUs with reduced idle power consumption\ngpustack/gguf-parser\n- review/check the GGUF file and estimate the memory usage\nStyled Lines\n(proprietary licensed, async wrapper of inference part for game development in Unity3d with pre-built Mobile and Web platform wrappers and a model example)\nInfrastructure\nPaddler\n- Open-source LLMOps platform for hosting and scaling AI in your own infrastructure\nGPUStack\n- Manage GPU clusters for running LLMs\nllama_cpp_canister\n- llama.cpp as a smart contract on the Internet Computer, using WebAssembly\nllama-swap\n- transparent proxy that adds automatic model switching with llama-server\nKalavai\n- Crowdsource end to end LLM deployment at any scale\nllmaz\n- ☸️ Easy, advanced inference platform for large language models on Kubernetes.\nGames\nLucy's Labyrinth\n- A simple maze game where agents controlled by an AI model will try to trick you.\nSupported backends\nBackend\nTarget devices\nMetal\nApple Silicon\nBLAS\nAll\nBLIS\nAll\nSYCL\nIntel and Nvidia GPU\nMUSA\nMoore Threads GPU\nCUDA\nNvidia GPU\nHIP\nAMD GPU\nVulkan\nGPU\nCANN\nAscend NPU\nOpenCL\nAdreno GPU\nIBM zDNN\nIBM Z & LinuxONE\nWebGPU [In Progress]\nAll\nRPC\nAll\nObtaining and quantizing models\nThe\nHugging Face\nplatform hosts a\nnumber of LLMs\ncompatible with\nllama.cpp\n:\nTrending\nLLaMA\nYou can either manually download the GGUF file or directly use any\nllama.cpp\n-compatible models from\nHugging Face\nor other model hosting sites, such as\nModelScope\n, by using this CLI argument:\n-hf <user>/<model>[:quant]\n. For example:\nllama-cli -hf ggml-org/gemma-3-1b-it-GGUF\nBy default, the CLI would download from Hugging Face, you can switch to other options with the environment variable\nMODEL_ENDPOINT\n. For example, you may opt to downloading model checkpoints from ModelScope or other model sharing communities by setting the environment variable, e.g.\nMODEL_ENDPOINT=https://www.modelscope.cn/\n.\nAfter downloading a model, use the CLI tools to run it locally - see below.\nllama.cpp\nrequires the model to be stored in the\nGGUF\nfile format. Models in other data formats can be converted to GGUF using the\nconvert_*.py\nPython scripts in this repo.\nThe Hugging Face platform provides a variety of online tools for converting, quantizing and hosting models with\nllama.cpp\n:\nUse the\nGGUF-my-repo space\nto convert to GGUF format and quantize model weights to smaller sizes\nUse the\nGGUF-my-LoRA space\nto convert LoRA adapters to GGUF format (more info:\n#10123\n)\nUse the\nGGUF-editor space\nto edit GGUF meta data in the browser (more info:\n#9268\n)\nUse the\nInference Endpoints\nto directly host\nllama.cpp\nin the cloud (more info:\n#9669\n)\nTo learn more about model quantization,\nread this documentation\nllama-cli\nA CLI tool for accessing and experimenting with most of\nllama.cpp\n's functionality.\nRun in conversation mode\nModels with a built-in chat template will automatically activate conversation mode. If this doesn't occur, you can manually enable it by adding\n-cnv\nand specifying a suitable chat template with\n--chat-template NAME\nllama-cli -m model.gguf\n#\n> hi, who are you?\n#\nHi there! I'm your helpful assistant! I'm an AI-powered chatbot designed to assist and provide information to users like you. I'm here to help answer your questions, provide guidance, and offer support on a wide range of topics. I'm a friendly and knowledgeable AI, and I'm always happy to help with anything you need. What's on your mind, and how can I assist you today?\n#\n#\n> what is 1+1?\n#\nEasy peasy! The answer to 1+1 is... 2!\nRun in conversation mode with custom chat template\n#\nuse the \"chatml\" template (use -h to see the list of supported templates)\nllama-cli -m model.gguf -cnv --chat-template chatml\n#\nuse a custom template\nllama-cli -m model.gguf -cnv --in-prefix\n'\nUser:\n'\n--reverse-prompt\n'\nUser:\n'\nRun simple text completion\nTo disable conversation mode explicitly, use\n-no-cnv\nllama-cli -m model.gguf -p\n\"\nI believe the meaning of life is\n\"\n-n 128 -no-cnv\n#\nI believe the meaning of life is to find your own truth and to live in accordance with it. For me, this means being true to myself and following my passions, even if they don't align with societal expectations. I think that's what I love about yoga – it's not just a physical practice, but a spiritual one too. It's about connecting with yourself, listening to your inner voice, and honoring your own unique journey.\nConstrain the output with a custom grammar\nllama-cli -m model.gguf -n 256 --grammar-file grammars/json.gbnf -p\n'\nRequest: schedule a call at 8pm; Command:\n'\n#\n{\"appointmentTime\": \"8pm\", \"appointmentDetails\": \"schedule a a call\"}\nThe\ngrammars/\nfolder contains a handful of sample grammars. To write your own, check out the\nGBNF Guide\n.\nFor authoring more complex JSON grammars, check out\nhttps://grammar.intrinsiclabs.ai/\nllama-server\nA lightweight,\nOpenAI API\ncompatible, HTTP server for serving LLMs.\nStart a local HTTP server with default configuration on port 8080\nllama-server -m model.gguf --port 8080\n#\nBasic web UI can be accessed via browser: http://localhost:8080\n#\nChat completion endpoint: http://localhost:8080/v1/chat/completions\nSupport multiple-users and parallel decoding\n#\nup to 4 concurrent requests, each with 4096 max context\nllama-server -m model.gguf -c 16384 -np 4\nEnable speculative decoding\n#\nthe draft.gguf model should be a small variant of the target model.gguf\nllama-server -m model.gguf -md draft.gguf\nServe an embedding model\n#\nuse the /embedding endpoint\nllama-server -m model.gguf --embedding --pooling cls -ub 8192\nServe a reranking model\n#\nuse the /reranking endpoint\nllama-server -m model.gguf --reranking\nConstrain all outputs with a grammar\n#\ncustom grammar\nllama-server -m model.gguf --grammar-file grammar.gbnf\n#\nJSON\nllama-server -m model.gguf --grammar-file grammars/json.gbnf\nllama-perplexity\nA tool for measuring the\nperplexity\n1\n(and other quality metrics) of a model over a given text.\nMeasure the perplexity over a text file\nllama-perplexity -m model.gguf -f file.txt\n#\n[1]15.2701,[2]5.4007,[3]5.3073,[4]6.2965,[5]5.8940,[6]5.6096,[7]5.7942,[8]4.9297, ...\n#\nFinal estimate: PPL = 5.4007 +/- 0.67339\nMeasure KL divergence\n#\nTODO\nllama-bench\nBenchmark the performance of the inference for various parameters.\nRun default benchmark\nllama-bench -m model.gguf\n#\nOutput:\n#\n| model               |       size |     params | backend    | threads |          test |                  t/s |\n#\n| ------------------- | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\n#\n| qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         pp512 |      5765.41 ± 20.55 |\n#\n| qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         tg128 |        197.71 ± 0.81 |\n#\n#\nbuild: 3e0ba0e60 (4229)\nllama-run\nA comprehensive example for running\nllama.cpp\nmodels. Useful for inferencing. Used with RamaLama\n2\n.\nRun a model with a specific prompt (by default it's pulled from Ollama registry)\nllama-run granite-code\nllama-simple\nA minimal example for implementing apps with\nllama.cpp\n. Useful for developers.\nBasic text completion\nllama-simple -m model.gguf\n#\nHello my name is Kaitlyn and I am a 16 year old girl. I am a junior in high school and I am currently taking a class called \"The Art of\nContributing\nContributors can open PRs\nCollaborators will be invited based on contributions\nMaintainers can push to branches in the\nllama.cpp\nrepo and merge PRs into the\nmaster\nbranch\nAny help with managing issues, PRs and projects is very appreciated!\nSee\ngood first issues\nfor tasks suitable for first contributions\nRead the\nCONTRIBUTING.md\nfor more information\nMake sure to read this:\nInference at the edge\nA bit of backstory for those who are interested:\nChangelog podcast\nOther documentation\nmain (cli)\nserver\nGBNF grammars\nDevelopment documentation\nHow to build\nRunning on Docker\nBuild on Android\nPerformance troubleshooting\nGGML tips & tricks\nSeminal papers and background on the models\nIf your issue is with model generation quality, then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:\nLLaMA:\nIntroducing LLaMA: A foundational, 65-billion-parameter large language model\nLLaMA: Open and Efficient Foundation Language Models\nGPT-3\nLanguage Models are Few-Shot Learners\nGPT-3.5 / InstructGPT / ChatGPT:\nAligning language models to follow instructions\nTraining language models to follow instructions with human feedback\nXCFramework\nThe XCFramework is a precompiled version of the library for iOS, visionOS, tvOS,\nand macOS. It can be used in Swift projects without the need to compile the\nlibrary from source. For example:\n// swift-tools-version: 5.10\n// The swift-tools-version declares the minimum version of Swift required to build this package.\nimport\nPackageDescription\nlet\npackage\n=\nPackage\n(\nname\n:\n\"\nMyLlamaPackage\n\"\n,\ntargets\n:\n[\n.\nexecutableTarget\n(\nname\n:\n\"\nMyLlamaPackage\n\"\n,\ndependencies\n:\n[\n\"\nLlamaFramework\n\"\n]\n)\n,\n.\nbinaryTarget\n(\nname\n:\n\"\nLlamaFramework\n\"\n,\nurl\n:\n\"\nhttps://github.com/ggml-org/llama.cpp/releases/download/b5046/llama-b5046-xcframework.zip\n\"\n,\nchecksum\n:\n\"\nc19be78b5f00d8d29a25da41042cb7afa094cbf6280a225abe614b03b20029ab\n\"\n)\n]\n)\nThe above example is using an intermediate build\nb5046\nof the library. This can be modified\nto use a different version by changing the URL and checksum.\nCompletions\nCommand-line completion is available for some environments.\nBash Completion\n$ build/bin/llama-cli --completion-bash\n>\n~\n/.llama-completion.bash\n$\nsource\n~\n/.llama-completion.bash\nOptionally this can be added to your\n.bashrc\nor\n.bash_profile\nto load it\nautomatically. For example:\n$\necho\n\"\nsource ~/.llama-completion.bash\n\"\n>>\n~\n/.bashrc\nDependencies\nyhirose/cpp-httplib\n- Single-header HTTP server, used by\nllama-server\n- MIT license\nstb-image\n- Single-header image format decoder, used by multimodal subsystem - Public domain\nnlohmann/json\n- Single-header JSON library, used by various tools/examples - MIT License\nminja\n- Minimal Jinja parser in C++, used by various tools/examples - MIT License\nlinenoise.cpp\n- C++ library that provides readline-like line editing capabilities, used by\nllama-run\n- BSD 2-Clause License\ncurl\n- Client-side URL transfer library, used by various tools/examples -\nCURL License\nminiaudio.h\n- Single-header audio format decoder, used by multimodal subsystem - Public domain\nFootnotes\nhttps://huggingface.co/docs/transformers/perplexity\n↩\nRamaLama\n↩",
        "今日の獲得スター数: 49",
        "累積スター数: 87,367"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/ggml-org/llama.cpp"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/supermemoryai/supermemory",
      "title": "supermemoryai/supermemory",
      "date": null,
      "executive_summary": [
        "Memory engine and app that is extremely fast, scalable. The Memory API for the AI era.",
        "---",
        "Features\nCore Functionality\nAdd Memories from Any Content\n: Easily add memories from URLs, PDFs, and plain text—just paste, upload, or link.\nChat with Your Memories\n: Converse with your stored content using natural language chat.\nSupermemory MCP Integration\n: Seamlessly connect with all major AI tools (Claude, Cursor, etc.) via Supermemory MCP.\nHow do i use this?\nGo to\napp.supermemory.ai\nand sign into with your account\nStart Adding Memory with your choose of format (Note, Link, File)\nYou can also Connect to your favourite services (Notion, Google Drive, OneDrive)\nOnce Memories are added, you can chat with Supermemory by clicking on \"Open Chat\" and retrieve info from your saved memories\nAdd MCP to your AI Tools (by clicking on \"Connect to your AI\" and select the AI tool you are trying to integrate)\nSupport\nHave questions or feedback? We're here to help:\nEmail:\ndhravya@supermemory.com\nDocumentation:\ndocs.supermemory.ai\nContributing\nWe welcome contributions from developers of all skill levels! Whether you're fixing bugs, adding features, or improving documentation, your help makes supermemory better for everyone.\nQuick Start for Contributors\nFork and clone\nthe repository\nInstall dependencies\nwith\nbun install\nSet up your environment\nby copying\n.env.example\nto\n.env.local\nStart developing\nwith\nbun run dev\nFor detailed guidelines, development setup, coding standards, and the complete contribution workflow, please see our\nContributing Guide\n.\nWays to Contribute\n🐛\nBug fixes\n- Help us squash those pesky issues\n✨\nNew features\n- Add functionality that users will love\n🎨\nUI/UX improvements\n- Make the interface more intuitive\n⚡\nPerformance optimizations\n- Help us make supermemory faster\nCheck out our\nIssues\npage for\ngood first issue\nand\nhelp wanted\nlabels to get started!\nUpdates & Roadmap\nStay up to date with the latest improvements:\nChangelog\nX\n.",
        "今日の獲得スター数: 47",
        "累積スター数: 11,152"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/supermemoryai/supermemory"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/browserbase/stagehand",
      "title": "browserbase/stagehand",
      "date": null,
      "executive_summary": [
        "The AI Browser Automation Framework",
        "---",
        "The AI Browser Automation Framework\nRead the Docs\nIf you're looking for the Python implementation, you can find it\nhere\nVibe code\nStagehand with\nDirector\nWhy Stagehand?\nMost existing browser automation tools either require you to write low-level code in a framework like Selenium, Playwright, or Puppeteer, or use high-level agents that can be unpredictable in production. By letting developers choose what to write in code vs. natural language, Stagehand is the natural choice for browser automations in production.\nChoose when to write code vs. natural language\n: use AI when you want to navigate unfamiliar pages, and use code (\nPlaywright\n) when you know exactly what you want to do.\nPreview and cache actions\n: Stagehand lets you preview AI actions before running them, and also helps you easily cache repeatable actions to save time and tokens.\nComputer use models with one line of code\n: Stagehand lets you integrate SOTA computer use models from OpenAI and Anthropic into the browser with one line of code.\nExample\nHere's how to build a sample browser automation with Stagehand:\n// Use Playwright functions on the page object\nconst\npage\n=\nstagehand\n.\npage\n;\nawait\npage\n.\ngoto\n(\n\"https://github.com/browserbase\"\n)\n;\n// Use act() to execute individual actions\nawait\npage\n.\nact\n(\n\"click on the stagehand repo\"\n)\n;\n// Use Computer Use agents for larger actions\nconst\nagent\n=\nstagehand\n.\nagent\n(\n{\nprovider\n:\n\"openai\"\n,\nmodel\n:\n\"computer-use-preview\"\n,\n}\n)\n;\nawait\nagent\n.\nexecute\n(\n\"Get to the latest PR\"\n)\n;\n// Use extract() to read data from the page\nconst\n{\nauthor\n,\ntitle\n}\n=\nawait\npage\n.\nextract\n(\n{\ninstruction\n:\n\"extract the author and title of the PR\"\n,\nschema\n:\nz\n.\nobject\n(\n{\nauthor\n:\nz\n.\nstring\n(\n)\n.\ndescribe\n(\n\"The username of the PR author\"\n)\n,\ntitle\n:\nz\n.\nstring\n(\n)\n.\ndescribe\n(\n\"The title of the PR\"\n)\n,\n}\n)\n,\n}\n)\n;\nDocumentation\nVisit\ndocs.stagehand.dev\nto view the full documentation.\nGetting Started\nStart with Stagehand with one line of code, or check out our\nQuickstart Guide\nfor more information:\nnpx create-browser-app\nWatch Anirudh demo create-browser-app to create a Stagehand project!\nBuild and Run from Source\ngit clone https://github.com/browserbase/stagehand.git\ncd\nstagehand\npnpm install\npnpm playwright install\npnpm run build\npnpm run example\n#\nrun the blank script at ./examples/example.ts\npnpm run example 2048\n#\nrun the 2048 example at ./examples/2048.ts\npnpm run evals -man\n#\nsee evaluation suite options\nStagehand is best when you have an API key for an LLM provider and Browserbase credentials. To add these to your project, run:\ncp .env.example .env\nnano .env\n#\nEdit the .env file to add API keys\nContributing\nNote\nWe highly value contributions to Stagehand! For questions or support, please join our\nSlack community\n.\nAt a high level, we're focused on improving reliability, speed, and cost in that order of priority. If you're interested in contributing, we strongly recommend reaching out to\nMiguel Gonzalez\nor\nPaul Klein\nin our\nSlack community\nbefore starting to ensure that your contribution aligns with our goals.\nFor more information, please see our\nContributing Guide\n.\nAcknowledgements\nThis project heavily relies on\nPlaywright\nas a resilient backbone to automate the web. It also would not be possible without the awesome techniques and discoveries made by\ntarsier\n,\ngemini-zod\n, and\nfuji-web\n.\nWe'd like to thank the following people for their major contributions to Stagehand:\nPaul Klein\nAnirudh Kamath\nSean McGuire\nMiguel Gonzalez\nSameel Arif\nFilip Michalsky\nJeremy Press\nNavid Pour\nLicense\nLicensed under the MIT License.\nCopyright 2025 Browserbase, Inc.",
        "今日の獲得スター数: 46",
        "累積スター数: 17,563"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/browserbase/stagehand"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/oracle/graal",
      "title": "oracle/graal",
      "date": null,
      "executive_summary": [
        "GraalVM compiles applications into native executables that start instantly, scale fast, and use fewer compute resources 🚀",
        "---",
        "GraalVM is a high-performance JDK distribution that compiles your Java applications ahead of time into standalone binaries. These binaries start instantly, provide peak performance with no warmup, and use fewer resources.\nYou can use GraalVM just like any other Java Development Kit in your IDE.\nThe project website at\nhttps://www.graalvm.org/\ndescribes how to\nget started\n, how to\nstay connected\n, and how to\ncontribute\n.\nDocumentation\nPlease refer to the\nGraalVM website for documentation\n.\nYou can find most of the documentation sources in the\ndocs/\ndirectory in the same hierarchy as displayed on the website.\nAdditional documentation including developer instructions for individual components can be found in corresponding\ndocs/\nsub-directories.\nThe documentation for the Truffle framework, for example, is in\ntruffle/docs/\n.\nThis also applies to languages, tools, and other components maintained in\nrelated repositories\n.\nGet Support\nOpen a\nGitHub issue\nfor bug reports, questions, or requests for enhancements.\nJoin the\nGraalVM Slack\nto connect with the community and the GraalVM team.\nReport a security vulnerability according to the\nReporting Vulnerabilities guide\n.\nRepository Structure\nThis source repository is the main repository for GraalVM and includes the following components:\nDirectory\nDescription\n.devcontainer/\nConfiguration files for GitHub dev containers.\n.github/\nConfiguration files for GitHub issues, workflows, ….\ncompiler/\nGraal compiler\n, a modern, versatile compiler written in Java.\nespresso/\nEspresso\n, a meta-circular Java bytecode interpreter for the GraalVM.\nregex/\nTRegex, a regular expression engine for other GraalVM languages.\nsdk/\nGraalVM SDK\n, long-term supported APIs of GraalVM.\nsubstratevm/\nFramework for ahead-of-time (AOT) compilation with\nNative Image\n.\nsulong/\nSulong\n, an engine for running LLVM bitcode on GraalVM.\ntools/\nTools for GraalVM languages implemented with the instrumentation framework.\ntruffle/\nGraalVM's\nlanguage implementation framework\nfor creating languages and tools.\nvisualizer/\nIdeal Graph Visualizer (IGV)\n, a tool for analyzing Graal compiler graphs.\nvm/\nComponents for building GraalVM distributions.\nwasm/\nGraalWasm\n, an engine for running WebAssembly programs on GraalVM.\nRelated Repositories\nGraalVM provides additional languages, tools, and other components developed in related repositories. These are:\nName\nDescription\nFastR\nImplementation of the R language.\nGraalJS\nImplementation of JavaScript and Node.js.\nGraalPy\nImplementation of the Python language.\nNative Build Tools\nBuild tool plugins for GraalVM Native Image.\nSimpleLanguage\nA simple example language built with the Truffle framework.\nSimpleTool\nA simple example tool built with the Truffle framework.\nTruffleRuby\nImplementation of the Ruby language.\nExamples and Tutorials\nExplore practical examples, deep-dive workshops, and language-specific demos for working with GraalVM.\nName\nDescription\nGraalVM Demos\nExample applications highlighting GraalVM key features and best practices.\nGraalVM Workshops and Tutorials\nWorkshops and tutorials to help you learn and apply GraalVM tools and capabilities.\nGraal Languages - Demos and Guides\nDemo applications and guides for GraalJS, GraalPy, GraalWasm, and other Graal Languages.\nLicense\nGraalVM Community Edition is open source and distributed under\nversion 2 of the GNU General Public License with the “Classpath” Exception\n, which are the same terms as for Java. The licenses of the individual GraalVM components are generally derivative of the license of a particular language (see the table below).\nComponent(s)\nLicense\nEspresso\n,\nIdeal Graph Visualizer\nGPL 2\nGraalVM Compiler\n,\nSubstrateVM\n,\nTools\n,\nVM\nGPL 2 with Classpath Exception\nGraalVM SDK\n,\nGraalWasm\n,\nTruffle Framework\n,\nTRegex\nUniversal Permissive License\nSulong\n3-clause BSD",
        "今日の獲得スター数: 45",
        "累積スター数: 21,200"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/oracle/graal"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/qaiu/netdisk-fast-download",
      "title": "qaiu/netdisk-fast-download",
      "date": null,
      "executive_summary": [
        "各类网盘直链解析服务, 已支持蓝奏云/蓝奏优享/小飞机盘/123云盘/移动联通/天翼云等. 支持文件夹分享解析. 体验地址: https://lz.qaiu.top http://www.722shop.top:6401",
        "---",
        "netdisk-fast-download 网盘分享链接云解析服务\nQQ群：1017480890\nnetdisk-fast-download网盘直链云解析(nfd云解析)能把网盘分享下载链接转化为直链，支持多款云盘，已支持蓝奏云/蓝奏云优享/奶牛快传/移动云云空间/小飞机盘/亿方云/123云盘/Cloudreve等，支持加密分享，以及部分网盘文件夹分享。\n快速开始\n命令行下载分享文件：\ncurl -LOJ\n\"\nhttps://lz.qaiu.top/parser?url=https://share.feijipan.com/s/nQOaNRPW&pwd=1234\n\"\n或者使用wget:\nwget -O bilibili.mp4\n\"\nhttps://lz.qaiu.top/parser?url=https://share.feijipan.com/s/nQOaNRPW&pwd=1234\n\"\n或者使用浏览器\n直接访问\n:\n### 调用演示站下载：\nhttps://lz.qaiu.top/parser?url=https://share.feijipan.com/s/nQOaNRPW&pwd=1234  \n### 调用演示站预览：\nhttps://nfd-parser.github.io/nfd-preview/preview.html?src=https%3A%2F%2Flz.qaiu.top%2Fparser%3Furl%3Dhttps%3A%2F%2Fshare.feijipan.com%2Fs%2FnQOaNRPW&name=bilibili.mp4&ext=mp4\n预览地址\n预览地址1\n预览地址2\n天翼云盘大文件解析限时开放\nmain分支依赖JDK17, 提供了JDK11分支\nmain-jdk11\n0.1.8及以上版本json接口格式有调整 参考json返回数据格式示例\n小飞机解析有IP限制，多数云服务商的大陆IP会被拦截（可以自行配置代理），和本程序无关\n注意: 请不要过度依赖lz.qaiu.top预览地址服务，建议本地搭建或者云服务器自行搭建。解析次数过多IP会被部分网盘厂商限制，不推荐做公共解析。\n网盘支持情况:\n20230905 奶牛云直链做了防盗链，需加入请求头：Referer:\nhttps://cowtransfer.com/\n20230824 123云盘解析大文件(>100MB)失效，需要登录\n20230722 UC网盘解析失效，需要登录\n网盘名称-网盘标识:\n蓝奏云-lz\n蓝奏云优享-iz\n奶牛快传-cow\n移动云云空间-ec\n小飞机网盘-fj\n亿方云-fc\n123云盘-ye\n115网盘(失效)-p115\n118网盘(已停服)-p118\n文叔叔-ws\n联想乐云-le\nQQ邮箱云盘-qqw\nQQ闪传-qqsc\n城通网盘-ct\n网易云音乐分享链接-mnes\n酷狗音乐分享链接-mkgs\n酷我音乐分享链接-mkws\nQQ音乐分享链接-mqqs\n咪咕音乐分享链接(开发中)\nCloudreve自建网盘-ce\n微雨云存储-pvvy\n超星云盘(需要referer: https://pan-yz.chaoxing.com)-pcx\nGoogle云盘-pgd\nOnedrive-pod\nDropbox-pdp\niCloud-pic\n仅专属版提供\n移动云盘-p139\n联通云盘-pwo\n天翼云盘-p189\nAPI接口说明\nyour_host指的是您的域名或者IP，实际使用时替换为实际域名或者IP，端口默认6400，可以使用nginx代理来做域名访问。\n解析方式分为两种类型直接跳转下载文件和获取下载链接,\n每一种都提供了两种接口形式:\n通用接口parser?url=\n和\n网盘标志/分享key拼接的短地址（标志短链）\n，所有规则参考示例。\n通用接口:\n/parser?url=分享链接&pwd=密码\n没有分享密码去掉&pwd参数;\n标志短链:\n/d/网盘标识/分享key@密码\n没有分享密码去掉@密码;\n直链JSON:\n/json/网盘标识/分享key@密码\n和\n/json/parser?url=分享链接&pwd=密码\n网盘标识参考上面网盘支持情况\n当带有分享密码时需要加上密码参数(pwd)\n移动云云空间,小飞机网盘的加密分享的密码可以忽略\n移动云空间分享key取分享链接中的data参数,比如\n&data=xxx\n的参数就是xxx\nAPI规则:\n建议使用UrlEncode编码分享链接\n解析并自动302跳转\nhttp://your_host/parser?url=分享链接&pwd=xxx\nhttp://your_host/parser?url=UrlEncode(分享链接)&pwd=xxx\nhttp://your_host/d/网盘标识/分享key@分享密码\n获取解析后的直链--JSON格式\nhttp://your_host/json/parser?url=分享链接&pwd=xxx\nhttp://your_host/json/网盘标识/分享key@分享密码\n文件夹解析v0.1.8fixed3新增\nhttp://your_host/json/getFileList?url=分享链接&pwd=xxx\njson接口说明\n1. 文件解析：/json/parser?url=分享链接&pwd=xxx\njson返回数据格式示例:\nshareKey\n:    全局分享key\ndirectLink\n:  下载链接\ncacheHit\n:    是否为缓存链接\nexpires\n:     缓存到期时间\n{\n\"code\"\n:\n200\n,\n\"msg\"\n:\n\"\nsuccess\n\"\n,\n\"success\"\n:\ntrue\n,\n\"count\"\n:\n0\n,\n\"data\"\n: {\n\"shareKey\"\n:\n\"\nlz:xxx\n\"\n,\n\"directLink\"\n:\n\"\n下载直链\n\"\n,\n\"cacheHit\"\n:\ntrue\n,\n\"expires\"\n:\n\"\n2024-09-18 01:48:02\n\"\n,\n\"expiration\"\n:\n1726638482825\n},\n\"timestamp\"\n:\n1726637151902\n}\n2. 分享链接详情接口 /v2/linkInfo?url=分享链接\n{\n\"code\"\n:\n200\n,\n\"msg\"\n:\n\"\nsuccess\n\"\n,\n\"success\"\n:\ntrue\n,\n\"count\"\n:\n0\n,\n\"data\"\n: {\n\"downLink\"\n:\n\"\nhttps://lz.qaiu.top/d/fj/xx\n\"\n,\n\"apiLink\"\n:\n\"\nhttps://lz.qaiu.top/json/fj/xx\n\"\n,\n\"cacheHitTotal\"\n:\n5\n,\n\"parserTotal\"\n:\n2\n,\n\"sumTotal\"\n:\n7\n,\n\"shareLinkInfo\"\n: {\n\"shareKey\"\n:\n\"\nxx\n\"\n,\n\"panName\"\n:\n\"\n小飞机网盘\n\"\n,\n\"type\"\n:\n\"\nfj\n\"\n,\n\"sharePassword\"\n:\n\"\n\"\n,\n\"shareUrl\"\n:\n\"\nhttps://share.feijipan.com/s/xx\n\"\n,\n\"standardUrl\"\n:\n\"\nhttps://www.feijix.com/s/xx\n\"\n,\n\"otherParam\"\n: {\n\"UA\"\n:\n\"\nMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\n\"\n},\n\"cacheKey\"\n:\n\"\nfj:xx\n\"\n}\n    },\n\"timestamp\"\n:\n1736489219402\n}\n3. 文件夹解析(仅支持蓝奏云/蓝奏优享/小飞机网盘)\n/v2/getFileList?url=分享链接&pwd=分享密码\n{\n\"code\"\n:\n200\n,\n\"msg\"\n:\n\"\nsuccess\n\"\n,\n\"success\"\n:\ntrue\n,\n\"data\"\n: [\n    {\n\"fileName\"\n:\n\"\nxxx\n\"\n,\n\"fileId\"\n:\n\"\nxxx\n\"\n,\n\"fileIcon\"\n:\nnull\n,\n\"size\"\n:\n999\n,\n\"sizeStr\"\n:\n\"\n999 M\n\"\n,\n\"fileType\"\n:\n\"\nfile/folder\n\"\n,\n\"filePath\"\n:\nnull\n,\n\"createTime\"\n:\n\"\n17 小时前\n\"\n,\n\"updateTime\"\n:\nnull\n,\n\"createBy\"\n:\nnull\n,\n\"description\"\n:\nnull\n,\n\"downloadCount\"\n:\n\"\n下载次数\n\"\n,\n\"panType\"\n:\n\"\nlz\n\"\n,\n\"parserUrl\"\n:\n\"\n下载链接/文件夹链接\n\"\n,\n\"extParameters\"\n:\nnull\n}\n  ]\n}\n4. 解析次数统计接口 /v2/statisticsInfo\n{\n\"code\"\n:\n200\n,\n\"msg\"\n:\n\"\nsuccess\n\"\n,\n\"success\"\n:\ntrue\n,\n\"count\"\n:\n0\n,\n\"data\"\n: {\n\"parserTotal\"\n:\n320508\n,\n\"cacheTotal\"\n:\n5957910\n,\n\"total\"\n:\n6278418\n},\n\"timestamp\"\n:\n1736489378770\n}\nIDEA HttpClient示例:\n# 解析并重定向到直链\n### 蓝奏云普通分享\n# @no-redirect\nGET http://127.0.0.1:6400/parser?url=https://lanzoux.com/ia2cntg\n### 奶牛快传普通分享\n# @no-redirect\nGET http://127.0.0.1:6400/parser?url=https://cowtransfer.com/s/9a644fe3e3a748\n### 360亿方云加密分享\n# @no-redirect\nGET http://127.0.0.1:6400/parser?url=https://v2.fangcloud.com/sharing/e5079007dc31226096628870c7&pwd=QAIU\n\n# Rest请求自动302跳转(只提供共享文件Id):\n### 蓝奏云普通分享\n# @no-redirect\nGET http://127.0.0.1:6400/lz/ia2cntg\n### 奶牛快传普通分享\n# @no-redirect\nGET http://127.0.0.1:6400/cow/9a644fe3e3a748\n### 360亿方云加密分享\nGET http://127.0.0.1:6400/json/fc/e5079007dc31226096628870c7@QAIU\n\n\n# 解析返回json直链\n### 蓝奏云普通分享\nGET http://127.0.0.1:6400/json/lz/ia2cntg\n### 奶牛快传普通分享\nGET http://127.0.0.1:6400/json/cow/9a644fe3e3a748\n### 360亿方云加密分享\nGET http://127.0.0.1:6400/json/fc/e5079007dc31226096628870c7@QAIU\n网盘对比\n网盘名称\n免登陆下载分享\n加密分享\n初始网盘空间\n单文件大小限制\n蓝奏云\n√\n√\n不限空间\n100M\n奶牛快传\n√\nX\n10G\n不限大小\n移动云云空间(个人版)\n√\n√(密码可忽略)\n5G(个人)\n不限大小\n小飞机网盘\n√\n√(密码可忽略)\n10G\n不限大小\n360亿方云\n√\n√(密码可忽略)\n100G(须实名)\n不限大小\n123云盘\n√\n√\n2T\n100G（>100M需要登录）\n文叔叔\n√\n√\n10G\n5GB\n夸克网盘\nx\n√\n10G\n不限大小\nUC网盘\nx\n√\n10G\n不限大小\n打包部署\nJDK下载（lz.qaiu.top提供直链云解析服务）\n阿里jdk17(Dragonwell17-windows-x86)\n阿里jdk17(Dragonwell17-linux-x86)\n阿里jdk17(Dragonwell17-linux-aarch64)\n解析有效性测试-移动云云空间-阿里jdk17-linux-x86\n开发和打包\n#\n环境要求: Jdk17 + maven;\nmvn clean\nmvn package\n打包好的文件位于 web-service/target/netdisk-fast-download-bin.zip\nLinux服务部署\nDocker 部署（Main分支）\n海外服务器Docker部署\n#\n创建目录\nmkdir -p netdisk-fast-download\ncd\nnetdisk-fast-download\n#\n拉取镜像\ndocker pull ghcr.io/qaiu/netdisk-fast-download:latest\n#\n复制配置文件（或下载仓库web-service\\src\\main\\resources）\ndocker create --name netdisk-fast-download ghcr.io/qaiu/netdisk-fast-download:latest\ndocker cp netdisk-fast-download:/app/resources ./resources\ndocker rm netdisk-fast-download\n#\n启动容器\ndocker run -d -it --name netdisk-fast-download -p 6401:6401 --restart unless-stopped -e TZ=Asia/Shanghai -v ./resources:/app/resources -v ./db:/app/db -v ./logs:/app/logs ghcr.io/qaiu/netdisk-fast-download:latest\n#\n反代6401端口\n#\n升级容器\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --cleanup --run-once netdisk-fast-download\n国内Docker部署\n#\n创建目录\nmkdir -p netdisk-fast-download\ncd\nnetdisk-fast-download\n#\n拉取镜像\ndocker pull ghcr.nju.edu.cn/qaiu/netdisk-fast-download:latest\n#\n复制配置文件（或下载仓库web-service\\src\\main\\resources）\ndocker create --name netdisk-fast-download ghcr.nju.edu.cn/qaiu/netdisk-fast-download:latest\ndocker cp netdisk-fast-download:/app/resources ./resources\ndocker rm netdisk-fast-download\n#\n启动容器\ndocker run -d -it --name netdisk-fast-download -p 6401:6401 --restart unless-stopped -e TZ=Asia/Shanghai -v ./resources:/app/resources -v ./db:/app/db -v ./logs:/app/logs ghcr.nju.edu.cn/qaiu/netdisk-fast-download:latest\n#\n反代6401端口\n#\n升级容器\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --cleanup --run-once netdisk-fast-download\n宝塔部署指引 ->\n点击进入宝塔部署教程\nLinux命令行部署\n注意: netdisk-fast-download.service中的ExecStart的路径改为实际路径\ncd\n~\nwget -O netdisk-fast-download.zip  https://github.com/qaiu/netdisk-fast-download/releases/download/0.1.8-release-fixed2/netdisk-fast-download-bin-fixed2.zip\nunzip netdisk-fast-download-bin.zip\ncd\nnetdisk-fast-download\nbash service-install.sh\n服务相关命令:\n查看服务状态\nsystemctl status netdisk-fast-download.service\n启动服务\nsystemctl start netdisk-fast-download.service\n重启服务\nsystemctl restart netdisk-fast-download.service\n停止服务\nsystemctl stop netdisk-fast-download.service\n开机启动服务\nsystemctl enable netdisk-fast-download.service\n停止开机启动\nsystemctl disable netdisk-fast-download.service\nWindows服务部署\n下载并解压releases版本netdisk-fast-download-bin.zip\n进入netdisk-fast-download下的bin目录\n使用管理员权限运行nfd-service-install.bat\n如果不想使用服务运行可以直接运行run.bat\n注意: 如果jdk环境变量的java版本不是17请修改nfd-service-template.xml中的java命令的路径改为实际路径\n相关配置说明\nresources目录下包含服务端配置文件 配置文件自带说明，具体请查看配置文件内容，\napp-dev.yml 可以配置解析服务相关信息， 包括端口，域名，缓存时长等\nserver-proxy.yml 可以配置代理服务运行的相关信息， 包括前端反向代理端口，路径等\nip代理配置说明\n有时候解析量很大，IP容易被ban，这时候可以使用其他服务器搭建nfd-proxy代理服务。\n修改配置文件：\napp-dev.yml\nproxy\n:\n  -\npanTypes\n:\npgd,pdb,pod\n#\n网盘标识\ntype\n:\nhttp\n#\n支持http/socks4/socks5\nhost\n:\n127.0.0.1\n#\n代理IP\nport\n:\n7890\n#\n端口\nusername\n:\n#\n用户名\npassword\n:\n#\n密码\nnfd-proxy搭建http代理服务器\n参考\nhttps://github.com/nfd-parser/nfd-proxy\n0.1.9 开发计划\n目录解析(专属版)\n带cookie/token参数解析大文件(专属版)\n技术栈:\nJdk17+Vert.x4\nCore模块集成Vert.x实现类似spring的注解式路由API\nStar History\n免责声明\n用户在使用本项目时，应自行承担风险，并确保其行为符合当地法律法规及网盘服务提供商的使用条款。\n开发者不对用户因使用本项目而导致的任何后果负责，包括但不限于数据丢失、隐私泄露、账号封禁或其他任何形式的损害。\n支持该项目\n开源不易，用爱发电，本项目长期维护如果觉得有帮助, 可以请作者喝杯咖啡, 感谢支持\n关于专属版\n99元, 提供对小飞机,蓝奏优享大文件解析的支持, 提供天翼云盘,移动云盘,联调云盘的解析支持\n199元, 包含部署服务和首页定制, 需提供宝塔环境\n可以提供功能定制开发, 加v价格详谈:\nqq: 197575894\nwechat: imcoding_",
        "今日の獲得スター数: 41",
        "累積スター数: 2,220"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/qaiu/netdisk-fast-download"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/DioxusLabs/dioxus",
      "title": "DioxusLabs/dioxus",
      "date": null,
      "executive_summary": [
        "Fullstack app framework for web, desktop, and mobile.",
        "---",
        "Website\n|\nExamples\n|\nGuide\n|\n中文\n|\nPT-BR\n|\n日本語\n|\nTürkçe\n|\n한국어\n✨ Dioxus 0.7 is in alpha - test it out! ✨\nBuild for web, desktop, and mobile, and more with a single codebase. Zero-config setup, integrated hot-reloading, and signals-based state management. Add backend functionality with Server Functions and bundle with our CLI.\nfn\napp\n(\n)\n->\nElement\n{\nlet\nmut\ncount =\nuse_signal\n(\n||\n0\n)\n;\nrsx\n!\n{\nh1\n{\n\"High-Five counter: {count}\"\n}\nbutton\n{\nonclick\n:\nmove |_| count +=\n1\n,\n\"Up high!\"\n}\nbutton\n{\nonclick\n:\nmove |_| count -=\n1\n,\n\"Down low!\"\n}\n}\n}\n⭐️ Unique features:\nCross-platform apps in three lines of code (web, desktop, mobile, server, and more)\nErgonomic state management\ncombines the best of React, Solid, and Svelte\nBuilt-in featureful, type-safe, fullstack web framework\nIntegrated bundler for deploying to the web, macOS, Linux, and Windows\nSubsecond Rust hot-patching and asset hot-reloading\nAnd more!\nTake a tour of Dioxus\n.\nInstant hot-reloading\nWith one command,\ndx serve\nand your app is running. Edit your markup, styles, and see changes in milliseconds. Use our experimental\ndx serve --hotpatch\nto update Rust code in real time.\nBuild Beautiful Apps\nDioxus apps are styled with HTML and CSS. Use the built-in TailwindCSS support or load your favorite CSS library. Easily call into native code (objective-c, JNI, Web-Sys) for a perfect native touch.\nTruly fullstack applications\nDioxus deeply integrates with\naxum\nto provide powerful fullstack capabilities for both clients and servers. Pick from a wide array of built-in batteries like WebSockets, SSE, Streaming, File Upload/Download, Server-Side-Rendering, Forms, Middleware, and Hot-Reload, or go fully custom and integrate your existing axum backend.\nExperimental Native Renderer\nRender using web-sys, webview, server-side-rendering, liveview, or even with our experimental WGPU-based renderer. Embed Dioxus in Bevy, WGPU, or even run on embedded Linux!\nFirst-party primitive components\nGet started quickly with a complete set of primitives modeled after shadcn/ui and Radix-Primitives.\nFirst-class Android and iOS support\nDioxus is the fastest way to build native mobile apps with Rust. Simply run\ndx serve --platform android\nand your app is running in an emulator or on device in seconds. Call directly into JNI and Native APIs.\nBundle for web, desktop, and mobile\nSimply run\ndx bundle\nand your app will be built and bundled with maximization optimizations. On the web, take advantage of\n.avif\ngeneration,\n.wasm\ncompression, minification\n, and more. Build WebApps weighing\nless than 50kb\nand desktop/mobile apps less than 5mb.\nFantastic documentation\nWe've put a ton of effort into building clean, readable, and comprehensive documentation. All html elements and listeners are documented with MDN docs, and our Docs runs continuous integration with Dioxus itself to ensure that the docs are always up to date. Check out the\nDioxus website\nfor guides, references, recipes, and more. Fun fact: we use the Dioxus website as a testbed for new Dioxus features -\ncheck it out!\nModular and Customizable\nBuild your own renderer, or use a community renderer like\nFreya\n. Use our modular components like RSX, VirtualDom, Blitz, Taffy, and Subsecond.\nCommunity\nDioxus is a community-driven project, with a very active\nDiscord\nand\nGitHub\ncommunity. We're always looking for help, and we're happy to answer questions and help you get started.\nOur SDK\nis community-run and we even have a\nGitHub organization\nfor the best Dioxus crates that receive free upgrades and support.\nFull-time core team\nDioxus has grown from a side project to a small team of fulltime engineers. Thanks to the generous support of FutureWei, Satellite.im, the GitHub Accelerator program, we're able to work on Dioxus full-time. Our long term goal is for Dioxus to become self-sustaining by providing paid high-quality enterprise tools. If your company is interested in adopting Dioxus and would like to work with us, please reach out!\nSupported Platforms\nWeb\nRender directly to the DOM using WebAssembly\nPre-render with SSR and rehydrate on the client\nSimple \"hello world\" at about 50kb, comparable to React\nBuilt-in dev server and hot reloading for quick iteration\nDesktop\nRender using Webview or - experimentally - with WGPU or\nFreya\n(Skia)\nZero-config setup. Simply `cargo run` or `dx serve` to build your app\nFull support for native system access without IPC\nSupports macOS, Linux, and Windows. Portable <3mb binaries\nMobile\nRender using Webview or - experimentally - with WGPU or Skia\nBuild .ipa and .apk files for iOS and Android\nCall directly into Java and Objective-C with minimal overhead\nFrom \"hello world\" to running on device in seconds\nServer-side Rendering\nSuspense, hydration, and server-side rendering\nQuickly drop in backend functionality with server functions\nExtractors, middleware, and routing integrations\nStatic-site generation and incremental regeneration\nRunning the examples\nThe examples in the main branch of this repository target the git version of dioxus and the CLI. If you are looking for examples that work with the latest stable release of dioxus, check out the\n0.6 branch\n.\nThe examples in the top level of this repository can be run with:\ncargo run --example\n<\nexample\n>\nHowever, we encourage you to download the dioxus-cli to test out features like hot-reloading. To install the most recent binary CLI, you can use cargo binstall.\ncargo binstall dioxus-cli@0.7.0-rc.1 --force\nIf this CLI is out-of-date, you can install it directly from git\ncargo install --git https://github.com/DioxusLabs/dioxus dioxus-cli --locked\nWith the CLI, you can also run examples with the web platform. You will need to disable the default desktop feature and enable the web feature with this command:\ndx serve --example\n<\nexample\n>\n--platform web -- --no-default-features\nContributing\nCheck out the website\nsection on contributing\n.\nReport issues on our\nissue tracker\n.\nJoin\nthe discord and ask questions!\nLicense\nThis project is licensed under either the\nMIT license\nor the\nApache-2 License\n.\nUnless you explicitly state otherwise, any contribution intentionally submitted\nfor inclusion in Dioxus by you, shall be licensed as MIT or Apache-2, without any additional\nterms or conditions.",
        "今日の獲得スター数: 37",
        "累積スター数: 30,886"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/DioxusLabs/dioxus"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/modelcontextprotocol/python-sdk",
      "title": "modelcontextprotocol/python-sdk",
      "date": null,
      "executive_summary": [
        "The official Python SDK for Model Context Protocol servers and clients",
        "---",
        "MCP Python SDK\nPython implementation of the Model Context Protocol (MCP)\nTable of Contents\nMCP Python SDK\nOverview\nInstallation\nAdding MCP to your python project\nRunning the standalone MCP development tools\nQuickstart\nWhat is MCP?\nCore Concepts\nServer\nResources\nTools\nStructured Output\nPrompts\nImages\nContext\nGetting Context in Functions\nContext Properties and Methods\nCompletions\nElicitation\nSampling\nLogging and Notifications\nAuthentication\nFastMCP Properties\nSession Properties and Methods\nRequest Context Properties\nRunning Your Server\nDevelopment Mode\nClaude Desktop Integration\nDirect Execution\nStreamable HTTP Transport\nCORS Configuration for Browser-Based Clients\nMounting to an Existing ASGI Server\nStreamableHTTP servers\nBasic mounting\nHost-based routing\nMultiple servers with path configuration\nPath configuration at initialization\nSSE servers\nAdvanced Usage\nLow-Level Server\nStructured Output Support\nPagination (Advanced)\nWriting MCP Clients\nClient Display Utilities\nOAuth Authentication for Clients\nParsing Tool Results\nMCP Primitives\nServer Capabilities\nDocumentation\nContributing\nLicense\nOverview\nThe Model Context Protocol allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. This Python SDK implements the full MCP specification, making it easy to:\nBuild MCP clients that can connect to any MCP server\nCreate MCP servers that expose resources, prompts and tools\nUse standard transports like stdio, SSE, and Streamable HTTP\nHandle all MCP protocol messages and lifecycle events\nInstallation\nAdding MCP to your python project\nWe recommend using\nuv\nto manage your Python projects.\nIf you haven't created a uv-managed project yet, create one:\nuv init mcp-server-demo\ncd\nmcp-server-demo\nThen add MCP to your project dependencies:\nuv add\n\"\nmcp[cli]\n\"\nAlternatively, for projects using pip for dependencies:\npip install\n\"\nmcp[cli]\n\"\nRunning the standalone MCP development tools\nTo run the mcp command with uv:\nuv run mcp\nQuickstart\nLet's create a simple MCP server that exposes a calculator tool and some data:\n\"\"\"\nFastMCP quickstart example.\ncd to the `examples/snippets/clients` directory and run:\nuv run server fastmcp_quickstart stdio\n\"\"\"\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Create an MCP server\nmcp\n=\nFastMCP\n(\n\"Demo\"\n)\n# Add an addition tool\n@\nmcp\n.\ntool\n()\ndef\nadd\n(\na\n:\nint\n,\nb\n:\nint\n)\n->\nint\n:\n\"\"\"Add two numbers\"\"\"\nreturn\na\n+\nb\n# Add a dynamic greeting resource\n@\nmcp\n.\nresource\n(\n\"greeting://{name}\"\n)\ndef\nget_greeting\n(\nname\n:\nstr\n)\n->\nstr\n:\n\"\"\"Get a personalized greeting\"\"\"\nreturn\nf\"Hello,\n{\nname\n}\n!\"\n# Add a prompt\n@\nmcp\n.\nprompt\n()\ndef\ngreet_user\n(\nname\n:\nstr\n,\nstyle\n:\nstr\n=\n\"friendly\"\n)\n->\nstr\n:\n\"\"\"Generate a greeting prompt\"\"\"\nstyles\n=\n{\n\"friendly\"\n:\n\"Please write a warm, friendly greeting\"\n,\n\"formal\"\n:\n\"Please write a formal, professional greeting\"\n,\n\"casual\"\n:\n\"Please write a casual, relaxed greeting\"\n,\n    }\nreturn\nf\"\n{\nstyles\n.\nget\n(\nstyle\n,\nstyles\n[\n'friendly'\n])\n}\nfor someone named\n{\nname\n}\n.\"\nFull example:\nexamples/snippets/servers/fastmcp_quickstart.py\nYou can install this server in\nClaude Desktop\nand interact with it right away by running:\nuv run mcp install server.py\nAlternatively, you can test it with the MCP Inspector:\nuv run mcp dev server.py\nWhat is MCP?\nThe\nModel Context Protocol (MCP)\nlets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:\nExpose data through\nResources\n(think of these sort of like GET endpoints; they are used to load information into the LLM's context)\nProvide functionality through\nTools\n(sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)\nDefine interaction patterns through\nPrompts\n(reusable templates for LLM interactions)\nAnd more!\nCore Concepts\nServer\nThe FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:\n\"\"\"Example showing lifespan support for startup/shutdown with strong typing.\"\"\"\nfrom\ncollections\n.\nabc\nimport\nAsyncIterator\nfrom\ncontextlib\nimport\nasynccontextmanager\nfrom\ndataclasses\nimport\ndataclass\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nContext\n,\nFastMCP\nfrom\nmcp\n.\nserver\n.\nsession\nimport\nServerSession\n# Mock database class for example\nclass\nDatabase\n:\n\"\"\"Mock database class for example.\"\"\"\n@\nclassmethod\nasync\ndef\nconnect\n(\ncls\n)\n->\n\"Database\"\n:\n\"\"\"Connect to database.\"\"\"\nreturn\ncls\n()\nasync\ndef\ndisconnect\n(\nself\n)\n->\nNone\n:\n\"\"\"Disconnect from database.\"\"\"\npass\ndef\nquery\n(\nself\n)\n->\nstr\n:\n\"\"\"Execute a query.\"\"\"\nreturn\n\"Query result\"\n@\ndataclass\nclass\nAppContext\n:\n\"\"\"Application context with typed dependencies.\"\"\"\ndb\n:\nDatabase\n@\nasynccontextmanager\nasync\ndef\napp_lifespan\n(\nserver\n:\nFastMCP\n)\n->\nAsyncIterator\n[\nAppContext\n]:\n\"\"\"Manage application lifecycle with type-safe context.\"\"\"\n# Initialize on startup\ndb\n=\nawait\nDatabase\n.\nconnect\n()\ntry\n:\nyield\nAppContext\n(\ndb\n=\ndb\n)\nfinally\n:\n# Cleanup on shutdown\nawait\ndb\n.\ndisconnect\n()\n# Pass lifespan to server\nmcp\n=\nFastMCP\n(\n\"My App\"\n,\nlifespan\n=\napp_lifespan\n)\n# Access type-safe lifespan context in tools\n@\nmcp\n.\ntool\n()\ndef\nquery_db\n(\nctx\n:\nContext\n[\nServerSession\n,\nAppContext\n])\n->\nstr\n:\n\"\"\"Tool that uses initialized resources.\"\"\"\ndb\n=\nctx\n.\nrequest_context\n.\nlifespan_context\n.\ndb\nreturn\ndb\n.\nquery\n()\nFull example:\nexamples/snippets/servers/lifespan_example.py\nResources\nResources are how you expose data to LLMs. They're similar to GET endpoints in a REST API - they provide data but shouldn't perform significant computation or have side effects:\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\nmcp\n=\nFastMCP\n(\nname\n=\n\"Resource Example\"\n)\n@\nmcp\n.\nresource\n(\n\"file://documents/{name}\"\n)\ndef\nread_document\n(\nname\n:\nstr\n)\n->\nstr\n:\n\"\"\"Read a document by name.\"\"\"\n# This would normally read from disk\nreturn\nf\"Content of\n{\nname\n}\n\"\n@\nmcp\n.\nresource\n(\n\"config://settings\"\n)\ndef\nget_settings\n()\n->\nstr\n:\n\"\"\"Get application settings.\"\"\"\nreturn\n\"\"\"{\n\"theme\": \"dark\",\n\"language\": \"en\",\n\"debug\": false\n}\"\"\"\nFull example:\nexamples/snippets/servers/basic_resource.py\nTools\nTools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\nmcp\n=\nFastMCP\n(\nname\n=\n\"Tool Example\"\n)\n@\nmcp\n.\ntool\n()\ndef\nsum\n(\na\n:\nint\n,\nb\n:\nint\n)\n->\nint\n:\n\"\"\"Add two numbers together.\"\"\"\nreturn\na\n+\nb\n@\nmcp\n.\ntool\n()\ndef\nget_weather\n(\ncity\n:\nstr\n,\nunit\n:\nstr\n=\n\"celsius\"\n)\n->\nstr\n:\n\"\"\"Get weather for a city.\"\"\"\n# This would normally call a weather API\nreturn\nf\"Weather in\n{\ncity\n}\n: 22degrees\n{\nunit\n[\n0\n].\nupper\n()\n}\n\"\nFull example:\nexamples/snippets/servers/basic_tool.py\nTools can optionally receive a Context object by including a parameter with the\nContext\ntype annotation. This context is automatically injected by the FastMCP framework and provides access to MCP capabilities:\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nContext\n,\nFastMCP\nfrom\nmcp\n.\nserver\n.\nsession\nimport\nServerSession\nmcp\n=\nFastMCP\n(\nname\n=\n\"Progress Example\"\n)\n@\nmcp\n.\ntool\n()\nasync\ndef\nlong_running_task\n(\ntask_name\n:\nstr\n,\nctx\n:\nContext\n[\nServerSession\n,\nNone\n],\nsteps\n:\nint\n=\n5\n)\n->\nstr\n:\n\"\"\"Execute a task with progress updates.\"\"\"\nawait\nctx\n.\ninfo\n(\nf\"Starting:\n{\ntask_name\n}\n\"\n)\nfor\ni\nin\nrange\n(\nsteps\n):\nprogress\n=\n(\ni\n+\n1\n)\n/\nsteps\nawait\nctx\n.\nreport_progress\n(\nprogress\n=\nprogress\n,\ntotal\n=\n1.0\n,\nmessage\n=\nf\"Step\n{\ni\n+\n1\n}\n/\n{\nsteps\n}\n\"\n,\n        )\nawait\nctx\n.\ndebug\n(\nf\"Completed step\n{\ni\n+\n1\n}\n\"\n)\nreturn\nf\"Task '\n{\ntask_name\n}\n' completed\"\nFull example:\nexamples/snippets/servers/tool_progress.py\nStructured Output\nTools will return structured results by default, if their return type\nannotation is compatible. Otherwise, they will return unstructured results.\nStructured output supports these return types:\nPydantic models (BaseModel subclasses)\nTypedDicts\nDataclasses and other classes with type hints\ndict[str, T]\n(where T is any JSON-serializable type)\nPrimitive types (str, int, float, bool, bytes, None) - wrapped in\n{\"result\": value}\nGeneric types (list, tuple, Union, Optional, etc.) - wrapped in\n{\"result\": value}\nClasses without type hints cannot be serialized for structured output. Only\nclasses with properly annotated attributes will be converted to Pydantic models\nfor schema generation and validation.\nStructured results are automatically validated against the output schema\ngenerated from the annotation. This ensures the tool returns well-typed,\nvalidated data that clients can easily process.\nNote:\nFor backward compatibility, unstructured results are also\nreturned. Unstructured results are provided for backward compatibility\nwith previous versions of the MCP specification, and are quirks-compatible\nwith previous versions of FastMCP in the current version of the SDK.\nNote:\nIn cases where a tool function's return type annotation\ncauses the tool to be classified as structured\nand this is undesirable\n,\nthe  classification can be suppressed by passing\nstructured_output=False\nto the\n@tool\ndecorator.\n\"\"\"Example showing structured output with tools.\"\"\"\nfrom\ntyping\nimport\nTypedDict\nfrom\npydantic\nimport\nBaseModel\n,\nField\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\nmcp\n=\nFastMCP\n(\n\"Structured Output Example\"\n)\n# Using Pydantic models for rich structured data\nclass\nWeatherData\n(\nBaseModel\n):\n\"\"\"Weather information structure.\"\"\"\ntemperature\n:\nfloat\n=\nField\n(\ndescription\n=\n\"Temperature in Celsius\"\n)\nhumidity\n:\nfloat\n=\nField\n(\ndescription\n=\n\"Humidity percentage\"\n)\ncondition\n:\nstr\nwind_speed\n:\nfloat\n@\nmcp\n.\ntool\n()\ndef\nget_weather\n(\ncity\n:\nstr\n)\n->\nWeatherData\n:\n\"\"\"Get weather for a city - returns structured data.\"\"\"\n# Simulated weather data\nreturn\nWeatherData\n(\ntemperature\n=\n22.5\n,\nhumidity\n=\n45.0\n,\ncondition\n=\n\"sunny\"\n,\nwind_speed\n=\n5.2\n,\n    )\n# Using TypedDict for simpler structures\nclass\nLocationInfo\n(\nTypedDict\n):\nlatitude\n:\nfloat\nlongitude\n:\nfloat\nname\n:\nstr\n@\nmcp\n.\ntool\n()\ndef\nget_location\n(\naddress\n:\nstr\n)\n->\nLocationInfo\n:\n\"\"\"Get location coordinates\"\"\"\nreturn\nLocationInfo\n(\nlatitude\n=\n51.5074\n,\nlongitude\n=\n-\n0.1278\n,\nname\n=\n\"London, UK\"\n)\n# Using dict[str, Any] for flexible schemas\n@\nmcp\n.\ntool\n()\ndef\nget_statistics\n(\ndata_type\n:\nstr\n)\n->\ndict\n[\nstr\n,\nfloat\n]:\n\"\"\"Get various statistics\"\"\"\nreturn\n{\n\"mean\"\n:\n42.5\n,\n\"median\"\n:\n40.0\n,\n\"std_dev\"\n:\n5.2\n}\n# Ordinary classes with type hints work for structured output\nclass\nUserProfile\n:\nname\n:\nstr\nage\n:\nint\nemail\n:\nstr\n|\nNone\n=\nNone\ndef\n__init__\n(\nself\n,\nname\n:\nstr\n,\nage\n:\nint\n,\nemail\n:\nstr\n|\nNone\n=\nNone\n):\nself\n.\nname\n=\nname\nself\n.\nage\n=\nage\nself\n.\nemail\n=\nemail\n@\nmcp\n.\ntool\n()\ndef\nget_user\n(\nuser_id\n:\nstr\n)\n->\nUserProfile\n:\n\"\"\"Get user profile - returns structured data\"\"\"\nreturn\nUserProfile\n(\nname\n=\n\"Alice\"\n,\nage\n=\n30\n,\nemail\n=\n\"alice@example.com\"\n)\n# Classes WITHOUT type hints cannot be used for structured output\nclass\nUntypedConfig\n:\ndef\n__init__\n(\nself\n,\nsetting1\n,\nsetting2\n):\n# type: ignore[reportMissingParameterType]\nself\n.\nsetting1\n=\nsetting1\nself\n.\nsetting2\n=\nsetting2\n@\nmcp\n.\ntool\n()\ndef\nget_config\n()\n->\nUntypedConfig\n:\n\"\"\"This returns unstructured output - no schema generated\"\"\"\nreturn\nUntypedConfig\n(\n\"value1\"\n,\n\"value2\"\n)\n# Lists and other types are wrapped automatically\n@\nmcp\n.\ntool\n()\ndef\nlist_cities\n()\n->\nlist\n[\nstr\n]:\n\"\"\"Get a list of cities\"\"\"\nreturn\n[\n\"London\"\n,\n\"Paris\"\n,\n\"Tokyo\"\n]\n# Returns: {\"result\": [\"London\", \"Paris\", \"Tokyo\"]}\n@\nmcp\n.\ntool\n()\ndef\nget_temperature\n(\ncity\n:\nstr\n)\n->\nfloat\n:\n\"\"\"Get temperature as a simple float\"\"\"\nreturn\n22.5\n# Returns: {\"result\": 22.5}\nFull example:\nexamples/snippets/servers/structured_output.py\nPrompts\nPrompts are reusable templates that help LLMs interact with your server effectively:\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\nfrom\nmcp\n.\nserver\n.\nfastmcp\n.\nprompts\nimport\nbase\nmcp\n=\nFastMCP\n(\nname\n=\n\"Prompt Example\"\n)\n@\nmcp\n.\nprompt\n(\ntitle\n=\n\"Code Review\"\n)\ndef\nreview_code\n(\ncode\n:\nstr\n)\n->\nstr\n:\nreturn\nf\"Please review this code:\n\\n\n\\n\n{\ncode\n}\n\"\n@\nmcp\n.\nprompt\n(\ntitle\n=\n\"Debug Assistant\"\n)\ndef\ndebug_error\n(\nerror\n:\nstr\n)\n->\nlist\n[\nbase\n.\nMessage\n]:\nreturn\n[\nbase\n.\nUserMessage\n(\n\"I'm seeing this error:\"\n),\nbase\n.\nUserMessage\n(\nerror\n),\nbase\n.\nAssistantMessage\n(\n\"I'll help debug that. What have you tried so far?\"\n),\n    ]\nFull example:\nexamples/snippets/servers/basic_prompt.py\nIcons\nMCP servers can provide icons for UI display. Icons can be added to the server implementation, tools, resources, and prompts:\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n,\nIcon\n# Create an icon from a file path or URL\nicon\n=\nIcon\n(\nsrc\n=\n\"icon.png\"\n,\nmimeType\n=\n\"image/png\"\n,\nsizes\n=\n\"64x64\"\n)\n# Add icons to server\nmcp\n=\nFastMCP\n(\n\"My Server\"\n,\nwebsite_url\n=\n\"https://example.com\"\n,\nicons\n=\n[\nicon\n]\n)\n# Add icons to tools, resources, and prompts\n@\nmcp\n.\ntool\n(\nicons\n=\n[\nicon\n])\ndef\nmy_tool\n():\n\"\"\"Tool with an icon.\"\"\"\nreturn\n\"result\"\n@\nmcp\n.\nresource\n(\n\"demo://resource\"\n,\nicons\n=\n[\nicon\n])\ndef\nmy_resource\n():\n\"\"\"Resource with an icon.\"\"\"\nreturn\n\"content\"\nFull example:\nexamples/fastmcp/icons_demo.py\nImages\nFastMCP provides an\nImage\nclass that automatically handles image data:\n\"\"\"Example showing image handling with FastMCP.\"\"\"\nfrom\nPIL\nimport\nImage\nas\nPILImage\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n,\nImage\nmcp\n=\nFastMCP\n(\n\"Image Example\"\n)\n@\nmcp\n.\ntool\n()\ndef\ncreate_thumbnail\n(\nimage_path\n:\nstr\n)\n->\nImage\n:\n\"\"\"Create a thumbnail from an image\"\"\"\nimg\n=\nPILImage\n.\nopen\n(\nimage_path\n)\nimg\n.\nthumbnail\n((\n100\n,\n100\n))\nreturn\nImage\n(\ndata\n=\nimg\n.\ntobytes\n(),\nformat\n=\n\"png\"\n)\nFull example:\nexamples/snippets/servers/images.py\nContext\nThe Context object is automatically injected into tool and resource functions that request it via type hints. It provides access to MCP capabilities like logging, progress reporting, resource reading, user interaction, and request metadata.\nGetting Context in Functions\nTo use context in a tool or resource function, add a parameter with the\nContext\ntype annotation:\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nContext\n,\nFastMCP\nmcp\n=\nFastMCP\n(\nname\n=\n\"Context Example\"\n)\n@\nmcp\n.\ntool\n()\nasync\ndef\nmy_tool\n(\nx\n:\nint\n,\nctx\n:\nContext\n)\n->\nstr\n:\n\"\"\"Tool that uses context capabilities.\"\"\"\n# The context parameter can have any name as long as it's type-annotated\nreturn\nawait\nprocess_with_context\n(\nx\n,\nctx\n)\nContext Properties and Methods\nThe Context object provides the following capabilities:\nctx.request_id\n- Unique ID for the current request\nctx.client_id\n- Client ID if available\nctx.fastmcp\n- Access to the FastMCP server instance (see\nFastMCP Properties\n)\nctx.session\n- Access to the underlying session for advanced communication (see\nSession Properties and Methods\n)\nctx.request_context\n- Access to request-specific data and lifespan resources (see\nRequest Context Properties\n)\nawait ctx.debug(message)\n- Send debug log message\nawait ctx.info(message)\n- Send info log message\nawait ctx.warning(message)\n- Send warning log message\nawait ctx.error(message)\n- Send error log message\nawait ctx.log(level, message, logger_name=None)\n- Send log with custom level\nawait ctx.report_progress(progress, total=None, message=None)\n- Report operation progress\nawait ctx.read_resource(uri)\n- Read a resource by URI\nawait ctx.elicit(message, schema)\n- Request additional information from user with validation\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nContext\n,\nFastMCP\nfrom\nmcp\n.\nserver\n.\nsession\nimport\nServerSession\nmcp\n=\nFastMCP\n(\nname\n=\n\"Progress Example\"\n)\n@\nmcp\n.\ntool\n()\nasync\ndef\nlong_running_task\n(\ntask_name\n:\nstr\n,\nctx\n:\nContext\n[\nServerSession\n,\nNone\n],\nsteps\n:\nint\n=\n5\n)\n->\nstr\n:\n\"\"\"Execute a task with progress updates.\"\"\"\nawait\nctx\n.\ninfo\n(\nf\"Starting:\n{\ntask_name\n}\n\"\n)\nfor\ni\nin\nrange\n(\nsteps\n):\nprogress\n=\n(\ni\n+\n1\n)\n/\nsteps\nawait\nctx\n.\nreport_progress\n(\nprogress\n=\nprogress\n,\ntotal\n=\n1.0\n,\nmessage\n=\nf\"Step\n{\ni\n+\n1\n}\n/\n{\nsteps\n}\n\"\n,\n        )\nawait\nctx\n.\ndebug\n(\nf\"Completed step\n{\ni\n+\n1\n}\n\"\n)\nreturn\nf\"Task '\n{\ntask_name\n}\n' completed\"\nFull example:\nexamples/snippets/servers/tool_progress.py\nCompletions\nMCP supports providing completion suggestions for prompt arguments and resource template parameters. With the context parameter, servers can provide completions based on previously resolved values:\nClient usage:\n\"\"\"\ncd to the `examples/snippets` directory and run:\nuv run completion-client\n\"\"\"\nimport\nasyncio\nimport\nos\nfrom\nmcp\nimport\nClientSession\n,\nStdioServerParameters\nfrom\nmcp\n.\nclient\n.\nstdio\nimport\nstdio_client\nfrom\nmcp\n.\ntypes\nimport\nPromptReference\n,\nResourceTemplateReference\n# Create server parameters for stdio connection\nserver_params\n=\nStdioServerParameters\n(\ncommand\n=\n\"uv\"\n,\n# Using uv to run the server\nargs\n=\n[\n\"run\"\n,\n\"server\"\n,\n\"completion\"\n,\n\"stdio\"\n],\n# Server with completion support\nenv\n=\n{\n\"UV_INDEX\"\n:\nos\n.\nenviron\n.\nget\n(\n\"UV_INDEX\"\n,\n\"\"\n)},\n)\nasync\ndef\nrun\n():\n\"\"\"Run the completion client example.\"\"\"\nasync\nwith\nstdio_client\n(\nserver_params\n)\nas\n(\nread\n,\nwrite\n):\nasync\nwith\nClientSession\n(\nread\n,\nwrite\n)\nas\nsession\n:\n# Initialize the connection\nawait\nsession\n.\ninitialize\n()\n# List available resource templates\ntemplates\n=\nawait\nsession\n.\nlist_resource_templates\n()\nprint\n(\n\"Available resource templates:\"\n)\nfor\ntemplate\nin\ntemplates\n.\nresourceTemplates\n:\nprint\n(\nf\"  -\n{\ntemplate\n.\nuriTemplate\n}\n\"\n)\n# List available prompts\nprompts\n=\nawait\nsession\n.\nlist_prompts\n()\nprint\n(\n\"\n\\n\nAvailable prompts:\"\n)\nfor\nprompt\nin\nprompts\n.\nprompts\n:\nprint\n(\nf\"  -\n{\nprompt\n.\nname\n}\n\"\n)\n# Complete resource template arguments\nif\ntemplates\n.\nresourceTemplates\n:\ntemplate\n=\ntemplates\n.\nresourceTemplates\n[\n0\n]\nprint\n(\nf\"\n\\n\nCompleting arguments for resource template:\n{\ntemplate\n.\nuriTemplate\n}\n\"\n)\n# Complete without context\nresult\n=\nawait\nsession\n.\ncomplete\n(\nref\n=\nResourceTemplateReference\n(\ntype\n=\n\"ref/resource\"\n,\nuri\n=\ntemplate\n.\nuriTemplate\n),\nargument\n=\n{\n\"name\"\n:\n\"owner\"\n,\n\"value\"\n:\n\"model\"\n},\n                )\nprint\n(\nf\"Completions for 'owner' starting with 'model':\n{\nresult\n.\ncompletion\n.\nvalues\n}\n\"\n)\n# Complete with context - repo suggestions based on owner\nresult\n=\nawait\nsession\n.\ncomplete\n(\nref\n=\nResourceTemplateReference\n(\ntype\n=\n\"ref/resource\"\n,\nuri\n=\ntemplate\n.\nuriTemplate\n),\nargument\n=\n{\n\"name\"\n:\n\"repo\"\n,\n\"value\"\n:\n\"\"\n},\ncontext_arguments\n=\n{\n\"owner\"\n:\n\"modelcontextprotocol\"\n},\n                )\nprint\n(\nf\"Completions for 'repo' with owner='modelcontextprotocol':\n{\nresult\n.\ncompletion\n.\nvalues\n}\n\"\n)\n# Complete prompt arguments\nif\nprompts\n.\nprompts\n:\nprompt_name\n=\nprompts\n.\nprompts\n[\n0\n].\nname\nprint\n(\nf\"\n\\n\nCompleting arguments for prompt:\n{\nprompt_name\n}\n\"\n)\nresult\n=\nawait\nsession\n.\ncomplete\n(\nref\n=\nPromptReference\n(\ntype\n=\n\"ref/prompt\"\n,\nname\n=\nprompt_name\n),\nargument\n=\n{\n\"name\"\n:\n\"style\"\n,\n\"value\"\n:\n\"\"\n},\n                )\nprint\n(\nf\"Completions for 'style' argument:\n{\nresult\n.\ncompletion\n.\nvalues\n}\n\"\n)\ndef\nmain\n():\n\"\"\"Entry point for the completion client.\"\"\"\nasyncio\n.\nrun\n(\nrun\n())\nif\n__name__\n==\n\"__main__\"\n:\nmain\n()\nFull example:\nexamples/snippets/clients/completion_client.py\nElicitation\nRequest additional information from users. This example shows an Elicitation during a Tool Call:\nfrom\npydantic\nimport\nBaseModel\n,\nField\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nContext\n,\nFastMCP\nfrom\nmcp\n.\nserver\n.\nsession\nimport\nServerSession\nmcp\n=\nFastMCP\n(\nname\n=\n\"Elicitation Example\"\n)\nclass\nBookingPreferences\n(\nBaseModel\n):\n\"\"\"Schema for collecting user preferences.\"\"\"\ncheckAlternative\n:\nbool\n=\nField\n(\ndescription\n=\n\"Would you like to check another date?\"\n)\nalternativeDate\n:\nstr\n=\nField\n(\ndefault\n=\n\"2024-12-26\"\n,\ndescription\n=\n\"Alternative date (YYYY-MM-DD)\"\n,\n    )\n@\nmcp\n.\ntool\n()\nasync\ndef\nbook_table\n(\ndate\n:\nstr\n,\ntime\n:\nstr\n,\nparty_size\n:\nint\n,\nctx\n:\nContext\n[\nServerSession\n,\nNone\n])\n->\nstr\n:\n\"\"\"Book a table with date availability check.\"\"\"\n# Check if date is available\nif\ndate\n==\n\"2024-12-25\"\n:\n# Date unavailable - ask user for alternative\nresult\n=\nawait\nctx\n.\nelicit\n(\nmessage\n=\n(\nf\"No tables available for\n{\nparty_size\n}\non\n{\ndate\n}\n. Would you like to try another date?\"\n),\nschema\n=\nBookingPreferences\n,\n        )\nif\nresult\n.\naction\n==\n\"accept\"\nand\nresult\n.\ndata\n:\nif\nresult\n.\ndata\n.\ncheckAlternative\n:\nreturn\nf\"[SUCCESS] Booked for\n{\nresult\n.\ndata\n.\nalternativeDate\n}\n\"\nreturn\n\"[CANCELLED] No booking made\"\nreturn\n\"[CANCELLED] Booking cancelled\"\n# Date available\nreturn\nf\"[SUCCESS] Booked for\n{\ndate\n}\nat\n{\ntime\n}\n\"\nFull example:\nexamples/snippets/servers/elicitation.py\nElicitation schemas support default values for all field types. Default values are automatically included in the JSON schema sent to clients, allowing them to pre-populate forms.\nThe\nelicit()\nmethod returns an\nElicitationResult\nwith:\naction\n: \"accept\", \"decline\", or \"cancel\"\ndata\n: The validated response (only when accepted)\nvalidation_error\n: Any validation error message\nSampling\nTools can interact with LLMs through sampling (generating text):\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nContext\n,\nFastMCP\nfrom\nmcp\n.\nserver\n.\nsession\nimport\nServerSession\nfrom\nmcp\n.\ntypes\nimport\nSamplingMessage\n,\nTextContent\nmcp\n=\nFastMCP\n(\nname\n=\n\"Sampling Example\"\n)\n@\nmcp\n.\ntool\n()\nasync\ndef\ngenerate_poem\n(\ntopic\n:\nstr\n,\nctx\n:\nContext\n[\nServerSession\n,\nNone\n])\n->\nstr\n:\n\"\"\"Generate a poem using LLM sampling.\"\"\"\nprompt\n=\nf\"Write a short poem about\n{\ntopic\n}\n\"\nresult\n=\nawait\nctx\n.\nsession\n.\ncreate_message\n(\nmessages\n=\n[\nSamplingMessage\n(\nrole\n=\n\"user\"\n,\ncontent\n=\nTextContent\n(\ntype\n=\n\"text\"\n,\ntext\n=\nprompt\n),\n            )\n        ],\nmax_tokens\n=\n100\n,\n    )\nif\nresult\n.\ncontent\n.\ntype\n==\n\"text\"\n:\nreturn\nresult\n.\ncontent\n.\ntext\nreturn\nstr\n(\nresult\n.\ncontent\n)\nFull example:\nexamples/snippets/servers/sampling.py\nLogging and Notifications\nTools can send logs and notifications through the context:\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nContext\n,\nFastMCP\nfrom\nmcp\n.\nserver\n.\nsession\nimport\nServerSession\nmcp\n=\nFastMCP\n(\nname\n=\n\"Notifications Example\"\n)\n@\nmcp\n.\ntool\n()\nasync\ndef\nprocess_data\n(\ndata\n:\nstr\n,\nctx\n:\nContext\n[\nServerSession\n,\nNone\n])\n->\nstr\n:\n\"\"\"Process data with logging.\"\"\"\n# Different log levels\nawait\nctx\n.\ndebug\n(\nf\"Debug: Processing '\n{\ndata\n}\n'\"\n)\nawait\nctx\n.\ninfo\n(\n\"Info: Starting processing\"\n)\nawait\nctx\n.\nwarning\n(\n\"Warning: This is experimental\"\n)\nawait\nctx\n.\nerror\n(\n\"Error: (This is just a demo)\"\n)\n# Notify about resource changes\nawait\nctx\n.\nsession\n.\nsend_resource_list_changed\n()\nreturn\nf\"Processed:\n{\ndata\n}\n\"\nFull example:\nexamples/snippets/servers/notifications.py\nAuthentication\nAuthentication can be used by servers that want to expose tools accessing protected resources.\nmcp.server.auth\nimplements OAuth 2.1 resource server functionality, where MCP servers act as Resource Servers (RS) that validate tokens issued by separate Authorization Servers (AS). This follows the\nMCP authorization specification\nand implements RFC 9728 (Protected Resource Metadata) for AS discovery.\nMCP servers can use authentication by providing an implementation of the\nTokenVerifier\nprotocol:\n\"\"\"\nRun from the repository root:\nuv run examples/snippets/servers/oauth_server.py\n\"\"\"\nfrom\npydantic\nimport\nAnyHttpUrl\nfrom\nmcp\n.\nserver\n.\nauth\n.\nprovider\nimport\nAccessToken\n,\nTokenVerifier\nfrom\nmcp\n.\nserver\n.\nauth\n.\nsettings\nimport\nAuthSettings\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\nclass\nSimpleTokenVerifier\n(\nTokenVerifier\n):\n\"\"\"Simple token verifier for demonstration.\"\"\"\nasync\ndef\nverify_token\n(\nself\n,\ntoken\n:\nstr\n)\n->\nAccessToken\n|\nNone\n:\npass\n# This is where you would implement actual token validation\n# Create FastMCP instance as a Resource Server\nmcp\n=\nFastMCP\n(\n\"Weather Service\"\n,\n# Token verifier for authentication\ntoken_verifier\n=\nSimpleTokenVerifier\n(),\n# Auth settings for RFC 9728 Protected Resource Metadata\nauth\n=\nAuthSettings\n(\nissuer_url\n=\nAnyHttpUrl\n(\n\"https://auth.example.com\"\n),\n# Authorization Server URL\nresource_server_url\n=\nAnyHttpUrl\n(\n\"http://localhost:3001\"\n),\n# This server's URL\nrequired_scopes\n=\n[\n\"user\"\n],\n    ),\n)\n@\nmcp\n.\ntool\n()\nasync\ndef\nget_weather\n(\ncity\n:\nstr\n=\n\"London\"\n)\n->\ndict\n[\nstr\n,\nstr\n]:\n\"\"\"Get weather data for a city\"\"\"\nreturn\n{\n\"city\"\n:\ncity\n,\n\"temperature\"\n:\n\"22\"\n,\n\"condition\"\n:\n\"Partly cloudy\"\n,\n\"humidity\"\n:\n\"65%\"\n,\n    }\nif\n__name__\n==\n\"__main__\"\n:\nmcp\n.\nrun\n(\ntransport\n=\n\"streamable-http\"\n)\nFull example:\nexamples/snippets/servers/oauth_server.py\nFor a complete example with separate Authorization Server and Resource Server implementations, see\nexamples/servers/simple-auth/\n.\nArchitecture:\nAuthorization Server (AS)\n: Handles OAuth flows, user authentication, and token issuance\nResource Server (RS)\n: Your MCP server that validates tokens and serves protected resources\nClient\n: Discovers AS through RFC 9728, obtains tokens, and uses them with the MCP server\nSee\nTokenVerifier\nfor more details on implementing token validation.\nFastMCP Properties\nThe FastMCP server instance accessible via\nctx.fastmcp\nprovides access to server configuration and metadata:\nctx.fastmcp.name\n- The server's name as defined during initialization\nctx.fastmcp.instructions\n- Server instructions/description provided to clients\nctx.fastmcp.website_url\n- Optional website URL for the server\nctx.fastmcp.icons\n- Optional list of icons for UI display\nctx.fastmcp.settings\n- Complete server configuration object containing:\ndebug\n- Debug mode flag\nlog_level\n- Current logging level\nhost\nand\nport\n- Server network configuration\nmount_path\n,\nsse_path\n,\nstreamable_http_path\n- Transport paths\nstateless_http\n- Whether the server operates in stateless mode\nAnd other configuration options\n@\nmcp\n.\ntool\n()\ndef\nserver_info\n(\nctx\n:\nContext\n)\n->\ndict\n:\n\"\"\"Get information about the current server.\"\"\"\nreturn\n{\n\"name\"\n:\nctx\n.\nfastmcp\n.\nname\n,\n\"instructions\"\n:\nctx\n.\nfastmcp\n.\ninstructions\n,\n\"debug_mode\"\n:\nctx\n.\nfastmcp\n.\nsettings\n.\ndebug\n,\n\"log_level\"\n:\nctx\n.\nfastmcp\n.\nsettings\n.\nlog_level\n,\n\"host\"\n:\nctx\n.\nfastmcp\n.\nsettings\n.\nhost\n,\n\"port\"\n:\nctx\n.\nfastmcp\n.\nsettings\n.\nport\n,\n    }\nSession Properties and Methods\nThe session object accessible via\nctx.session\nprovides advanced control over client communication:\nctx.session.client_params\n- Client initialization parameters and declared capabilities\nawait ctx.session.send_log_message(level, data, logger)\n- Send log messages with full control\nawait ctx.session.create_message(messages, max_tokens)\n- Request LLM sampling/completion\nawait ctx.session.send_progress_notification(token, progress, total, message)\n- Direct progress updates\nawait ctx.session.send_resource_updated(uri)\n- Notify clients that a specific resource changed\nawait ctx.session.send_resource_list_changed()\n- Notify clients that the resource list changed\nawait ctx.session.send_tool_list_changed()\n- Notify clients that the tool list changed\nawait ctx.session.send_prompt_list_changed()\n- Notify clients that the prompt list changed\n@\nmcp\n.\ntool\n()\nasync\ndef\nnotify_data_update\n(\nresource_uri\n:\nstr\n,\nctx\n:\nContext\n)\n->\nstr\n:\n\"\"\"Update data and notify clients of the change.\"\"\"\n# Perform data update logic here\n# Notify clients that this specific resource changed\nawait\nctx\n.\nsession\n.\nsend_resource_updated\n(\nAnyUrl\n(\nresource_uri\n))\n# If this affects the overall resource list, notify about that too\nawait\nctx\n.\nsession\n.\nsend_resource_list_changed\n()\nreturn\nf\"Updated\n{\nresource_uri\n}\nand notified clients\"\nRequest Context Properties\nThe request context accessible via\nctx.request_context\ncontains request-specific information and resources:\nctx.request_context.lifespan_context\n- Access to resources initialized during server startup\nDatabase connections, configuration objects, shared services\nType-safe access to resources defined in your server's lifespan function\nctx.request_context.meta\n- Request metadata from the client including:\nprogressToken\n- Token for progress notifications\nOther client-provided metadata\nctx.request_context.request\n- The original MCP request object for advanced processing\nctx.request_context.request_id\n- Unique identifier for this request\n# Example with typed lifespan context\n@\ndataclass\nclass\nAppContext\n:\ndb\n:\nDatabase\nconfig\n:\nAppConfig\n@\nmcp\n.\ntool\n()\ndef\nquery_with_config\n(\nquery\n:\nstr\n,\nctx\n:\nContext\n)\n->\nstr\n:\n\"\"\"Execute a query using shared database and configuration.\"\"\"\n# Access typed lifespan context\napp_ctx\n:\nAppContext\n=\nctx\n.\nrequest_context\n.\nlifespan_context\n# Use shared resources\nconnection\n=\napp_ctx\n.\ndb\nsettings\n=\napp_ctx\n.\nconfig\n# Execute query with configuration\nresult\n=\nconnection\n.\nexecute\n(\nquery\n,\ntimeout\n=\nsettings\n.\nquery_timeout\n)\nreturn\nstr\n(\nresult\n)\nFull lifespan example:\nexamples/snippets/servers/lifespan_example.py\nRunning Your Server\nDevelopment Mode\nThe fastest way to test and debug your server is with the MCP Inspector:\nuv run mcp dev server.py\n#\nAdd dependencies\nuv run mcp dev server.py --with pandas --with numpy\n#\nMount local code\nuv run mcp dev server.py --with-editable\n.\nClaude Desktop Integration\nOnce your server is ready, install it in Claude Desktop:\nuv run mcp install server.py\n#\nCustom name\nuv run mcp install server.py --name\n\"\nMy Analytics Server\n\"\n#\nEnvironment variables\nuv run mcp install server.py -v API_KEY=abc123 -v DB_URL=postgres://...\nuv run mcp install server.py -f .env\nDirect Execution\nFor advanced scenarios like custom deployments:\n\"\"\"Example showing direct execution of an MCP server.\nThis is the simplest way to run an MCP server directly.\ncd to the `examples/snippets` directory and run:\nuv run direct-execution-server\nor\npython servers/direct_execution.py\n\"\"\"\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\nmcp\n=\nFastMCP\n(\n\"My App\"\n)\n@\nmcp\n.\ntool\n()\ndef\nhello\n(\nname\n:\nstr\n=\n\"World\"\n)\n->\nstr\n:\n\"\"\"Say hello to someone.\"\"\"\nreturn\nf\"Hello,\n{\nname\n}\n!\"\ndef\nmain\n():\n\"\"\"Entry point for the direct execution server.\"\"\"\nmcp\n.\nrun\n()\nif\n__name__\n==\n\"__main__\"\n:\nmain\n()\nFull example:\nexamples/snippets/servers/direct_execution.py\nRun it with:\npython servers/direct_execution.py\n#\nor\nuv run mcp run servers/direct_execution.py\nNote that\nuv run mcp run\nor\nuv run mcp dev\nonly supports server using FastMCP and not the low-level server variant.\nStreamable HTTP Transport\nNote\n: Streamable HTTP transport is superseding SSE transport for production deployments.\n\"\"\"\nRun from the repository root:\nuv run examples/snippets/servers/streamable_config.py\n\"\"\"\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Stateful server (maintains session state)\nmcp\n=\nFastMCP\n(\n\"StatefulServer\"\n)\n# Other configuration options:\n# Stateless server (no session persistence)\n# mcp = FastMCP(\"StatelessServer\", stateless_http=True)\n# Stateless server (no session persistence, no sse stream with supported client)\n# mcp = FastMCP(\"StatelessServer\", stateless_http=True, json_response=True)\n# Add a simple tool to demonstrate the server\n@\nmcp\n.\ntool\n()\ndef\ngreet\n(\nname\n:\nstr\n=\n\"World\"\n)\n->\nstr\n:\n\"\"\"Greet someone by name.\"\"\"\nreturn\nf\"Hello,\n{\nname\n}\n!\"\n# Run server with streamable_http transport\nif\n__name__\n==\n\"__main__\"\n:\nmcp\n.\nrun\n(\ntransport\n=\n\"streamable-http\"\n)\nFull example:\nexamples/snippets/servers/streamable_config.py\nYou can mount multiple FastMCP servers in a Starlette application:\n\"\"\"\nRun from the repository root:\nuvicorn examples.snippets.servers.streamable_starlette_mount:app --reload\n\"\"\"\nimport\ncontextlib\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nrouting\nimport\nMount\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Create the Echo server\necho_mcp\n=\nFastMCP\n(\nname\n=\n\"EchoServer\"\n,\nstateless_http\n=\nTrue\n)\n@\necho_mcp\n.\ntool\n()\ndef\necho\n(\nmessage\n:\nstr\n)\n->\nstr\n:\n\"\"\"A simple echo tool\"\"\"\nreturn\nf\"Echo:\n{\nmessage\n}\n\"\n# Create the Math server\nmath_mcp\n=\nFastMCP\n(\nname\n=\n\"MathServer\"\n,\nstateless_http\n=\nTrue\n)\n@\nmath_mcp\n.\ntool\n()\ndef\nadd_two\n(\nn\n:\nint\n)\n->\nint\n:\n\"\"\"Tool to add two to the input\"\"\"\nreturn\nn\n+\n2\n# Create a combined lifespan to manage both session managers\n@\ncontextlib\n.\nasynccontextmanager\nasync\ndef\nlifespan\n(\napp\n:\nStarlette\n):\nasync\nwith\ncontextlib\n.\nAsyncExitStack\n()\nas\nstack\n:\nawait\nstack\n.\nenter_async_context\n(\necho_mcp\n.\nsession_manager\n.\nrun\n())\nawait\nstack\n.\nenter_async_context\n(\nmath_mcp\n.\nsession_manager\n.\nrun\n())\nyield\n# Create the Starlette app and mount the MCP servers\napp\n=\nStarlette\n(\nroutes\n=\n[\nMount\n(\n\"/echo\"\n,\necho_mcp\n.\nstreamable_http_app\n()),\nMount\n(\n\"/math\"\n,\nmath_mcp\n.\nstreamable_http_app\n()),\n    ],\nlifespan\n=\nlifespan\n,\n)\n# Note: Clients connect to http://localhost:8000/echo/mcp and http://localhost:8000/math/mcp\n# To mount at the root of each path (e.g., /echo instead of /echo/mcp):\n# echo_mcp.settings.streamable_http_path = \"/\"\n# math_mcp.settings.streamable_http_path = \"/\"\nFull example:\nexamples/snippets/servers/streamable_starlette_mount.py\nFor low level server with Streamable HTTP implementations, see:\nStateful server:\nexamples/servers/simple-streamablehttp/\nStateless server:\nexamples/servers/simple-streamablehttp-stateless/\nThe streamable HTTP transport supports:\nStateful and stateless operation modes\nResumability with event stores\nJSON or SSE response formats\nBetter scalability for multi-node deployments\nCORS Configuration for Browser-Based Clients\nIf you'd like your server to be accessible by browser-based MCP clients, you'll need to configure CORS headers. The\nMcp-Session-Id\nheader must be exposed for browser clients to access it:\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nmiddleware\n.\ncors\nimport\nCORSMiddleware\n# Create your Starlette app first\nstarlette_app\n=\nStarlette\n(\nroutes\n=\n[...])\n# Then wrap it with CORS middleware\nstarlette_app\n=\nCORSMiddleware\n(\nstarlette_app\n,\nallow_origins\n=\n[\n\"*\"\n],\n# Configure appropriately for production\nallow_methods\n=\n[\n\"GET\"\n,\n\"POST\"\n,\n\"DELETE\"\n],\n# MCP streamable HTTP methods\nexpose_headers\n=\n[\n\"Mcp-Session-Id\"\n],\n)\nThis configuration is necessary because:\nThe MCP streamable HTTP transport uses the\nMcp-Session-Id\nheader for session management\nBrowsers restrict access to response headers unless explicitly exposed via CORS\nWithout this configuration, browser-based clients won't be able to read the session ID from initialization responses\nMounting to an Existing ASGI Server\nBy default, SSE servers are mounted at\n/sse\nand Streamable HTTP servers are mounted at\n/mcp\n. You can customize these paths using the methods described below.\nFor more information on mounting applications in Starlette, see the\nStarlette documentation\n.\nStreamableHTTP servers\nYou can mount the StreamableHTTP server to an existing ASGI server using the\nstreamable_http_app\nmethod. This allows you to integrate the StreamableHTTP server with other ASGI applications.\nBasic mounting\n\"\"\"\nBasic example showing how to mount StreamableHTTP server in Starlette.\nRun from the repository root:\nuvicorn examples.snippets.servers.streamable_http_basic_mounting:app --reload\n\"\"\"\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nrouting\nimport\nMount\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Create MCP server\nmcp\n=\nFastMCP\n(\n\"My App\"\n)\n@\nmcp\n.\ntool\n()\ndef\nhello\n()\n->\nstr\n:\n\"\"\"A simple hello tool\"\"\"\nreturn\n\"Hello from MCP!\"\n# Mount the StreamableHTTP server to the existing ASGI server\napp\n=\nStarlette\n(\nroutes\n=\n[\nMount\n(\n\"/\"\n,\napp\n=\nmcp\n.\nstreamable_http_app\n()),\n    ]\n)\nFull example:\nexamples/snippets/servers/streamable_http_basic_mounting.py\nHost-based routing\n\"\"\"\nExample showing how to mount StreamableHTTP server using Host-based routing.\nRun from the repository root:\nuvicorn examples.snippets.servers.streamable_http_host_mounting:app --reload\n\"\"\"\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nrouting\nimport\nHost\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Create MCP server\nmcp\n=\nFastMCP\n(\n\"MCP Host App\"\n)\n@\nmcp\n.\ntool\n()\ndef\ndomain_info\n()\n->\nstr\n:\n\"\"\"Get domain-specific information\"\"\"\nreturn\n\"This is served from mcp.acme.corp\"\n# Mount using Host-based routing\napp\n=\nStarlette\n(\nroutes\n=\n[\nHost\n(\n\"mcp.acme.corp\"\n,\napp\n=\nmcp\n.\nstreamable_http_app\n()),\n    ]\n)\nFull example:\nexamples/snippets/servers/streamable_http_host_mounting.py\nMultiple servers with path configuration\n\"\"\"\nExample showing how to mount multiple StreamableHTTP servers with path configuration.\nRun from the repository root:\nuvicorn examples.snippets.servers.streamable_http_multiple_servers:app --reload\n\"\"\"\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nrouting\nimport\nMount\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Create multiple MCP servers\napi_mcp\n=\nFastMCP\n(\n\"API Server\"\n)\nchat_mcp\n=\nFastMCP\n(\n\"Chat Server\"\n)\n@\napi_mcp\n.\ntool\n()\ndef\napi_status\n()\n->\nstr\n:\n\"\"\"Get API status\"\"\"\nreturn\n\"API is running\"\n@\nchat_mcp\n.\ntool\n()\ndef\nsend_message\n(\nmessage\n:\nstr\n)\n->\nstr\n:\n\"\"\"Send a chat message\"\"\"\nreturn\nf\"Message sent:\n{\nmessage\n}\n\"\n# Configure servers to mount at the root of each path\n# This means endpoints will be at /api and /chat instead of /api/mcp and /chat/mcp\napi_mcp\n.\nsettings\n.\nstreamable_http_path\n=\n\"/\"\nchat_mcp\n.\nsettings\n.\nstreamable_http_path\n=\n\"/\"\n# Mount the servers\napp\n=\nStarlette\n(\nroutes\n=\n[\nMount\n(\n\"/api\"\n,\napp\n=\napi_mcp\n.\nstreamable_http_app\n()),\nMount\n(\n\"/chat\"\n,\napp\n=\nchat_mcp\n.\nstreamable_http_app\n()),\n    ]\n)\nFull example:\nexamples/snippets/servers/streamable_http_multiple_servers.py\nPath configuration at initialization\n\"\"\"\nExample showing path configuration during FastMCP initialization.\nRun from the repository root:\nuvicorn examples.snippets.servers.streamable_http_path_config:app --reload\n\"\"\"\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nrouting\nimport\nMount\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Configure streamable_http_path during initialization\n# This server will mount at the root of wherever it's mounted\nmcp_at_root\n=\nFastMCP\n(\n\"My Server\"\n,\nstreamable_http_path\n=\n\"/\"\n)\n@\nmcp_at_root\n.\ntool\n()\ndef\nprocess_data\n(\ndata\n:\nstr\n)\n->\nstr\n:\n\"\"\"Process some data\"\"\"\nreturn\nf\"Processed:\n{\ndata\n}\n\"\n# Mount at /process - endpoints will be at /process instead of /process/mcp\napp\n=\nStarlette\n(\nroutes\n=\n[\nMount\n(\n\"/process\"\n,\napp\n=\nmcp_at_root\n.\nstreamable_http_app\n()),\n    ]\n)\nFull example:\nexamples/snippets/servers/streamable_http_path_config.py\nSSE servers\nNote\n: SSE transport is being superseded by\nStreamable HTTP transport\n.\nYou can mount the SSE server to an existing ASGI server using the\nsse_app\nmethod. This allows you to integrate the SSE server with other ASGI applications.\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nrouting\nimport\nMount\n,\nHost\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\nmcp\n=\nFastMCP\n(\n\"My App\"\n)\n# Mount the SSE server to the existing ASGI server\napp\n=\nStarlette\n(\nroutes\n=\n[\nMount\n(\n'/'\n,\napp\n=\nmcp\n.\nsse_app\n()),\n    ]\n)\n# or dynamically mount as host\napp\n.\nrouter\n.\nroutes\n.\nappend\n(\nHost\n(\n'mcp.acme.corp'\n,\napp\n=\nmcp\n.\nsse_app\n()))\nWhen mounting multiple MCP servers under different paths, you can configure the mount path in several ways:\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nrouting\nimport\nMount\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Create multiple MCP servers\ngithub_mcp\n=\nFastMCP\n(\n\"GitHub API\"\n)\nbrowser_mcp\n=\nFastMCP\n(\n\"Browser\"\n)\ncurl_mcp\n=\nFastMCP\n(\n\"Curl\"\n)\nsearch_mcp\n=\nFastMCP\n(\n\"Search\"\n)\n# Method 1: Configure mount paths via settings (recommended for persistent configuration)\ngithub_mcp\n.\nsettings\n.\nmount_path\n=\n\"/github\"\nbrowser_mcp\n.\nsettings\n.\nmount_path\n=\n\"/browser\"\n# Method 2: Pass mount path directly to sse_app (preferred for ad-hoc mounting)\n# This approach doesn't modify the server's settings permanently\n# Create Starlette app with multiple mounted servers\napp\n=\nStarlette\n(\nroutes\n=\n[\n# Using settings-based configuration\nMount\n(\n\"/github\"\n,\napp\n=\ngithub_mcp\n.\nsse_app\n()),\nMount\n(\n\"/browser\"\n,\napp\n=\nbrowser_mcp\n.\nsse_app\n()),\n# Using direct mount path parameter\nMount\n(\n\"/curl\"\n,\napp\n=\ncurl_mcp\n.\nsse_app\n(\n\"/curl\"\n)),\nMount\n(\n\"/search\"\n,\napp\n=\nsearch_mcp\n.\nsse_app\n(\n\"/search\"\n)),\n    ]\n)\n# Method 3: For direct execution, you can also pass the mount path to run()\nif\n__name__\n==\n\"__main__\"\n:\nsearch_mcp\n.\nrun\n(\ntransport\n=\n\"sse\"\n,\nmount_path\n=\n\"/search\"\n)\nFor more information on mounting applications in Starlette, see the\nStarlette documentation\n.\nAdvanced Usage\nLow-Level Server\nFor more control, you can use the low-level server implementation directly. This gives you full access to the protocol and allows you to customize every aspect of your server, including lifecycle management through the lifespan API:\n\"\"\"\nRun from the repository root:\nuv run examples/snippets/servers/lowlevel/lifespan.py\n\"\"\"\nfrom\ncollections\n.\nabc\nimport\nAsyncIterator\nfrom\ncontextlib\nimport\nasynccontextmanager\nfrom\ntyping\nimport\nAny\nimport\nmcp\n.\nserver\n.\nstdio\nimport\nmcp\n.\ntypes\nas\ntypes\nfrom\nmcp\n.\nserver\n.\nlowlevel\nimport\nNotificationOptions\n,\nServer\nfrom\nmcp\n.\nserver\n.\nmodels\nimport\nInitializationOptions\n# Mock database class for example\nclass\nDatabase\n:\n\"\"\"Mock database class for example.\"\"\"\n@\nclassmethod\nasync\ndef\nconnect\n(\ncls\n)\n->\n\"Database\"\n:\n\"\"\"Connect to database.\"\"\"\nprint\n(\n\"Database connected\"\n)\nreturn\ncls\n()\nasync\ndef\ndisconnect\n(\nself\n)\n->\nNone\n:\n\"\"\"Disconnect from database.\"\"\"\nprint\n(\n\"Database disconnected\"\n)\nasync\ndef\nquery\n(\nself\n,\nquery_str\n:\nstr\n)\n->\nlist\n[\ndict\n[\nstr\n,\nstr\n]]:\n\"\"\"Execute a query.\"\"\"\n# Simulate database query\nreturn\n[{\n\"id\"\n:\n\"1\"\n,\n\"name\"\n:\n\"Example\"\n,\n\"query\"\n:\nquery_str\n}]\n@\nasynccontextmanager\nasync\ndef\nserver_lifespan\n(\n_server\n:\nServer\n)\n->\nAsyncIterator\n[\ndict\n[\nstr\n,\nAny\n]]:\n\"\"\"Manage server startup and shutdown lifecycle.\"\"\"\n# Initialize resources on startup\ndb\n=\nawait\nDatabase\n.\nconnect\n()\ntry\n:\nyield\n{\n\"db\"\n:\ndb\n}\nfinally\n:\n# Clean up on shutdown\nawait\ndb\n.\ndisconnect\n()\n# Pass lifespan to server\nserver\n=\nServer\n(\n\"example-server\"\n,\nlifespan\n=\nserver_lifespan\n)\n@\nserver\n.\nlist_tools\n()\nasync\ndef\nhandle_list_tools\n()\n->\nlist\n[\ntypes\n.\nTool\n]:\n\"\"\"List available tools.\"\"\"\nreturn\n[\ntypes\n.\nTool\n(\nname\n=\n\"query_db\"\n,\ndescription\n=\n\"Query the database\"\n,\ninputSchema\n=\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n: {\n\"query\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"SQL query to execute\"\n}},\n\"required\"\n: [\n\"query\"\n],\n            },\n        )\n    ]\n@\nserver\n.\ncall_tool\n()\nasync\ndef\nquery_db\n(\nname\n:\nstr\n,\narguments\n:\ndict\n[\nstr\n,\nAny\n])\n->\nlist\n[\ntypes\n.\nTextContent\n]:\n\"\"\"Handle database query tool call.\"\"\"\nif\nname\n!=\n\"query_db\"\n:\nraise\nValueError\n(\nf\"Unknown tool:\n{\nname\n}\n\"\n)\n# Access lifespan context\nctx\n=\nserver\n.\nrequest_context\ndb\n=\nctx\n.\nlifespan_context\n[\n\"db\"\n]\n# Execute query\nresults\n=\nawait\ndb\n.\nquery\n(\narguments\n[\n\"query\"\n])\nreturn\n[\ntypes\n.\nTextContent\n(\ntype\n=\n\"text\"\n,\ntext\n=\nf\"Query results:\n{\nresults\n}\n\"\n)]\nasync\ndef\nrun\n():\n\"\"\"Run the server with lifespan management.\"\"\"\nasync\nwith\nmcp\n.\nserver\n.\nstdio\n.\nstdio_server\n()\nas\n(\nread_stream\n,\nwrite_stream\n):\nawait\nserver\n.\nrun\n(\nread_stream\n,\nwrite_stream\n,\nInitializationOptions\n(\nserver_name\n=\n\"example-server\"\n,\nserver_version\n=\n\"0.1.0\"\n,\ncapabilities\n=\nserver\n.\nget_capabilities\n(\nnotification_options\n=\nNotificationOptions\n(),\nexperimental_capabilities\n=\n{},\n                ),\n            ),\n        )\nif\n__name__\n==\n\"__main__\"\n:\nimport\nasyncio\nasyncio\n.\nrun\n(\nrun\n())\nFull example:\nexamples/snippets/servers/lowlevel/lifespan.py\nThe lifespan API provides:\nA way to initialize resources when the server starts and clean them up when it stops\nAccess to initialized resources through the request context in handlers\nType-safe context passing between lifespan and request handlers\n\"\"\"\nRun from the repository root:\nuv run examples/snippets/servers/lowlevel/basic.py\n\"\"\"\nimport\nasyncio\nimport\nmcp\n.\nserver\n.\nstdio\nimport\nmcp\n.\ntypes\nas\ntypes\nfrom\nmcp\n.\nserver\n.\nlowlevel\nimport\nNotificationOptions\n,\nServer\nfrom\nmcp\n.\nserver\n.\nmodels\nimport\nInitializationOptions\n# Create a server instance\nserver\n=\nServer\n(\n\"example-server\"\n)\n@\nserver\n.\nlist_prompts\n()\nasync\ndef\nhandle_list_prompts\n()\n->\nlist\n[\ntypes\n.\nPrompt\n]:\n\"\"\"List available prompts.\"\"\"\nreturn\n[\ntypes\n.\nPrompt\n(\nname\n=\n\"example-prompt\"\n,\ndescription\n=\n\"An example prompt template\"\n,\narguments\n=\n[\ntypes\n.\nPromptArgument\n(\nname\n=\n\"arg1\"\n,\ndescription\n=\n\"Example argument\"\n,\nrequired\n=\nTrue\n)],\n        )\n    ]\n@\nserver\n.\nget_prompt\n()\nasync\ndef\nhandle_get_prompt\n(\nname\n:\nstr\n,\narguments\n:\ndict\n[\nstr\n,\nstr\n]\n|\nNone\n)\n->\ntypes\n.\nGetPromptResult\n:\n\"\"\"Get a specific prompt by name.\"\"\"\nif\nname\n!=\n\"example-prompt\"\n:\nraise\nValueError\n(\nf\"Unknown prompt:\n{\nname\n}\n\"\n)\narg1_value\n=\n(\narguments\nor\n{}).\nget\n(\n\"arg1\"\n,\n\"default\"\n)\nreturn\ntypes\n.\nGetPromptResult\n(\ndescription\n=\n\"Example prompt\"\n,\nmessages\n=\n[\ntypes\n.\nPromptMessage\n(\nrole\n=\n\"user\"\n,\ncontent\n=\ntypes\n.\nTextContent\n(\ntype\n=\n\"text\"\n,\ntext\n=\nf\"Example prompt text with argument:\n{\narg1_value\n}\n\"\n),\n            )\n        ],\n    )\nasync\ndef\nrun\n():\n\"\"\"Run the basic low-level server.\"\"\"\nasync\nwith\nmcp\n.\nserver\n.\nstdio\n.\nstdio_server\n()\nas\n(\nread_stream\n,\nwrite_stream\n):\nawait\nserver\n.\nrun\n(\nread_stream\n,\nwrite_stream\n,\nInitializationOptions\n(\nserver_name\n=\n\"example\"\n,\nserver_version\n=\n\"0.1.0\"\n,\ncapabilities\n=\nserver\n.\nget_capabilities\n(\nnotification_options\n=\nNotificationOptions\n(),\nexperimental_capabilities\n=\n{},\n                ),\n            ),\n        )\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nrun\n())\nFull example:\nexamples/snippets/servers/lowlevel/basic.py\nCaution: The\nuv run mcp run\nand\nuv run mcp dev\ntool doesn't support low-level server.\nStructured Output Support\nThe low-level server supports structured output for tools, allowing you to return both human-readable content and machine-readable structured data. Tools can define an\noutputSchema\nto validate their structured output:\n\"\"\"\nRun from the repository root:\nuv run examples/snippets/servers/lowlevel/structured_output.py\n\"\"\"\nimport\nasyncio\nfrom\ntyping\nimport\nAny\nimport\nmcp\n.\nserver\n.\nstdio\nimport\nmcp\n.\ntypes\nas\ntypes\nfrom\nmcp\n.\nserver\n.\nlowlevel\nimport\nNotificationOptions\n,\nServer\nfrom\nmcp\n.\nserver\n.\nmodels\nimport\nInitializationOptions\nserver\n=\nServer\n(\n\"example-server\"\n)\n@\nserver\n.\nlist_tools\n()\nasync\ndef\nlist_tools\n()\n->\nlist\n[\ntypes\n.\nTool\n]:\n\"\"\"List available tools with structured output schemas.\"\"\"\nreturn\n[\ntypes\n.\nTool\n(\nname\n=\n\"get_weather\"\n,\ndescription\n=\n\"Get current weather for a city\"\n,\ninputSchema\n=\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n: {\n\"city\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"City name\"\n}},\n\"required\"\n: [\n\"city\"\n],\n            },\noutputSchema\n=\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n: {\n\"temperature\"\n: {\n\"type\"\n:\n\"number\"\n,\n\"description\"\n:\n\"Temperature in Celsius\"\n},\n\"condition\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"Weather condition\"\n},\n\"humidity\"\n: {\n\"type\"\n:\n\"number\"\n,\n\"description\"\n:\n\"Humidity percentage\"\n},\n\"city\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"City name\"\n},\n                },\n\"required\"\n: [\n\"temperature\"\n,\n\"condition\"\n,\n\"humidity\"\n,\n\"city\"\n],\n            },\n        )\n    ]\n@\nserver\n.\ncall_tool\n()\nasync\ndef\ncall_tool\n(\nname\n:\nstr\n,\narguments\n:\ndict\n[\nstr\n,\nAny\n])\n->\ndict\n[\nstr\n,\nAny\n]:\n\"\"\"Handle tool calls with structured output.\"\"\"\nif\nname\n==\n\"get_weather\"\n:\ncity\n=\narguments\n[\n\"city\"\n]\n# Simulated weather data - in production, call a weather API\nweather_data\n=\n{\n\"temperature\"\n:\n22.5\n,\n\"condition\"\n:\n\"partly cloudy\"\n,\n\"humidity\"\n:\n65\n,\n\"city\"\n:\ncity\n,\n# Include the requested city\n}\n# low-level server will validate structured output against the tool's\n# output schema, and additionally serialize it into a TextContent block\n# for backwards compatibility with pre-2025-06-18 clients.\nreturn\nweather_data\nelse\n:\nraise\nValueError\n(\nf\"Unknown tool:\n{\nname\n}\n\"\n)\nasync\ndef\nrun\n():\n\"\"\"Run the structured output server.\"\"\"\nasync\nwith\nmcp\n.\nserver\n.\nstdio\n.\nstdio_server\n()\nas\n(\nread_stream\n,\nwrite_stream\n):\nawait\nserver\n.\nrun\n(\nread_stream\n,\nwrite_stream\n,\nInitializationOptions\n(\nserver_name\n=\n\"structured-output-example\"\n,\nserver_version\n=\n\"0.1.0\"\n,\ncapabilities\n=\nserver\n.\nget_capabilities\n(\nnotification_options\n=\nNotificationOptions\n(),\nexperimental_capabilities\n=\n{},\n                ),\n            ),\n        )\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nrun\n())\nFull example:\nexamples/snippets/servers/lowlevel/structured_output.py\nTools can return data in three ways:\nContent only\n: Return a list of content blocks (default behavior before spec revision 2025-06-18)\nStructured data only\n: Return a dictionary that will be serialized to JSON (Introduced in spec revision 2025-06-18)\nBoth\n: Return a tuple of (content, structured_data) preferred option to use for backwards compatibility\nWhen an\noutputSchema\nis defined, the server automatically validates the structured output against the schema. This ensures type safety and helps catch errors early.\nPagination (Advanced)\nFor servers that need to handle large datasets, the low-level server provides paginated versions of list operations. This is an optional optimization - most servers won't need pagination unless they're dealing with hundreds or thousands of items.\nServer-side Implementation\n\"\"\"\nExample of implementing pagination with MCP server decorators.\n\"\"\"\nfrom\npydantic\nimport\nAnyUrl\nimport\nmcp\n.\ntypes\nas\ntypes\nfrom\nmcp\n.\nserver\n.\nlowlevel\nimport\nServer\n# Initialize the server\nserver\n=\nServer\n(\n\"paginated-server\"\n)\n# Sample data to paginate\nITEMS\n=\n[\nf\"Item\n{\ni\n}\n\"\nfor\ni\nin\nrange\n(\n1\n,\n101\n)]\n# 100 items\n@\nserver\n.\nlist_resources\n()\nasync\ndef\nlist_resources_paginated\n(\nrequest\n:\ntypes\n.\nListResourcesRequest\n)\n->\ntypes\n.\nListResourcesResult\n:\n\"\"\"List resources with pagination support.\"\"\"\npage_size\n=\n10\n# Extract cursor from request params\ncursor\n=\nrequest\n.\nparams\n.\ncursor\nif\nrequest\n.\nparams\nis\nnot\nNone\nelse\nNone\n# Parse cursor to get offset\nstart\n=\n0\nif\ncursor\nis\nNone\nelse\nint\n(\ncursor\n)\nend\n=\nstart\n+\npage_size\n# Get page of resources\npage_items\n=\n[\ntypes\n.\nResource\n(\nuri\n=\nAnyUrl\n(\nf\"resource://items/\n{\nitem\n}\n\"\n),\nname\n=\nitem\n,\ndescription\n=\nf\"Description for\n{\nitem\n}\n\"\n)\nfor\nitem\nin\nITEMS\n[\nstart\n:\nend\n]\n    ]\n# Determine next cursor\nnext_cursor\n=\nstr\n(\nend\n)\nif\nend\n<\nlen\n(\nITEMS\n)\nelse\nNone\nreturn\ntypes\n.\nListResourcesResult\n(\nresources\n=\npage_items\n,\nnextCursor\n=\nnext_cursor\n)\nFull example:\nexamples/snippets/servers/pagination_example.py\nClient-side Consumption\n\"\"\"\nExample of consuming paginated MCP endpoints from a client.\n\"\"\"\nimport\nasyncio\nfrom\nmcp\n.\nclient\n.\nsession\nimport\nClientSession\nfrom\nmcp\n.\nclient\n.\nstdio\nimport\nStdioServerParameters\n,\nstdio_client\nfrom\nmcp\n.\ntypes\nimport\nResource\nasync\ndef\nlist_all_resources\n()\n->\nNone\n:\n\"\"\"Fetch all resources using pagination.\"\"\"\nasync\nwith\nstdio_client\n(\nStdioServerParameters\n(\ncommand\n=\n\"uv\"\n,\nargs\n=\n[\n\"run\"\n,\n\"mcp-simple-pagination\"\n]))\nas\n(\nread\n,\nwrite\n,\n    ):\nasync\nwith\nClientSession\n(\nread\n,\nwrite\n)\nas\nsession\n:\nawait\nsession\n.\ninitialize\n()\nall_resources\n:\nlist\n[\nResource\n]\n=\n[]\ncursor\n=\nNone\nwhile\nTrue\n:\n# Fetch a page of resources\nresult\n=\nawait\nsession\n.\nlist_resources\n(\ncursor\n=\ncursor\n)\nall_resources\n.\nextend\n(\nresult\n.\nresources\n)\nprint\n(\nf\"Fetched\n{\nlen\n(\nresult\n.\nresources\n)\n}\nresources\"\n)\n# Check if there are more pages\nif\nresult\n.\nnextCursor\n:\ncursor\n=\nresult\n.\nnextCursor\nelse\n:\nbreak\nprint\n(\nf\"Total resources:\n{\nlen\n(\nall_resources\n)\n}\n\"\n)\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nlist_all_resources\n())\nFull example:\nexamples/snippets/clients/pagination_client.py\nKey Points\nCursors are opaque strings\n- the server defines the format (numeric offsets, timestamps, etc.)\nReturn\nnextCursor=None\nwhen there are no more pages\nBackward compatible\n- clients that don't support pagination will still work (they'll just get the first page)\nFlexible page sizes\n- Each endpoint can define its own page size based on data characteristics\nSee the\nsimple-pagination example\nfor a complete implementation.\nWriting MCP Clients\nThe SDK provides a high-level client interface for connecting to MCP servers using various\ntransports\n:\n\"\"\"\ncd to the `examples/snippets/clients` directory and run:\nuv run client\n\"\"\"\nimport\nasyncio\nimport\nos\nfrom\npydantic\nimport\nAnyUrl\nfrom\nmcp\nimport\nClientSession\n,\nStdioServerParameters\n,\ntypes\nfrom\nmcp\n.\nclient\n.\nstdio\nimport\nstdio_client\nfrom\nmcp\n.\nshared\n.\ncontext\nimport\nRequestContext\n# Create server parameters for stdio connection\nserver_params\n=\nStdioServerParameters\n(\ncommand\n=\n\"uv\"\n,\n# Using uv to run the server\nargs\n=\n[\n\"run\"\n,\n\"server\"\n,\n\"fastmcp_quickstart\"\n,\n\"stdio\"\n],\n# We're already in snippets dir\nenv\n=\n{\n\"UV_INDEX\"\n:\nos\n.\nenviron\n.\nget\n(\n\"UV_INDEX\"\n,\n\"\"\n)},\n)\n# Optional: create a sampling callback\nasync\ndef\nhandle_sampling_message\n(\ncontext\n:\nRequestContext\n[\nClientSession\n,\nNone\n],\nparams\n:\ntypes\n.\nCreateMessageRequestParams\n)\n->\ntypes\n.\nCreateMessageResult\n:\nprint\n(\nf\"Sampling request:\n{\nparams\n.\nmessages\n}\n\"\n)\nreturn\ntypes\n.\nCreateMessageResult\n(\nrole\n=\n\"assistant\"\n,\ncontent\n=\ntypes\n.\nTextContent\n(\ntype\n=\n\"text\"\n,\ntext\n=\n\"Hello, world! from model\"\n,\n        ),\nmodel\n=\n\"gpt-3.5-turbo\"\n,\nstopReason\n=\n\"endTurn\"\n,\n    )\nasync\ndef\nrun\n():\nasync\nwith\nstdio_client\n(\nserver_params\n)\nas\n(\nread\n,\nwrite\n):\nasync\nwith\nClientSession\n(\nread\n,\nwrite\n,\nsampling_callback\n=\nhandle_sampling_message\n)\nas\nsession\n:\n# Initialize the connection\nawait\nsession\n.\ninitialize\n()\n# List available prompts\nprompts\n=\nawait\nsession\n.\nlist_prompts\n()\nprint\n(\nf\"Available prompts:\n{\n[\np\n.\nname\nfor\np\nin\nprompts\n.\nprompts\n]\n}\n\"\n)\n# Get a prompt (greet_user prompt from fastmcp_quickstart)\nif\nprompts\n.\nprompts\n:\nprompt\n=\nawait\nsession\n.\nget_prompt\n(\n\"greet_user\"\n,\narguments\n=\n{\n\"name\"\n:\n\"Alice\"\n,\n\"style\"\n:\n\"friendly\"\n})\nprint\n(\nf\"Prompt result:\n{\nprompt\n.\nmessages\n[\n0\n].\ncontent\n}\n\"\n)\n# List available resources\nresources\n=\nawait\nsession\n.\nlist_resources\n()\nprint\n(\nf\"Available resources:\n{\n[\nr\n.\nuri\nfor\nr\nin\nresources\n.\nresources\n]\n}\n\"\n)\n# List available tools\ntools\n=\nawait\nsession\n.\nlist_tools\n()\nprint\n(\nf\"Available tools:\n{\n[\nt\n.\nname\nfor\nt\nin\ntools\n.\ntools\n]\n}\n\"\n)\n# Read a resource (greeting resource from fastmcp_quickstart)\nresource_content\n=\nawait\nsession\n.\nread_resource\n(\nAnyUrl\n(\n\"greeting://World\"\n))\ncontent_block\n=\nresource_content\n.\ncontents\n[\n0\n]\nif\nisinstance\n(\ncontent_block\n,\ntypes\n.\nTextContent\n):\nprint\n(\nf\"Resource content:\n{\ncontent_block\n.\ntext\n}\n\"\n)\n# Call a tool (add tool from fastmcp_quickstart)\nresult\n=\nawait\nsession\n.\ncall_tool\n(\n\"add\"\n,\narguments\n=\n{\n\"a\"\n:\n5\n,\n\"b\"\n:\n3\n})\nresult_unstructured\n=\nresult\n.\ncontent\n[\n0\n]\nif\nisinstance\n(\nresult_unstructured\n,\ntypes\n.\nTextContent\n):\nprint\n(\nf\"Tool result:\n{\nresult_unstructured\n.\ntext\n}\n\"\n)\nresult_structured\n=\nresult\n.\nstructuredContent\nprint\n(\nf\"Structured tool result:\n{\nresult_structured\n}\n\"\n)\ndef\nmain\n():\n\"\"\"Entry point for the client script.\"\"\"\nasyncio\n.\nrun\n(\nrun\n())\nif\n__name__\n==\n\"__main__\"\n:\nmain\n()\nFull example:\nexamples/snippets/clients/stdio_client.py\nClients can also connect using\nStreamable HTTP transport\n:\n\"\"\"\nRun from the repository root:\nuv run examples/snippets/clients/streamable_basic.py\n\"\"\"\nimport\nasyncio\nfrom\nmcp\nimport\nClientSession\nfrom\nmcp\n.\nclient\n.\nstreamable_http\nimport\nstreamablehttp_client\nasync\ndef\nmain\n():\n# Connect to a streamable HTTP server\nasync\nwith\nstreamablehttp_client\n(\n\"http://localhost:8000/mcp\"\n)\nas\n(\nread_stream\n,\nwrite_stream\n,\n_\n,\n    ):\n# Create a session using the client streams\nasync\nwith\nClientSession\n(\nread_stream\n,\nwrite_stream\n)\nas\nsession\n:\n# Initialize the connection\nawait\nsession\n.\ninitialize\n()\n# List available tools\ntools\n=\nawait\nsession\n.\nlist_tools\n()\nprint\n(\nf\"Available tools:\n{\n[\ntool\n.\nname\nfor\ntool\nin\ntools\n.\ntools\n]\n}\n\"\n)\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nFull example:\nexamples/snippets/clients/streamable_basic.py\nClient Display Utilities\nWhen building MCP clients, the SDK provides utilities to help display human-readable names for tools, resources, and prompts:\n\"\"\"\ncd to the `examples/snippets` directory and run:\nuv run display-utilities-client\n\"\"\"\nimport\nasyncio\nimport\nos\nfrom\nmcp\nimport\nClientSession\n,\nStdioServerParameters\nfrom\nmcp\n.\nclient\n.\nstdio\nimport\nstdio_client\nfrom\nmcp\n.\nshared\n.\nmetadata_utils\nimport\nget_display_name\n# Create server parameters for stdio connection\nserver_params\n=\nStdioServerParameters\n(\ncommand\n=\n\"uv\"\n,\n# Using uv to run the server\nargs\n=\n[\n\"run\"\n,\n\"server\"\n,\n\"fastmcp_quickstart\"\n,\n\"stdio\"\n],\nenv\n=\n{\n\"UV_INDEX\"\n:\nos\n.\nenviron\n.\nget\n(\n\"UV_INDEX\"\n,\n\"\"\n)},\n)\nasync\ndef\ndisplay_tools\n(\nsession\n:\nClientSession\n):\n\"\"\"Display available tools with human-readable names\"\"\"\ntools_response\n=\nawait\nsession\n.\nlist_tools\n()\nfor\ntool\nin\ntools_response\n.\ntools\n:\n# get_display_name() returns the title if available, otherwise the name\ndisplay_name\n=\nget_display_name\n(\ntool\n)\nprint\n(\nf\"Tool:\n{\ndisplay_name\n}\n\"\n)\nif\ntool\n.\ndescription\n:\nprint\n(\nf\"\n{\ntool\n.\ndescription\n}\n\"\n)\nasync\ndef\ndisplay_resources\n(\nsession\n:\nClientSession\n):\n\"\"\"Display available resources with human-readable names\"\"\"\nresources_response\n=\nawait\nsession\n.\nlist_resources\n()\nfor\nresource\nin\nresources_response\n.\nresources\n:\ndisplay_name\n=\nget_display_name\n(\nresource\n)\nprint\n(\nf\"Resource:\n{\ndisplay_name\n}\n(\n{\nresource\n.\nuri\n}\n)\"\n)\ntemplates_response\n=\nawait\nsession\n.\nlist_resource_templates\n()\nfor\ntemplate\nin\ntemplates_response\n.\nresourceTemplates\n:\ndisplay_name\n=\nget_display_name\n(\ntemplate\n)\nprint\n(\nf\"Resource Template:\n{\ndisplay_name\n}\n\"\n)\nasync\ndef\nrun\n():\n\"\"\"Run the display utilities example.\"\"\"\nasync\nwith\nstdio_client\n(\nserver_params\n)\nas\n(\nread\n,\nwrite\n):\nasync\nwith\nClientSession\n(\nread\n,\nwrite\n)\nas\nsession\n:\n# Initialize the connection\nawait\nsession\n.\ninitialize\n()\nprint\n(\n\"=== Available Tools ===\"\n)\nawait\ndisplay_tools\n(\nsession\n)\nprint\n(\n\"\n\\n\n=== Available Resources ===\"\n)\nawait\ndisplay_resources\n(\nsession\n)\ndef\nmain\n():\n\"\"\"Entry point for the display utilities client.\"\"\"\nasyncio\n.\nrun\n(\nrun\n())\nif\n__name__\n==\n\"__main__\"\n:\nmain\n()\nFull example:\nexamples/snippets/clients/display_utilities.py\nThe\nget_display_name()\nfunction implements the proper precedence rules for displaying names:\nFor tools:\ntitle\n>\nannotations.title\n>\nname\nFor other objects:\ntitle\n>\nname\nThis ensures your client UI shows the most user-friendly names that servers provide.\nOAuth Authentication for Clients\nThe SDK includes\nauthorization support\nfor connecting to protected MCP servers:\n\"\"\"\nBefore running, specify running MCP RS server URL.\nTo spin up RS server locally, see\nexamples/servers/simple-auth/README.md\ncd to the `examples/snippets` directory and run:\nuv run oauth-client\n\"\"\"\nimport\nasyncio\nfrom\nurllib\n.\nparse\nimport\nparse_qs\n,\nurlparse\nfrom\npydantic\nimport\nAnyUrl\nfrom\nmcp\nimport\nClientSession\nfrom\nmcp\n.\nclient\n.\nauth\nimport\nOAuthClientProvider\n,\nTokenStorage\nfrom\nmcp\n.\nclient\n.\nstreamable_http\nimport\nstreamablehttp_client\nfrom\nmcp\n.\nshared\n.\nauth\nimport\nOAuthClientInformationFull\n,\nOAuthClientMetadata\n,\nOAuthToken\nclass\nInMemoryTokenStorage\n(\nTokenStorage\n):\n\"\"\"Demo In-memory token storage implementation.\"\"\"\ndef\n__init__\n(\nself\n):\nself\n.\ntokens\n:\nOAuthToken\n|\nNone\n=\nNone\nself\n.\nclient_info\n:\nOAuthClientInformationFull\n|\nNone\n=\nNone\nasync\ndef\nget_tokens\n(\nself\n)\n->\nOAuthToken\n|\nNone\n:\n\"\"\"Get stored tokens.\"\"\"\nreturn\nself\n.\ntokens\nasync\ndef\nset_tokens\n(\nself\n,\ntokens\n:\nOAuthToken\n)\n->\nNone\n:\n\"\"\"Store tokens.\"\"\"\nself\n.\ntokens\n=\ntokens\nasync\ndef\nget_client_info\n(\nself\n)\n->\nOAuthClientInformationFull\n|\nNone\n:\n\"\"\"Get stored client information.\"\"\"\nreturn\nself\n.\nclient_info\nasync\ndef\nset_client_info\n(\nself\n,\nclient_info\n:\nOAuthClientInformationFull\n)\n->\nNone\n:\n\"\"\"Store client information.\"\"\"\nself\n.\nclient_info\n=\nclient_info\nasync\ndef\nhandle_redirect\n(\nauth_url\n:\nstr\n)\n->\nNone\n:\nprint\n(\nf\"Visit:\n{\nauth_url\n}\n\"\n)\nasync\ndef\nhandle_callback\n()\n->\ntuple\n[\nstr\n,\nstr\n|\nNone\n]:\ncallback_url\n=\ninput\n(\n\"Paste callback URL: \"\n)\nparams\n=\nparse_qs\n(\nurlparse\n(\ncallback_url\n).\nquery\n)\nreturn\nparams\n[\n\"code\"\n][\n0\n],\nparams\n.\nget\n(\n\"state\"\n, [\nNone\n])[\n0\n]\nasync\ndef\nmain\n():\n\"\"\"Run the OAuth client example.\"\"\"\noauth_auth\n=\nOAuthClientProvider\n(\nserver_url\n=\n\"http://localhost:8001\"\n,\nclient_metadata\n=\nOAuthClientMetadata\n(\nclient_name\n=\n\"Example MCP Client\"\n,\nredirect_uris\n=\n[\nAnyUrl\n(\n\"http://localhost:3000/callback\"\n)],\ngrant_types\n=\n[\n\"authorization_code\"\n,\n\"refresh_token\"\n],\nresponse_types\n=\n[\n\"code\"\n],\nscope\n=\n\"user\"\n,\n        ),\nstorage\n=\nInMemoryTokenStorage\n(),\nredirect_handler\n=\nhandle_redirect\n,\ncallback_handler\n=\nhandle_callback\n,\n    )\nasync\nwith\nstreamablehttp_client\n(\n\"http://localhost:8001/mcp\"\n,\nauth\n=\noauth_auth\n)\nas\n(\nread\n,\nwrite\n,\n_\n):\nasync\nwith\nClientSession\n(\nread\n,\nwrite\n)\nas\nsession\n:\nawait\nsession\n.\ninitialize\n()\ntools\n=\nawait\nsession\n.\nlist_tools\n()\nprint\n(\nf\"Available tools:\n{\n[\ntool\n.\nname\nfor\ntool\nin\ntools\n.\ntools\n]\n}\n\"\n)\nresources\n=\nawait\nsession\n.\nlist_resources\n()\nprint\n(\nf\"Available resources:\n{\n[\nr\n.\nuri\nfor\nr\nin\nresources\n.\nresources\n]\n}\n\"\n)\ndef\nrun\n():\nasyncio\n.\nrun\n(\nmain\n())\nif\n__name__\n==\n\"__main__\"\n:\nrun\n()\nFull example:\nexamples/snippets/clients/oauth_client.py\nFor a complete working example, see\nexamples/clients/simple-auth-client/\n.\nParsing Tool Results\nWhen calling tools through MCP, the\nCallToolResult\nobject contains the tool's response in a structured format. Understanding how to parse this result is essential for properly handling tool outputs.\n\"\"\"examples/snippets/clients/parsing_tool_results.py\"\"\"\nimport\nasyncio\nfrom\nmcp\nimport\nClientSession\n,\nStdioServerParameters\n,\ntypes\nfrom\nmcp\n.\nclient\n.\nstdio\nimport\nstdio_client\nasync\ndef\nparse_tool_results\n():\n\"\"\"Demonstrates how to parse different types of content in CallToolResult.\"\"\"\nserver_params\n=\nStdioServerParameters\n(\ncommand\n=\n\"python\"\n,\nargs\n=\n[\n\"path/to/mcp_server.py\"\n]\n    )\nasync\nwith\nstdio_client\n(\nserver_params\n)\nas\n(\nread\n,\nwrite\n):\nasync\nwith\nClientSession\n(\nread\n,\nwrite\n)\nas\nsession\n:\nawait\nsession\n.\ninitialize\n()\n# Example 1: Parsing text content\nresult\n=\nawait\nsession\n.\ncall_tool\n(\n\"get_data\"\n, {\n\"format\"\n:\n\"text\"\n})\nfor\ncontent\nin\nresult\n.\ncontent\n:\nif\nisinstance\n(\ncontent\n,\ntypes\n.\nTextContent\n):\nprint\n(\nf\"Text:\n{\ncontent\n.\ntext\n}\n\"\n)\n# Example 2: Parsing structured content from JSON tools\nresult\n=\nawait\nsession\n.\ncall_tool\n(\n\"get_user\"\n, {\n\"id\"\n:\n\"123\"\n})\nif\nhasattr\n(\nresult\n,\n\"structuredContent\"\n)\nand\nresult\n.\nstructuredContent\n:\n# Access structured data directly\nuser_data\n=\nresult\n.\nstructuredContent\nprint\n(\nf\"User:\n{\nuser_data\n.\nget\n(\n'name'\n)\n}\n, Age:\n{\nuser_data\n.\nget\n(\n'age'\n)\n}\n\"\n)\n# Example 3: Parsing embedded resources\nresult\n=\nawait\nsession\n.\ncall_tool\n(\n\"read_config\"\n, {})\nfor\ncontent\nin\nresult\n.\ncontent\n:\nif\nisinstance\n(\ncontent\n,\ntypes\n.\nEmbeddedResource\n):\nresource\n=\ncontent\n.\nresource\nif\nisinstance\n(\nresource\n,\ntypes\n.\nTextResourceContents\n):\nprint\n(\nf\"Config from\n{\nresource\n.\nuri\n}\n:\n{\nresource\n.\ntext\n}\n\"\n)\nelif\nisinstance\n(\nresource\n,\ntypes\n.\nBlobResourceContents\n):\nprint\n(\nf\"Binary data from\n{\nresource\n.\nuri\n}\n\"\n)\n# Example 4: Parsing image content\nresult\n=\nawait\nsession\n.\ncall_tool\n(\n\"generate_chart\"\n, {\n\"data\"\n: [\n1\n,\n2\n,\n3\n]})\nfor\ncontent\nin\nresult\n.\ncontent\n:\nif\nisinstance\n(\ncontent\n,\ntypes\n.\nImageContent\n):\nprint\n(\nf\"Image (\n{\ncontent\n.\nmimeType\n}\n):\n{\nlen\n(\ncontent\n.\ndata\n)\n}\nbytes\"\n)\n# Example 5: Handling errors\nresult\n=\nawait\nsession\n.\ncall_tool\n(\n\"failing_tool\"\n, {})\nif\nresult\n.\nisError\n:\nprint\n(\n\"Tool execution failed!\"\n)\nfor\ncontent\nin\nresult\n.\ncontent\n:\nif\nisinstance\n(\ncontent\n,\ntypes\n.\nTextContent\n):\nprint\n(\nf\"Error:\n{\ncontent\n.\ntext\n}\n\"\n)\nasync\ndef\nmain\n():\nawait\nparse_tool_results\n()\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nMCP Primitives\nThe MCP protocol defines three core primitives that servers can implement:\nPrimitive\nControl\nDescription\nExample Use\nPrompts\nUser-controlled\nInteractive templates invoked by user choice\nSlash commands, menu options\nResources\nApplication-controlled\nContextual data managed by the client application\nFile contents, API responses\nTools\nModel-controlled\nFunctions exposed to the LLM to take actions\nAPI calls, data updates\nServer Capabilities\nMCP servers declare capabilities during initialization:\nCapability\nFeature Flag\nDescription\nprompts\nlistChanged\nPrompt template management\nresources\nsubscribe\nlistChanged\nResource exposure and updates\ntools\nlistChanged\nTool discovery and execution\nlogging\n-\nServer logging configuration\ncompletions\n-\nArgument completion suggestions\nDocumentation\nAPI Reference\nModel Context Protocol documentation\nModel Context Protocol specification\nOfficially supported servers\nContributing\nWe are passionate about supporting contributors of all levels of experience and would love to see you get involved in the project. See the\ncontributing guide\nto get started.\nLicense\nThis project is licensed under the MIT License - see the LICENSE file for details.",
        "今日の獲得スター数: 32",
        "累積スター数: 19,129"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/modelcontextprotocol/python-sdk"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/openai/openai-agents-js",
      "title": "openai/openai-agents-js",
      "date": null,
      "executive_summary": [
        "A lightweight, powerful framework for multi-agent workflows and voice agents",
        "---",
        "OpenAI Agents SDK (JavaScript/TypeScript)\nThe OpenAI Agents SDK is a lightweight yet powerful framework for building multi-agent workflows in JavaScript/TypeScript. It is provider-agnostic, supporting OpenAI APIs and more.\nNote\nLooking for the Python version? Check out\nAgents SDK Python\n.\nCore concepts\nAgents\n: LLMs configured with instructions, tools, guardrails, and handoffs.\nHandoffs\n: Specialized tool calls for transferring control between agents.\nGuardrails\n: Configurable safety checks for input and output validation.\nTracing\n: Built-in tracking of agent runs, allowing you to view, debug, and optimize your workflows.\nExplore the\nexamples/\ndirectory to see the SDK in action.\nSupported Features\nMulti-Agent Workflows\n: Compose and orchestrate multiple agents in a single workflow.\nTool Integration\n: Seamlessly call tools/functions from within agent responses.\nHandoffs\n: Transfer control between agents dynamically during a run.\nStructured Outputs\n: Support for both plain text and schema-validated structured outputs.\nStreaming Responses\n: Stream agent outputs and events in real time.\nTracing & Debugging\n: Built-in tracing for visualizing and debugging agent runs.\nGuardrails\n: Input and output validation for safety and reliability.\nParallelization\n: Run agents or tool calls in parallel and aggregate results.\nHuman-in-the-Loop\n: Integrate human approval or intervention into workflows.\nRealtime Voice Agents\n: Build realtime voice agents using WebRTC or WebSockets\nLocal MCP Server Support\n: Give an Agent access to a locally running MCP server to provide tools\nSeparate optimized browser package\n: Dedicated package meant to run in the browser for Realtime agents.\nBroader model support\n: Use non-OpenAI models through the Vercel AI SDK adapter\nLong running functions\n: Suspend an agent loop to execute a long-running function and revive it later\nVoice pipeline\n: Chain text-based agents using speech-to-text and text-to-speech into a voice agent\nGet started\nSupported environments\nNode.js 22 or later\nDeno\nBun\nExperimental support:\nCloudflare Workers with\nnodejs_compat\nenabled\nCheck out the documentation\nfor more detailed information.\nInstallation\nnpm install @openai/agents zod@3\nHello world example\nimport\n{\nAgent\n,\nrun\n}\nfrom\n'@openai/agents'\n;\nconst\nagent\n=\nnew\nAgent\n(\n{\nname\n:\n'Assistant'\n,\ninstructions\n:\n'You are a helpful assistant'\n,\n}\n)\n;\nconst\nresult\n=\nawait\nrun\n(\nagent\n,\n'Write a haiku about recursion in programming.'\n,\n)\n;\nconsole\n.\nlog\n(\nresult\n.\nfinalOutput\n)\n;\n// Code within the code,\n// Functions calling themselves,\n// Infinite loop's dance.\n(\nIf running this, ensure you set the\nOPENAI_API_KEY\nenvironment variable\n)\nFunctions example\nimport\n{\nz\n}\nfrom\n'zod'\n;\nimport\n{\nAgent\n,\nrun\n,\ntool\n}\nfrom\n'@openai/agents'\n;\nconst\ngetWeatherTool\n=\ntool\n(\n{\nname\n:\n'get_weather'\n,\ndescription\n:\n'Get the weather for a given city'\n,\nparameters\n:\nz\n.\nobject\n(\n{\ncity\n:\nz\n.\nstring\n(\n)\n}\n)\n,\nexecute\n:\nasync\n(\ninput\n)\n=>\n{\nreturn\n`The weather in\n${\ninput\n.\ncity\n}\nis sunny`\n;\n}\n,\n}\n)\n;\nconst\nagent\n=\nnew\nAgent\n(\n{\nname\n:\n'Data agent'\n,\ninstructions\n:\n'You are a data agent'\n,\ntools\n:\n[\ngetWeatherTool\n]\n,\n}\n)\n;\nasync\nfunction\nmain\n(\n)\n{\nconst\nresult\n=\nawait\nrun\n(\nagent\n,\n'What is the weather in Tokyo?'\n)\n;\nconsole\n.\nlog\n(\nresult\n.\nfinalOutput\n)\n;\n}\nmain\n(\n)\n.\ncatch\n(\nconsole\n.\nerror\n)\n;\nHandoffs example\nimport\n{\nz\n}\nfrom\n'zod'\n;\nimport\n{\nAgent\n,\nrun\n,\ntool\n}\nfrom\n'@openai/agents'\n;\nconst\ngetWeatherTool\n=\ntool\n(\n{\nname\n:\n'get_weather'\n,\ndescription\n:\n'Get the weather for a given city'\n,\nparameters\n:\nz\n.\nobject\n(\n{\ncity\n:\nz\n.\nstring\n(\n)\n}\n)\n,\nexecute\n:\nasync\n(\ninput\n)\n=>\n{\nreturn\n`The weather in\n${\ninput\n.\ncity\n}\nis sunny`\n;\n}\n,\n}\n)\n;\nconst\ndataAgent\n=\nnew\nAgent\n(\n{\nname\n:\n'Data agent'\n,\ninstructions\n:\n'You are a data agent'\n,\nhandoffDescription\n:\n'You know everything about the weather'\n,\ntools\n:\n[\ngetWeatherTool\n]\n,\n}\n)\n;\n// Use Agent.create method to ensure the finalOutput type considers handoffs\nconst\nagent\n=\nAgent\n.\ncreate\n(\n{\nname\n:\n'Basic test agent'\n,\ninstructions\n:\n'You are a basic agent'\n,\nhandoffs\n:\n[\ndataAgent\n]\n,\n}\n)\n;\nasync\nfunction\nmain\n(\n)\n{\nconst\nresult\n=\nawait\nrun\n(\nagent\n,\n'What is the weather in San Francisco?'\n)\n;\nconsole\n.\nlog\n(\nresult\n.\nfinalOutput\n)\n;\n}\nmain\n(\n)\n.\ncatch\n(\nconsole\n.\nerror\n)\n;\nVoice Agent\nimport\n{\nz\n}\nfrom\n'zod'\n;\nimport\n{\nRealtimeAgent\n,\nRealtimeSession\n,\ntool\n}\nfrom\n'@openai/agents-realtime'\n;\nconst\ngetWeatherTool\n=\ntool\n(\n{\nname\n:\n'get_weather'\n,\ndescription\n:\n'Get the weather for a given city'\n,\nparameters\n:\nz\n.\nobject\n(\n{\ncity\n:\nz\n.\nstring\n(\n)\n}\n)\n,\nexecute\n:\nasync\n(\ninput\n)\n=>\n{\nreturn\n`The weather in\n${\ninput\n.\ncity\n}\nis sunny`\n;\n}\n,\n}\n)\n;\nconst\nagent\n=\nnew\nRealtimeAgent\n(\n{\nname\n:\n'Data agent'\n,\ninstructions\n:\n'You are a data agent. When you are asked to check weather, you must use the available tools.'\n,\ntools\n:\n[\ngetWeatherTool\n]\n,\n}\n)\n;\n// Intended to run in the browser\nconst\n{\napiKey\n}\n=\nawait\nfetch\n(\n'/path/to/ephemeral/key/generation'\n)\n.\nthen\n(\n(\nresp\n)\n=>\nresp\n.\njson\n(\n)\n,\n)\n;\n// Automatically configures audio input/output — start talking\nconst\nsession\n=\nnew\nRealtimeSession\n(\nagent\n)\n;\nawait\nsession\n.\nconnect\n(\n{\napiKey\n}\n)\n;\nRunning Complete Examples\nThe\nexamples/\ndirectory contains a series of examples to get started:\npnpm examples:basic\n- Basic example with handoffs and tool calling\npnpm examples:agents-as-tools\n- Using agents as tools for translation\npnpm examples:tools-web-search\n- Using the web search tool\npnpm examples:tools-file-search\n- Using the file search tool\npnpm examples:deterministic\n- Deterministic multi-agent workflow\npnpm examples:parallelization\n- Running agents in parallel and picking the best result\npnpm examples:human-in-the-loop\n- Human approval for certain tool calls\npnpm examples:streamed\n- Streaming agent output and events in real time\npnpm examples:streamed:human-in-the-loop\n- Streaming output with human-in-the-loop approval\npnpm examples:routing\n- Routing between agents based on language or context\npnpm examples:realtime-demo\n- Framework agnostic Voice Agent example\npnpm examples:realtime-next\n- Next.js Voice Agent example application\nThe agent loop\nWhen you call\nRunner.run()\n, the SDK executes a loop until a final output is produced.\nThe agent is invoked with the given input, using the model and settings configured on the agent (or globally).\nThe LLM returns a response, which may include tool calls or handoff requests.\nIf the response contains a final output (see below), the loop ends and the result is returned.\nIf the response contains a handoff, the agent is switched to the new agent and the loop continues.\nIf there are tool calls, the tools are executed, their results are appended to the message history, and the loop continues.\nYou can control the maximum number of iterations with the\nmaxTurns\nparameter.\nFinal output\nThe final output is the last thing the agent produces in the loop.\nIf the agent has an\noutputType\n(structured output), the loop ends when the LLM returns a response matching that type.\nIf there is no\noutputType\n(plain text), the first LLM response without tool calls or handoffs is considered the final output.\nSummary of the agent loop:\nIf the current agent has an\noutputType\n, the loop runs until structured output of that type is produced.\nIf not, the loop runs until a message is produced with no tool calls or handoffs.\nError handling\nIf the maximum number of turns is exceeded, a\nMaxTurnsExceededError\nis thrown.\nIf a guardrail is triggered, a\nGuardrailTripwireTriggered\nexception is raised.\nDocumentation\nTo view the documentation locally:\npnpm docs:dev\nThen visit\nhttp://localhost:4321\nin your browser.\nDevelopment\nIf you want to contribute or edit the SDK/examples:\nInstall dependencies\npnpm install\nBuild the project\npnpm build\n&&\npnpm -r build-check\nRun tests and linter\npnpm\ntest\n&&\npnpm lint\nSee\nAGENTS.md\nand\nCONTRIBUTING.md\nfor the full contributor guide.\nAcknowledgements\nWe'd like to acknowledge the excellent work of the open-source community, especially:\nzod\n(schema validation)\nStarlight\nvite\nand\nvitest\npnpm\nNext.js\nWe're committed to building the Agents SDK as an open source framework so others in the community can expand on our approach.\nFor more details, see the\ndocumentation\nor explore the\nexamples/\ndirectory.",
        "今日の獲得スター数: 31",
        "累積スター数: 1,610"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/openai/openai-agents-js"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    }
  ]
}