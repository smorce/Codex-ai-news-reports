{
  "generated_at": "2025-10-09T02:06:04Z",
  "site": "github-trending",
  "num_articles": 31,
  "articles": [
    {
      "url": "https://github.com/Stremio/stremio-web",
      "title": "Stremio/stremio-web",
      "date": null,
      "executive_summary": [
        "Stremio - Freedom to Stream",
        "---",
        "Stremio - Freedom to Stream\nStremio is a modern media center that's a one-stop solution for your video entertainment. You discover, watch and organize video content from easy to install addons.\nBuild\nPrerequisites\nNode.js 12 or higher\npnpm\n10 or higher\nInstall dependencies\npnpm install\nStart development server\npnpm start\nProduction build\npnpm run build\nRun with Docker\ndocker build -t stremio-web\n.\ndocker run -p 8080:8080 stremio-web\nScreenshots\nBoard\nDiscover\nMeta Details\nLicense\nStremio is copyright 2017-2023 Smart code and available under GPLv2 license. See the\nLICENSE\nfile in the project for more information.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 1,799",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 5,573"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/Stremio/stremio-web"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/aandrew-me/ytDownloader",
      "title": "aandrew-me/ytDownloader",
      "date": null,
      "executive_summary": [
        "Desktop App for downloading Videos and Audios from hundreds of sites",
        "---",
        "ytDownloader\nA modern GUI video and audio downloader supporting\nhundreds of sites\nFeatures üöÄ\n‚úÖ Supports hundreds of sites including Youtube, Facebook, Instagram, Tiktok, Twitter and so on.\n‚úÖ Multiple themes\n‚úÖ Video Compressor with Hardware Acceleration\n‚úÖ Advanced options like Range Selection, Subtitles\n‚úÖ Download playlists\n‚úÖ Available on Linux, Windows & macOS\n‚úÖ Fast download speeds\n‚úÖ And of-course no trackers or ads\nScreenshots\nInstallation\nWindows ü™ü\nTraditional way\nDownload and install the exe or msi file. Exe file lets you choose custom download location, msi file doesn't ask for location. Windows defender may show a popup saying\nWindows Protected Your PC\n. Just click on\nMore info\nand click on\nRun Anyway\nChocolatey\nApp can be installed from\nChocolatey\nusing the following command\nchoco install ytdownloader\nScoop\nApp can be installed with\nScoop\nusing the following command\nscoop install https://raw.githubusercontent.com/aandrew-me/ytDownloader/main/ytdownloader.json\nWinget\nApp can be installed with\nWinget\nusing the following command\nwinget install aandrew-me.ytDownloader\nLinux üêß\nLinux has several options available - Flatpak, AppImage and Snap.\nFlatpak is recommended. For arm processors, download from flathub.\nAppImage\nAppImage\nformat is supported on most Linux distros and has Auto-Update support.\nIt just needs to be executed after downloading. See more about\nAppImages here\n.\nAppImageLauncher\nis recommended for integrating AppImages.\nFlatpak\nflatpak install flathub io.github.aandrew_me.ytdn\nSnapcraft\nsudo snap install ytdownloader\nmacOS üçé\nSince the app is not signed, when you will try to open the app, macOS will not allow you to open it.\nYou need to open terminal and execute:\nsudo xattr -r -d com.apple.quarantine /Applications/YTDownloader.app\nYou will also need to install\nyt-dlp\nwith\nhomebrew\nbrew install yt-dlp\nInternationalization (Localization) üåç\nTranslations into other languages would be highly appreciated. If you want to help translating the app to other languages, you can join from\nhere\n. Open a new issue and that language will be added to Crowdin. Please don't make pull requests with json files, instead use Crowdin.\n‚úÖ Available languages\nName\nStatus\nArabic\n‚úîÔ∏è\nEnglish\n‚úîÔ∏è\nSimplified Chinese\n‚úîÔ∏è\nFinnish\n‚úîÔ∏è\nFrench\n‚úîÔ∏è\nGerman\n‚úîÔ∏è\nGreek\n‚úîÔ∏è\nHungarian\n‚úîÔ∏è\nItalian\n‚úîÔ∏è\nJapanese\n‚úîÔ∏è\nPersian\n‚úîÔ∏è\nPolish\n‚úîÔ∏è\nPortuguese (Brazil)\n‚úîÔ∏è\nRussian\n‚úîÔ∏è\nSpanish\n‚úîÔ∏è\nTurkish\n‚úîÔ∏è\nUkrainian\n‚úîÔ∏è\nVietnamese\n‚úîÔ∏è\nThanks to\nnxjosephofficial\n,\nLINUX-SAUNA\n,\nProxycon\n,\nalbanobattistella\n,\nTheBlueQuasar\n,\nMrQuerter\n,\nKotoWhiskas\n,\nAndr√©\n,\nhaggen88\n,\nXfedeX\n,\nJok3r\n,\nTitouanReal\n,\nsoredake\n,\nyoi\n,\nHowlingWerewolf\n,\nKum\n,\nMohammed Bakry\n,\nHuang Bingfeng\nand others for helping.\nUsed technologies\nyt-dlp\nElectron\nffmpeg\nnodeJS\nflaticon\nFor building or running from source code\nNodejs\n(along with npm) needs to be installed.\nRequired commands to get started.\ngit clone https://github.com/aandrew-me/ytDownloader.git\ncd ytDownloader\nnpm i\nTo run with\nElectron\n:\nnpm start\nYou need to download ffmpeg and put it in the root directory of the project. If you don't need to build for arm processor, you can download ffmpeg by executing any of the files - linux.sh / mac.sh / windows.sh depending on the platform. Otherwise you need to download ffmpeg from\nhere\nfor windows/linux and from\nhere\nfor mac (not tested)\nTo build for Linux (It will create packages as specified in package.json). The builds are stored in\nrelease\nfolder.\nnpm run linux\nTo build for Windows\nnpm run windows\nTo build for macOS\nnpm run mac\nIf you only want to build for one format, you can do\nnpx electron-builder -l appimage\nIt will just create a linux appimage build.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 612",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 3,894"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/aandrew-me/ytDownloader"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/FlowiseAI/Flowise",
      "title": "FlowiseAI/Flowise",
      "date": null,
      "executive_summary": [
        "Build AI Agents, Visually",
        "---",
        "English |\nÁπÅÈ´î‰∏≠Êñá\n|\nÁÆÄ‰Ωì‰∏≠Êñá\n|\nÊó•Êú¨Ë™û\n|\nÌïúÍµ≠Ïñ¥\nBuild AI Agents, Visually\nüìö Table of Contents\n‚ö° Quick Start\nüê≥ Docker\nüë®‚Äçüíª Developers\nüå± Env Variables\nüìñ Documentation\nüåê Self Host\n‚òÅÔ∏è Flowise Cloud\nüôã Support\nüôå Contributing\nüìÑ License\n‚ö°Quick Start\nDownload and Install\nNodeJS\n>= 18.15.0\nInstall Flowise\nnpm install -g flowise\nStart Flowise\nnpx flowise start\nOpen\nhttp://localhost:3000\nüê≥ Docker\nDocker Compose\nClone the Flowise project\nGo to\ndocker\nfolder at the root of the project\nCopy\n.env.example\nfile, paste it into the same location, and rename to\n.env\nfile\ndocker compose up -d\nOpen\nhttp://localhost:3000\nYou can bring the containers down by\ndocker compose stop\nDocker Image\nBuild the image locally:\ndocker build --no-cache -t flowise\n.\nRun image:\ndocker run -d --name flowise -p 3000:3000 flowise\nStop image:\ndocker stop flowise\nüë®‚Äçüíª Developers\nFlowise has 3 different modules in a single mono repository.\nserver\n: Node backend to serve API logics\nui\n: React frontend\ncomponents\n: Third-party nodes integrations\napi-documentation\n: Auto-generated swagger-ui API docs from express\nPrerequisite\nInstall\nPNPM\nnpm i -g pnpm\nSetup\nClone the repository:\ngit clone https://github.com/FlowiseAI/Flowise.git\nGo into repository folder:\ncd\nFlowise\nInstall all dependencies of all modules:\npnpm install\nBuild all the code:\npnpm build\nExit code 134 (JavaScript heap out of memory)\nIf you get this error when running the above `build` script, try increasing the Node.js heap size and run the script again:\n#\nmacOS / Linux / Git Bash\nexport\nNODE_OPTIONS=\n\"\n--max-old-space-size=4096\n\"\n#\nWindows PowerShell\n$env\n:NODE_OPTIONS=\n\"\n--max-old-space-size=4096\n\"\n#\nWindows CMD\nset\nNODE_OPTIONS=--max-old-space-size=4096\nThen run:\npnpm build\nStart the app:\npnpm start\nYou can now access the app on\nhttp://localhost:3000\nFor development build:\nCreate\n.env\nfile and specify the\nVITE_PORT\n(refer to\n.env.example\n) in\npackages/ui\nCreate\n.env\nfile and specify the\nPORT\n(refer to\n.env.example\n) in\npackages/server\nRun:\npnpm dev\nAny code changes will reload the app automatically on\nhttp://localhost:8080\nüå± Env Variables\nFlowise supports different environment variables to configure your instance. You can specify the following variables in the\n.env\nfile inside\npackages/server\nfolder. Read\nmore\nüìñ Documentation\nYou can view the Flowise Docs\nhere\nüåê Self Host\nDeploy Flowise self-hosted in your existing infrastructure, we support various\ndeployments\nAWS\nAzure\nDigital Ocean\nGCP\nAlibaba Cloud\nOthers\nRailway\nRender\nHuggingFace Spaces\nElestio\nSealos\nRepoCloud\n‚òÅÔ∏è Flowise Cloud\nGet Started with\nFlowise Cloud\n.\nüôã Support\nFeel free to ask any questions, raise problems, and request new features in\nDiscussion\n.\nüôå Contributing\nThanks go to these awesome contributors\nSee\nContributing Guide\n. Reach out to us at\nDiscord\nif you have any questions or issues.\nüìÑ License\nSource code in this repository is made available under the\nApache License Version 2.0\n.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 366",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 44,832"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/FlowiseAI/Flowise"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/BeehiveInnovations/zen-mcp-server",
      "title": "BeehiveInnovations/zen-mcp-server",
      "date": null,
      "executive_summary": [
        "The power of Claude Code / GeminiCLI / CodexCLI + [Gemini / OpenAI / OpenRouter / Azure / Grok / Ollama / Custom Model / All Of The Above] working as one.",
        "---",
        "Zen MCP: Many Workflows. One Context.\nZen_CLink_web.mp4\nüëâ\nWatch more examples\nYour CLI + Multiple Models = Your AI Dev Team\nUse the ü§ñ CLI you love:\nClaude Code\n¬∑\nGemini CLI\n¬∑\nCodex CLI\n¬∑\nQwen Code CLI\n¬∑\nCursor\n¬∑\nand more\nWith multiple models within a single prompt:\nGemini ¬∑ OpenAI ¬∑ Anthropic ¬∑ Grok ¬∑ Azure ¬∑ Ollama ¬∑ OpenRouter ¬∑ DIAL ¬∑ On-Device Model\nüÜï Now with CLI-to-CLI Bridge\nThe new\nclink\n(CLI + Link) tool connects external AI CLIs directly into your workflow:\nConnect external CLIs\nlike\nGemini CLI\n,\nCodex CLI\n, and\nClaude Code\ndirectly into your workflow\nCLI Subagents\n- Launch isolated CLI instances from\nwithin\nyour current CLI! Claude Code can spawn Codex subagents, Codex can spawn Gemini CLI subagents, etc. Offload heavy tasks (code reviews, bug hunting) to fresh contexts while your main session's context window remains unpolluted. Each subagent returns only final results.\nContext Isolation\n- Run separate investigations without polluting your primary workspace\nRole Specialization\n- Spawn\nplanner\n,\ncodereviewer\n, or custom role agents with specialized system prompts\nFull CLI Capabilities\n- Web search, file inspection, MCP tool access, latest documentation lookups\nSeamless Continuity\n- Sub-CLIs participate as first-class members with full conversation context between tools\n#\nCodex spawns Codex subagent for isolated code review in fresh context\nclink with codex codereviewer to audit auth module\nfor\nsecurity issues\n#\nSubagent reviews in isolation, returns final report without cluttering your context as codex reads each file and walks the directory structure\n#\nConsensus from different AI models ‚Üí Implementation handoff with full context preservation between tools\nUse consensus with gpt-5 and gemini-pro to decide: dark mode or offline support next\nContinue with clink gemini - implement the recommended feature\n#\nGemini receives full debate context and starts coding immediately\nüëâ\nLearn more about clink\nWhy Zen MCP?\nWhy rely on one AI model when you can orchestrate them all?\nA Model Context Protocol server that supercharges tools like\nClaude Code\n,\nCodex CLI\n, and IDE clients such\nas\nCursor\nor the\nClaude Dev VS Code extension\n.\nZen MCP connects your favorite AI tool\nto multiple AI models\nfor enhanced code analysis, problem-solving, and collaborative development.\nTrue AI Collaboration with Conversation Continuity\nZen supports\nconversation threading\nso your CLI can\ndiscuss ideas with multiple AI models, exchange reasoning, get second opinions, and even run collaborative debates between models\nto help you reach deeper insights and better solutions.\nYour CLI always stays in control but gets perspectives from the best AI for each subtask. Context carries forward seamlessly across tools and models, enabling complex workflows like: code reviews with multiple models ‚Üí automated planning ‚Üí implementation ‚Üí pre-commit validation.\nYou're in control.\nYour CLI of choice orchestrates the AI team, but you decide the workflow. Craft powerful prompts that bring in Gemini Pro, GPT 5, Flash, or local offline models exactly when needed.\nReasons to Use Zen MCP\nA typical workflow with Claude Code as an example:\nMulti-Model Orchestration\n- Claude coordinates with Gemini Pro, O3, GPT-5, and 50+ other models to get the best analysis for each task\nContext Revival Magic\n- Even after Claude's context resets, continue conversations seamlessly by having other models \"remind\" Claude of the discussion\nGuided Workflows\n- Enforces systematic investigation phases that prevent rushed analysis and ensure thorough code examination\nExtended Context Windows\n- Break Claude's limits by delegating to Gemini (1M tokens) or O3 (200K tokens) for massive codebases\nTrue Conversation Continuity\n- Full context flows across tools and models - Gemini remembers what O3 said 10 steps ago\nModel-Specific Strengths\n- Extended thinking with Gemini Pro, blazing speed with Flash, strong reasoning with O3, privacy with local Ollama\nProfessional Code Reviews\n- Multi-pass analysis with severity levels, actionable feedback, and consensus from multiple AI experts\nSmart Debugging Assistant\n- Systematic root cause analysis with hypothesis tracking and confidence levels\nAutomatic Model Selection\n- Claude intelligently picks the right model for each subtask (or you can specify)\nVision Capabilities\n- Analyze screenshots, diagrams, and visual content with vision-enabled models\nLocal Model Support\n- Run Llama, Mistral, or other models locally for complete privacy and zero API costs\nBypass MCP Token Limits\n- Automatically works around MCP's 25K limit for large prompts and responses\nThe Killer Feature:\nWhen Claude's context resets, just ask to \"continue with O3\" - the other model's response magically revives Claude's understanding without re-ingesting documents!\nExample: Multi-Model Code Review Workflow\nPerform a codereview using gemini pro and o3 and use planner to generate a detailed plan, implement the fixes and do a final precommit check by continuing from the previous codereview\nThis triggers a\ncodereview\nworkflow where Claude walks the code, looking for all kinds of issues\nAfter multiple passes, collects relevant code and makes note of issues along the way\nMaintains a\nconfidence\nlevel between\nexploring\n,\nlow\n,\nmedium\n,\nhigh\nand\ncertain\nto track how confidently it's been able to find and identify issues\nGenerates a detailed list of critical -> low issues\nShares the relevant files, findings, etc with\nGemini Pro\nto perform a deep dive for a second\ncodereview\nComes back with a response and next does the same with o3, adding to the prompt if a new discovery comes to light\nWhen done, Claude takes in all the feedback and combines a single list of all critical -> low issues, including good patterns in your code. The final list includes new findings or revisions in case Claude misunderstood or missed something crucial and one of the other models pointed this out\nIt then uses the\nplanner\nworkflow to break the work down into simpler steps if a major refactor is required\nClaude then performs the actual work of fixing highlighted issues\nWhen done, Claude returns to Gemini Pro for a\nprecommit\nreview\nAll within a single conversation thread! Gemini Pro in step 11\nknows\nwhat was recommended by O3 in step 7! Taking that context\nand review into consideration to aid with its final pre-commit review.\nThink of it as Claude Code\nfor\nClaude Code.\nThis MCP isn't magic. It's just\nsuper-glue\n.\nRemember:\nClaude stays in full control ‚Äî but\nYOU\ncall the shots.\nZen is designed to have Claude engage other models only when needed ‚Äî and to follow through with meaningful back-and-forth.\nYou're\nthe one who crafts the powerful prompt that makes Claude bring in Gemini, Flash, O3 ‚Äî or fly solo.\nYou're the guide. The prompter. The puppeteer.\nYou are the AI -\nActually Intelligent\n.\nRecommended AI Stack\nFor Claude Code Users\nFor best results when using\nClaude Code\n:\nSonnet 4.5\n- All agentic work and orchestration\nGemini 2.5 Pro\nOR\nGPT-5-Pro\n- Deep thinking, additional code reviews, debugging and validations, pre-commit analysis\nFor Codex Users\nFor best results when using\nCodex CLI\n:\nGPT-5 Codex Medium\n- All agentic work and orchestration\nGemini 2.5 Pro\nOR\nGPT-5-Pro\n- Deep thinking, additional code reviews, debugging and validations, pre-commit analysis\nQuick Start (5 minutes)\nPrerequisites:\nPython 3.10+, Git,\nuv installed\n1. Get API Keys\n(choose one or more):\nOpenRouter\n- Access multiple models with one API\nGemini\n- Google's latest models\nOpenAI\n- O3, GPT-5 series\nAzure OpenAI\n- Enterprise deployments of GPT-4o, GPT-4.1, GPT-5 family\nX.AI\n- Grok models\nDIAL\n- Vendor-agnostic model access\nOllama\n- Local models (free)\n2. Install\n(choose one):\nOption A: Clone and Automatic Setup\n(recommended)\ngit clone https://github.com/BeehiveInnovations/zen-mcp-server.git\ncd\nzen-mcp-server\n#\nHandles everything: setup, config, API keys from system environment.\n#\nAuto-configures Claude Desktop, Claude Code, Gemini CLI, Codex CLI, Qwen CLI\n#\nEnable / disable additional settings in .env\n./run-server.sh\nOption B: Instant Setup with\nuvx\n// Add to ~/.claude/settings.json or .mcp.json\n// Don't forget to add your API keys under env\n{\n\"mcpServers\"\n: {\n\"zen\"\n: {\n\"command\"\n:\n\"\nbash\n\"\n,\n\"args\"\n: [\n\"\n-c\n\"\n,\n\"\nfor p in $(which uvx 2>/dev/null) $HOME/.local/bin/uvx /opt/homebrew/bin/uvx /usr/local/bin/uvx uvx; do [ -x\n\\\"\n$p\n\\\"\n] && exec\n\\\"\n$p\n\\\"\n--from git+https://github.com/BeehiveInnovations/zen-mcp-server.git zen-mcp-server; done; echo 'uvx not found' >&2; exit 1\n\"\n],\n\"env\"\n: {\n\"PATH\"\n:\n\"\n/usr/local/bin:/usr/bin:/bin:/opt/homebrew/bin:~/.local/bin\n\"\n,\n\"GEMINI_API_KEY\"\n:\n\"\nyour-key-here\n\"\n,\n\"DISABLED_TOOLS\"\n:\n\"\nanalyze,refactor,testgen,secaudit,docgen,tracer\n\"\n,\n\"DEFAULT_MODEL\"\n:\n\"\nauto\n\"\n}\n    }\n  }\n}\n3. Start Using!\n\"Use zen to analyze this code for security issues with gemini pro\"\n\"Debug this error with o3 and then get flash to suggest optimizations\"\n\"Plan the migration strategy with zen, get consensus from multiple models\"\n\"clink with cli_name=\\\"gemini\\\" role=\\\"planner\\\" to draft a phased rollout plan\"\nüëâ\nComplete Setup Guide\nwith detailed installation, configuration for Gemini / Codex / Qwen, and troubleshooting\nüëâ\nCursor & VS Code Setup\nfor IDE integration instructions\nüì∫\nWatch tools in action\nto see real-world examples\nProvider Configuration\nZen activates any provider that has credentials in your\n.env\n. See\n.env.example\nfor deeper customization.\nCore Tools\nNote:\nEach tool comes with its own multi-step workflow, parameters, and descriptions that consume valuable context window space even when not in use. To optimize performance, some tools are disabled by default. See\nTool Configuration\nbelow to enable them.\nCollaboration & Planning\n(Enabled by default)\nclink\n- Bridge requests to external AI CLIs (Gemini planner, codereviewer, etc.)\nchat\n- Brainstorm ideas, get second opinions, validate approaches. With capable models (GPT-5 Pro, Gemini 2.5 Pro), generates complete code / implementation\nthinkdeep\n- Extended reasoning, edge case analysis, alternative perspectives\nplanner\n- Break down complex projects into structured, actionable plans\nconsensus\n- Get expert opinions from multiple AI models with stance steering\nCode Analysis & Quality\ndebug\n- Systematic investigation and root cause analysis\nprecommit\n- Validate changes before committing, prevent regressions\ncodereview\n- Professional reviews with severity levels and actionable feedback\nanalyze\n(disabled by default -\nenable\n)\n- Understand architecture, patterns, dependencies across entire codebases\nDevelopment Tools\n(Disabled by default -\nenable\n)\nrefactor\n- Intelligent code refactoring with decomposition focus\ntestgen\n- Comprehensive test generation with edge cases\nsecaudit\n- Security audits with OWASP Top 10 analysis\ndocgen\n- Generate documentation with complexity analysis\nUtilities\napilookup\n- Forces current-year API/SDK documentation lookups in a sub-process (saves tokens within the current context window), prevents outdated training data responses\nchallenge\n- Prevent \"You're absolutely right!\" responses with critical analysis\ntracer\n(disabled by default -\nenable\n)\n- Static analysis prompts for call-flow mapping\nüëâ Tool Configuration\nDefault Configuration\nTo optimize context window usage, only essential tools are enabled by default:\nEnabled by default:\nchat\n,\nthinkdeep\n,\nplanner\n,\nconsensus\n- Core collaboration tools\ncodereview\n,\nprecommit\n,\ndebug\n- Essential code quality tools\napilookup\n- Rapid API/SDK information lookup\nchallenge\n- Critical thinking utility\nDisabled by default:\nanalyze\n,\nrefactor\n,\ntestgen\n,\nsecaudit\n,\ndocgen\n,\ntracer\nEnabling Additional Tools\nTo enable additional tools, remove them from the\nDISABLED_TOOLS\nlist:\nOption 1: Edit your .env file\n#\nDefault configuration (from .env.example)\nDISABLED_TOOLS=analyze,refactor,testgen,secaudit,docgen,tracer\n#\nTo enable specific tools, remove them from the list\n#\nExample: Enable analyze tool\nDISABLED_TOOLS=refactor,testgen,secaudit,docgen,tracer\n#\nTo enable ALL tools\nDISABLED_TOOLS=\nOption 2: Configure in MCP settings\n// In ~/.claude/settings.json or .mcp.json\n{\n\"mcpServers\"\n: {\n\"zen\"\n: {\n\"env\"\n: {\n// Tool configuration\n\"DISABLED_TOOLS\"\n:\n\"\nrefactor,testgen,secaudit,docgen,tracer\n\"\n,\n\"DEFAULT_MODEL\"\n:\n\"\npro\n\"\n,\n\"DEFAULT_THINKING_MODE_THINKDEEP\"\n:\n\"\nhigh\n\"\n,\n// API configuration\n\"GEMINI_API_KEY\"\n:\n\"\nyour-gemini-key\n\"\n,\n\"OPENAI_API_KEY\"\n:\n\"\nyour-openai-key\n\"\n,\n\"OPENROUTER_API_KEY\"\n:\n\"\nyour-openrouter-key\n\"\n,\n// Logging and performance\n\"LOG_LEVEL\"\n:\n\"\nINFO\n\"\n,\n\"CONVERSATION_TIMEOUT_HOURS\"\n:\n\"\n6\n\"\n,\n\"MAX_CONVERSATION_TURNS\"\n:\n\"\n50\n\"\n}\n    }\n  }\n}\nOption 3: Enable all tools\n// Remove or empty the DISABLED_TOOLS to enable everything\n{\n\"mcpServers\"\n: {\n\"zen\"\n: {\n\"env\"\n: {\n\"DISABLED_TOOLS\"\n:\n\"\n\"\n}\n    }\n  }\n}\nNote:\nEssential tools (\nversion\n,\nlistmodels\n) cannot be disabled\nAfter changing tool configuration, restart your Claude session for changes to take effect\nEach tool adds to context window usage, so only enable what you need\nüì∫ Watch Tools In Action\nChat Tool\n- Collaborative decision making and multi-turn conversations\nPicking Redis vs Memcached:\nChat.Redis.or.Memcached_web.webm\nMulti-turn conversation with continuation:\nChat.With.Gemini_web.webm\nConsensus Tool\n- Multi-model debate and decision making\nMulti-model consensus debate:\nZen.Debate_web.webm\nPreCommit Tool\n- Comprehensive change validation\nPre-commit validation workflow:\nAPI Lookup Tool\n- Current vs outdated API documentation\nWithout Zen - outdated APIs:\nAPI_without_zen_web.mp4\nWith Zen - current APIs:\nAPI_with_zen.mp4\nChallenge Tool\n- Critical thinking vs reflexive agreement\nWithout Zen:\nWith Zen:\nKey Features\nAI Orchestration\nAuto model selection\n- Claude picks the right AI for each task\nMulti-model workflows\n- Chain different models in single conversations\nConversation continuity\n- Context preserved across tools and models\nContext revival\n- Continue conversations even after context resets\nModel Support\nMultiple providers\n- Gemini, OpenAI, Azure, X.AI, OpenRouter, DIAL, Ollama\nLatest models\n- GPT-5, Gemini 2.5 Pro, O3, Grok-4, local Llama\nThinking modes\n- Control reasoning depth vs cost\nVision support\n- Analyze images, diagrams, screenshots\nDeveloper Experience\nGuided workflows\n- Systematic investigation prevents rushed analysis\nSmart file handling\n- Auto-expand directories, manage token limits\nWeb search integration\n- Access current documentation and best practices\nLarge prompt support\n- Bypass MCP's 25K token limit\nExample Workflows\nMulti-model Code Review:\n\"Perform a codereview using gemini pro and o3, then use planner to create a fix strategy\"\n‚Üí Claude reviews code systematically ‚Üí Consults Gemini Pro ‚Üí Gets O3's perspective ‚Üí Creates unified action plan\nCollaborative Debugging:\n\"Debug this race condition with max thinking mode, then validate the fix with precommit\"\n‚Üí Deep investigation ‚Üí Expert analysis ‚Üí Solution implementation ‚Üí Pre-commit validation\nArchitecture Planning:\n\"Plan our microservices migration, get consensus from pro and o3 on the approach\"\n‚Üí Structured planning ‚Üí Multiple expert opinions ‚Üí Consensus building ‚Üí Implementation roadmap\nüëâ\nAdvanced Usage Guide\nfor complex workflows, model configuration, and power-user features\nQuick Links\nüìñ Documentation\nDocs Overview\n- High-level map of major guides\nGetting Started\n- Complete setup guide\nTools Reference\n- All tools with examples\nAdvanced Usage\n- Power user features\nConfiguration\n- Environment variables, restrictions\nAdding Providers\n- Provider-specific setup (OpenAI, Azure, custom gateways)\nModel Ranking Guide\n- How intelligence scores drive auto-mode suggestions\nüîß Setup & Support\nWSL Setup\n- Windows users\nTroubleshooting\n- Common issues\nContributing\n- Code standards, PR process\nLicense\nApache 2.0 License - see\nLICENSE\nfile for details.\nAcknowledgments\nBuilt with the power of\nMulti-Model AI\ncollaboration ü§ù\nA\nctual\nI\nntelligence by real Humans\nMCP (Model Context Protocol)\nCodex CLI\nClaude Code\nGemini\nOpenAI\nAzure OpenAI\nStar History",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 329",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 8,339"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/BeehiveInnovations/zen-mcp-server"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/trycua/cua",
      "title": "trycua/cua",
      "date": null,
      "executive_summary": [
        "Open-source infrastructure for Computer-Use Agents. Sandboxes, SDKs, and benchmarks to train and evaluate AI agents that can control full desktops (macOS, Linux, Windows).",
        "---",
        "We‚Äôre hosting the\nComputer-Use Agents SOTA Challenge\nat\nHack the North\nand online!\nTrack A (On-site @ UWaterloo)\n: Reserved for participants accepted to Hack the North. üèÜ Prize:\nYC interview guaranteed\n.\nTrack B (Remote)\n: Open to everyone worldwide. üèÜ Prize:\nCash award\n.\nüëâ Sign up here:\ntrycua.com/hackathon\ncua\n(\"koo-ah\") is Docker for\nComputer-Use Agents\n- it enables AI agents to control full operating systems in virtual containers and deploy them locally or to the cloud.\nvibe-photoshop.mp4\nWith the Computer SDK, you can:\nautomate Windows, Linux, and macOS VMs with a consistent,\npyautogui-like API\ncreate & manage VMs\nlocally\nor using\ncua cloud\nWith the Agent SDK, you can:\nrun computer-use models with a\nconsistent schema\nbenchmark on OSWorld-Verified, SheetBench-V2, and more\nwith a single line of code using HUD\n(\nNotebook\n)\ncombine UI grounding models with any LLM using\ncomposed agents\nuse new UI agent models and UI grounding models from the Model Zoo below with just a model string (e.g.,\nComputerAgent(model=\"openai/computer-use-preview\")\n)\nuse API or local inference by changing a prefix (e.g.,\nopenai/\n,\nopenrouter/\n,\nollama/\n,\nhuggingface-local/\n,\nmlx/\n,\netc.\n)\nCUA Model Zoo üê®\nAll-in-one CUAs\nUI Grounding Models\nUI Planning Models\nanthropic/claude-sonnet-4-5-20250929\nhuggingface-local/xlangai/OpenCUA-{7B,32B}\nany all-in-one CUA\nopenai/computer-use-preview\nhuggingface-local/HelloKKMe/GTA1-{7B,32B,72B}\nany VLM (using liteLLM, requires\ntools\nparameter)\nopenrouter/z-ai/glm-4.5v\nhuggingface-local/Hcompany/Holo1.5-{3B,7B,72B}\nany LLM (using liteLLM, requires\nmoondream3+\nprefix )\nhuggingface-local/OpenGVLab/InternVL3_5-{1B,2B,4B,8B,...}\nany all-in-one CUA\nhuggingface-local/ByteDance-Seed/UI-TARS-1.5-7B\nmoondream3+{ui planning}\n(supports text-only models)\nomniparser+{ui planning}\n{ui grounding}+{ui planning}\nhuman/human\n‚Üí\nHuman-in-the-Loop\nMissing a model?\nRaise a feature request\nor\ncontribute\n!\nQuick Start\nGet started with a Computer-Use Agent UI\nGet started with the Computer-Use Agent CLI\nGet started with the Python SDKs\nUsage (\nDocs\n)\npip install cua-agent[all]\nfrom\nagent\nimport\nComputerAgent\nagent\n=\nComputerAgent\n(\nmodel\n=\n\"anthropic/claude-3-5-sonnet-20241022\"\n,\ntools\n=\n[\ncomputer\n],\nmax_trajectory_budget\n=\n5.0\n)\nmessages\n=\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Take a screenshot and tell me what you see\"\n}]\nasync\nfor\nresult\nin\nagent\n.\nrun\n(\nmessages\n):\nfor\nitem\nin\nresult\n[\n\"output\"\n]:\nif\nitem\n[\n\"type\"\n]\n==\n\"message\"\n:\nprint\n(\nitem\n[\n\"content\"\n][\n0\n][\n\"text\"\n])\nOutput format (OpenAI Agent Responses Format):\n{\n\"output\"\n: [\n# user input\n{\n\"role\"\n:\n\"\nuser\n\"\n,\n\"content\"\n:\n\"\ngo to trycua on gh\n\"\n},\n# first agent turn adds the model output to the history\n{\n\"summary\"\n: [\n            {\n\"text\"\n:\n\"\nSearching Firefox for Trycua GitHub\n\"\n,\n\"type\"\n:\n\"\nsummary_text\n\"\n}\n        ],\n\"type\"\n:\n\"\nreasoning\n\"\n},\n    {\n\"action\"\n: {\n\"text\"\n:\n\"\nTrycua GitHub\n\"\n,\n\"type\"\n:\n\"\ntype\n\"\n},\n\"call_id\"\n:\n\"\ncall_QI6OsYkXxl6Ww1KvyJc4LKKq\n\"\n,\n\"status\"\n:\n\"\ncompleted\n\"\n,\n\"type\"\n:\n\"\ncomputer_call\n\"\n},\n# second agent turn adds the computer output to the history\n{\n\"type\"\n:\n\"\ncomputer_call_output\n\"\n,\n\"call_id\"\n:\n\"\ncall_QI6OsYkXxl6Ww1KvyJc4LKKq\n\"\n,\n\"output\"\n: {\n\"type\"\n:\n\"\ninput_image\n\"\n,\n\"image_url\"\n:\n\"\ndata:image/png;base64,...\n\"\n}\n    },\n# final agent turn adds the agent output text to the history\n{\n\"type\"\n:\n\"\nmessage\n\"\n,\n\"role\"\n:\n\"\nassistant\n\"\n,\n\"content\"\n: [\n          {\n\"text\"\n:\n\"\nSuccess! The Trycua GitHub page has been opened.\n\"\n,\n\"type\"\n:\n\"\noutput_text\n\"\n}\n        ]\n    }\n  ],\n\"usage\"\n: {\n\"prompt_tokens\"\n:\n150\n,\n\"completion_tokens\"\n:\n75\n,\n\"total_tokens\"\n:\n225\n,\n\"response_cost\"\n:\n0.01\n,\n  }\n}\nComputer (\nDocs\n)\npip install cua-computer[all]\nfrom\ncomputer\nimport\nComputer\nasync\nwith\nComputer\n(\nos_type\n=\n\"linux\"\n,\nprovider_type\n=\n\"cloud\"\n,\nname\n=\n\"your-container-name\"\n,\napi_key\n=\n\"your-api-key\"\n)\nas\ncomputer\n:\n# Take screenshot\nscreenshot\n=\nawait\ncomputer\n.\ninterface\n.\nscreenshot\n()\n# Click and type\nawait\ncomputer\n.\ninterface\n.\nleft_click\n(\n100\n,\n100\n)\nawait\ncomputer\n.\ninterface\n.\ntype\n(\n\"Hello!\"\n)\nResources\nHow to use the MCP Server with Claude Desktop or other MCP clients\n- One of the easiest ways to get started with Cua\nHow to use OpenAI Computer-Use, Anthropic, OmniParser, or UI-TARS for your Computer-Use Agent\nHow to use Lume CLI for managing desktops\nTraining Computer-Use Models: Collecting Human Trajectories with Cua (Part 1)\nModules\nModule\nDescription\nInstallation\nLume\nVM management for macOS/Linux using Apple's Virtualization.Framework\ncurl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh | bash\nLumier\nDocker interface for macOS and Linux VMs\ndocker pull trycua/lumier:latest\nComputer (Python)\nPython Interface for controlling virtual machines\npip install \"cua-computer[all]\"\nComputer (Typescript)\nTypescript Interface for controlling virtual machines\nnpm install @trycua/computer\nAgent\nAI agent framework for automating tasks\npip install \"cua-agent[all]\"\nMCP Server\nMCP server for using CUA with Claude Desktop\npip install cua-mcp-server\nSOM\nSelf-of-Mark library for Agent\npip install cua-som\nComputer Server\nServer component for Computer\npip install cua-computer-server\nCore (Python)\nPython Core utilities\npip install cua-core\nCore (Typescript)\nTypescript Core utilities\nnpm install @trycua/core\nCommunity\nJoin our\nDiscord community\nto discuss ideas, get assistance, or share your demos!\nLicense\nCua is open-sourced under the MIT License - see the\nLICENSE\nfile for details.\nPortions of this project, specifically components adapted from Kasm Technologies Inc., are also licensed under the MIT License. See\nlibs/kasm/LICENSE\nfor details.\nMicrosoft's OmniParser, which is used in this project, is licensed under the Creative Commons Attribution 4.0 International License (CC-BY-4.0). See the\nOmniParser LICENSE\nfor details.\nThird-Party Licenses and Optional Components\nSome optional extras for this project depend on third-party packages that are licensed under terms different from the MIT License.\nThe optional \"omni\" extra (installed via\npip install \"cua-agent[omni]\"\n) installs the\ncua-som\nmodule, which includes\nultralytics\nand is licensed under the AGPL-3.0.\nWhen you choose to install and use such optional extras, your use, modification, and distribution of those third-party components are governed by their respective licenses (e.g., AGPL-3.0 for\nultralytics\n).\nContributing\nWe welcome contributions to Cua! Please refer to our\nContributing Guidelines\nfor details.\nTrademarks\nApple, macOS, and Apple Silicon are trademarks of Apple Inc.\nUbuntu and Canonical are registered trademarks of Canonical Ltd.\nMicrosoft is a registered trademark of Microsoft Corporation.\nThis project is not affiliated with, endorsed by, or sponsored by Apple Inc., Canonical Ltd., Microsoft Corporation, or Kasm Technologies.\nStargazers\nThank you to all our supporters!",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 310",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 10,369"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/trycua/cua"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/TapXWorld/ChinaTextbook",
      "title": "TapXWorld/ChinaTextbook",
      "date": null,
      "executive_summary": [
        "ÊâÄÊúâÂ∞èÂàùÈ´ò„ÄÅÂ§ßÂ≠¶PDFÊïôÊùê„ÄÇ",
        "---",
        "È°πÁõÆÁöÑÁî±Êù•\nËôΩÁÑ∂ÂõΩÂÜÖÊïôËÇ≤ÁΩëÁ´ôÂ∑≤Êèê‰æõÂÖçË¥πËµÑÊ∫êÔºå‰ΩÜÂ§ßÂ§öÊï∞ÊôÆÈÄö‰∫∫Ëé∑Âèñ‰ø°ÊÅØÁöÑÈÄîÂæÑ‰æùÁÑ∂ÂèóÈôê„ÄÇÊúâ‰∫õ‰∫∫Âà©Áî®Ëøô‰∏ÄÁÇπÔºåÂú®ÊüêÁ´ô‰∏äÈîÄÂîÆËøô‰∫õÂ∏¶ÊúâÁßÅ‰∫∫Ê∞¥Âç∞ÁöÑËµÑÊ∫ê„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøôÁßçÊÉÖÂÜµÔºåÊàëËÆ°ÂàíÂ∞ÜËøô‰∫õËµÑÊ∫êÈõÜ‰∏≠Âπ∂ÂºÄÊ∫êÔºå‰ª•‰øÉËøõ‰πâÂä°ÊïôËÇ≤ÁöÑÊôÆÂèäÂíåÊ∂àÈô§Âú∞Âå∫Èó¥ÁöÑÊïôËÇ≤Ë¥´Âõ∞„ÄÇ\nËøòÊúâ‰∏Ä‰∏™ÊúÄÈáçË¶ÅÁöÑÂéüÂõ†ÊòØÔºåÂ∏åÊúõÊµ∑Â§ñÂçé‰∫∫ËÉΩÂ§üËÆ©Ëá™Â∑±ÁöÑÂ≠©Â≠êÁªßÁª≠‰∫ÜËß£ÂõΩÂÜÖÊïôËÇ≤„ÄÇ\nÂ≠¶‰π†Êï∞Â≠¶\nÂ∏åÊúõÊú™Êù•Âá∫Áé∞Êõ¥Â§ö‰∏çÊòØ‰∏∫‰∫ÜËÄÉÂ≠¶ËÄåËØª‰π¶ÁöÑ‰∫∫„ÄÇ\nÂ∞èÂ≠¶Êï∞Â≠¶\n‰∏ÄÂπ¥Á∫ß‰∏äÂÜå\n‰∏ÄÂπ¥Á∫ß‰∏ãÂÜå\n‰∫åÂπ¥Á∫ß‰∏äÂÜå\n‰∫åÂπ¥Á∫ß‰∏ãÂÜå\n‰∏âÂπ¥Á∫ß‰∏äÂÜå\n‰∏âÂπ¥Á∫ß‰∏ãÂÜå\nÂõõÂπ¥Á∫ß‰∏äÂÜå\nÂõõÂπ¥Á∫ß‰∏ãÂÜå\n‰∫îÂπ¥Á∫ß‰∏äÂÜå\n‰∫îÂπ¥Á∫ß‰∏ãÂÜå\nÂÖ≠Âπ¥Á∫ß‰∏äÂÜå\nÂÖ≠Âπ¥Á∫ß‰∏ãÂÜå\nÂàù‰∏≠Êï∞Â≠¶\nÂàù‰∏Ä‰∏äÂÜå\nÂàù‰∏Ä‰∏ãÂÜå\nÂàù‰∫å‰∏äÂÜå\nÂàù‰∫å‰∏ãÂÜå\nÂàù‰∏â‰∏äÂÜå\nÂàù‰∏â‰∏ãÂÜå\nÈ´ò‰∏≠Êï∞Â≠¶\nÁõÆÂΩï\nÂ§ßÂ≠¶Êï∞Â≠¶\nÈ´òÁ≠âÊï∞Â≠¶\nÁ∫øÊÄß‰ª£Êï∞\nÁ¶ªÊï£Êï∞Â≠¶\nÊ¶ÇÁéáËÆ∫\nÊõ¥Â§öÊï∞Â≠¶ËµÑÊñô-(Â§ßÂ≠¶Êï∞Â≠¶ÁΩë)\nÈóÆÈ¢òÔºöÂ¶Ç‰ΩïÂêàÂπ∂Ë¢´ÊãÜÂàÜÁöÑÊñá‰ª∂Ôºü\nÁî±‰∫é GitHub ÂØπÂçï‰∏™Êñá‰ª∂ÁöÑ‰∏ä‰º†ÊúâÊúÄÂ§ßÈôêÂà∂ÔºåË∂ÖËøá 100MB ÁöÑÊñá‰ª∂‰ºöË¢´ÊãíÁªù‰∏ä‰º†ÔºåË∂ÖËøá 50MB ÁöÑÊñá‰ª∂‰∏ä‰º†Êó∂‰ºöÊî∂Âà∞Ë≠¶Âëä„ÄÇÂõ†Ê≠§ÔºåÊñá‰ª∂Â§ßÂ∞èË∂ÖËøá 50MB ÁöÑÊñá‰ª∂‰ºöË¢´ÊãÜÂàÜÊàêÊØè‰∏™ 35MB ÁöÑÂ§ö‰∏™Êñá‰ª∂„ÄÇ\nÁ§∫‰æã\nÊñá‰ª∂Ë¢´ÊãÜÂàÜÁöÑÁ§∫‰æãÔºö\n‰πâÂä°ÊïôËÇ≤ÊïôÁßë‰π¶ ¬∑ Êï∞Â≠¶‰∏ÄÂπ¥Á∫ß‰∏äÂÜå.pdf.1\n‰πâÂä°ÊïôËÇ≤ÊïôÁßë‰π¶ ¬∑ Êï∞Â≠¶‰∏ÄÂπ¥Á∫ß‰∏äÂÜå.pdf.2\nËß£ÂÜ≥ÂäûÊ≥ï\nË¶ÅÂêàÂπ∂Ëøô‰∫õË¢´ÊãÜÂàÜÁöÑÊñá‰ª∂ÔºåÊÇ®Âè™ÈúÄÊâßË°å‰ª•‰∏ãÊ≠•È™§(ÂÖ∂‰ªñÊìç‰ΩúÁ≥ªÁªüÂêåÁêÜ)Ôºö\nÂ∞ÜÂêàÂπ∂Á®ãÂ∫è\nmergePDFs-windows-amd64.exe\n‰∏ãËΩΩÂà∞ÂåÖÂê´ PDF Êñá‰ª∂ÁöÑÊñá‰ª∂Â§π‰∏≠„ÄÇ\nÁ°Æ‰øù\nmergePDFs-windows-amd64.exe\nÂíåË¢´ÊãÜÂàÜÁöÑ PDF Êñá‰ª∂Âú®Âêå‰∏ÄÁõÆÂΩï‰∏ã„ÄÇ\nÂèåÂáª\nmergePDFs-windows-amd64.exe\nÁ®ãÂ∫èÂç≥ÂèØËá™Âä®ÂÆåÊàêÊñá‰ª∂ÂêàÂπ∂„ÄÇ\n‰∏ãËΩΩÊñπÂºè\nÊÇ®ÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÈìæÊé•Ôºå‰∏ãËΩΩÊñá‰ª∂ÂêàÂπ∂Á®ãÂ∫èÔºö\n‰∏ãËΩΩÊñá‰ª∂ÂêàÂπ∂Á®ãÂ∫è\nÊñá‰ª∂ÂíåÁ®ãÂ∫èÁ§∫‰æã\nmergePDFs-windows-amd64.exe\n‰πâÂä°ÊïôËÇ≤ÊïôÁßë‰π¶ ¬∑ Êï∞Â≠¶‰∏ÄÂπ¥Á∫ß‰∏äÂÜå.pdf.1\n‰πâÂä°ÊïôËÇ≤ÊïôÁßë‰π¶ ¬∑ Êï∞Â≠¶‰∏ÄÂπ¥Á∫ß‰∏äÂÜå.pdf.2\nÈáçÊñ∞‰∏ãËΩΩ\nÂ¶ÇÊûúÊÇ®‰Ωç‰∫éÂÜÖÂú∞ÔºåÂπ∂‰∏îÁΩëÁªú‰∏çÈîôÔºåÊÉ≥ÈáçÊñ∞‰∏ãËΩΩÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî®\ntchMaterial-parser\nÈ°πÁõÆÔºàÈºìÂä±ÂºÄÊ∫êÔºâÔºåËøõË°åÈáçÊñ∞‰∏ãËΩΩ„ÄÇ\nÂ¶ÇÊûúÊÇ®‰Ωç‰∫éÂõΩÂ§ñÔºåÂíåÂÜÖÂú∞ÁΩëÁªúÈÄö‰ø°ÈÄüÂ∫¶ËæÉÊÖ¢ÔºåÂª∫ËÆÆ‰ΩøÁî®Êú¨Â≠òÂÇ®Â∫ìËøõË°åÁ≠æÂá∫„ÄÇ\nÊïôÊùêÊçêÁåÆ\nÂ¶ÇÊûúËøô‰∏™È°πÁõÆÂ∏ÆÂä©ÊÇ®ÂÖçË¥πËé∑ÂèñÊïôËÇ≤ËµÑÊ∫êÔºåËØ∑ËÄÉËôëÊîØÊåÅÊàë‰ª¨Êé®ÂπøÂºÄÊîæÊïôËÇ≤ÁöÑÂä™ÂäõÔºÅÊÇ®ÁöÑÊçêÁåÆÂ∞ÜÂ∏ÆÂä©Êàë‰ª¨Áª¥Êä§ÂíåÊâ©Â±ïËøô‰∏™ËµÑÊ∫êÂ∫ì„ÄÇ\nÂä†ÂÖ•Êàë‰ª¨ÁöÑ Telegram Á§æÂå∫ÔºåËé∑ÂèñÊúÄÊñ∞Âä®ÊÄÅÂπ∂ÂàÜ‰∫´ÊÇ®ÁöÑÊÉ≥Ê≥ïÔºö\nhttps://t.me/+1V6WjEq8WEM4MDM1\nÊîØÊåÅÊàë\nÂ¶ÇÊûúÊÇ®ËßâÂæóËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåÊÇ®ÂèØ‰ª•Êâ´Êèè‰ª•‰∏ã‰∫åÁª¥Á†ÅËøõË°åÊçêËµ†Ôºö",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 304",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 51,626"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/TapXWorld/ChinaTextbook"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/openai/openai-agents-python",
      "title": "openai/openai-agents-python",
      "date": null,
      "executive_summary": [
        "A lightweight, powerful framework for multi-agent workflows",
        "---",
        "OpenAI Agents SDK\nThe OpenAI Agents SDK is a lightweight yet powerful framework for building multi-agent workflows. It is provider-agnostic, supporting the OpenAI Responses and Chat Completions APIs, as well as 100+ other LLMs.\nNote\nLooking for the JavaScript/TypeScript version? Check out\nAgents SDK JS/TS\n.\nCore concepts:\nAgents\n: LLMs configured with instructions, tools, guardrails, and handoffs\nHandoffs\n: A specialized tool call used by the Agents SDK for transferring control between agents\nGuardrails\n: Configurable safety checks for input and output validation\nSessions\n: Automatic conversation history management across agent runs\nTracing\n: Built-in tracking of agent runs, allowing you to view, debug and optimize your workflows\nExplore the\nexamples\ndirectory to see the SDK in action, and read our\ndocumentation\nfor more details.\nGet started\nTo get started, set up your Python environment (Python 3.9 or newer required), and then install OpenAI Agents SDK package.\nvenv\npython -m venv .venv\nsource\n.venv/bin/activate\n#\nOn Windows: .venv\\Scripts\\activate\npip install openai-agents\nFor voice support, install with the optional\nvoice\ngroup:\npip install 'openai-agents[voice]'\n.\nFor Redis session support, install with the optional\nredis\ngroup:\npip install 'openai-agents[redis]'\n.\nuv\nIf you're familiar with\nuv\n, using the tool would be even similar:\nuv init\nuv add openai-agents\nFor voice support, install with the optional\nvoice\ngroup:\nuv add 'openai-agents[voice]'\n.\nFor Redis session support, install with the optional\nredis\ngroup:\nuv add 'openai-agents[redis]'\n.\nHello world example\nfrom\nagents\nimport\nAgent\n,\nRunner\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n,\ninstructions\n=\n\"You are a helpful assistant\"\n)\nresult\n=\nRunner\n.\nrun_sync\n(\nagent\n,\n\"Write a haiku about recursion in programming.\"\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# Code within the code,\n# Functions calling themselves,\n# Infinite loop's dance.\n(\nIf running this, ensure you set the\nOPENAI_API_KEY\nenvironment variable\n)\n(\nFor Jupyter notebook users, see\nhello_world_jupyter.ipynb\n)\nHandoffs example\nfrom\nagents\nimport\nAgent\n,\nRunner\nimport\nasyncio\nspanish_agent\n=\nAgent\n(\nname\n=\n\"Spanish agent\"\n,\ninstructions\n=\n\"You only speak Spanish.\"\n,\n)\nenglish_agent\n=\nAgent\n(\nname\n=\n\"English agent\"\n,\ninstructions\n=\n\"You only speak English\"\n,\n)\ntriage_agent\n=\nAgent\n(\nname\n=\n\"Triage agent\"\n,\ninstructions\n=\n\"Handoff to the appropriate agent based on the language of the request.\"\n,\nhandoffs\n=\n[\nspanish_agent\n,\nenglish_agent\n],\n)\nasync\ndef\nmain\n():\nresult\n=\nawait\nRunner\n.\nrun\n(\ntriage_agent\n,\ninput\n=\n\"Hola, ¬øc√≥mo est√°s?\"\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# ¬°Hola! Estoy bien, gracias por preguntar. ¬øY t√∫, c√≥mo est√°s?\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nFunctions example\nimport\nasyncio\nfrom\nagents\nimport\nAgent\n,\nRunner\n,\nfunction_tool\n@\nfunction_tool\ndef\nget_weather\n(\ncity\n:\nstr\n)\n->\nstr\n:\nreturn\nf\"The weather in\n{\ncity\n}\nis sunny.\"\nagent\n=\nAgent\n(\nname\n=\n\"Hello world\"\n,\ninstructions\n=\n\"You are a helpful agent.\"\n,\ntools\n=\n[\nget_weather\n],\n)\nasync\ndef\nmain\n():\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\ninput\n=\n\"What's the weather in Tokyo?\"\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# The weather in Tokyo is sunny.\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nThe agent loop\nWhen you call\nRunner.run()\n, we run a loop until we get a final output.\nWe call the LLM, using the model and settings on the agent, and the message history.\nThe LLM returns a response, which may include tool calls.\nIf the response has a final output (see below for more on this), we return it and end the loop.\nIf the response has a handoff, we set the agent to the new agent and go back to step 1.\nWe process the tool calls (if any) and append the tool responses messages. Then we go to step 1.\nThere is a\nmax_turns\nparameter that you can use to limit the number of times the loop executes.\nFinal output\nFinal output is the last thing the agent produces in the loop.\nIf you set an\noutput_type\non the agent, the final output is when the LLM returns something of that type. We use\nstructured outputs\nfor this.\nIf there's no\noutput_type\n(i.e. plain text responses), then the first LLM response without any tool calls or handoffs is considered as the final output.\nAs a result, the mental model for the agent loop is:\nIf the current agent has an\noutput_type\n, the loop runs until the agent produces structured output matching that type.\nIf the current agent does not have an\noutput_type\n, the loop runs until the current agent produces a message without any tool calls/handoffs.\nCommon agent patterns\nThe Agents SDK is designed to be highly flexible, allowing you to model a wide range of LLM workflows including deterministic flows, iterative loops, and more. See examples in\nexamples/agent_patterns\n.\nTracing\nThe Agents SDK automatically traces your agent runs, making it easy to track and debug the behavior of your agents. Tracing is extensible by design, supporting custom spans and a wide variety of external destinations, including\nLogfire\n,\nAgentOps\n,\nBraintrust\n,\nScorecard\n, and\nKeywords AI\n. For more details about how to customize or disable tracing, see\nTracing\n, which also includes a larger list of\nexternal tracing processors\n.\nLong running agents & human-in-the-loop\nYou can use the Agents SDK\nTemporal\nintegration to run durable, long-running workflows, including human-in-the-loop tasks. View a demo of Temporal and the Agents SDK working in action to complete long-running tasks\nin this video\n, and\nview docs here\n.\nSessions\nThe Agents SDK provides built-in session memory to automatically maintain conversation history across multiple agent runs, eliminating the need to manually handle\n.to_input_list()\nbetween turns.\nQuick start\nfrom\nagents\nimport\nAgent\n,\nRunner\n,\nSQLiteSession\n# Create agent\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n,\ninstructions\n=\n\"Reply very concisely.\"\n,\n)\n# Create a session instance\nsession\n=\nSQLiteSession\n(\n\"conversation_123\"\n)\n# First turn\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"What city is the Golden Gate Bridge in?\"\n,\nsession\n=\nsession\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# \"San Francisco\"\n# Second turn - agent automatically remembers previous context\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"What state is it in?\"\n,\nsession\n=\nsession\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# \"California\"\n# Also works with synchronous runner\nresult\n=\nRunner\n.\nrun_sync\n(\nagent\n,\n\"What's the population?\"\n,\nsession\n=\nsession\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# \"Approximately 39 million\"\nSession options\nNo memory\n(default): No session memory when session parameter is omitted\nsession: Session = DatabaseSession(...)\n: Use a Session instance to manage conversation history\nfrom\nagents\nimport\nAgent\n,\nRunner\n,\nSQLiteSession\n# SQLite - file-based or in-memory database\nsession\n=\nSQLiteSession\n(\n\"user_123\"\n,\n\"conversations.db\"\n)\n# Redis - for scalable, distributed deployments\n# from agents.extensions.memory import RedisSession\n# session = RedisSession.from_url(\"user_123\", url=\"redis://localhost:6379/0\")\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n)\n# Different session IDs maintain separate conversation histories\nresult1\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"Hello\"\n,\nsession\n=\nsession\n)\nresult2\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"Hello\"\n,\nsession\n=\nSQLiteSession\n(\n\"user_456\"\n,\n\"conversations.db\"\n)\n)\nCustom session implementations\nYou can implement your own session memory by creating a class that follows the\nSession\nprotocol:\nfrom\nagents\n.\nmemory\nimport\nSession\nfrom\ntyping\nimport\nList\nclass\nMyCustomSession\n:\n\"\"\"Custom session implementation following the Session protocol.\"\"\"\ndef\n__init__\n(\nself\n,\nsession_id\n:\nstr\n):\nself\n.\nsession_id\n=\nsession_id\n# Your initialization here\nasync\ndef\nget_items\n(\nself\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n)\n->\nList\n[\ndict\n]:\n# Retrieve conversation history for the session\npass\nasync\ndef\nadd_items\n(\nself\n,\nitems\n:\nList\n[\ndict\n])\n->\nNone\n:\n# Store new items for the session\npass\nasync\ndef\npop_item\n(\nself\n)\n->\ndict\n|\nNone\n:\n# Remove and return the most recent item from the session\npass\nasync\ndef\nclear_session\n(\nself\n)\n->\nNone\n:\n# Clear all items for the session\npass\n# Use your custom session\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n)\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"Hello\"\n,\nsession\n=\nMyCustomSession\n(\n\"my_session\"\n)\n)\nDevelopment (only needed if you need to edit the SDK/examples)\nEnsure you have\nuv\ninstalled.\nuv --version\nInstall dependencies\nmake sync\n(After making changes) lint/test\nmake check # run tests linter and typechecker\nOr to run them individually:\nmake tests  # run tests\nmake mypy   # run typechecker\nmake lint   # run linter\nmake format-check # run style checker\nAcknowledgements\nWe'd like to acknowledge the excellent work of the open-source community, especially:\nPydantic\n(data validation) and\nPydanticAI\n(advanced agent framework)\nLiteLLM\n(unified interface for 100+ LLMs)\nMkDocs\nGriffe\nuv\nand\nruff\nWe're committed to continuing to build the Agents SDK as an open source framework so others in the community can expand on our approach.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 276",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 15,755"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/openai/openai-agents-python"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/Infisical/infisical",
      "title": "Infisical/infisical",
      "date": null,
      "executive_summary": [
        "Infisical is the open-source platform for secrets management, PKI, and SSH access.",
        "---",
        "The open-source secret management platform\n: Sync secrets/configs across your team/infrastructure and prevent secret leaks.\nSlack\n|\nInfisical Cloud\n|\nSelf-Hosting\n|\nDocs\n|\nWebsite\n|\nHiring (Remote/SF)\nIntroduction\nInfisical\nis the open source secret management platform that teams use to centralize their application configuration and secrets like API keys and database credentials as well as manage their internal PKI.\nWe're on a mission to make security tooling more accessible to everyone, not just security teams, and that means redesigning the entire developer experience from ground up.\nFeatures\nSecrets Management:\nDashboard\n: Manage secrets across projects and environments (e.g. development, production, etc.) through a user-friendly interface.\nNative Integrations\n: Sync secrets to platforms like\nGitHub\n,\nVercel\n,\nAWS\n, and use tools like\nTerraform\n,\nAnsible\n, and more.\nSecret versioning\nand\nPoint-in-Time Recovery\n: Keep track of every secret and project state; roll back when needed.\nSecret Rotation\n: Rotate secrets at regular intervals for services like\nPostgreSQL\n,\nMySQL\n,\nAWS IAM\n, and more.\nDynamic Secrets\n: Generate ephemeral secrets on-demand for services like\nPostgreSQL\n,\nMySQL\n,\nRabbitMQ\n, and more.\nSecret Scanning and Leak Prevention\n: Prevent secrets from leaking to git.\nInfisical Kubernetes Operator\n: Deliver secrets to your Kubernetes workloads and automatically reload deployments.\nInfisical Agent\n: Inject secrets into applications without modifying any code logic.\nInfisical (Internal) PKI:\nPrivate Certificate Authority\n: Create CA hierarchies, configure\ncertificate templates\nfor policy enforcement, and start issuing X.509 certificates.\nCertificate Management\n: Manage the certificate lifecycle from\nissuance\nto\nrevocation\nwith support for CRL.\nAlerting\n: Configure alerting for expiring CA and end-entity certificates.\nInfisical PKI Issuer for Kubernetes\n: Deliver TLS certificates to your Kubernetes workloads with automatic renewal.\nEnrollment over Secure Transport\n: Enroll and manage certificates via EST protocol.\nInfisical Key Management System (KMS):\nCryptographic Keys\n: Centrally manage keys across projects through a user-friendly interface or via the API.\nEncrypt and Decrypt Data\n: Use symmetric keys to encrypt and decrypt data.\nInfisical SSH\nSigned SSH Certificates\n: Issue ephemeral SSH credentials for secure, short-lived, and centralized access to infrastructure.\nGeneral Platform:\nAuthentication Methods\n: Authenticate machine identities with Infisical using a cloud-native or platform agnostic authentication method (\nKubernetes Auth\n,\nGCP Auth\n,\nAzure Auth\n,\nAWS Auth\n,\nOIDC Auth\n,\nUniversal Auth\n).\nAccess Controls\n: Define advanced authorization controls for users and machine identities with\nRBAC\n,\nadditional privileges\n,\ntemporary access\n,\naccess requests\n,\napproval workflows\n, and more.\nAudit logs\n: Track every action taken on the platform.\nSelf-hosting\n: Deploy Infisical on-prem or cloud with ease; keep data on your own infrastructure.\nInfisical SDK\n: Interact with Infisical via client SDKs (\nNode\n,\nPython\n,\nGo\n,\nRuby\n,\nJava\n,\n.NET\n)\nInfisical CLI\n: Interact with Infisical via CLI; useful for injecting secrets into local development and CI/CD pipelines.\nInfisical API\n: Interact with Infisical via API.\nGetting started\nCheck out the\nQuickstart Guides\nUse Infisical Cloud\nDeploy Infisical on premise\nThe fastest and most reliable way to\nget started with Infisical is signing up\nfor free to\nInfisical Cloud\n.\nView all\ndeployment options\nRun Infisical locally\nTo set up and run Infisical locally, make sure you have Git and Docker installed on your system. Then run the command for your system:\nLinux/macOS:\ngit clone https://github.com/Infisical/infisical && cd \"$(basename $_ .git)\" && cp .env.example .env && docker compose -f docker-compose.prod.yml up\nWindows Command Prompt:\ngit clone https://github.com/Infisical/infisical && cd infisical && copy .env.example .env && docker compose -f docker-compose.prod.yml up\nCreate an account at\nhttp://localhost:80\nScan and prevent secret leaks\nOn top managing secrets with Infisical, you can also\nscan for over 140+ secret types\nin your files, directories and git repositories.\nTo scan your full git history, run:\ninfisical scan --verbose\nInstall pre commit hook to scan each commit before you push to your repository\ninfisical scan install --pre-commit-hook\nLearn about Infisical's code scanning feature\nhere\nOpen-source vs. paid\nThis repo available under the\nMIT expat license\n, with the exception of the\nee\ndirectory which will contain premium enterprise features requiring a Infisical license.\nIf you are interested in managed Infisical Cloud of self-hosted Enterprise Offering, take a look at\nour website\nor\nbook a meeting with us\n.\nSecurity\nPlease do not file GitHub issues or post on our public forum for security vulnerabilities, as they are public!\nInfisical takes security issues very seriously. If you have any concerns about Infisical or believe you have uncovered a vulnerability, please get in touch via the e-mail address\nsecurity@infisical.com\n. In the message, try to provide a description of the issue and ideally a way of reproducing it. The security team will get back to you as soon as possible.\nNote that this security address should be used only for undisclosed vulnerabilities. Please report any security problems to us before disclosing it publicly.\nContributing\nWhether it's big or small, we love contributions. Check out our guide to see how to\nget started\n.\nNot sure where to get started? You can:\nJoin our\nSlack\n, and ask us any questions there.\nWe are hiring!\nIf you're reading this, there is a strong chance you like the products we created.\nYou might also make a great addition to our team. We're growing fast and would love for you to\njoin us\n.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 266",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 22,291"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/Infisical/infisical"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/openemr/openemr",
      "title": "openemr/openemr",
      "date": null,
      "executive_summary": [
        "The most popular open source electronic health records and medical practice management solution.",
        "---",
        "OpenEMR\nOpenEMR\nis a Free and Open Source electronic health records and medical practice management application. It features fully integrated electronic health records, practice management, scheduling, electronic billing, internationalization, free support, a vibrant community, and a whole lot more. It runs on Windows, Linux, Mac OS X, and many other platforms.\nContributing\nOpenEMR is a leader in healthcare open source software and comprises a large and diverse community of software developers, medical providers and educators with a very healthy mix of both volunteers and professionals.\nJoin us and learn how to start contributing today!\nAlready comfortable with git? Check out\nCONTRIBUTING.md\nfor quick setup instructions and requirements for contributing to OpenEMR by resolving a bug or adding an awesome feature üòä.\nSupport\nCommunity and Professional support can be found\nhere\n.\nExtensive documentation and forums can be found on the\nOpenEMR website\nthat can help you to become more familiar about the project üìñ.\nReporting Issues and Bugs\nReport these on the\nIssue Tracker\n. If you are unsure if it is an issue/bug, then always feel free to use the\nForum\nand\nChat\nto discuss about the issue ü™≤.\nReporting Security Vulnerabilities\nCheck out\nSECURITY.md\nAPI\nCheck out\nAPI_README.md\nDocker\nCheck out\nDOCKER_README.md\nFHIR\nCheck out\nFHIR_README.md\nFor Developers\nIf using OpenEMR directly from the code repository, then the following commands will build OpenEMR (Node.js version 22.* is required) :\ncomposer install --no-dev\nnpm install\nnpm run build\ncomposer dump-autoload -o\nContributors\nThis project exists thanks to all the people who have contributed.\n[Contribute]\n.\nSponsors\nThanks to our\nONC Certification Major Sponsors\n!\nLicense\nGNU GPL",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 219",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 4,208"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/openemr/openemr"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/openai/codex",
      "title": "openai/codex",
      "date": null,
      "executive_summary": [
        "Lightweight coding agent that runs in your terminal",
        "---",
        "npm i -g @openai/codex\nor\nbrew install codex\nCodex CLI\nis a coding agent from OpenAI that runs locally on your computer.\nIf you want Codex in your code editor (VS Code, Cursor, Windsurf),\ninstall in your IDE\nIf you are looking for the\ncloud-based agent\nfrom OpenAI,\nCodex Web\n, go to\nchatgpt.com/codex\nQuickstart\nInstalling and running Codex CLI\nInstall globally with your preferred package manager. If you use npm:\nnpm install -g @openai/codex\nAlternatively, if you use Homebrew:\nbrew install codex\nThen simply run\ncodex\nto get started:\ncodex\nYou can also go to the\nlatest GitHub Release\nand download the appropriate binary for your platform.\nEach GitHub Release contains many executables, but in practice, you likely want one of these:\nmacOS\nApple Silicon/arm64:\ncodex-aarch64-apple-darwin.tar.gz\nx86_64 (older Mac hardware):\ncodex-x86_64-apple-darwin.tar.gz\nLinux\nx86_64:\ncodex-x86_64-unknown-linux-musl.tar.gz\narm64:\ncodex-aarch64-unknown-linux-musl.tar.gz\nEach archive contains a single entry with the platform baked into the name (e.g.,\ncodex-x86_64-unknown-linux-musl\n), so you likely want to rename it to\ncodex\nafter extracting it.\nUsing Codex with your ChatGPT plan\nRun\ncodex\nand select\nSign in with ChatGPT\n. We recommend signing into your ChatGPT account to use Codex as part of your Plus, Pro, Team, Edu, or Enterprise plan.\nLearn more about what's included in your ChatGPT plan\n.\nYou can also use Codex with an API key, but this requires\nadditional setup\n. If you previously used an API key for usage-based billing, see the\nmigration steps\n. If you're having trouble with login, please comment on\nthis issue\n.\nModel Context Protocol (MCP)\nCodex can access MCP servers. To configure them, refer to the\nconfig docs\n.\nConfiguration\nCodex CLI supports a rich set of configuration options, with preferences stored in\n~/.codex/config.toml\n. For full configuration options, see\nConfiguration\n.\nDocs & FAQ\nGetting started\nCLI usage\nRunning with a prompt as input\nExample prompts\nMemory with AGENTS.md\nConfiguration\nSandbox & approvals\nAuthentication\nAuth methods\nLogin on a \"Headless\" machine\nAutomating Codex\nGitHub Action\nTypeScript SDK\nNon-interactive mode (\ncodex exec\n)\nAdvanced\nTracing / verbose logging\nModel Context Protocol (MCP)\nZero data retention (ZDR)\nContributing\nInstall & build\nSystem Requirements\nDotSlash\nBuild from source\nFAQ\nOpen source fund\nLicense\nThis repository is licensed under the\nApache-2.0 License\n.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 210",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 46,520"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/openai/codex"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/dyad-sh/dyad",
      "title": "dyad-sh/dyad",
      "date": null,
      "executive_summary": [
        "Free, local, open-source AI app builder ‚ú® v0 / lovable / Bolt alternative üåü Star if you like it!",
        "---",
        "Dyad\nDyad is a local, open-source AI app builder. It's fast, private, and fully under your control ‚Äî like Lovable, v0, or Bolt, but running right on your machine.\nMore info at:\nhttp://dyad.sh/\nüöÄ Features\n‚ö°Ô∏è\nLocal\n: Fast, private and no lock-in.\nüõ†\nBring your own keys\n: Use your own AI API keys ‚Äî no vendor lock-in.\nüñ•Ô∏è\nCross-platform\n: Easy to run on Mac or Windows.\nüì¶ Download\nNo sign-up required. Just download and go.\nüëâ Download for your platform\nü§ù Community\nJoin our growing community of AI app builders on\nReddit\n:\nr/dyadbuilders\n- share your projects and get help from the community!\nüõ†Ô∏è Contributing\nDyad\nis open-source (Apache 2.0 licensed).\nIf you're interested in contributing to dyad, please read our\ncontributing\ndoc.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 201",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 15,759"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/dyad-sh/dyad"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/audacity/audacity",
      "title": "audacity/audacity",
      "date": null,
      "executive_summary": [
        "Audio Editor",
        "---",
        "Audacity\nAudacity\nis an easy-to-use, multi-track audio editor and recorder for Windows, macOS, GNU/Linux and other operating systems. More info can be found on\nhttps://www.audacityteam.org\nThis repository is currently undergoing major structural change.\nWe're currently working on Audacity 4, which means an entirely new UI and also refactorings aplenty. As such, the\nmaster\nbranch is currently not particularly friendly to new contributors. It is still possible to submit patches to Audacity 3.x; make sure you branch off\naudacity3\nif you choose to do so. Build instructions for 3.x can be found\nhere\n; build instructions for Audacity 4 can be found\nhere\n.\nYou can stay updated with our efforts on\nYouTube\n,\nDiscord\nand\nour blog\n.\nLicense\nAudacity is open source software licensed GPLv3. Most code files are GPLv2-or-later, with the notable exceptions being\n/au3/lib-src\n(which contains third party libraries), as well as VST3-related code. Documentation is licensed CC-by 3.0 unless otherwise noted. Details can be found in the\nlicense file\n.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 144",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 15,192"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/audacity/audacity"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/is-a-dev/register",
      "title": "is-a-dev/register",
      "date": null,
      "executive_summary": [
        "Grab your own sweet-looking '.is-a.dev' subdomain.",
        "---",
        "is-a.dev\nis-a.dev\nis a service that allows developers to get a sweet-looking\n.is-a.dev\nsubdomain for their personal websites.\nAnnouncements & Status Updates\nPlease join our\nDiscord server\nfor announcements, updates & upgrades, and downtime notifications regarding the service.\nNot all of these will be posted on GitHub\n1\n, however they will always be posted in our Discord server.\nRegister\nIf you want a visual guide, check out\nthis blog post\n.\nFork\nthis repository.\nRead the documentation\n.\nIf you are applying for NS records please read\nthis\n.\nYour pull request will be reviewed and merged.\nKeep an eye on it in case changes are needed!\nAfter the pull request is merged, your DNS records should be published with-in a few minutes.\nEnjoy your new\n.is-a.dev\nsubdomain! Please consider leaving us a star ‚≠êÔ∏è to help support us!\nNS Records\nWhen applying for NS records, please be aware we already support a\nwide range of DNS records\n, so you likely do not need them.\nIn your PR, please explain why you need NS records, including examples, to help mitigate potential abuse. Refer to the\nFAQ\nfor guidelines on allowed usage.\nPull requests adding NS records without sufficient reasoning will be closed.\nAlso see:\nWhy are NS records restricted?\nReport Abuse\nIf you find any subdomains being used for abusive purposes, please report them by\ncreating an issue\nwith the relevant evidence.\nWe are proud to announce that we are supported by Cloudflare's\nProject Alexandria\nsponsorship program. We would not be able to operate without their help! üíñ\nFootnotes\nWe only post announcements on GitHub in the case of a serious incident, which you'll see at the top of this README.\n‚Ü©",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 124",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 8,486"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/is-a-dev/register"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/firefly-iii/firefly-iii",
      "title": "firefly-iii/firefly-iii",
      "date": null,
      "executive_summary": [
        "Firefly III: a personal finances manager",
        "---",
        "Firefly III\nA free and open source personal finance manager\nExplore the documentation\nView the demo\n¬∑\nReport a bug\n¬∑\nRequest a feature\n¬∑\nAsk questions\nAbout Firefly III\nPurpose\nFeatures\nWho's it for?\nThe Firefly III eco-system\nGetting Started\nContributing\nSupport the development of Firefly III\nLicense\nDo you need help, or do you want to get in touch?\nAcknowledgements\nAbout Firefly III\n\"Firefly III\" is a (self-hosted) manager for your personal finances. It can help you keep track of your expenses and income, so you can spend less and save more. Firefly III supports the use of budgets, categories and tags. Using a bunch of external tools, you can import data. It also has many neat financial reports available.\nFirefly III should give you\ninsight\ninto and\ncontrol\nover your finances. Money should be useful, not scary. You should be able to\nsee\nwhere it is going, to\nfeel\nyour expenses and to... wow, I'm going overboard with this aren't I?\nBut you get the idea: this is your money. These are your expenses. Stop them from controlling you. I built this tool because I started to dislike money. Having money, not having money, paying bills with money, you get the idea. But no more. I want to feel \"safe\", whatever my balance is. And I hope this tool can help you. I know it helps me.\nPurpose\nPersonal financial management is pretty difficult, and everybody has their own approach to it. Some people make budgets, other people limit their cashflow by throwing away their credit cards, others try to increase their current cashflow. There are tons of ways to save and earn money. Firefly III works on the principle that if you know where your money is going, you can stop it from going there.\nBy keeping track of your expenses and your income you can budget accordingly and save money. Stop living from paycheck to paycheck but give yourself the financial wiggle room you need.\nYou can read more about the purpose of Firefly III in the\ndocumentation\n.\nFeatures\nFirefly III is pretty feature packed. Some important stuff first:\nIt is completely self-hosted and isolated, and will never contact external servers until you explicitly tell it to.\nIt features a REST JSON API that covers almost every part of Firefly III.\nThe most exciting features are:\nCreate\nrecurring transactions to manage your money\n.\nRule based transaction handling\nwith the ability to create your own rules.\nThen the things that make you go \"yeah OK, makes sense\".\nA\ndouble-entry\nbookkeeping system.\nSave towards a goal using\npiggy banks\n.\nView\nincome and expense reports\n.\nAnd the things you would hope for but not expect:\n2 factor authentication for extra security üîí.\nSupports\nany currency you want\n.\nThere is a\nDocker image\n.\nAnd to organise everything:\nClear views that should show you how you're doing.\nEasy navigation through your records.\nLots of charts because we all love them.\nMany more features are listed in the\ndocumentation\n.\nWho's it for?\nThis application is for people who want to track their finances, keep an eye on their money\nwithout having to upload their financial records to the cloud\n. You're a bit tech-savvy, you like open source software and you don't mind tinkering with (self-hosted) servers.\nThe Firefly III eco-system\nSeveral users have built pretty awesome stuff around the Firefly III API.\nCheck out these tools in the documentation\n.\nGetting Started\nThere are many ways to run Firefly III\nThere is a\ndemo site\nwith an example financial administration already present.\nYou can\ninstall it on your server\n.\nYou can\nrun it using Docker\n.\nYou can\ndeploy via Kubernetes\n.\nYou can\ninstall it using Softaculous\n.\nYou can\ninstall it using AMPPS\n.\nYou can\ninstall it on Cloudron\n.\nYou can\ninstall it on Lando\n.\nYou can\ninstall it on Yunohost\n.\nContributing\nYou can contact me at\njames@firefly-iii.org\n, you may open an issue in the\nmain repository\nor contact me through\ngitter\nand\nMastodon\n.\nOf course, there are some\ncontributing guidelines\nand a\ncode of conduct\n, which I invite you to check out.\nI can always use your help\nsquashing bugs\n, thinking about\nnew features\nor\ntranslating Firefly III\ninto other languages.\nSonarcloud\nscans the code of Firefly III. If you want to help improve Firefly III, check out the latest reports and take your pick!\nThere is also a\nsecurity policy\n.\nSupport the development of Firefly III\nIf you like Firefly III and if it helps you save lots of money, why not send me a dime for every dollar saved! ü•≥\nOK that was a joke. If you feel Firefly III made your life better, please consider contributing as a sponsor. Please check out my\nPatreon\nand\nGitHub Sponsors\npage for more information. You can also\nbuy me a ‚òïÔ∏è coffee at ko-fi.com\n. Thank you for your consideration.\nLicense\nThis work\nis licensed\nunder the\nGNU Affero General Public License v3\n.\nDo you need help, or do you want to get in touch?\nDo you want to contact me? You can email me at\njames@firefly-iii.org\nor get in touch through one of the following support channels:\nGitHub Discussions\nfor questions and support\nGitter.im\nfor a good chat and a quick answer\nGitHub Issues\nfor bugs and issues\nMastodon\nfor news and updates\nAcknowledgements\nOver time,\nmany people have contributed to Firefly III\n. I'm grateful for their support and code contributions.\nThe Firefly III logo is made by the excellent Cherie Woo.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 110",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 20,970"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/firefly-iii/firefly-iii"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/sharkdp/bat",
      "title": "sharkdp/bat",
      "date": null,
      "executive_summary": [
        "A cat(1) clone with wings.",
        "---",
        "A\ncat(1)\nclone with syntax highlighting and Git integration.\nKey Features\n‚Ä¢\nHow To Use\n‚Ä¢\nInstallation\n‚Ä¢\nCustomization\n‚Ä¢\nProject goals, alternatives\n[English]\n  [\n‰∏≠Êñá\n]\n  [\nÊó•Êú¨Ë™û\n]\n  [\nÌïúÍµ≠Ïñ¥\n]\n  [\n–†—É—Å—Å–∫–∏–π\n]\nSponsors\nA special\nthank you\ngoes to our biggest\nsponsors\n:\nWarp, the intelligent terminal\nAvailable on MacOS, Linux, Windows\nGraphite is the AI developer productivity platform helping\nteams on GitHub ship higher quality software, faster\nSyntax highlighting\nbat\nsupports syntax highlighting for a large number of programming and markup\nlanguages:\nGit integration\nbat\ncommunicates with\ngit\nto show modifications with respect to the index\n(see left side bar):\nShow non-printable characters\nYou can use the\n-A\n/\n--show-all\noption to show and highlight non-printable\ncharacters:\nAutomatic paging\nBy default,\nbat\npipes its own output to a pager (e.g.\nless\n) if the output is too large for one screen.\nIf you would rather\nbat\nwork like\ncat\nall the time (never page output), you can set\n--paging=never\nas an option, either on the command line or in your configuration file.\nIf you intend to alias\ncat\nto\nbat\nin your shell configuration, you can use\nalias cat='bat --paging=never'\nto preserve the default behavior.\nFile concatenation\nEven with a pager set, you can still use\nbat\nto concatenate files üòâ.\nWhenever\nbat\ndetects a non-interactive terminal (i.e. when you pipe into another process or into a file),\nbat\nwill act as a drop-in replacement for\ncat\nand fall back to printing the plain file contents, regardless of the\n--pager\noption's value.\nHow to use\nDisplay a single file on the terminal\nbat README.md\nDisplay multiple files at once\nbat src/\n*\n.rs\nRead from stdin, determine the syntax automatically (note, highlighting will\nonly work if the syntax can be determined from the first line of the file,\nusually through a shebang such as\n#!/bin/sh\n)\ncurl -s https://sh.rustup.rs\n|\nbat\nRead from stdin, specify the language explicitly\nyaml2json .travis.yml\n|\njson_pp\n|\nbat -l json\nShow and highlight non-printable characters:\nbat -A /etc/hosts\nUse it as a\ncat\nreplacement:\nbat\n>\nnote.md\n#\nquickly create a new file\nbat header.md content.md footer.md\n>\ndocument.md\n\nbat -n main.rs\n#\nshow line numbers (only)\nbat f - g\n#\noutput 'f', then stdin, then 'g'.\nIntegration with other tools\nfzf\nYou can use\nbat\nas a previewer for\nfzf\n. To do this,\nuse\nbat\n's\n--color=always\noption to force colorized output. You can also use\n--line-range\noption to restrict the load times for long files:\nfzf --preview\n\"\nbat --color=always --style=numbers --line-range=:500 {}\n\"\nFor more information, see\nfzf\n's\nREADME\n.\nfind\nor\nfd\nYou can use the\n-exec\noption of\nfind\nto preview all search results with\nbat\n:\nfind ‚Ä¶ -exec bat {} +\nIf you happen to use\nfd\n, you can use the\n-X\n/\n--exec-batch\noption to do the same:\nfd ‚Ä¶ -X bat\nripgrep\nWith\nbatgrep\n,\nbat\ncan be used as the printer for\nripgrep\nsearch results.\nbatgrep needle src/\ntail -f\nbat\ncan be combined with\ntail -f\nto continuously monitor a given file with syntax highlighting.\ntail -f /var/log/pacman.log\n|\nbat --paging=never -l log\nNote that we have to switch off paging in order for this to work. We have also specified the syntax\nexplicitly (\n-l log\n), as it can not be auto-detected in this case.\ngit\nYou can combine\nbat\nwith\ngit show\nto view an older version of a given file with proper syntax\nhighlighting:\ngit show v0.6.0:src/main.rs\n|\nbat -l rs\ngit diff\nYou can combine\nbat\nwith\ngit diff\nto view lines around code changes with proper syntax\nhighlighting:\nbatdiff\n() {\n    git diff --name-only --relative --diff-filter=d -z\n|\nxargs -0 bat --diff\n}\nIf you prefer to use this as a separate tool, check out\nbatdiff\nin\nbat-extras\n.\nIf you are looking for more support for git and diff operations, check out\ndelta\n.\nxclip\nThe line numbers and Git modification markers in the output of\nbat\ncan make it hard to copy\nthe contents of a file. To prevent this, you can call\nbat\nwith the\n-p\n/\n--plain\noption or\nsimply pipe the output into\nxclip\n:\nbat main.cpp\n|\nxclip\nbat\nwill detect that the output is being redirected and print the plain file contents.\nman\nbat\ncan be used as a colorizing pager for\nman\n, by setting the\nMANPAGER\nenvironment variable:\nexport\nMANPAGER=\n\"\nsh -c 'awk '\\''{ gsub(/\\x1B\\[[0-9;]*m/,\n\\\"\\\"\n,\n\\$\n0); gsub(/.\\x08/,\n\\\"\\\"\n,\n\\$\n0); print }'\\'' | bat -p -lman'\n\"\nman 2\nselect\n(replace\nbat\nwith\nbatcat\nif you are on Debian or Ubuntu)\nIf you prefer to have this bundled in a new command, you can also use\nbatman\n.\nWarning\nThis will\nnot work\nout of the box with Mandoc's\nman\nimplementation.\nPlease either use\nbatman\n, or convert the shell script to a\nshebang executable\nand point\nMANPAGER\nto that.\nNote that the\nManpage syntax\nis developed in this repository and still needs some work.\nprettier\n/\nshfmt\n/\nrustfmt\nThe\nprettybat\nscript is a wrapper that will format code and print it with\nbat\n.\nWarp\nHighlighting\n--help\nmessages\nYou can use\nbat\nto colorize help text:\n$ cp --help | bat -plhelp\nYou can also use a wrapper around this:\n#\nin your .bashrc/.zshrc/*rc\nalias\nbathelp=\n'\nbat --plain --language=help\n'\nhelp\n() {\n\"\n$@\n\"\n--help\n2>&1\n|\nbathelp\n}\nThen you can do\n$ help cp\nor\n$ help git commit\n.\nWhen you are using\nzsh\n, you can also use global aliases to override\n-h\nand\n--help\nentirely:\nalias\n-g -- -h=\n'\n-h 2>&1 | bat --language=help --style=plain\n'\nalias\n-g -- --help=\n'\n--help 2>&1 | bat --language=help --style=plain\n'\nFor\nfish\n, you can use abbreviations:\nabbr\n-a\n--position\nanywhere --\n--help\n'\n--help | bat -plhelp\n'\nabbr\n-a\n--position\nanywhere --\n-h\n'\n-h | bat -plhelp\n'\nThis way, you can keep on using\ncp --help\n, but get colorized help pages.\nBe aware that in some cases,\n-h\nmay not be a shorthand of\n--help\n(for example with\nls\n). In cases where you need to use\n-h\nas a command argument you can prepend\n\\\nto the arguement (eg.\nls \\-h\n) to escape the aliasing defined above.\nPlease report any issues with the help syntax in\nthis repository\n.\nInstallation\nOn Ubuntu (using\napt\n)\n... and other Debian-based Linux distributions.\nbat\nis available on\nUbuntu since 20.04 (\"Focal\")\nand\nDebian since August 2021 (Debian 11 - \"Bullseye\")\n.\nIf your Ubuntu/Debian installation is new enough you can simply run:\nsudo apt install bat\nImportant\n: If you install\nbat\nthis way, please note that the executable may be installed as\nbatcat\ninstead of\nbat\n(due to\na name\nclash with another package\n). You can set up a\nbat -> batcat\nsymlink or alias to prevent any issues that may come up because of this and to be consistent with other distributions:\nmkdir -p\n~\n/.local/bin\nln -s /usr/bin/batcat\n~\n/.local/bin/bat\nan example alias for\nbatcat\nas\nbat\n:\nalias\nbat=\n\"\nbatcat\n\"\nOn Ubuntu (using most recent\n.deb\npackages)\n... and other Debian-based Linux distributions.\nIf the package has not yet been promoted to your Ubuntu/Debian installation, or you want\nthe most recent release of\nbat\n, download the latest\n.deb\npackage from the\nrelease page\nand install it via:\nsudo dpkg -i bat_0.18.3_amd64.deb\n#\nadapt version number and architecture\nOn Alpine Linux\nYou can install\nthe\nbat\npackage\nfrom the official sources, provided you have the appropriate repository enabled:\napk add bat\nOn Arch Linux\nYou can install\nthe\nbat\npackage\nfrom the official sources:\npacman -S bat\nOn Fedora\nYou can install\nthe\nbat\npackage\nfrom the official\nFedora Modular\nrepository.\ndnf install bat\nOn Gentoo Linux\nYou can install\nthe\nbat\npackage\nfrom the official sources:\nemerge sys-apps/bat\nOn FreeBSD\nYou can install a precompiled\nbat\npackage\nwith pkg:\npkg install bat\nor build it on your own from the FreeBSD ports:\ncd\n/usr/ports/textproc/bat\nmake install\nOn OpenBSD\nYou can install\nbat\npackage using\npkg_add(1)\n:\npkg_add bat\nVia nix\nYou can install\nbat\nusing the\nnix package manager\n:\nnix-env -i bat\nOn openSUSE\nYou can install\nbat\nwith zypper:\nzypper install bat\nVia snap package\nThere is currently no recommended snap package available.\nExisting packages may be available, but are not officially supported and may contain\nissues\n.\nOn macOS (or Linux) via Homebrew\nYou can install\nbat\nwith\nHomebrew\n:\nbrew install bat\nOn macOS via MacPorts\nOr install\nbat\nwith\nMacPorts\n:\nport install bat\nOn Windows\nThere are a few options to install\nbat\non Windows. Once you have installed\nbat\n,\ntake a look at the\n\"Using\nbat\non Windows\"\nsection.\nPrerequisites\nYou will need to install the\nVisual C++ Redistributable\nWith WinGet\nYou can install\nbat\nvia\nWinGet\n:\nwinget install sharkdp.bat\nWith Chocolatey\nYou can install\nbat\nvia\nChocolatey\n:\nchoco install bat\nWith Scoop\nYou can install\nbat\nvia\nscoop\n:\nscoop install bat\nFrom prebuilt binaries:\nYou can download prebuilt binaries from the\nRelease page\n,\nYou will need to install the\nVisual C++ Redistributable\npackage.\nFrom binaries\nCheck out the\nRelease page\nfor\nprebuilt versions of\nbat\nfor many different architectures. Statically-linked\nbinaries are also available: look for archives with\nmusl\nin the file name.\nFrom source\nIf you want to build\nbat\nfrom source, you need Rust 1.74.0 or\nhigher. You can then use\ncargo\nto build everything:\nFrom local source\ncargo install --path\n.\n--locked\nNote\nThe\n--path .\nabove specifies the directory of the source code and NOT where\nbat\nwill be installed.\nFor more information see the docs for\ncargo install\n.\nFrom\ncrates.io\ncargo install --locked bat\nNote that additional files like the man page or shell completion\nfiles can not be installed automatically in both these ways.\nIf installing from a local source, they will be generated by\ncargo\nand should be available in the cargo target folder under\nbuild\n.\nFurthermore, shell completions are also available by running:\nbat --completion\n<\nshell\n>\n#\nsee --help for supported shells\nCustomization\nHighlighting theme\nUse\nbat --list-themes\nto get a list of all available themes for syntax\nhighlighting. By default,\nbat\nuses\nMonokai Extended\nor\nMonokai Extended Light\nfor dark and light themes respectively. To select the\nTwoDark\ntheme, call\nbat\nwith the\n--theme=TwoDark\noption or set the\nBAT_THEME\nenvironment variable to\nTwoDark\n. Use\nexport BAT_THEME=\"TwoDark\"\nin your shell's startup file to\nmake the change permanent. Alternatively, use\nbat\n's\nconfiguration file\n.\nIf you want to preview the different themes on a custom file, you can use\nthe following command (you need\nfzf\nfor this):\nbat --list-themes\n|\nfzf --preview=\n\"\nbat --theme={} --color=always /path/to/file\n\"\nbat\nautomatically picks a fitting theme depending on your terminal's background color.\nYou can use the\n--theme-dark\n/\n--theme-light\noptions or the\nBAT_THEME_DARK\n/\nBAT_THEME_LIGHT\nenvironment variables\nto customize the themes used. This is especially useful if you frequently switch between dark and light mode.\nYou can also use a custom theme by following the\n'Adding new themes' section below\n.\n8-bit themes\nbat\nhas three themes that always use\n8-bit colors\n,\neven when truecolor support is available:\nansi\nlooks decent on any terminal. It uses 3-bit colors: black, red, green,\nyellow, blue, magenta, cyan, and white.\nbase16\nis designed for\nbase16\nterminal themes. It uses\n4-bit colors (3-bit colors plus bright variants) in accordance with the\nbase16 styling guidelines\n.\nbase16-256\nis designed for\ntinted-shell\n.\nIt replaces certain bright colors with 8-bit colors from 16 to 21.\nDo not\nuse this simply\nbecause you have a 256-color terminal but are not using tinted-shell.\nAlthough these themes are more restricted, they have three advantages over truecolor themes. They:\nEnjoy maximum compatibility. Some terminal utilities do not support more than 3-bit colors.\nAdapt to terminal theme changes. Even for already printed output.\nVisually harmonize better with other terminal software.\nOutput style\nYou can use the\n--style\noption to control the appearance of\nbat\n's output.\nYou can use\n--style=numbers,changes\n, for example, to show only Git changes\nand line numbers but no grid and no file header. Set the\nBAT_STYLE\nenvironment\nvariable to make these changes permanent or use\nbat\n's\nconfiguration file\n.\nTip\nIf you specify a default style in\nbat\n's config file, you can change which components\nare displayed during a single run of\nbat\nusing the\n--style\ncommand-line argument.\nBy prefixing a component with\n+\nor\n-\n, it can be added or removed from the current style.\nFor example, if your config contains\n--style=full,-snip\n, you can run bat with\n--style=-grid,+snip\nto remove the grid and add back the\nsnip\ncomponent.\nOr, if you want to override the styles completely, you use\n--style=numbers\nto\nonly show the line numbers.\nAdding new syntaxes / language definitions\nShould you find that a particular syntax is not available within\nbat\n, you can follow these\ninstructions to easily add new syntaxes to your current\nbat\ninstallation.\nbat\nuses the excellent\nsyntect\nlibrary for syntax highlighting.\nsyntect\ncan read any\nSublime Text\n.sublime-syntax\nfile\nand theme.\nA good resource for finding Sublime Syntax packages is\nPackage Control\n. Once you found a\nsyntax:\nCreate a folder with syntax definition files:\nmkdir -p\n\"\n$(\nbat --config-dir\n)\n/syntaxes\n\"\ncd\n\"\n$(\nbat --config-dir\n)\n/syntaxes\n\"\n#\nPut new '.sublime-syntax' language definition files\n#\nin this folder (or its subdirectories), for example:\ngit clone https://github.com/tellnobody1/sublime-purescript-syntax\nNow use the following command to parse these files into a binary cache:\nbat cache --build\nFinally, use\nbat --list-languages\nto check if the new languages are available.\nIf you ever want to go back to the default settings, call:\nbat cache --clear\nIf you think that a specific syntax should be included in\nbat\nby default, please\nconsider opening a \"syntax request\" ticket after reading the policies and\ninstructions\nhere\n:\nOpen Syntax Request\n.\nAdding new themes\nThis works very similar to how we add new syntax definitions.\nNote\nThemes are stored in\n.tmTheme\nfiles\n.\nFirst, create a folder with the new syntax highlighting themes:\nmkdir -p\n\"\n$(\nbat --config-dir\n)\n/themes\n\"\ncd\n\"\n$(\nbat --config-dir\n)\n/themes\n\"\n#\nDownload a theme in '.tmTheme' format, for example:\ngit clone https://github.com/greggb/sublime-snazzy\n#\nUpdate the binary cache\nbat cache --build\nFinally, use\nbat --list-themes\nto check if the new themes are available.\nNote\nbat\nuses the name of the\n.tmTheme\nfile for the theme's name.\nAdding or changing file type associations\nYou can add new (or change existing) file name patterns using the\n--map-syntax\ncommand line option. The option takes an argument of the form\npattern:syntax\nwhere\npattern\nis a glob pattern that is matched against the file name and\nthe absolute file path. The\nsyntax\npart is the full name of a supported language\n(use\nbat --list-languages\nfor an overview).\nNote:\nYou probably want to use this option as\nan entry in\nbat\n's configuration file\nfor persistence instead of passing it on the command line as a one-off. Generally\nyou'd just use\n-l\nif you want to manually specify a language for a file.\nExample: To use \"INI\" syntax highlighting for all files with a\n.conf\nfile extension, use\n--map-syntax=\n'\n*.conf:INI\n'\nExample: To open all files called\n.ignore\n(exact match) with the \"Git Ignore\" syntax, use:\n--map-syntax=\n'\n.ignore:Git Ignore\n'\nExample: To open all\n.conf\nfiles in subfolders of\n/etc/apache2\nwith the \"Apache Conf\"\nsyntax, use (this mapping is already built in):\n--map-syntax=\n'\n/etc/apache2/**/*.conf:Apache Conf\n'\nUsing a different pager\nbat\nuses the pager that is specified in the\nPAGER\nenvironment variable. If this variable is not\nset,\nless\nis used by default. You can also use bat's built-in pager with\n--pager=builtin\nor\nby setting the\nBAT_PAGER\nenvironment variable to \"builtin\".\nIf you want to use a different pager, you can either modify the\nPAGER\nvariable or set the\nBAT_PAGER\nenvironment variable to override what is specified in\nPAGER\n.\nNote\nIf\nPAGER\nis\nmore\nor\nmost\n,\nbat\nwill silently use\nless\ninstead to ensure support for colors.\nIf you want to pass command-line arguments to the pager, you can also set them via the\nPAGER\n/\nBAT_PAGER\nvariables:\nexport\nBAT_PAGER=\n\"\nless -RFK\n\"\nInstead of using environment variables, you can also use\nbat\n's\nconfiguration file\nto configure the pager (\n--pager\noption).\nUsing\nless\nas a pager\nWhen using\nless\nas a pager,\nbat\nwill automatically pass extra options along to\nless\nto improve the experience. Specifically,\n-R\n/\n--RAW-CONTROL-CHARS\n,\n-F\n/\n--quit-if-one-screen\n,\n-K\n/\n--quit-on-intr\nand under certain conditions,\n-X\n/\n--no-init\nand/or\n-S\n/\n--chop-long-lines\n.\nImportant\nThese options will not be added if:\nThe pager is not named\nless\n.\nThe\n--pager\nargument contains any command-line arguments (e.g.\n--pager=\"less -R\"\n).\nThe\nBAT_PAGER\nenvironment variable contains any command-line arguments (e.g.\nexport BAT_PAGER=\"less -R\"\n)\nThe\n--quit-if-one-screen\noption will not be added when:\nThe\n--paging=always\nargument is used.\nThe\nBAT_PAGING\nenvironment is set to\nalways\n.\nThe\n-R\noption is needed to interpret ANSI colors correctly.\nThe\n-F\noption instructs\nless\nto exit immediately if the output size is smaller than\nthe vertical size of the terminal. This is convenient for small files because you do not\nhave to press\nq\nto quit the pager.\nThe\n-K\noption instructs\nless\nto exit immediately when an interrupt signal is received.\nThis is useful to ensure that\nless\nquits together with\nbat\non SIGINT.\nThe\n-X\noption is needed to fix a bug with the\n--quit-if-one-screen\nfeature in versions\nof\nless\nolder than version 530. Unfortunately, it also breaks mouse-wheel support in\nless\n.\nIf you want to enable mouse-wheel scrolling on older versions of\nless\nand do not mind losing\nthe quit-if-one-screen feature, you can set the pager (via\n--pager\nor\nBAT_PAGER\n) to\nless -R\n.\nFor\nless\n530 or newer, it should work out of the box.\nThe\n-S\noption is added when\nbat\n's\n-S\n/\n--chop-long-lines\noption is used. This tells\nless\nto truncate any lines larger than the terminal width.\nIndentation\nbat\nexpands tabs to 4 spaces by itself, not relying on the pager. To change this, simply add the\n--tabs\nargument with the number of spaces you want to be displayed.\nNote\n: Defining tab stops for the pager (via the\n--pager\nargument by\nbat\n, or via the\nLESS\nenvironment variable for\nless\n) won't be taken into account because the pager will already get\nexpanded spaces instead of tabs. This behaviour is added to avoid indentation issues caused by the\nsidebar. Calling\nbat\nwith\n--tabs=0\nwill override it and let tabs be consumed by the pager.\nDark mode\nIf you make use of the dark mode feature in\nmacOS\n, you might want to configure\nbat\nto use a different\ntheme based on the OS theme. The following snippet uses the\ndefault\ntheme when in the\ndark mode\nand the\nGitHub\ntheme when in the\nlight mode\n.\nalias\ncat=\n\"\nbat --theme auto:system --theme-dark default --theme-light GitHub\n\"\nThe same dark mode feature is now available in\nGNOME\nand affects the\norg.gnome.desktop.interface color-scheme\nsetting. The following code converts the above to use said setting.\n#\n.bashrc\nsys_color_scheme_is_dark\n() {\n    condition=\n$(\ngsettings get org.gnome.desktop.interface color-scheme\n)\ncondition=\n$(\necho\n\"\n$condition\n\"\n|\ntr -d\n\"\n[:space:]'\n\"\n)\nif\n[\n$condition\n==\n\"\nprefer-dark\n\"\n]\n;\nthen\nreturn\n0\nelse\nreturn\n1\nfi\n}\nbat_alias_wrapper\n() {\n#\nget color scheme\nsys_color_scheme_is_dark\nif\n[[\n$?\n-eq\n0 ]]\n;\nthen\n#\nbat command with dark color scheme\nbat --theme=default\n\"\n$@\n\"\nelse\n#\nbat command with light color scheme\nbat --theme=GitHub\n\"\n$@\n\"\nfi\n}\nalias\ncat=\n'\nbat_alias_wrapper\n'\nConfiguration file\nbat\ncan also be customized with a configuration file. The location of the file is dependent\non your operating system. To get the default path for your system, call\nbat --config-file\nAlternatively, you can use\nBAT_CONFIG_PATH\nor\nBAT_CONFIG_DIR\nenvironment variables to point\nbat\nto a non-default location of the configuration file or the configuration directory respectively:\nexport\nBAT_CONFIG_PATH=\n\"\n/path/to/bat/bat.conf\n\"\nexport\nBAT_CONFIG_DIR=\n\"\n/path/to/bat\n\"\nA default configuration file can be created with the\n--generate-config-file\noption.\nbat --generate-config-file\nThere is also now a systemwide configuration file, which is located under\n/etc/bat/config\non\nLinux and Mac OS and\nC:\\ProgramData\\bat\\config\non windows. If the system wide configuration\nfile is present, the content of the user configuration will simply be appended to it.\nFormat\nThe configuration file is a simple list of command line arguments. Use\nbat --help\nto see a full list of possible options and values. In addition, you can add comments by prepending a line with the\n#\ncharacter.\nExample configuration file:\n#\nSet the theme to \"TwoDark\"\n--theme=\n\"\nTwoDark\n\"\n#\nShow line numbers, Git modifications and file header (but no grid)\n--style=\n\"\nnumbers,changes,header\n\"\n#\nUse italic text on the terminal (not supported on all terminals)\n--italic-text=always\n#\nUse C++ syntax for Arduino .ino files\n--map-syntax\n\"\n*.ino:C++\n\"\nUsing\nbat\non Windows\nbat\nmostly works out-of-the-box on Windows, but a few features may need extra configuration.\nPrerequisites\nYou will need to install the\nVisual C++ Redistributable\npackage.\nPaging\nWindows only includes a very limited pager in the form of\nmore\n. You can download a Windows binary\nfor\nless\nfrom its homepage\nor\nthrough\nChocolatey\n. To use it, place the binary in a directory in\nyour\nPATH\nor\ndefine an environment variable\n. The\nChocolatey package\ninstalls\nless\nautomatically.\nColors\nWindows 10 natively supports colors in both\nconhost.exe\n(Command Prompt) and PowerShell since\nv1511\n, as\nwell as in newer versions of bash. On earlier versions of Windows, you can use\nCmder\n, which includes\nConEmu\n.\nNote:\nOld versions of\nless\ndo not correctly interpret colors on Windows. To fix this, you can add the optional Unix tools to your PATH when installing Git. If you don‚Äôt have any other pagers installed, you can disable paging entirely by passing\n--paging=never\nor by setting\nBAT_PAGER\nto an empty string.\nCygwin\nbat\non Windows does not natively support Cygwin's unix-style paths (\n/cygdrive/*\n). When passed an absolute cygwin path as an argument,\nbat\nwill encounter the following error:\nThe system cannot find the path specified. (os error 3)\nThis can be solved by creating a wrapper or adding the following function to your\n.bash_profile\nfile:\nbat\n() {\nlocal\nindex\nlocal\nargs=(\n\"\n$@\n\"\n)\nfor\nindex\nin\n$(\nseq 0\n${\n#\nargs[@]}\n)\n;\ndo\ncase\n\"\n${args[index]}\n\"\nin\n-\n*\n)\ncontinue\n;;\n*\n)  [\n-e\n\"\n${args[index]}\n\"\n]\n&&\nargs[index]=\n\"\n$(\ncygpath --windows\n\"\n${args[index]}\n\"\n)\n\"\n;;\nesac\ndone\ncommand\nbat\n\"\n${args[@]}\n\"\n}\nTroubleshooting\nGarbled output\nIf an input file contains color codes or other ANSI escape sequences or control characters,\nbat\nwill have problems\nperforming syntax highlighting and text wrapping, and thus the output can become garbled.\nIf your version of\nbat\nsupports the\n--strip-ansi=auto\noption, it can be used to remove such sequences\nbefore syntax highlighting. Alternatively, you may disable both syntax highlighting and wrapping by\npassing the\n--color=never --wrap=never\noptions to\nbat\n.\nNote\nThe\nauto\noption of\n--strip-ansi\navoids removing escape sequences when the syntax is plain text.\nTerminals & colors\nbat\nhandles terminals\nwith\nand\nwithout\ntruecolor support. However, the colors in most syntax\nhighlighting themes are not optimized for 8-bit colors. It is therefore strongly recommended\nthat you use a terminal with 24-bit truecolor support (\nterminator\n,\nkonsole\n,\niTerm2\n, ...),\nor use one of the basic\n8-bit themes\ndesigned for a restricted set of colors.\nSee\nthis article\nfor more details and a full list of\nterminals with truecolor support.\nMake sure that your truecolor terminal sets the\nCOLORTERM\nvariable to either\ntruecolor\nor\n24bit\n. Otherwise,\nbat\nwill not be able to determine whether or not 24-bit escape sequences\nare supported (and fall back to 8-bit colors).\nLine numbers and grid are hardly visible\nPlease try a different theme (see\nbat --list-themes\nfor a list). The\nOneHalfDark\nand\nOneHalfLight\nthemes provide grid and line colors that are brighter.\nFile encodings\nbat\nnatively supports UTF-8 as well as UTF-16. For every other file encoding, you may need to\nconvert to UTF-8 first because the encodings can typically not be auto-detected. You can\niconv\nto do so.\nExample: if you have a PHP file in Latin-1 (ISO-8859-1) encoding, you can call:\niconv -f ISO-8859-1 -t UTF-8 my-file.php\n|\nbat\nNote: you might have to use the\n-l\n/\n--language\noption if the syntax can not be auto-detected\nby\nbat\n.\nDevelopment\n#\nRecursive clone to retrieve all submodules\ngit clone --recursive https://github.com/sharkdp/bat\n#\nBuild (debug version)\ncd\nbat\ncargo build --bins\n#\nRun unit tests and integration tests\ncargo\ntest\n#\nInstall (release version)\ncargo install --path\n.\n--locked\n#\nBuild a bat binary with modified syntaxes and themes\nbash assets/create.sh\ncargo install --path\n.\n--locked --force\nIf you want to build an application that uses\nbat\n's pretty-printing\nfeatures as a library, check out the\nthe API documentation\n.\nNote that you have to use either\nregex-onig\nor\nregex-fancy\nas a feature\nwhen you depend on\nbat\nas a library.\nContributing\nTake a look at the\nCONTRIBUTING.md\nguide.\nMaintainers\nsharkdp\neth-p\nkeith-hall\nEnselic\nSecurity vulnerabilities\nSee\nSECURITY.md\n.\nProject goals and alternatives\nbat\ntries to achieve the following goals:\nProvide beautiful, advanced syntax highlighting\nIntegrate with Git to show file modifications\nBe a drop-in replacement for (POSIX)\ncat\nOffer a user-friendly command-line interface\nThere are a lot of alternatives, if you are looking for similar programs. See\nthis document\nfor a comparison.\nLicense\nCopyright (c) 2018-2025\nbat-developers\n.\nbat\nis made available under the terms of either the MIT License or the Apache License 2.0, at your option.\nSee the\nLICENSE-APACHE\nand\nLICENSE-MIT\nfiles for license details.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 97",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 54,887"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/sharkdp/bat"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/shadcn-ui/ui",
      "title": "shadcn-ui/ui",
      "date": null,
      "executive_summary": [
        "A set of beautifully-designed, accessible components and a code distribution platform. Works with your favorite frameworks. Open Source. Open Code.",
        "---",
        "shadcn/ui\nAccessible and customizable components that you can copy and paste into your apps. Free. Open Source.\nUse this to build your own component library\n.\nDocumentation\nVisit\nhttp://ui.shadcn.com/docs\nto view the documentation.\nContributing\nPlease read the\ncontributing guide\n.\nLicense\nLicensed under the\nMIT license\n.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 91",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 96,851"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/shadcn-ui/ui"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/MODSetter/SurfSense",
      "title": "MODSetter/SurfSense",
      "date": null,
      "executive_summary": [
        "Open Source Alternative to NotebookLM / Perplexity, connected to external sources such as Search Engines, Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more. Join our discord: https://discord.gg/ejRNvftDp9",
        "---",
        "SurfSense\nWhile tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar, Luma and more to come.\nVideo\ntemp_demo_v7.mp4\nPodcast Sample\nelon_vs_trump_podcast.mp4\nKey Features\nüí°\nIdea\n:\nHave your own highly customizable private NotebookLM and Perplexity integrated with external sources.\nüìÅ\nMultiple File Format Uploading Support\nSave content from your own personal files\n(Documents, images, videos and supports\n50+ file extensions\n)\nto your own personal knowledge base .\nüîç\nPowerful Search\nQuickly research or find anything in your saved content .\nüí¨\nChat with your Saved Content\nInteract in Natural Language and get cited answers.\nüìÑ\nCited Answers\nGet Cited answers just like Perplexity.\nüîî\nPrivacy & Local LLM Support\nWorks Flawlessly with Ollama local LLMs.\nüè†\nSelf Hostable\nOpen source and easy to deploy locally.\nüéôÔ∏è Podcasts\nBlazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)\nConvert your chat conversations into engaging audio content\nSupport for local TTS providers (Kokoro TTS)\nSupport for multiple TTS providers (OpenAI, Azure, Google Vertex AI)\nüìä\nAdvanced RAG Techniques\nSupports 100+ LLM's\nSupports 6000+ Embedding Models.\nSupports all major Rerankers (Pinecode, Cohere, Flashrank etc)\nUses Hierarchical Indices (2 tiered RAG setup).\nUtilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).\nRAG as a Service API Backend.\n‚ÑπÔ∏è\nExternal Sources\nSearch Engines (Tavily, LinkUp)\nSlack\nLinear\nJira\nClickUp\nConfluence\nNotion\nGmail\nYoutube Videos\nGitHub\nDiscord\nAirtable\nGoogle Calendar\nLuma\nand more to come.....\nüìÑ\nSupported File Extensions\nNote\n: File format support depends on your ETL service configuration. LlamaCloud supports 50+ formats, Unstructured supports 34+ core formats, and Docling (core formats, local processing, privacy-focused, no API key).\nDocuments & Text\nLlamaCloud\n:\n.pdf\n,\n.doc\n,\n.docx\n,\n.docm\n,\n.dot\n,\n.dotm\n,\n.rtf\n,\n.txt\n,\n.xml\n,\n.epub\n,\n.odt\n,\n.wpd\n,\n.pages\n,\n.key\n,\n.numbers\n,\n.602\n,\n.abw\n,\n.cgm\n,\n.cwk\n,\n.hwp\n,\n.lwp\n,\n.mw\n,\n.mcw\n,\n.pbd\n,\n.sda\n,\n.sdd\n,\n.sdp\n,\n.sdw\n,\n.sgl\n,\n.sti\n,\n.sxi\n,\n.sxw\n,\n.stw\n,\n.sxg\n,\n.uof\n,\n.uop\n,\n.uot\n,\n.vor\n,\n.wps\n,\n.zabw\nUnstructured\n:\n.doc\n,\n.docx\n,\n.odt\n,\n.rtf\n,\n.pdf\n,\n.xml\n,\n.txt\n,\n.md\n,\n.markdown\n,\n.rst\n,\n.html\n,\n.org\n,\n.epub\nDocling\n:\n.pdf\n,\n.docx\n,\n.html\n,\n.htm\n,\n.xhtml\n,\n.adoc\n,\n.asciidoc\nPresentations\nLlamaCloud\n:\n.ppt\n,\n.pptx\n,\n.pptm\n,\n.pot\n,\n.potm\n,\n.potx\n,\n.odp\n,\n.key\nUnstructured\n:\n.ppt\n,\n.pptx\nDocling\n:\n.pptx\nSpreadsheets & Data\nLlamaCloud\n:\n.xlsx\n,\n.xls\n,\n.xlsm\n,\n.xlsb\n,\n.xlw\n,\n.csv\n,\n.tsv\n,\n.ods\n,\n.fods\n,\n.numbers\n,\n.dbf\n,\n.123\n,\n.dif\n,\n.sylk\n,\n.slk\n,\n.prn\n,\n.et\n,\n.uos1\n,\n.uos2\n,\n.wk1\n,\n.wk2\n,\n.wk3\n,\n.wk4\n,\n.wks\n,\n.wq1\n,\n.wq2\n,\n.wb1\n,\n.wb2\n,\n.wb3\n,\n.qpw\n,\n.xlr\n,\n.eth\nUnstructured\n:\n.xls\n,\n.xlsx\n,\n.csv\n,\n.tsv\nDocling\n:\n.xlsx\n,\n.csv\nImages\nLlamaCloud\n:\n.jpg\n,\n.jpeg\n,\n.png\n,\n.gif\n,\n.bmp\n,\n.svg\n,\n.tiff\n,\n.webp\n,\n.html\n,\n.htm\n,\n.web\nUnstructured\n:\n.jpg\n,\n.jpeg\n,\n.png\n,\n.bmp\n,\n.tiff\n,\n.heic\nDocling\n:\n.jpg\n,\n.jpeg\n,\n.png\n,\n.bmp\n,\n.tiff\n,\n.tif\n,\n.webp\nAudio & Video\n(Always Supported)\n.mp3\n,\n.mpga\n,\n.m4a\n,\n.wav\n,\n.mp4\n,\n.mpeg\n,\n.webm\nEmail & Communication\nUnstructured\n:\n.eml\n,\n.msg\n,\n.p7s\nüîñ Cross Browser Extension\nThe SurfSense extension can be used to save any webpage you like.\nIts main usecase is to save any webpages protected beyond authentication.\nFEATURE REQUESTS AND FUTURE\nSurfSense is actively being developed.\nWhile it's not yet production-ready, you can help us speed up the process.\nJoin the\nSurfSense Discord\nand help shape the future of SurfSense!\nüöÄ Roadmap\nStay up to date with our development progress and upcoming features!\nCheck out our public roadmap and contribute your ideas or feedback:\nView the Roadmap:\nSurfSense Roadmap on GitHub Projects\nHow to get started?\nInstallation Options\nSurfSense provides two installation methods:\nDocker Installation\n- The easiest way to get SurfSense up and running with all dependencies containerized.\nIncludes pgAdmin for database management through a web UI\nSupports environment variable customization via\n.env\nfile\nFlexible deployment options (full stack or core services only)\nNo need to manually edit configuration files between environments\nSee\nDocker Setup Guide\nfor detailed instructions\nFor deployment scenarios and options, see\nDeployment Guide\nManual Installation (Recommended)\n- For users who prefer more control over their setup or need to customize their deployment.\nBoth installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.\nBefore installation, make sure to complete the\nprerequisite setup steps\nincluding:\nPGVector setup\nFile Processing ETL Service\n(choose one):\nUnstructured.io API key (supports 34+ formats)\nLlamaIndex API key (enhanced parsing, supports 50+ formats)\nDocling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)\nOther required API keys\nScreenshots\nResearch Agent\nSearch Spaces\nManage Documents\nPodcast Agent\nAgent Chat\nBrowser Extension\nTech Stack\nBackEnd\nFastAPI\n: Modern, fast web framework for building APIs with Python\nPostgreSQL with pgvector\n: Database with vector search capabilities for similarity searches\nSQLAlchemy\n: SQL toolkit and ORM (Object-Relational Mapping) for database interactions\nAlembic\n: A database migrations tool for SQLAlchemy.\nFastAPI Users\n: Authentication and user management with JWT and OAuth support\nLangGraph\n: Framework for developing AI-agents.\nLangChain\n: Framework for developing AI-powered applications.\nLLM Integration\n: Integration with LLM models through LiteLLM\nRerankers\n: Advanced result ranking for improved search relevance\nHybrid Search\n: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)\nVector Embeddings\n: Document and text embeddings for semantic search\npgvector\n: PostgreSQL extension for efficient vector similarity operations\nChonkie\n: Advanced document chunking and embedding library\nUses\nAutoEmbeddings\nfor flexible embedding model selection\nLateChunker\nfor optimized document chunking based on embedding model's max sequence length\nFrontEnd\nNext.js 15.2.3\n: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.\nReact 19.0.0\n: JavaScript library for building user interfaces.\nTypeScript\n: Static type-checking for JavaScript, enhancing code quality and developer experience.\nVercel AI SDK Kit UI Stream Protocol\n: To create scalable chat UI.\nTailwind CSS 4.x\n: Utility-first CSS framework for building custom UI designs.\nShadcn\n: Headless components library.\nLucide React\n: Icon set implemented as React components.\nFramer Motion\n: Animation library for React.\nSonner\n: Toast notification library.\nGeist\n: Font family from Vercel.\nReact Hook Form\n: Form state management and validation.\nZod\n: TypeScript-first schema validation with static type inference.\n@hookform/resolvers\n: Resolvers for using validation libraries with React Hook Form.\n@tanstack/react-table\n: Headless UI for building powerful tables & datagrids.\nDevOps\nDocker\n: Container platform for consistent deployment across environments\nDocker Compose\n: Tool for defining and running multi-container Docker applications\npgAdmin\n: Web-based PostgreSQL administration tool included in Docker setup\nExtension\nManifest v3 on Plasmo\nFuture Work\nAdd More Connectors.\nPatch minor bugs.\nDocument Podcasts\nContribute\nContributions are very welcome! A contribution can be as small as a ‚≠ê or even finding and creating issues.\nFine-tuning the Backend is always desired.\nFor detailed contribution guidelines, please see our\nCONTRIBUTING.md\nfile.\nStar History",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 88",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 8,601"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/MODSetter/SurfSense"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/jdx/mise",
      "title": "jdx/mise",
      "date": null,
      "executive_summary": [
        "dev tools, env vars, task runner",
        "---",
        "mise-en-place\nThe front-end to your dev env\nGetting Started\n‚Ä¢\nDocumentation\n‚Ä¢\nDev Tools\n‚Ä¢\nEnvironments\n‚Ä¢\nTasks\nWhat is it?\nLike\nasdf\n(or\nnvm\nor\npyenv\nbut for any language) it manages\ndev tools\nlike node, python, cmake, terraform, and\nhundreds more\n.\nLike\ndirenv\nit manages\nenvironment variables\nfor different project directories.\nLike\nmake\nit manages\ntasks\nused to build and test projects.\nDemo\nThe following demo shows how to install and use\nmise\nto manage multiple versions of\nnode\non the same system.\nNote that calling\nwhich node\ngives us a real path to node, not a shim.\nIt also shows that you can use\nmise\nto install and many other tools such as\njq\n,\nterraform\n, or\ngo\n.\nSee\ndemo transcript\n.\nQuickstart\nInstall mise\nSee\nGetting started\nfor more options.\n$\ncurl https://mise.run\n|\nsh\n$\n~\n/.local/bin/mise --version\n2025.10.6 macos-arm64 (a1b2d3e 2025-10-08)\nHook mise into your shell (pick the right one for your shell):\n#\nnote this assumes mise is located at\n~\n/.local/bin/mise\n#\nwhich is what https://mise.run does by default\necho 'eval \"$(~/.local/bin/mise activate bash)\"' >> ~/.bashrc\necho 'eval \"$(~/.local/bin/mise activate zsh)\"' >> ~/.zshrc\necho '~/.local/bin/mise activate fish | source' >> ~/.config/fish/config.fish\necho '~/.local/bin/mise activate pwsh | Out-String | Invoke-Expression' >> ~/.config/powershell/Microsoft.PowerShell_profile.ps1\nExecute commands with specific tools\n$\nmise\nexec\nnode@22 -- node -v\nmise node@22.x.x ‚úì installed\nv22.x.x\nInstall tools\n$\nmise use --global node@22 go@1\n$\nnode -v\nv22.x.x\n$\ngo version\ngo version go1.x.x macos/arm64\nSee\ndev tools\nfor more examples.\nManage environment variables\n#\nmise.toml\n[\nenv\n]\nSOME_VAR\n=\n\"\nfoo\n\"\n$\nmise\nset\nSOME_VAR=bar\n$\necho\n$SOME_VAR\nbar\nNote that\nmise\ncan also\nload\n.env\nfiles\n.\nRun tasks\n#\nmise.toml\n[\ntasks\n.\nbuild\n]\ndescription\n=\n\"\nbuild the project\n\"\nrun\n=\n\"\necho building...\n\"\n$\nmise run build\nbuilding...\nSee\ntasks\nfor more information.\nExample mise project\nHere is a combined example to give you an idea of how you can use mise to manage your a project's tools, environment, and tasks.\n#\nmise.toml\n[\ntools\n]\nterraform\n=\n\"\n1\n\"\naws-cli\n=\n\"\n2\n\"\n[\nenv\n]\nTF_WORKSPACE\n=\n\"\ndevelopment\n\"\nAWS_REGION\n=\n\"\nus-west-2\n\"\nAWS_PROFILE\n=\n\"\ndev\n\"\n[\ntasks\n.\nplan\n]\ndescription\n=\n\"\nRun terraform plan with configured workspace\n\"\nrun\n=\n\"\"\"\nterraform init\nterraform workspace select $TF_WORKSPACE\nterraform plan\n\"\"\"\n[\ntasks\n.\nvalidate\n]\ndescription\n=\n\"\nValidate AWS credentials and terraform config\n\"\nrun\n=\n\"\"\"\naws sts get-caller-identity\nterraform validate\n\"\"\"\n[\ntasks\n.\ndeploy\n]\ndescription\n=\n\"\nDeploy infrastructure after validation\n\"\ndepends\n= [\n\"\nvalidate\n\"\n,\n\"\nplan\n\"\n]\nrun\n=\n\"\nterraform apply -auto-approve\n\"\nRun it with:\nmise install # install tools specified in mise.toml\nmise run deploy\nFind more examples in the\nmise cookbook\n.\nFull Documentation\nSee\nmise.jdx.dev\nGitHub Issues & Discussions\nDue to the volume of issue submissions mise received, using GitHub Issues became unsustainable for\nthe project. Instead, mise uses GitHub Discussions which provide a more community-centric platform\nfor communication and require less management on the part of the maintainers.\nPlease note the following discussion categories, which match how issues are often used:\nAnnouncements\nIdeas\n: for feature requests, etc.\nTroubleshooting & Bug Reports\nSpecial Thanks\nWe're grateful for Cloudflare's support through\nProject Alexandria\n.\nContributors",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 77",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 20,215"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/jdx/mise"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/tursodatabase/turso",
      "title": "tursodatabase/turso",
      "date": null,
      "executive_summary": [
        "Turso is an in-process SQL database, compatible with SQLite.",
        "---",
        "Turso Database\nAn in-process SQL database, compatible with SQLite.\nAbout\nTurso Database is an in-process SQL database written in Rust, compatible with SQLite.\n‚ö†Ô∏è\nWarning:\nThis software is in BETA. It may still contain bugs and unexpected behavior. Use caution with production data and ensure you have backups.\nFeatures and Roadmap\nSQLite compatibility\nfor SQL dialect, file formats, and the C API [see\ndocument\nfor details]\nChange data capture (CDC)\nfor real-time tracking of database changes.\nMulti-language support\nfor\nGo\nJavaScript\nJava\nPython\nRust\nWebAssembly\nAsynchronous I/O\nsupport on Linux with\nio_uring\nCross-platform\nsupport for Linux, macOS, Windows and browsers (through WebAssembly)\nVector support\nsupport including exact search and vector manipulation\nImproved schema management\nincluding extended\nALTER\nsupport and faster schema changes.\nThe database has the following experimental features:\nBEGIN CONCURRENT\nfor improved write throughput using multi-version concurrency control (MVCC).\nEncryption at rest\nfor protecting the data locally.\nIncremental computation\nusing DBSP for incremental view mainatenance and query subscriptions.\nThe following features are on our current roadmap:\nVector indexing\nfor fast approximate vector search, similar to\nlibSQL vector search\n.\nGetting Started\nPlease see the\nTurso Database Manual\nfor more information.\nüíª Command Line\nYou can install the latest `turso` release with:\ncurl --proto\n'\n=https\n'\n--tlsv1.2 -LsSf \\\n  https://github.com/tursodatabase/turso/releases/latest/download/turso_cli-installer.sh\n|\nsh\nThen launch the interactive shell:\n$ tursodb\nThis will start the Turso interactive shell where you can execute SQL statements:\nTurso\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database\nturso> CREATE TABLE users (id INT, username TEXT);\nturso> INSERT INTO users VALUES (1, 'alice');\nturso> INSERT INTO users VALUES (2, 'bob');\nturso> SELECT * FROM users;\n1|alice\n2|bob\nYou can also build and run the latest development version with:\ncargo run\nIf you like docker, we got you covered. Simply run this in the root folder:\nmake docker-cli-build\n&&\n\\\nmake docker-cli-run\nü¶Ä Rust\ncargo add turso\nExample usage:\nlet\ndb =\nBuilder\n::\nnew_local\n(\n\"sqlite.db\"\n)\n.\nbuild\n(\n)\n.\nawait\n?\n;\nlet\nconn = db\n.\nconnect\n(\n)\n?\n;\nlet\nres = conn\n.\nquery\n(\n\"SELECT * FROM users\"\n,\n(\n)\n)\n.\nawait\n?\n;\n‚ú® JavaScript\nnpm i @tursodatabase/database\nExample usage:\nimport\n{\nconnect\n}\nfrom\n'@tursodatabase/database'\n;\nconst\ndb\n=\nawait\nconnect\n(\n'sqlite.db'\n)\n;\nconst\nstmt\n=\ndb\n.\nprepare\n(\n'SELECT * FROM users'\n)\n;\nconst\nusers\n=\nstmt\n.\nall\n(\n)\n;\nconsole\n.\nlog\n(\nusers\n)\n;\nüêç Python\nuv pip install pyturso\nExample usage:\nimport\nturso\ncon\n=\nturso\n.\nconnect\n(\n\"sqlite.db\"\n)\ncur\n=\ncon\n.\ncursor\n()\nres\n=\ncur\n.\nexecute\n(\n\"SELECT * FROM users\"\n)\nprint\n(\nres\n.\nfetchone\n())\nü¶´ Go\ngo get github.com/tursodatabase/turso-go\ngo install github.com/tursodatabase/turso-go\nExample usage:\nimport\n(\n\"database/sql\"\n_\n\"github.com/tursodatabase/turso-go\"\n)\nconn\n,\n_\n=\nsql\n.\nOpen\n(\n\"turso\"\n,\n\"sqlite.db\"\n)\ndefer\nconn\n.\nClose\n()\nstmt\n,\n_\n:=\nconn\n.\nPrepare\n(\n\"select * from users\"\n)\ndefer\nstmt\n.\nClose\n()\nrows\n,\n_\n=\nstmt\n.\nQuery\n()\nfor\nrows\n.\nNext\n() {\nvar\nid\nint\nvar\nusername\nstring\n_\n:=\nrows\n.\nScan\n(\n&\nid\n,\n&\nusername\n)\nfmt\n.\nPrintf\n(\n\"User: ID: %d, Username: %s\n\\n\n\"\n,\nid\n,\nusername\n)\n}\n‚òïÔ∏è Java\nWe integrated Turso Database into JDBC. For detailed instructions on how to use Turso Database with java, please refer to\nthe\nREADME.md under bindings/java\n.\nü§ñ MCP Server Mode\nThe Turso CLI includes a built-in\nModel Context Protocol (MCP)\nserver that allows AI assistants to interact with your databases.\nStart the MCP server with:\ntursodb your_database.db --mcp\nConfiguration\nAdd Turso to your MCP client configuration:\n{\n\"mcpServers\"\n: {\n\"turso\"\n: {\n\"command\"\n:\n\"\n/path/to/.turso/tursodb\n\"\n,\n\"args\"\n: [\n\"\n/path/to/your/database.db\n\"\n,\n\"\n--mcp\n\"\n]\n    }\n  }\n}\nAvailable Tools\nThe MCP server provides nine tools for database interaction:\nopen_database\n- Open a new database\ncurrent_database\n- Describe the current database\nlist_tables\n- List all tables in the database\ndescribe_table\n- Describe the structure of a specific table\nexecute_query\n- Execute read-only SELECT queries\ninsert_data\n- Insert new data into tables\nupdate_data\n- Update existing data in tables\ndelete_data\n- Delete data from tables\nschema_change\n- Execute schema modification statements (CREATE TABLE, ALTER TABLE, DROP TABLE)\nOnce connected, you can ask your AI assistant:\n\"Show me all tables in the database\"\n\"What's the schema for the users table?\"\n\"Find all posts with more than 100 upvotes\"\n\"Insert a new user with name 'Alice' and email '\nalice@example.com\n'\"\nMCP Clients\nClaude Code\nIf you're using\nClaude Code\n, you can easily connect to your Turso MCP server using the built-in MCP management commands:\nQuick Setup\nAdd the MCP server\nto Claude Code:\nclaude mcp add my-database -- tursodb ./path/to/your/database.db --mcp\nRestart Claude Code\nto activate the connection\nStart querying\nyour database through natural language!\nCommand Breakdown\nclaude mcp add my-database -- tursodb ./path/to/your/database.db --mcp\n#\n‚Üë            ‚Üë       ‚Üë                           ‚Üë\n#\n|            |       |                           |\n#\nName         |       Database path               MCP flag\n#\nSeparator\nmy-database\n- Choose any name for your MCP server\n--\n- Required separator between Claude options and your command\ntursodb\n- The Turso database CLI\n./path/to/your/database.db\n- Path to your SQLite database file\n--mcp\n- Enables MCP server mode\nExample Usage\n#\nFor a local project database\ncd\n/your/project\nclaude mcp add my-project-db -- tursodb ./data/app.db --mcp\n#\nFor an absolute path\nclaude mcp add analytics-db -- tursodb /Users/you/databases/analytics.db --mcp\n#\nFor a specific project (local scope)\nclaude mcp add project-db --local -- tursodb ./database.db --mcp\nManaging MCP Servers\n#\nList all configured MCP servers\nclaude mcp list\n#\nGet details about a specific server\nclaude mcp get my-database\n#\nRemove an MCP server\nclaude mcp remove my-database\nClaude Desktop\nFor Claude Desktop, add the configuration to your\nclaude_desktop_config.json\nfile:\n{\n\"mcpServers\"\n: {\n\"turso\"\n: {\n\"command\"\n:\n\"\n/path/to/.turso/tursodb\n\"\n,\n\"args\"\n: [\n\"\n./path/to/your/database.db.db\n\"\n,\n\"\n--mcp\n\"\n]\n    }\n  }\n}\nCursor\nFor Cursor, configure MCP in your settings:\nOpen Cursor settings\nNavigate to Extensions ‚Üí MCP\nAdd a new server with:\nName\n:\nturso\nCommand\n:\n/path/to/.turso/tursodb\nArgs\n:\n[\"./path/to/your/database.db.db\", \"--mcp\"]\nAlternatively, you can add it to your Cursor configuration file directly.\nDirect JSON-RPC Usage\nThe MCP server runs as a single process that handles multiple JSON-RPC requests over stdin/stdout. Here's how to interact with it directly:\nExample with In-Memory Database\ncat\n<<\n'\nEOF\n' | tursodb --mcp\n{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"initialize\", \"params\": {\"protocolVersion\": \"2024-11-05\", \"capabilities\": {}, \"clientInfo\": {\"name\": \"client\", \"version\": \"1.0\"}}}\n{\"jsonrpc\": \"2.0\", \"id\": 2, \"method\": \"tools/call\", \"params\": {\"name\": \"schema_change\", \"arguments\": {\"query\": \"CREATE TABLE users (id INTEGER, name TEXT, email TEXT)\"}}}\n{\"jsonrpc\": \"2.0\", \"id\": 3, \"method\": \"tools/call\", \"params\": {\"name\": \"list_tables\", \"arguments\": {}}}\n{\"jsonrpc\": \"2.0\", \"id\": 4, \"method\": \"tools/call\", \"params\": {\"name\": \"insert_data\", \"arguments\": {\"query\": \"INSERT INTO users VALUES (1, 'Alice', 'alice@example.com')\"}}}\n{\"jsonrpc\": \"2.0\", \"id\": 5, \"method\": \"tools/call\", \"params\": {\"name\": \"execute_query\", \"arguments\": {\"query\": \"SELECT * FROM users\"}}}\nEOF\nExample with Existing Database\n#\nWorking with an existing database file\ncat\n<<\n'\nEOF\n' | tursodb mydb.db --mcp\n{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"initialize\", \"params\": {\"protocolVersion\": \"2024-11-05\", \"capabilities\": {}, \"clientInfo\": {\"name\": \"client\", \"version\": \"1.0\"}}}\n{\"jsonrpc\": \"2.0\", \"id\": 2, \"method\": \"tools/call\", \"params\": {\"name\": \"list_tables\", \"arguments\": {}}}\nEOF\nContributing\nWe'd love to have you contribute to Turso Database! Please check out the\ncontribution guide\nto get started.\nFound a data corruption bug? Get up to $1,000.00\nSQLite is loved because it is the most reliable database in the world. The next evolution of SQLite has\nto match or surpass this level of reliability. Turso is built with\nDeterministic Simulation Testing\nfrom the ground up, and is also tested by\nAntithesis\n.\nEven during Alpha, if you find a bug that leads to a data corruption and demonstrate\nhow our simulator failed to catch it, you can get up to $1,000.00. As the project matures we will\nincrease the size of the prize, and the scope of the bugs.\nMore details\nhere\n.\nYou can see an example of an awarded case on\n#2049\n.\nTurso core staff are not eligible.\nFAQ\nIs Turso Database ready for production use?\nTurso Database is currently under heavy development and is\nnot\nready for production use.\nHow is Turso Database different from Turso's libSQL?\nTurso Database is a project to build the next evolution of SQLite in Rust, with a strong open contribution focus and features like native async support, vector search, and more. The libSQL project is also an attempt to evolve SQLite in a similar direction, but through a fork rather than a rewrite.\nRewriting SQLite in Rust started as an unassuming experiment, and due to its incredible success, replaces libSQL as our intended direction. At this point, libSQL is production ready, Turso Database is not - although it is evolving rapidly. More details\nhere\n.\nPublications\nPekka Enberg, Sasu Tarkoma, Jon Crowcroft Ashwin Rao (2024). Serverless Runtime / Database Co-Design With Asynchronous I/O. In\nEdgeSys ‚Äò24\n.\n[PDF]\nPekka Enberg, Sasu Tarkoma, and Ashwin Rao (2023). Towards Database and Serverless Runtime Co-Design. In\nCoNEXT-SW ‚Äô23\n. [\nPDF\n] [\nSlides\n]\nLicense\nThis project is licensed under the\nMIT license\n.\nContribution\nUnless you explicitly state otherwise, any contribution intentionally submitted\nfor inclusion in Turso Database by you, shall be licensed as MIT, without any additional\nterms or conditions.\nPartners\nThanks to all the partners of Turso!\nContributors\nThanks to all the contributors to Turso Database!",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 70",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 14,012"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/tursodatabase/turso"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/simular-ai/Agent-S",
      "title": "simular-ai/Agent-S",
      "date": null,
      "executive_summary": [
        "Agent S: an open agentic framework that uses computers like a human",
        "---",
        "Agent S:\n  Use Computer Like a Human\nüåê\n[S3 blog]\nüìÑ\n[S3 Paper]\nüé•\n[S3 Video]\nüåê\n[S2 blog]\nüìÑ\n[S2 Paper (COLM 2025)]\nüé•\n[S2 Video]\nüåê\n[S1 blog]\nüìÑ\n[S1 Paper (ICLR 2025)]\nüé•\n[S1 Video]\nDeutsch\n|\nEspa√±ol\n|\nfran√ßais\n|\nÊó•Êú¨Ë™û\n|\nÌïúÍµ≠Ïñ¥\n|\nPortugu√™s\n|\n–†—É—Å—Å–∫–∏–π\n|\n‰∏≠Êñá\nSkip the setup? Try Agent S in\nSimular Cloud\nü•≥ Updates\n2025/10/02\n: Released Agent S3 and its\ntechnical paper\n, setting a new SOTA of\n69.9%\non OSWorld (approaching 72% human performance), with strong generalizability on WindowsAgentArena and AndroidWorld! It is also simpler, faster, and more flexible.\n2025/08/01\n: Agent S2.5 is released (gui-agents v0.2.5): simpler, better, and faster! New SOTA on\nOSWorld-Verified\n!\n2025/07/07\n: The\nAgent S2 paper\nis accepted to COLM 2025! See you in Montreal!\n2025/04/27\n: The Agent S paper won the Best Paper Award üèÜ at ICLR 2025 Agentic AI for Science Workshop!\n2025/04/01\n: Released the\nAgent S2 paper\nwith new SOTA results on OSWorld, WindowsAgentArena, and AndroidWorld!\n2025/03/12\n: Released Agent S2 along with v0.2.0 of\ngui-agents\n, the new state-of-the-art for computer use agents (CUA), outperforming OpenAI's CUA/Operator and Anthropic's Claude 3.7 Sonnet Computer-Use!\n2025/01/22\n: The\nAgent S paper\nis accepted to ICLR 2025!\n2025/01/21\n: Released v0.1.2 of\ngui-agents\nlibrary, with support for Linux and Windows!\n2024/12/05\n: Released v0.1.0 of\ngui-agents\nlibrary, allowing you to use Agent-S for Mac, OSWorld, and WindowsAgentArena with ease!\n2024/10/10\n: Released the\nAgent S paper\nand codebase!\nTable of Contents\nüí° Introduction\nüéØ Current Results\nüõ†Ô∏è Installation & Setup\nüöÄ Usage\nü§ù Acknowledgements\nüí¨ Citation\nüí° Introduction\nWelcome to\nAgent S\n, an open-source framework designed to enable autonomous interaction with computers through Agent-Computer Interface. Our mission is to build intelligent GUI agents that can learn from past experiences and perform complex tasks autonomously on your computer.\nWhether you're interested in AI, automation, or contributing to cutting-edge agent-based systems, we're excited to have you here!\nüéØ Current Results\nOn OSWorld, Agent S3 alone reaches 62.6% in the 100-step setting, already exceeding the previous state of the art of 61.4% (Claude Sonnet 4.5). With the addition of Behavior Best-of-N, performance climbs even higher to 69.9%, bringing computer-use agents to within just a few points of human-level accuracy (72%).\nAgent S3 also demonstrates strong zero-shot generalization. On WindowsAgentArena, accuracy rises from 50.2% using only Agent S3 to 56.6% by selecting from 3 rollouts. Similarly on AndroidWorld, performance improves from 68.1% to 71.6%\nüõ†Ô∏è Installation & Setup\nPrerequisites\nSingle Monitor\n: Our agent is designed for single monitor screens\nSecurity\n: The agent runs Python code to control your computer - use with care\nSupported Platforms\n: Linux, Mac, and Windows\nInstallation\nTo install Agent S3 without cloning the repository, run\npip install gui-agents\nIf you would like to test Agent S3 while making changes, clone the repository and install using\npip install -e .\nDon't forget to also\nbrew install tesseract\n! Pytesseract requires this extra installation to work.\nAPI Configuration\nOption 1: Environment Variables\nAdd to your\n.bashrc\n(Linux) or\n.zshrc\n(MacOS):\nexport\nOPENAI_API_KEY=\n<\nYOUR_API_KEY\n>\nexport\nANTHROPIC_API_KEY=\n<\nYOUR_ANTHROPIC_API_KEY\n>\nexport\nHF_TOKEN=\n<\nYOUR_HF_TOKEN\n>\nOption 2: Python Script\nimport\nos\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\n\"<YOUR_API_KEY>\"\nSupported Models\nWe support Azure OpenAI, Anthropic, Gemini, Open Router, and vLLM inference. See\nmodels.md\nfor details.\nGrounding Models (Required)\nFor optimal performance, we recommend\nUI-TARS-1.5-7B\nhosted on Hugging Face Inference Endpoints or another provider. See\nHugging Face Inference Endpoints\nfor setup instructions.\nüöÄ Usage\n‚ö°Ô∏è\nRecommended Setup:\nFor the best configuration, we recommend using\nOpenAI gpt-5-2025-08-07\nas the main model, paired with\nUI-TARS-1.5-7B\nfor grounding.\nCLI\nNote, this is running Agent S3, our improved agent, without bBoN.\nRun Agent S3 with the required parameters:\nagent_s \\\n    --provider openai \\\n    --model gpt-5-2025-08-07 \\\n    --ground_provider huggingface \\\n    --ground_url http://localhost:8080 \\\n    --ground_model ui-tars-1.5-7b \\\n    --grounding_width 1920 \\\n    --grounding_height 1080\nLocal Coding Environment (Optional)\nFor tasks that require code execution (e.g., data processing, file manipulation, system automation), you can enable the local coding environment:\nagent_s \\\n    --provider openai \\\n    --model gpt-5-2025-08-07 \\\n    --ground_provider huggingface \\\n    --ground_url http://localhost:8080 \\\n    --ground_model ui-tars-1.5-7b \\\n    --grounding_width 1920 \\\n    --grounding_height 1080 \\\n    --enable_local_env\n‚ö†Ô∏è\nWARNING\n: The local coding environment executes arbitrary Python and Bash code locally on your machine. Only use this feature in trusted environments and with trusted inputs.\nRequired Parameters\n--provider\n: Main generation model provider (e.g., openai, anthropic, etc.) - Default: \"openai\"\n--model\n: Main generation model name (e.g., gpt-5-2025-08-07) - Default: \"gpt-5-2025-08-07\"\n--ground_provider\n: The provider for the grounding model -\nRequired\n--ground_url\n: The URL of the grounding model -\nRequired\n--ground_model\n: The model name for the grounding model -\nRequired\n--grounding_width\n: Width of the output coordinate resolution from the grounding model -\nRequired\n--grounding_height\n: Height of the output coordinate resolution from the grounding model -\nRequired\nOptional Parameters\n--model_temperature\n: The temperature to fix all model calls to (necessary to set to 1.0 for models like o3 but can be left blank for other models)\nGrounding Model Dimensions\nThe grounding width and height should match the output coordinate resolution of your grounding model:\nUI-TARS-1.5-7B\n: Use\n--grounding_width 1920 --grounding_height 1080\nUI-TARS-72B\n: Use\n--grounding_width 1000 --grounding_height 1000\nOptional Parameters\n--model_url\n: Custom API URL for main generation model - Default: \"\"\n--model_api_key\n: API key for main generation model - Default: \"\"\n--ground_api_key\n: API key for grounding model endpoint - Default: \"\"\n--max_trajectory_length\n: Maximum number of image turns to keep in trajectory - Default: 8\n--enable_reflection\n: Enable reflection agent to assist the worker agent - Default: True\n--enable_local_env\n: Enable local coding environment for code execution (WARNING: Executes arbitrary code locally) - Default: False\nLocal Coding Environment Details\nThe local coding environment enables Agent S3 to execute Python and Bash code directly on your machine. This is particularly useful for:\nData Processing\n: Manipulating spreadsheets, CSV files, or databases\nFile Operations\n: Bulk file processing, content extraction, or file organization\nSystem Automation\n: Configuration changes, system setup, or automation scripts\nCode Development\n: Writing, editing, or executing code files\nText Processing\n: Document manipulation, content editing, or formatting\nWhen enabled, the agent can use the\ncall_code_agent\naction to execute code blocks for tasks that can be completed through programming rather than GUI interaction.\nRequirements:\nPython\n: The same Python interpreter used to run Agent S3 (automatically detected)\nBash\n: Available at\n/bin/bash\n(standard on macOS and Linux)\nSystem Permissions\n: The agent runs with the same permissions as the user executing it\nSecurity Considerations:\nThe local environment executes arbitrary code with the same permissions as the user running the agent\nOnly enable this feature in trusted environments\nBe cautious when the agent generates code for system-level operations\nConsider running in a sandboxed environment for untrusted tasks\nBash scripts are executed with a 30-second timeout to prevent hanging processes\ngui_agents\nSDK\nFirst, we import the necessary modules.\nAgentS3\nis the main agent class for Agent S3.\nOSWorldACI\nis our grounding agent that translates agent actions into executable python code.\nimport\npyautogui\nimport\nio\nfrom\ngui_agents\n.\ns3\n.\nagents\n.\nagent_s\nimport\nAgentS3\nfrom\ngui_agents\n.\ns3\n.\nagents\n.\ngrounding\nimport\nOSWorldACI\nfrom\ngui_agents\n.\ns3\n.\nutils\n.\nlocal_env\nimport\nLocalEnv\n# Optional: for local coding environment\n# Load in your API keys.\nfrom\ndotenv\nimport\nload_dotenv\nload_dotenv\n()\ncurrent_platform\n=\n\"linux\"\n# \"darwin\", \"windows\"\nNext, we define our engine parameters.\nengine_params\nis used for the main agent, and\nengine_params_for_grounding\nis for grounding. For\nengine_params_for_grounding\n, we support custom endpoints like HuggingFace TGI, vLLM, and Open Router.\nengine_params\n=\n{\n\"engine_type\"\n:\nprovider\n,\n\"model\"\n:\nmodel\n,\n\"base_url\"\n:\nmodel_url\n,\n# Optional\n\"api_key\"\n:\nmodel_api_key\n,\n# Optional\n\"temperature\"\n:\nmodel_temperature\n# Optional\n}\n# Load the grounding engine from a custom endpoint\nground_provider\n=\n\"<your_ground_provider>\"\nground_url\n=\n\"<your_ground_url>\"\nground_model\n=\n\"<your_ground_model>\"\nground_api_key\n=\n\"<your_ground_api_key>\"\n# Set grounding dimensions based on your model's output coordinate resolution\n# UI-TARS-1.5-7B: grounding_width=1920, grounding_height=1080\n# UI-TARS-72B: grounding_width=1000, grounding_height=1000\ngrounding_width\n=\n1920\n# Width of output coordinate resolution\ngrounding_height\n=\n1080\n# Height of output coordinate resolution\nengine_params_for_grounding\n=\n{\n\"engine_type\"\n:\nground_provider\n,\n\"model\"\n:\nground_model\n,\n\"base_url\"\n:\nground_url\n,\n\"api_key\"\n:\nground_api_key\n,\n# Optional\n\"grounding_width\"\n:\ngrounding_width\n,\n\"grounding_height\"\n:\ngrounding_height\n,\n}\nThen, we define our grounding agent and Agent S3.\n# Optional: Enable local coding environment\nenable_local_env\n=\nFalse\n# Set to True to enable local code execution\nlocal_env\n=\nLocalEnv\n()\nif\nenable_local_env\nelse\nNone\ngrounding_agent\n=\nOSWorldACI\n(\nenv\n=\nlocal_env\n,\n# Pass local_env for code execution capability\nplatform\n=\ncurrent_platform\n,\nengine_params_for_generation\n=\nengine_params\n,\nengine_params_for_grounding\n=\nengine_params_for_grounding\n,\nwidth\n=\n1920\n,\n# Optional: screen width\nheight\n=\n1080\n# Optional: screen height\n)\nagent\n=\nAgentS3\n(\nengine_params\n,\ngrounding_agent\n,\nplatform\n=\ncurrent_platform\n,\nmax_trajectory_length\n=\n8\n,\n# Optional: maximum image turns to keep\nenable_reflection\n=\nTrue\n# Optional: enable reflection agent\n)\nFinally, let's query the agent!\n# Get screenshot.\nscreenshot\n=\npyautogui\n.\nscreenshot\n()\nbuffered\n=\nio\n.\nBytesIO\n()\nscreenshot\n.\nsave\n(\nbuffered\n,\nformat\n=\n\"PNG\"\n)\nscreenshot_bytes\n=\nbuffered\n.\ngetvalue\n()\nobs\n=\n{\n\"screenshot\"\n:\nscreenshot_bytes\n,\n}\ninstruction\n=\n\"Close VS Code\"\ninfo\n,\naction\n=\nagent\n.\npredict\n(\ninstruction\n=\ninstruction\n,\nobservation\n=\nobs\n)\nexec\n(\naction\n[\n0\n])\nRefer to\ngui_agents/s3/cli_app.py\nfor more details on how the inference loop works.\nOSWorld\nTo deploy Agent S3 in OSWorld, follow the\nOSWorld Deployment instructions\n.\nüí¨ Citations\nIf you find this codebase useful, please cite:\n@misc{Agent-S3,\n      title={The Unreasonable Effectiveness of Scaling Agents for Computer Use}, \n      author={Gonzalo Gonzalez-Pumariega and Vincent Tu and Chih-Lun Lee and Jiachen Yang and Ang Li and Xin Eric Wang},\n      year={2025},\n      eprint={2510.02250},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2510.02250}, \n}\n\n@misc{Agent-S2,\n      title={Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents}, \n      author={Saaket Agashe and Kyle Wong and Vincent Tu and Jiachen Yang and Ang Li and Xin Eric Wang},\n      year={2025},\n      eprint={2504.00906},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2504.00906}, \n}\n\n@inproceedings{Agent-S,\n    title={{Agent S: An Open Agentic Framework that Uses Computers Like a Human}},\n    author={Saaket Agashe and Jiuzhou Han and Shuyu Gan and Jiachen Yang and Ang Li and Xin Eric Wang},\n    booktitle={International Conference on Learning Representations (ICLR)},\n    year={2025},\n    url={https://arxiv.org/abs/2410.08164}\n}\nStar History",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 70",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 6,928"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/simular-ai/Agent-S"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/open-ani/animeko",
      "title": "open-ani/animeko",
      "date": null,
      "executive_summary": [
        "ÈõÜÊâæÁï™„ÄÅËøΩÁï™„ÄÅÁúãÁï™ÁöÑ‰∏ÄÁ´ôÂºèÂºπÂπïËøΩÁï™Âπ≥Âè∞Ôºå‰∫ëÊî∂ËóèÂêåÊ≠• (Bangumi)ÔºåÁ¶ªÁ∫øÁºìÂ≠òÔºåBitTorrentÔºåÂºπÂπï‰∫ëËøáÊª§„ÄÇ100% Kotlin/Compose Multiplatform",
        "---",
        "Ê≠£ÂºèÁâà\nÊµãËØïÁâà\nËÆ®ËÆ∫Áæ§\nAnimeko ÊîØÊåÅ‰∫ëÂêåÊ≠•ËßÇÁúãËÆ∞ÂΩï (\nBangumi\n)„ÄÅÂ§öËßÜÈ¢ëÊï∞ÊçÆÊ∫ê„ÄÅÁºìÂ≠ò„ÄÅÂºπÂπï„ÄÅ‰ª•ÂèäÊõ¥Â§öÂäüËÉΩÔºåÊèê‰æõÂ∞ΩÂèØËÉΩÁÆÄÂçï‰∏îËàíÈÄÇÁöÑËøΩÁï™‰ΩìÈ™å„ÄÇ\nAnimeko ÊõæÁî®Âêç AniÔºåÁé∞Âú®‰πüÁÆÄÁß∞ Ani„ÄÇ\nÁ´ãÂç≥‰∏ãËΩΩ\nplay.mp4\n‰∏ªË¶ÅÂäüËÉΩ\nÊµèËßàÊù•Ëá™\nBangumi\nÁöÑÁï™Ââß‰ø°ÊÅØ‰ª•ÂèäÁ§æÂå∫ËØÑ‰ª∑\n‰∏∞ÂØåÁöÑÊ£ÄÁ¥¢ÊñπÂºèÔºöÊñ∞Áï™Êó∂Èó¥Ë°®„ÄÅÊ†áÁ≠æÊêúÁ¥¢\nÁî± Bangumi Âíå Animeko ÊúçÂä°Á´ØÂÖ±ÂêåÊèê‰æõÁöÑÁ≤æÁ°ÆÊñ∞Áï™Êó∂Èó¥Ë°®\n‰∫ëÂêåÊ≠•ËøΩÁï™ËøõÂ∫¶\nÁúÅÂøÉÁöÑËøΩÁï™ËøõÂ∫¶ÁÆ°ÁêÜÔºåÁúãÂÆåËßÜÈ¢ëËá™Âä®Êõ¥Êñ∞ËøõÂ∫¶\nÊâìÂºÄ APP Á´ãÂç≥ÁªßÁª≠ËßÇÁúãÔºåÊó†ÈúÄÂõûÊÉ≥‰∏äÊ¨°ÁúãÂà∞‰∫ÜÂì™\nËÅöÂêàÊï∞ÊçÆÊ∫ê\nËÅöÂêàËßÜÈ¢ëÊï∞ÊçÆÊ∫êÔºåÂÖ®Ëá™Âä®ÈÄâÊã©\nËøòÊîØÊåÅ BitTorrent„ÄÅJellyfin„ÄÅEmby„ÄÅ‰ª•ÂèäËá™ÂÆö‰πâÊ∫ê\nËÅöÂêàÂÖ®ÁΩëÂºπÂπïÊ∫êÔºà\nÂºπÂºπplay\nÔºâÔºå‰ª•Âèä Animeko Ëá™Â∑±ÁöÑ\nÂºπÂπïÊúçÂä°\nÁ¶ªÁ∫øÁºìÂ≠ò\nÊâÄÊúâÊï∞ÊçÆÊ∫êÈÉΩËÉΩÁºìÂ≠ò\nÁ≤æÁæéÁïåÈù¢\nÈÄÇÈÖçÂπ≥ÊùøÂíåÂ§ßÂ±èËÆæÂ§á\nÊõ¥Â§ö‰∏™ÊÄßËÆæÁΩÆ\n‰∏ãËΩΩ\nAnimeko ÊîØÊåÅÊâÄÊúâ‰∏ªÊµÅÂπ≥Âè∞ÔºöAndroid„ÄÅiOS„ÄÅWindows„ÄÅmacOS„ÄÅLinux„ÄÇ\nÁ®≥ÂÆöÁâàÊú¨: ÊØè‰∏§Âë®Êõ¥Êñ∞, ÂäüËÉΩÁ®≥ÂÆö\n‰∏ãËΩΩÁ®≥ÂÆöÁâàÊú¨\nÈÄöÂ∏∏Âª∫ËÆÆ‰ΩøÁî®Á®≥ÂÆöÁâàÊú¨. Â¶ÇÊûú‰Ω†ÊÑøÊÑèÂèÇ‰∏éÊµãËØïÂπ∂Êã•Êúâ‰∏ÄÂÆöÁöÑÂØπ bug ÁöÑÂ§ÑÁêÜËÉΩÂäõ, ‰πüÊ¨¢Ëøé‰ΩøÁî®ÊµãËØïÁâàÊú¨Êõ¥Âø´‰ΩìÈ™åÊñ∞ÂäüËÉΩ.\nÂÖ∑‰ΩìÁâàÊú¨Á±ªÂûãÂèØÊü•Áúã‰∏ãÊñπ.\nÊµãËØïÁâàÊú¨: ÊØè‰∏§Â§©Êõ¥Êñ∞, ‰ΩìÈ™åÊúÄÊñ∞ÂäüËÉΩ\n‰∏ãËΩΩÊµãËØïÁâàÊú¨\nÁÇπÂáªÊü•ÁúãÂÖ∑‰ΩìÁâàÊú¨Á±ªÂûã\nAnimeko ÈááÁî®ËØ≠‰πâÂåñÁâàÊú¨Âè∑, ÁÆÄÂçïÊù•ËØ¥Â∞±ÊòØ\n4.x.y\nÁöÑÊ†ºÂºè. Êúâ‰ª•‰∏ãÂá†ÁßçÁâàÊú¨Á±ªÂûã:\nÁ®≥ÂÆöÁâàÊú¨:\nÊñ∞ÁâπÊÄßÂèëÂ∏É\n: ÂΩì\nx\nÊõ¥Êñ∞Êó∂, ‰ºöÊúâÊñ∞ÁâπÊÄßÁöÑÂèëÂ∏É. ÈÄöÂ∏∏‰∏∫ 2 Âë®‰∏ÄÊ¨°.\nBug ‰øÆÂ§ç\n: ÂΩì\ny\nÊõ¥Êñ∞Êó∂, Âè™‰ºöÊúâÈíàÂØπÂâç‰∏™ÁâàÊú¨ÁöÑÈáçË¶ÅÁöÑ bug ‰øÆÂ§ç. Ëøô‰∫õ Bug ‰øÆÂ§çÁâàÊú¨Á©øÊèíÂú®Êñ∞ÁâπÊÄßÊõ¥Êñ∞ÁöÑÈó¥Èöî‰∏≠,\nÊó∂Èó¥‰∏çÂõ∫ÂÆö.\nÂú®Á®≥ÂÆöÁâàÊú¨ÁöÑÂèëÂ∏ÉÂë®Êúü‰πãÈó¥, ‰ºöÂèëÂ∏ÉÊµãËØïÁâàÊú¨:\nAlpha ÊµãËØïÁâà\n: ÊâÄÊúâÈáçÂ§ßÊñ∞ÂäüËÉΩÈÉΩ‰ºöÈ¶ñÂÖàÂèëÂ∏ÉÂà∞\nalpha\nÊµãËØïÈÄöÈÅì, ÂÆ¢Êà∑Á´ØÂÜÖÂèØ‰ΩøÁî® \"ÊØèÊó•ÊûÑÂª∫\"\nÊé•Êî∂Êõ¥Êñ∞. Ëøô‰∫õÊñ∞ÂäüËÉΩÈùûÂ∏∏‰∏çÁ®≥ÂÆö, ÈÄÇÂêàÁÉ≠ÊÉÖÁöÑÂÖàÈîãÊµãËØïÂëò!\nBeta ÊµãËØïÁâà\n: Âú®ÂäüËÉΩÁªèËøá alpha ÊµãËØï‰øÆÂ§çÈáçÂ§ßÈóÆÈ¢òÂêé, ‰ºöËøõÂÖ•\nbeta\nÊµãËØïÈÄöÈÅì,\nÂú®ÂÆ¢Êà∑Á´ØÂÜÖÂêçÁß∞‰∏∫ \"ÊµãËØïÁâà\". Ê≠§ÁâàÊú¨‰ªçÁÑ∂‰∏çÁ®≥ÂÆö, ÊòØ‰∏Ä‰∏™Âπ≥Ë°°Êñ∞ÂäüËÉΩÂíåÁ®≥ÂÆöÊÄßÁöÑÈÄâÊã©\nÊäÄÊúØÊÄªËßà\nÂ¶ÇÊûú‰Ω†ÊòØÂºÄÂèëËÄÖÔºåÊàë‰ª¨ÊÄªÊòØÊ¨¢Ëøé‰Ω†Êèê‰∫§ PR ÂèÇ‰∏éÂºÄÂèëÔºÅ\n‰ª•‰∏ãÂá†ÁÇπÂèØ‰ª•Áªô‰Ω†‰∏Ä‰∏™ÊäÄÊúØ‰∏äÁöÑÂ§ßÊ¶Ç‰∫ÜËß£„ÄÇ\nKotlin Â§öÂπ≥Âè∞\nÊû∂ÊûÑÔºõ\n‰ΩøÁî®Êñ∞‰∏Ä‰ª£ÂìçÂ∫îÂºè UI Ê°ÜÊû∂\nCompose Multiplatform\nÊûÑÂª∫\nUIÔºõ\nÂÜÖÁΩÆ‰∏ì‰∏∫ Animeko ÊâìÈÄ†ÁöÑ‚ÄúÂü∫‰∫é\nlibtorrent\nÁöÑ BitTorrent ÂºïÊìéÔºå‰ºòÂåñËæπ‰∏ãËæπÊí≠ÁöÑ‰ΩìÈ™åÔºõ\nÈ´òÊÄßËÉΩÂºπÂπïÂºïÊìéÔºåÂÖ¨ÁõäÂºπÂπïÊúçÂä°Âô® + ÁΩëÁªúÂºπÂπïÊ∫êÔºõ\nÈÄÇÈÖçÂ§öÂπ≥Âè∞ÁöÑ\nËßÜÈ¢ëÊí≠ÊîæÂô®\nÔºåAndroid Â∫ïÂ±Ç‰∏∫\nExoPlayer\nÔºåPC Â∫ïÂ±Ç‰∏∫\nVLC\nÔºõ\nÂ§öÁ±ªÂûãÊï∞ÊçÆÊ∫êÈÄÇÈÖçÔºåÂÜÖÁΩÆ\nÂä®Êº´Ëä±Âõ≠\n„ÄÅ\nMikan\nÔºåÊã•ÊúâÂº∫Â§ßÁöÑËá™ÂÆö‰πâÊï∞ÊçÆÊ∫êÁºñËæëÂô®ÂíåËá™Âä®Êï∞ÊçÆÊ∫êÈÄâÊã©Âô®„ÄÇ\nÂèÇ‰∏éÂºÄÂèë\nÊ¨¢Ëøé‰Ω†Êèê‰∫§ PR ÂèÇ‰∏éÂºÄÂèëÔºå\nÊúâÂÖ≥È°πÁõÆÊäÄÊúØÁªÜËäÇËØ∑ÂèÇËÄÉ\nCONTRIBUTING\n„ÄÇ\nFAQ\nËµÑÊ∫êÊù•Ê∫êÊòØ‰ªÄ‰πà?\nÂÖ®ÈÉ®ËßÜÈ¢ëÊï∞ÊçÆÈÉΩÊù•Ëá™ÁΩëÁªú, Animeko Êú¨Ë∫´‰∏çÂ≠òÂÇ®‰ªª‰ΩïËßÜÈ¢ëÊï∞ÊçÆ„ÄÇ\nAnimeko ÊîØÊåÅ‰∏§Â§ßÊï∞ÊçÆÊ∫êÁ±ªÂûãÔºöBT ÂíåÂú®Á∫ø„ÄÇBT Ê∫êÂç≥‰∏∫ÂÖ¨ÂÖ± BitTorrent P2P ÁΩëÁªúÔºå\nÊØè‰∏™Âú® BT\nÁΩëÁªú‰∏äÁöÑ‰∫∫ÈÉΩÂèØÂàÜ‰∫´Ëá™Â∑±Êã•ÊúâÁöÑËµÑÊ∫ê‰æõ‰ªñ‰∫∫‰∏ãËΩΩ„ÄÇÂú®Á∫øÊ∫êÂç≥‰∏∫ÂÖ∂‰ªñËßÜÈ¢ëËµÑÊ∫êÁΩëÁ´ôÂàÜ‰∫´ÁöÑÂÜÖÂÆπ„ÄÇAnimeko\nÊú¨Ë∫´Âπ∂‰∏çÊèê‰æõ‰ªª‰ΩïËßÜÈ¢ëËµÑÊ∫ê„ÄÇ\nÊú¨ÁùÄ‰∫íÂä©Á≤æÁ•ûÔºå‰ΩøÁî® BT Ê∫êÊó∂ Animeko ‰ºöËá™Âä®ÂÅöÁßç (ÂàÜ‰∫´Êï∞ÊçÆ)„ÄÇ\nBT ÊåáÁ∫π‰∏∫\n-AL4123-\nÔºåÂÖ∂‰∏≠\n4123\n‰∏∫ÁâàÊú¨Âè∑\n4.12.3\nÔºõUA ‰∏∫Á±ª‰ºº\nani_libtorrent/4.12.3\n„ÄÇ\nÂºπÂπïÊù•Ê∫êÊòØ‰ªÄ‰πà?\nAnimeko Êã•ÊúâËá™Â∑±ÁöÑÂÖ¨ÁõäÂºπÂπïÊúçÂä°Âô®ÔºåÂú® Animeko Â∫îÁî®ÂÜÖÂèëÈÄÅÁöÑÂºπÂπïÂ∞Ü‰ºöÂèëÈÄÅÂà∞ÂºπÂπïÊúçÂä°Âô®‰∏ä„ÄÇÊØèÊù°ÂºπÂπïÈÉΩ‰ºö‰ª•\nBangumi\nÁî®Êà∑ÂêçÁªëÂÆö‰ª•Èò≤Êª•Áî®ÔºàÂπ∂ËÄÉËôëÊú™Êù•Â¢ûÂä†‰∏æÊä•ÂíåÂ±èËîΩÂäüËÉΩÔºâ„ÄÇ\nAnimeko Ëøò‰ºö‰ªé\nÂºπÂºπplay\nËé∑ÂèñÂÖ≥ËÅîÂºπÂπïÔºåÂºπÂºπplayËøò‰ºö‰ªéÂÖ∂‰ªñÂºπÂπïÂπ≥Âè∞‰æãÂ¶ÇÂìîÂì©ÂìîÂì©Ê∏ØÊæ≥Âè∞ÂíåÂ∑¥ÂìàÂßÜÁâπËé∑ÂèñÂºπÂπï„ÄÇ\nÁï™ÂâßÊØèÈõÜÂèØÊã•ÊúâÂá†ÂçÅÂà∞Âá†ÂçÉÊù°‰∏çÁ≠âÁöÑÂºπÂπïÈáè„ÄÇ",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 68",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 11,874"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/open-ani/animeko"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/LadybirdBrowser/ladybird",
      "title": "LadybirdBrowser/ladybird",
      "date": null,
      "executive_summary": [
        "Truly independent web browser",
        "---",
        "Ladybird\nLadybird\nis a truly independent web browser, using a novel engine based on web standards.\nImportant\nLadybird is in a pre-alpha state, and only suitable for use by developers\nFeatures\nWe aim to build a complete, usable browser for the modern web.\nLadybird uses a multi-process architecture with a main UI process, several WebContent renderer processes,\nan ImageDecoder process, and a RequestServer process.\nImage decoding and network connections are done out of process to be more robust against malicious content.\nEach tab has its own renderer process, which is sandboxed from the rest of the system.\nAt the moment, many core library support components are inherited from SerenityOS:\nLibWeb: Web rendering engine\nLibJS: JavaScript engine\nLibWasm: WebAssembly implementation\nLibCrypto/LibTLS: Cryptography primitives and Transport Layer Security\nLibHTTP: HTTP/1.1 client\nLibGfx: 2D Graphics Library, Image Decoding and Rendering\nLibUnicode: Unicode and locale support\nLibMedia: Audio and video playback\nLibCore: Event loop, OS abstraction layer\nLibIPC: Inter-process communication\nHow do I build and run this?\nSee\nbuild instructions\nfor information on how to build Ladybird.\nLadybird runs on Linux, macOS, Windows (with WSL2), and many other *Nixes.\nHow do I read the documentation?\nCode-related documentation can be found in the\ndocumentation\nfolder.\nGet in touch and participate!\nJoin\nour Discord server\nto participate in development discussion.\nPlease read\nGetting started contributing\nif you plan to contribute to Ladybird for the first time.\nBefore opening an issue, please see the\nissue policy\nand the\ndetailed issue-reporting guidelines\n.\nThe full contribution guidelines can be found in\nCONTRIBUTING.md\n.\nLicense\nLadybird is licensed under a 2-clause BSD license.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 61",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 49,502"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/LadybirdBrowser/ladybird"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/EFForg/rayhunter",
      "title": "EFForg/rayhunter",
      "date": null,
      "executive_summary": [
        "Rust tool to detect cell site simulators on an orbic mobile hotspot",
        "---",
        "Rayhunter\nRayhunter is a project for detecting IMSI catchers, also known as cell-site simulators or stingrays. It was first designed to run on a cheap mobile hotspot called the Orbic RC400L, but thanks to community efforts can\nsupport some other devices as well\n.\nIt's also designed to be as easy to install and use as possible, regardless of your level of technical skills, and to minimize false positives.\n‚Üí  Check out the\ninstallation guide\nto get started.\n‚Üí To learn more about the aim of the project, and about IMSI catchers in general, please check out our\nintroductory blog post\n.\n‚Üí For discussion, help, or to join the mattermost channel and get involved with the project and community check out the\nmany ways listed here\n!\n‚Üí To learn more about the project in general check out the\nRayhunter Book\n.\nLEGAL DISCLAIMER:\nUse this program at your own risk. We believe running this program does not currently violate any laws or regulations in the United States. However, we are not responsible for civil or criminal liability resulting from the use of this software. If you are located outside of the US please consult with an attorney in your country to help you assess the legal risks of running this program.\nGood Hunting!",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 54",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 3,053"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/EFForg/rayhunter"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/ggml-org/llama.cpp",
      "title": "ggml-org/llama.cpp",
      "date": null,
      "executive_summary": [
        "LLM inference in C/C++",
        "---",
        "llama.cpp\nManifesto\n/\nggml\n/\nops\nLLM inference in C/C++\nRecent API changes\nChangelog for\nlibllama\nAPI\nChangelog for\nllama-server\nREST API\nHot topics\nguide : running gpt-oss with llama.cpp\n[FEEDBACK] Better packaging for llama.cpp to support downstream consumers ü§ó\nSupport for the\ngpt-oss\nmodel with native MXFP4 format has been added |\nPR\n|\nCollaboration with NVIDIA\n|\nComment\nHot PRs:\nAll\n|\nOpen\nMultimodal support arrived in\nllama-server\n:\n#12898\n|\ndocumentation\nVS Code extension for FIM completions:\nhttps://github.com/ggml-org/llama.vscode\nVim/Neovim plugin for FIM completions:\nhttps://github.com/ggml-org/llama.vim\nIntroducing GGUF-my-LoRA\n#10123\nHugging Face Inference Endpoints now support GGUF out of the box!\n#9669\nHugging Face GGUF editor:\ndiscussion\n|\ntool\nQuick start\nGetting started with llama.cpp is straightforward. Here are several ways to install it on your machine:\nInstall\nllama.cpp\nusing\nbrew, nix or winget\nRun with Docker - see our\nDocker documentation\nDownload pre-built binaries from the\nreleases page\nBuild from source by cloning this repository - check out\nour build guide\nOnce installed, you'll need a model to work with. Head to the\nObtaining and quantizing models\nsection to learn more.\nExample command:\n#\nUse a local model file\nllama-cli -m my_model.gguf\n#\nOr download and run a model directly from Hugging Face\nllama-cli -hf ggml-org/gemma-3-1b-it-GGUF\n#\nLaunch OpenAI-compatible API server\nllama-server -hf ggml-org/gemma-3-1b-it-GGUF\nDescription\nThe main goal of\nllama.cpp\nis to enable LLM inference with minimal setup and state-of-the-art performance on a wide\nrange of hardware - locally and in the cloud.\nPlain C/C++ implementation without any dependencies\nApple silicon is a first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks\nAVX, AVX2, AVX512 and AMX support for x86 architectures\n1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit integer quantization for faster inference and reduced memory use\nCustom CUDA kernels for running LLMs on NVIDIA GPUs (support for AMD GPUs via HIP and Moore Threads GPUs via MUSA)\nVulkan and SYCL backend support\nCPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity\nThe\nllama.cpp\nproject is the main playground for developing new features for the\nggml\nlibrary.\nModels\nTypically finetunes of the base models below are supported as well.\nInstructions for adding support for new models:\nHOWTO-add-model.md\nText-only\nLLaMA ü¶ô\nLLaMA 2 ü¶ôü¶ô\nLLaMA 3 ü¶ôü¶ôü¶ô\nMistral 7B\nMixtral MoE\nDBRX\nFalcon\nChinese LLaMA / Alpaca\nand\nChinese LLaMA-2 / Alpaca-2\nVigogne (French)\nBERT\nKoala\nBaichuan 1 & 2\n+\nderivations\nAquila 1 & 2\nStarcoder models\nRefact\nMPT\nBloom\nYi models\nStableLM models\nDeepseek models\nQwen models\nPLaMo-13B\nPhi models\nPhiMoE\nGPT-2\nOrion 14B\nInternLM2\nCodeShell\nGemma\nMamba\nGrok-1\nXverse\nCommand-R models\nSEA-LION\nGritLM-7B\n+\nGritLM-8x7B\nOLMo\nOLMo 2\nOLMoE\nGranite models\nGPT-NeoX\n+\nPythia\nSnowflake-Arctic MoE\nSmaug\nPoro 34B\nBitnet b1.58 models\nFlan T5\nOpen Elm models\nChatGLM3-6b\n+\nChatGLM4-9b\n+\nGLMEdge-1.5b\n+\nGLMEdge-4b\nGLM-4-0414\nSmolLM\nEXAONE-3.0-7.8B-Instruct\nFalconMamba Models\nJais\nBielik-11B-v2.3\nRWKV-6\nQRWKV-6\nGigaChat-20B-A3B\nTrillion-7B-preview\nLing models\nLFM2 models\nHunyuan models\nMultimodal\nLLaVA 1.5 models\n,\nLLaVA 1.6 models\nBakLLaVA\nObsidian\nShareGPT4V\nMobileVLM 1.7B/3B models\nYi-VL\nMini CPM\nMoondream\nBunny\nGLM-EDGE\nQwen2-VL\nLFM2-VL\nBindings\nPython:\nddh0/easy-llama\nPython:\nabetlen/llama-cpp-python\nGo:\ngo-skynet/go-llama.cpp\nNode.js:\nwithcatai/node-llama-cpp\nJS/TS (llama.cpp server client):\nlgrammel/modelfusion\nJS/TS (Programmable Prompt Engine CLI):\noffline-ai/cli\nJavaScript/Wasm (works in browser):\ntangledgroup/llama-cpp-wasm\nTypescript/Wasm (nicer API, available on npm):\nngxson/wllama\nRuby:\nyoshoku/llama_cpp.rb\nRust (more features):\nedgenai/llama_cpp-rs\nRust (nicer API):\nmdrokz/rust-llama.cpp\nRust (more direct bindings):\nutilityai/llama-cpp-rs\nRust (automated build from crates.io):\nShelbyJenkins/llm_client\nC#/.NET:\nSciSharp/LLamaSharp\nC#/VB.NET (more features - community license):\nLM-Kit.NET\nScala 3:\ndonderom/llm4s\nClojure:\nphronmophobic/llama.clj\nReact Native:\nmybigday/llama.rn\nJava:\nkherud/java-llama.cpp\nJava:\nQuasarByte/llama-cpp-jna\nZig:\ndeins/llama.cpp.zig\nFlutter/Dart:\nnetdur/llama_cpp_dart\nFlutter:\nxuegao-tzx/Fllama\nPHP (API bindings and features built on top of llama.cpp):\ndistantmagic/resonance\n(more info)\nGuile Scheme:\nguile_llama_cpp\nSwift\nsrgtuszy/llama-cpp-swift\nSwift\nShenghaiWang/SwiftLlama\nDelphi\nEmbarcadero/llama-cpp-delphi\nUIs\n(to have a project listed here, it should clearly state that it depends on\nllama.cpp\n)\nAI Sublime Text plugin\n(MIT)\ncztomsik/ava\n(MIT)\nDot\n(GPL)\neva\n(MIT)\niohub/collama\n(Apache-2.0)\njanhq/jan\n(AGPL)\njohnbean393/Sidekick\n(MIT)\nKanTV\n(Apache-2.0)\nKodiBot\n(GPL)\nllama.vim\n(MIT)\nLARS\n(AGPL)\nLlama Assistant\n(GPL)\nLLMFarm\n(MIT)\nLLMUnity\n(MIT)\nLMStudio\n(proprietary)\nLocalAI\n(MIT)\nLostRuins/koboldcpp\n(AGPL)\nMindMac\n(proprietary)\nMindWorkAI/AI-Studio\n(FSL-1.1-MIT)\nMobile-Artificial-Intelligence/maid\n(MIT)\nMozilla-Ocho/llamafile\n(Apache-2.0)\nnat/openplayground\n(MIT)\nnomic-ai/gpt4all\n(MIT)\nollama/ollama\n(MIT)\noobabooga/text-generation-webui\n(AGPL)\nPocketPal AI\n(MIT)\npsugihara/FreeChat\n(MIT)\nptsochantaris/emeltal\n(MIT)\npythops/tenere\n(AGPL)\nramalama\n(MIT)\nsemperai/amica\n(MIT)\nwithcatai/catai\n(MIT)\nAutopen\n(GPL)\nTools\nakx/ggify\n‚Äì download PyTorch models from HuggingFace Hub and convert them to GGML\nakx/ollama-dl\n‚Äì download models from the Ollama library to be used directly with llama.cpp\ncrashr/gppm\n‚Äì launch llama.cpp instances utilizing NVIDIA Tesla P40 or P100 GPUs with reduced idle power consumption\ngpustack/gguf-parser\n- review/check the GGUF file and estimate the memory usage\nStyled Lines\n(proprietary licensed, async wrapper of inference part for game development in Unity3d with pre-built Mobile and Web platform wrappers and a model example)\nInfrastructure\nPaddler\n- Open-source LLMOps platform for hosting and scaling AI in your own infrastructure\nGPUStack\n- Manage GPU clusters for running LLMs\nllama_cpp_canister\n- llama.cpp as a smart contract on the Internet Computer, using WebAssembly\nllama-swap\n- transparent proxy that adds automatic model switching with llama-server\nKalavai\n- Crowdsource end to end LLM deployment at any scale\nllmaz\n- ‚ò∏Ô∏è Easy, advanced inference platform for large language models on Kubernetes.\nGames\nLucy's Labyrinth\n- A simple maze game where agents controlled by an AI model will try to trick you.\nSupported backends\nBackend\nTarget devices\nMetal\nApple Silicon\nBLAS\nAll\nBLIS\nAll\nSYCL\nIntel and Nvidia GPU\nMUSA\nMoore Threads GPU\nCUDA\nNvidia GPU\nHIP\nAMD GPU\nVulkan\nGPU\nCANN\nAscend NPU\nOpenCL\nAdreno GPU\nIBM zDNN\nIBM Z & LinuxONE\nWebGPU [In Progress]\nAll\nRPC\nAll\nObtaining and quantizing models\nThe\nHugging Face\nplatform hosts a\nnumber of LLMs\ncompatible with\nllama.cpp\n:\nTrending\nLLaMA\nYou can either manually download the GGUF file or directly use any\nllama.cpp\n-compatible models from\nHugging Face\nor other model hosting sites, such as\nModelScope\n, by using this CLI argument:\n-hf <user>/<model>[:quant]\n. For example:\nllama-cli -hf ggml-org/gemma-3-1b-it-GGUF\nBy default, the CLI would download from Hugging Face, you can switch to other options with the environment variable\nMODEL_ENDPOINT\n. For example, you may opt to downloading model checkpoints from ModelScope or other model sharing communities by setting the environment variable, e.g.\nMODEL_ENDPOINT=https://www.modelscope.cn/\n.\nAfter downloading a model, use the CLI tools to run it locally - see below.\nllama.cpp\nrequires the model to be stored in the\nGGUF\nfile format. Models in other data formats can be converted to GGUF using the\nconvert_*.py\nPython scripts in this repo.\nThe Hugging Face platform provides a variety of online tools for converting, quantizing and hosting models with\nllama.cpp\n:\nUse the\nGGUF-my-repo space\nto convert to GGUF format and quantize model weights to smaller sizes\nUse the\nGGUF-my-LoRA space\nto convert LoRA adapters to GGUF format (more info:\n#10123\n)\nUse the\nGGUF-editor space\nto edit GGUF meta data in the browser (more info:\n#9268\n)\nUse the\nInference Endpoints\nto directly host\nllama.cpp\nin the cloud (more info:\n#9669\n)\nTo learn more about model quantization,\nread this documentation\nllama-cli\nA CLI tool for accessing and experimenting with most of\nllama.cpp\n's functionality.\nRun in conversation mode\nModels with a built-in chat template will automatically activate conversation mode. If this doesn't occur, you can manually enable it by adding\n-cnv\nand specifying a suitable chat template with\n--chat-template NAME\nllama-cli -m model.gguf\n#\n> hi, who are you?\n#\nHi there! I'm your helpful assistant! I'm an AI-powered chatbot designed to assist and provide information to users like you. I'm here to help answer your questions, provide guidance, and offer support on a wide range of topics. I'm a friendly and knowledgeable AI, and I'm always happy to help with anything you need. What's on your mind, and how can I assist you today?\n#\n#\n> what is 1+1?\n#\nEasy peasy! The answer to 1+1 is... 2!\nRun in conversation mode with custom chat template\n#\nuse the \"chatml\" template (use -h to see the list of supported templates)\nllama-cli -m model.gguf -cnv --chat-template chatml\n#\nuse a custom template\nllama-cli -m model.gguf -cnv --in-prefix\n'\nUser:\n'\n--reverse-prompt\n'\nUser:\n'\nRun simple text completion\nTo disable conversation mode explicitly, use\n-no-cnv\nllama-cli -m model.gguf -p\n\"\nI believe the meaning of life is\n\"\n-n 128 -no-cnv\n#\nI believe the meaning of life is to find your own truth and to live in accordance with it. For me, this means being true to myself and following my passions, even if they don't align with societal expectations. I think that's what I love about yoga ‚Äì it's not just a physical practice, but a spiritual one too. It's about connecting with yourself, listening to your inner voice, and honoring your own unique journey.\nConstrain the output with a custom grammar\nllama-cli -m model.gguf -n 256 --grammar-file grammars/json.gbnf -p\n'\nRequest: schedule a call at 8pm; Command:\n'\n#\n{\"appointmentTime\": \"8pm\", \"appointmentDetails\": \"schedule a a call\"}\nThe\ngrammars/\nfolder contains a handful of sample grammars. To write your own, check out the\nGBNF Guide\n.\nFor authoring more complex JSON grammars, check out\nhttps://grammar.intrinsiclabs.ai/\nllama-server\nA lightweight,\nOpenAI API\ncompatible, HTTP server for serving LLMs.\nStart a local HTTP server with default configuration on port 8080\nllama-server -m model.gguf --port 8080\n#\nBasic web UI can be accessed via browser: http://localhost:8080\n#\nChat completion endpoint: http://localhost:8080/v1/chat/completions\nSupport multiple-users and parallel decoding\n#\nup to 4 concurrent requests, each with 4096 max context\nllama-server -m model.gguf -c 16384 -np 4\nEnable speculative decoding\n#\nthe draft.gguf model should be a small variant of the target model.gguf\nllama-server -m model.gguf -md draft.gguf\nServe an embedding model\n#\nuse the /embedding endpoint\nllama-server -m model.gguf --embedding --pooling cls -ub 8192\nServe a reranking model\n#\nuse the /reranking endpoint\nllama-server -m model.gguf --reranking\nConstrain all outputs with a grammar\n#\ncustom grammar\nllama-server -m model.gguf --grammar-file grammar.gbnf\n#\nJSON\nllama-server -m model.gguf --grammar-file grammars/json.gbnf\nllama-perplexity\nA tool for measuring the\nperplexity\n1\n(and other quality metrics) of a model over a given text.\nMeasure the perplexity over a text file\nllama-perplexity -m model.gguf -f file.txt\n#\n[1]15.2701,[2]5.4007,[3]5.3073,[4]6.2965,[5]5.8940,[6]5.6096,[7]5.7942,[8]4.9297, ...\n#\nFinal estimate: PPL = 5.4007 +/- 0.67339\nMeasure KL divergence\n#\nTODO\nllama-bench\nBenchmark the performance of the inference for various parameters.\nRun default benchmark\nllama-bench -m model.gguf\n#\nOutput:\n#\n| model               |       size |     params | backend    | threads |          test |                  t/s |\n#\n| ------------------- | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\n#\n| qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         pp512 |      5765.41 ¬± 20.55 |\n#\n| qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         tg128 |        197.71 ¬± 0.81 |\n#\n#\nbuild: 3e0ba0e60 (4229)\nllama-run\nA comprehensive example for running\nllama.cpp\nmodels. Useful for inferencing. Used with RamaLama\n2\n.\nRun a model with a specific prompt (by default it's pulled from Ollama registry)\nllama-run granite-code\nllama-simple\nA minimal example for implementing apps with\nllama.cpp\n. Useful for developers.\nBasic text completion\nllama-simple -m model.gguf\n#\nHello my name is Kaitlyn and I am a 16 year old girl. I am a junior in high school and I am currently taking a class called \"The Art of\nContributing\nContributors can open PRs\nCollaborators will be invited based on contributions\nMaintainers can push to branches in the\nllama.cpp\nrepo and merge PRs into the\nmaster\nbranch\nAny help with managing issues, PRs and projects is very appreciated!\nSee\ngood first issues\nfor tasks suitable for first contributions\nRead the\nCONTRIBUTING.md\nfor more information\nMake sure to read this:\nInference at the edge\nA bit of backstory for those who are interested:\nChangelog podcast\nOther documentation\nmain (cli)\nserver\nGBNF grammars\nDevelopment documentation\nHow to build\nRunning on Docker\nBuild on Android\nPerformance troubleshooting\nGGML tips & tricks\nSeminal papers and background on the models\nIf your issue is with model generation quality, then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:\nLLaMA:\nIntroducing LLaMA: A foundational, 65-billion-parameter large language model\nLLaMA: Open and Efficient Foundation Language Models\nGPT-3\nLanguage Models are Few-Shot Learners\nGPT-3.5 / InstructGPT / ChatGPT:\nAligning language models to follow instructions\nTraining language models to follow instructions with human feedback\nXCFramework\nThe XCFramework is a precompiled version of the library for iOS, visionOS, tvOS,\nand macOS. It can be used in Swift projects without the need to compile the\nlibrary from source. For example:\n// swift-tools-version: 5.10\n// The swift-tools-version declares the minimum version of Swift required to build this package.\nimport\nPackageDescription\nlet\npackage\n=\nPackage\n(\nname\n:\n\"\nMyLlamaPackage\n\"\n,\ntargets\n:\n[\n.\nexecutableTarget\n(\nname\n:\n\"\nMyLlamaPackage\n\"\n,\ndependencies\n:\n[\n\"\nLlamaFramework\n\"\n]\n)\n,\n.\nbinaryTarget\n(\nname\n:\n\"\nLlamaFramework\n\"\n,\nurl\n:\n\"\nhttps://github.com/ggml-org/llama.cpp/releases/download/b5046/llama-b5046-xcframework.zip\n\"\n,\nchecksum\n:\n\"\nc19be78b5f00d8d29a25da41042cb7afa094cbf6280a225abe614b03b20029ab\n\"\n)\n]\n)\nThe above example is using an intermediate build\nb5046\nof the library. This can be modified\nto use a different version by changing the URL and checksum.\nCompletions\nCommand-line completion is available for some environments.\nBash Completion\n$ build/bin/llama-cli --completion-bash\n>\n~\n/.llama-completion.bash\n$\nsource\n~\n/.llama-completion.bash\nOptionally this can be added to your\n.bashrc\nor\n.bash_profile\nto load it\nautomatically. For example:\n$\necho\n\"\nsource ~/.llama-completion.bash\n\"\n>>\n~\n/.bashrc\nDependencies\nyhirose/cpp-httplib\n- Single-header HTTP server, used by\nllama-server\n- MIT license\nstb-image\n- Single-header image format decoder, used by multimodal subsystem - Public domain\nnlohmann/json\n- Single-header JSON library, used by various tools/examples - MIT License\nminja\n- Minimal Jinja parser in C++, used by various tools/examples - MIT License\nlinenoise.cpp\n- C++ library that provides readline-like line editing capabilities, used by\nllama-run\n- BSD 2-Clause License\ncurl\n- Client-side URL transfer library, used by various tools/examples -\nCURL License\nminiaudio.h\n- Single-header audio format decoder, used by multimodal subsystem - Public domain\nFootnotes\nhttps://huggingface.co/docs/transformers/perplexity\n‚Ü©\nRamaLama\n‚Ü©",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 49",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 87,367"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/ggml-org/llama.cpp"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/supermemoryai/supermemory",
      "title": "supermemoryai/supermemory",
      "date": null,
      "executive_summary": [
        "Memory engine and app that is extremely fast, scalable. The Memory API for the AI era.",
        "---",
        "Features\nCore Functionality\nAdd Memories from Any Content\n: Easily add memories from URLs, PDFs, and plain text‚Äîjust paste, upload, or link.\nChat with Your Memories\n: Converse with your stored content using natural language chat.\nSupermemory MCP Integration\n: Seamlessly connect with all major AI tools (Claude, Cursor, etc.) via Supermemory MCP.\nHow do i use this?\nGo to\napp.supermemory.ai\nand sign into with your account\nStart Adding Memory with your choose of format (Note, Link, File)\nYou can also Connect to your favourite services (Notion, Google Drive, OneDrive)\nOnce Memories are added, you can chat with Supermemory by clicking on \"Open Chat\" and retrieve info from your saved memories\nAdd MCP to your AI Tools (by clicking on \"Connect to your AI\" and select the AI tool you are trying to integrate)\nSupport\nHave questions or feedback? We're here to help:\nEmail:\ndhravya@supermemory.com\nDocumentation:\ndocs.supermemory.ai\nContributing\nWe welcome contributions from developers of all skill levels! Whether you're fixing bugs, adding features, or improving documentation, your help makes supermemory better for everyone.\nQuick Start for Contributors\nFork and clone\nthe repository\nInstall dependencies\nwith\nbun install\nSet up your environment\nby copying\n.env.example\nto\n.env.local\nStart developing\nwith\nbun run dev\nFor detailed guidelines, development setup, coding standards, and the complete contribution workflow, please see our\nContributing Guide\n.\nWays to Contribute\nüêõ\nBug fixes\n- Help us squash those pesky issues\n‚ú®\nNew features\n- Add functionality that users will love\nüé®\nUI/UX improvements\n- Make the interface more intuitive\n‚ö°\nPerformance optimizations\n- Help us make supermemory faster\nCheck out our\nIssues\npage for\ngood first issue\nand\nhelp wanted\nlabels to get started!\nUpdates & Roadmap\nStay up to date with the latest improvements:\nChangelog\nX\n.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 47",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 11,152"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/supermemoryai/supermemory"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/browserbase/stagehand",
      "title": "browserbase/stagehand",
      "date": null,
      "executive_summary": [
        "The AI Browser Automation Framework",
        "---",
        "The AI Browser Automation Framework\nRead the Docs\nIf you're looking for the Python implementation, you can find it\nhere\nVibe code\nStagehand with\nDirector\nWhy Stagehand?\nMost existing browser automation tools either require you to write low-level code in a framework like Selenium, Playwright, or Puppeteer, or use high-level agents that can be unpredictable in production. By letting developers choose what to write in code vs. natural language, Stagehand is the natural choice for browser automations in production.\nChoose when to write code vs. natural language\n: use AI when you want to navigate unfamiliar pages, and use code (\nPlaywright\n) when you know exactly what you want to do.\nPreview and cache actions\n: Stagehand lets you preview AI actions before running them, and also helps you easily cache repeatable actions to save time and tokens.\nComputer use models with one line of code\n: Stagehand lets you integrate SOTA computer use models from OpenAI and Anthropic into the browser with one line of code.\nExample\nHere's how to build a sample browser automation with Stagehand:\n// Use Playwright functions on the page object\nconst\npage\n=\nstagehand\n.\npage\n;\nawait\npage\n.\ngoto\n(\n\"https://github.com/browserbase\"\n)\n;\n// Use act() to execute individual actions\nawait\npage\n.\nact\n(\n\"click on the stagehand repo\"\n)\n;\n// Use Computer Use agents for larger actions\nconst\nagent\n=\nstagehand\n.\nagent\n(\n{\nprovider\n:\n\"openai\"\n,\nmodel\n:\n\"computer-use-preview\"\n,\n}\n)\n;\nawait\nagent\n.\nexecute\n(\n\"Get to the latest PR\"\n)\n;\n// Use extract() to read data from the page\nconst\n{\nauthor\n,\ntitle\n}\n=\nawait\npage\n.\nextract\n(\n{\ninstruction\n:\n\"extract the author and title of the PR\"\n,\nschema\n:\nz\n.\nobject\n(\n{\nauthor\n:\nz\n.\nstring\n(\n)\n.\ndescribe\n(\n\"The username of the PR author\"\n)\n,\ntitle\n:\nz\n.\nstring\n(\n)\n.\ndescribe\n(\n\"The title of the PR\"\n)\n,\n}\n)\n,\n}\n)\n;\nDocumentation\nVisit\ndocs.stagehand.dev\nto view the full documentation.\nGetting Started\nStart with Stagehand with one line of code, or check out our\nQuickstart Guide\nfor more information:\nnpx create-browser-app\nWatch Anirudh demo create-browser-app to create a Stagehand project!\nBuild and Run from Source\ngit clone https://github.com/browserbase/stagehand.git\ncd\nstagehand\npnpm install\npnpm playwright install\npnpm run build\npnpm run example\n#\nrun the blank script at ./examples/example.ts\npnpm run example 2048\n#\nrun the 2048 example at ./examples/2048.ts\npnpm run evals -man\n#\nsee evaluation suite options\nStagehand is best when you have an API key for an LLM provider and Browserbase credentials. To add these to your project, run:\ncp .env.example .env\nnano .env\n#\nEdit the .env file to add API keys\nContributing\nNote\nWe highly value contributions to Stagehand! For questions or support, please join our\nSlack community\n.\nAt a high level, we're focused on improving reliability, speed, and cost in that order of priority. If you're interested in contributing, we strongly recommend reaching out to\nMiguel Gonzalez\nor\nPaul Klein\nin our\nSlack community\nbefore starting to ensure that your contribution aligns with our goals.\nFor more information, please see our\nContributing Guide\n.\nAcknowledgements\nThis project heavily relies on\nPlaywright\nas a resilient backbone to automate the web. It also would not be possible without the awesome techniques and discoveries made by\ntarsier\n,\ngemini-zod\n, and\nfuji-web\n.\nWe'd like to thank the following people for their major contributions to Stagehand:\nPaul Klein\nAnirudh Kamath\nSean McGuire\nMiguel Gonzalez\nSameel Arif\nFilip Michalsky\nJeremy Press\nNavid Pour\nLicense\nLicensed under the MIT License.\nCopyright 2025 Browserbase, Inc.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 46",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 17,563"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/browserbase/stagehand"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/oracle/graal",
      "title": "oracle/graal",
      "date": null,
      "executive_summary": [
        "GraalVM compiles applications into native executables that start instantly, scale fast, and use fewer compute resources üöÄ",
        "---",
        "GraalVM is a high-performance JDK distribution that compiles your Java applications ahead of time into standalone binaries. These binaries start instantly, provide peak performance with no warmup, and use fewer resources.\nYou can use GraalVM just like any other Java Development Kit in your IDE.\nThe project website at\nhttps://www.graalvm.org/\ndescribes how to\nget started\n, how to\nstay connected\n, and how to\ncontribute\n.\nDocumentation\nPlease refer to the\nGraalVM website for documentation\n.\nYou can find most of the documentation sources in the\ndocs/\ndirectory in the same hierarchy as displayed on the website.\nAdditional documentation including developer instructions for individual components can be found in corresponding\ndocs/\nsub-directories.\nThe documentation for the Truffle framework, for example, is in\ntruffle/docs/\n.\nThis also applies to languages, tools, and other components maintained in\nrelated repositories\n.\nGet Support\nOpen a\nGitHub issue\nfor bug reports, questions, or requests for enhancements.\nJoin the\nGraalVM Slack\nto connect with the community and the GraalVM team.\nReport a security vulnerability according to the\nReporting Vulnerabilities guide\n.\nRepository Structure\nThis source repository is the main repository for GraalVM and includes the following components:\nDirectory\nDescription\n.devcontainer/\nConfiguration files for GitHub dev containers.\n.github/\nConfiguration files for GitHub issues, workflows, ‚Ä¶.\ncompiler/\nGraal compiler\n, a modern, versatile compiler written in Java.\nespresso/\nEspresso\n, a meta-circular Java bytecode interpreter for the GraalVM.\nregex/\nTRegex, a regular expression engine for other GraalVM languages.\nsdk/\nGraalVM SDK\n, long-term supported APIs of GraalVM.\nsubstratevm/\nFramework for ahead-of-time (AOT) compilation with\nNative Image\n.\nsulong/\nSulong\n, an engine for running LLVM bitcode on GraalVM.\ntools/\nTools for GraalVM languages implemented with the instrumentation framework.\ntruffle/\nGraalVM's\nlanguage implementation framework\nfor creating languages and tools.\nvisualizer/\nIdeal Graph Visualizer (IGV)\n, a tool for analyzing Graal compiler graphs.\nvm/\nComponents for building GraalVM distributions.\nwasm/\nGraalWasm\n, an engine for running WebAssembly programs on GraalVM.\nRelated Repositories\nGraalVM provides additional languages, tools, and other components developed in related repositories. These are:\nName\nDescription\nFastR\nImplementation of the R language.\nGraalJS\nImplementation of JavaScript and Node.js.\nGraalPy\nImplementation of the Python language.\nNative Build Tools\nBuild tool plugins for GraalVM Native Image.\nSimpleLanguage\nA simple example language built with the Truffle framework.\nSimpleTool\nA simple example tool built with the Truffle framework.\nTruffleRuby\nImplementation of the Ruby language.\nExamples and Tutorials\nExplore practical examples, deep-dive workshops, and language-specific demos for working with GraalVM.\nName\nDescription\nGraalVM Demos\nExample applications highlighting GraalVM key features and best practices.\nGraalVM Workshops and Tutorials\nWorkshops and tutorials to help you learn and apply GraalVM tools and capabilities.\nGraal Languages - Demos and Guides\nDemo applications and guides for GraalJS, GraalPy, GraalWasm, and other Graal Languages.\nLicense\nGraalVM Community Edition is open source and distributed under\nversion 2 of the GNU General Public License with the ‚ÄúClasspath‚Äù Exception\n, which are the same terms as for Java. The licenses of the individual GraalVM components are generally derivative of the license of a particular language (see the table below).\nComponent(s)\nLicense\nEspresso\n,\nIdeal Graph Visualizer\nGPL 2\nGraalVM Compiler\n,\nSubstrateVM\n,\nTools\n,\nVM\nGPL 2 with Classpath Exception\nGraalVM SDK\n,\nGraalWasm\n,\nTruffle Framework\n,\nTRegex\nUniversal Permissive License\nSulong\n3-clause BSD",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 45",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 21,200"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/oracle/graal"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/qaiu/netdisk-fast-download",
      "title": "qaiu/netdisk-fast-download",
      "date": null,
      "executive_summary": [
        "ÂêÑÁ±ªÁΩëÁõòÁõ¥ÈìæËß£ÊûêÊúçÂä°, Â∑≤ÊîØÊåÅËìùÂ•è‰∫ë/ËìùÂ•è‰ºò‰∫´/Â∞èÈ£ûÊú∫Áõò/123‰∫ëÁõò/ÁßªÂä®ËÅîÈÄö/Â§©Áøº‰∫ëÁ≠â. ÊîØÊåÅÊñá‰ª∂Â§πÂàÜ‰∫´Ëß£Êûê. ‰ΩìÈ™åÂú∞ÂùÄ: https://lz.qaiu.top http://www.722shop.top:6401",
        "---",
        "netdisk-fast-download ÁΩëÁõòÂàÜ‰∫´ÈìæÊé•‰∫ëËß£ÊûêÊúçÂä°\nQQÁæ§Ôºö1017480890\nnetdisk-fast-downloadÁΩëÁõòÁõ¥Èìæ‰∫ëËß£Êûê(nfd‰∫ëËß£Êûê)ËÉΩÊääÁΩëÁõòÂàÜ‰∫´‰∏ãËΩΩÈìæÊé•ËΩ¨Âåñ‰∏∫Áõ¥ÈìæÔºåÊîØÊåÅÂ§öÊ¨æ‰∫ëÁõòÔºåÂ∑≤ÊîØÊåÅËìùÂ•è‰∫ë/ËìùÂ•è‰∫ë‰ºò‰∫´/Â•∂ÁâõÂø´‰º†/ÁßªÂä®‰∫ë‰∫ëÁ©∫Èó¥/Â∞èÈ£ûÊú∫Áõò/‰∫øÊñπ‰∫ë/123‰∫ëÁõò/CloudreveÁ≠âÔºåÊîØÊåÅÂä†ÂØÜÂàÜ‰∫´Ôºå‰ª•ÂèäÈÉ®ÂàÜÁΩëÁõòÊñá‰ª∂Â§πÂàÜ‰∫´„ÄÇ\nÂø´ÈÄüÂºÄÂßã\nÂëΩ‰ª§Ë°å‰∏ãËΩΩÂàÜ‰∫´Êñá‰ª∂Ôºö\ncurl -LOJ\n\"\nhttps://lz.qaiu.top/parser?url=https://share.feijipan.com/s/nQOaNRPW&pwd=1234\n\"\nÊàñËÄÖ‰ΩøÁî®wget:\nwget -O bilibili.mp4\n\"\nhttps://lz.qaiu.top/parser?url=https://share.feijipan.com/s/nQOaNRPW&pwd=1234\n\"\nÊàñËÄÖ‰ΩøÁî®ÊµèËßàÂô®\nÁõ¥Êé•ËÆøÈóÆ\n:\n### Ë∞ÉÁî®ÊºîÁ§∫Á´ô‰∏ãËΩΩÔºö\nhttps://lz.qaiu.top/parser?url=https://share.feijipan.com/s/nQOaNRPW&pwd=1234  \n### Ë∞ÉÁî®ÊºîÁ§∫Á´ôÈ¢ÑËßàÔºö\nhttps://nfd-parser.github.io/nfd-preview/preview.html?src=https%3A%2F%2Flz.qaiu.top%2Fparser%3Furl%3Dhttps%3A%2F%2Fshare.feijipan.com%2Fs%2FnQOaNRPW&name=bilibili.mp4&ext=mp4\nÈ¢ÑËßàÂú∞ÂùÄ\nÈ¢ÑËßàÂú∞ÂùÄ1\nÈ¢ÑËßàÂú∞ÂùÄ2\nÂ§©Áøº‰∫ëÁõòÂ§ßÊñá‰ª∂Ëß£ÊûêÈôêÊó∂ÂºÄÊîæ\nmainÂàÜÊîØ‰æùËµñJDK17, Êèê‰æõ‰∫ÜJDK11ÂàÜÊîØ\nmain-jdk11\n0.1.8Âèä‰ª•‰∏äÁâàÊú¨jsonÊé•Âè£Ê†ºÂºèÊúâË∞ÉÊï¥ ÂèÇËÄÉjsonËøîÂõûÊï∞ÊçÆÊ†ºÂºèÁ§∫‰æã\nÂ∞èÈ£ûÊú∫Ëß£ÊûêÊúâIPÈôêÂà∂ÔºåÂ§öÊï∞‰∫ëÊúçÂä°ÂïÜÁöÑÂ§ßÈôÜIP‰ºöË¢´Êã¶Êà™ÔºàÂèØ‰ª•Ëá™Ë°åÈÖçÁΩÆ‰ª£ÁêÜÔºâÔºåÂíåÊú¨Á®ãÂ∫èÊó†ÂÖ≥\nÊ≥®ÊÑè: ËØ∑‰∏çË¶ÅËøáÂ∫¶‰æùËµñlz.qaiu.topÈ¢ÑËßàÂú∞ÂùÄÊúçÂä°ÔºåÂª∫ËÆÆÊú¨Âú∞Êê≠Âª∫ÊàñËÄÖ‰∫ëÊúçÂä°Âô®Ëá™Ë°åÊê≠Âª∫„ÄÇËß£ÊûêÊ¨°Êï∞ËøáÂ§öIP‰ºöË¢´ÈÉ®ÂàÜÁΩëÁõòÂéÇÂïÜÈôêÂà∂Ôºå‰∏çÊé®ËçêÂÅöÂÖ¨ÂÖ±Ëß£Êûê„ÄÇ\nÁΩëÁõòÊîØÊåÅÊÉÖÂÜµ:\n20230905 Â•∂Áâõ‰∫ëÁõ¥ÈìæÂÅö‰∫ÜÈò≤ÁõóÈìæÔºåÈúÄÂä†ÂÖ•ËØ∑Ê±ÇÂ§¥ÔºöReferer:\nhttps://cowtransfer.com/\n20230824 123‰∫ëÁõòËß£ÊûêÂ§ßÊñá‰ª∂(>100MB)Â§±ÊïàÔºåÈúÄË¶ÅÁôªÂΩï\n20230722 UCÁΩëÁõòËß£ÊûêÂ§±ÊïàÔºåÈúÄË¶ÅÁôªÂΩï\nÁΩëÁõòÂêçÁß∞-ÁΩëÁõòÊ†áËØÜ:\nËìùÂ•è‰∫ë-lz\nËìùÂ•è‰∫ë‰ºò‰∫´-iz\nÂ•∂ÁâõÂø´‰º†-cow\nÁßªÂä®‰∫ë‰∫ëÁ©∫Èó¥-ec\nÂ∞èÈ£ûÊú∫ÁΩëÁõò-fj\n‰∫øÊñπ‰∫ë-fc\n123‰∫ëÁõò-ye\n115ÁΩëÁõò(Â§±Êïà)-p115\n118ÁΩëÁõò(Â∑≤ÂÅúÊúç)-p118\nÊñáÂèîÂèî-ws\nËÅîÊÉ≥‰πê‰∫ë-le\nQQÈÇÆÁÆ±‰∫ëÁõò-qqw\nQQÈó™‰º†-qqsc\nÂüéÈÄöÁΩëÁõò-ct\nÁΩëÊòì‰∫ëÈü≥‰πêÂàÜ‰∫´ÈìæÊé•-mnes\nÈÖ∑ÁãóÈü≥‰πêÂàÜ‰∫´ÈìæÊé•-mkgs\nÈÖ∑ÊàëÈü≥‰πêÂàÜ‰∫´ÈìæÊé•-mkws\nQQÈü≥‰πêÂàÜ‰∫´ÈìæÊé•-mqqs\nÂí™ÂíïÈü≥‰πêÂàÜ‰∫´ÈìæÊé•(ÂºÄÂèë‰∏≠)\nCloudreveËá™Âª∫ÁΩëÁõò-ce\nÂæÆÈõ®‰∫ëÂ≠òÂÇ®-pvvy\nË∂ÖÊòü‰∫ëÁõò(ÈúÄË¶Åreferer: https://pan-yz.chaoxing.com)-pcx\nGoogle‰∫ëÁõò-pgd\nOnedrive-pod\nDropbox-pdp\niCloud-pic\n‰ªÖ‰∏ìÂ±ûÁâàÊèê‰æõ\nÁßªÂä®‰∫ëÁõò-p139\nËÅîÈÄö‰∫ëÁõò-pwo\nÂ§©Áøº‰∫ëÁõò-p189\nAPIÊé•Âè£ËØ¥Êòé\nyour_hostÊåáÁöÑÊòØÊÇ®ÁöÑÂüüÂêçÊàñËÄÖIPÔºåÂÆûÈôÖ‰ΩøÁî®Êó∂ÊõøÊç¢‰∏∫ÂÆûÈôÖÂüüÂêçÊàñËÄÖIPÔºåÁ´ØÂè£ÈªòËÆ§6400ÔºåÂèØ‰ª•‰ΩøÁî®nginx‰ª£ÁêÜÊù•ÂÅöÂüüÂêçËÆøÈóÆ„ÄÇ\nËß£ÊûêÊñπÂºèÂàÜ‰∏∫‰∏§ÁßçÁ±ªÂûãÁõ¥Êé•Ë∑≥ËΩ¨‰∏ãËΩΩÊñá‰ª∂ÂíåËé∑Âèñ‰∏ãËΩΩÈìæÊé•,\nÊØè‰∏ÄÁßçÈÉΩÊèê‰æõ‰∫Ü‰∏§ÁßçÊé•Âè£ÂΩ¢Âºè:\nÈÄöÁî®Êé•Âè£parser?url=\nÂíå\nÁΩëÁõòÊ†áÂøó/ÂàÜ‰∫´keyÊãºÊé•ÁöÑÁü≠Âú∞ÂùÄÔºàÊ†áÂøóÁü≠ÈìæÔºâ\nÔºåÊâÄÊúâËßÑÂàôÂèÇËÄÉÁ§∫‰æã„ÄÇ\nÈÄöÁî®Êé•Âè£:\n/parser?url=ÂàÜ‰∫´ÈìæÊé•&pwd=ÂØÜÁ†Å\nÊ≤°ÊúâÂàÜ‰∫´ÂØÜÁ†ÅÂéªÊéâ&pwdÂèÇÊï∞;\nÊ†áÂøóÁü≠Èìæ:\n/d/ÁΩëÁõòÊ†áËØÜ/ÂàÜ‰∫´key@ÂØÜÁ†Å\nÊ≤°ÊúâÂàÜ‰∫´ÂØÜÁ†ÅÂéªÊéâ@ÂØÜÁ†Å;\nÁõ¥ÈìæJSON:\n/json/ÁΩëÁõòÊ†áËØÜ/ÂàÜ‰∫´key@ÂØÜÁ†Å\nÂíå\n/json/parser?url=ÂàÜ‰∫´ÈìæÊé•&pwd=ÂØÜÁ†Å\nÁΩëÁõòÊ†áËØÜÂèÇËÄÉ‰∏äÈù¢ÁΩëÁõòÊîØÊåÅÊÉÖÂÜµ\nÂΩìÂ∏¶ÊúâÂàÜ‰∫´ÂØÜÁ†ÅÊó∂ÈúÄË¶ÅÂä†‰∏äÂØÜÁ†ÅÂèÇÊï∞(pwd)\nÁßªÂä®‰∫ë‰∫ëÁ©∫Èó¥,Â∞èÈ£ûÊú∫ÁΩëÁõòÁöÑÂä†ÂØÜÂàÜ‰∫´ÁöÑÂØÜÁ†ÅÂèØ‰ª•ÂøΩÁï•\nÁßªÂä®‰∫ëÁ©∫Èó¥ÂàÜ‰∫´keyÂèñÂàÜ‰∫´ÈìæÊé•‰∏≠ÁöÑdataÂèÇÊï∞,ÊØîÂ¶Ç\n&data=xxx\nÁöÑÂèÇÊï∞Â∞±ÊòØxxx\nAPIËßÑÂàô:\nÂª∫ËÆÆ‰ΩøÁî®UrlEncodeÁºñÁ†ÅÂàÜ‰∫´ÈìæÊé•\nËß£ÊûêÂπ∂Ëá™Âä®302Ë∑≥ËΩ¨\nhttp://your_host/parser?url=ÂàÜ‰∫´ÈìæÊé•&pwd=xxx\nhttp://your_host/parser?url=UrlEncode(ÂàÜ‰∫´ÈìæÊé•)&pwd=xxx\nhttp://your_host/d/ÁΩëÁõòÊ†áËØÜ/ÂàÜ‰∫´key@ÂàÜ‰∫´ÂØÜÁ†Å\nËé∑ÂèñËß£ÊûêÂêéÁöÑÁõ¥Èìæ--JSONÊ†ºÂºè\nhttp://your_host/json/parser?url=ÂàÜ‰∫´ÈìæÊé•&pwd=xxx\nhttp://your_host/json/ÁΩëÁõòÊ†áËØÜ/ÂàÜ‰∫´key@ÂàÜ‰∫´ÂØÜÁ†Å\nÊñá‰ª∂Â§πËß£Êûêv0.1.8fixed3Êñ∞Â¢û\nhttp://your_host/json/getFileList?url=ÂàÜ‰∫´ÈìæÊé•&pwd=xxx\njsonÊé•Âè£ËØ¥Êòé\n1. Êñá‰ª∂Ëß£ÊûêÔºö/json/parser?url=ÂàÜ‰∫´ÈìæÊé•&pwd=xxx\njsonËøîÂõûÊï∞ÊçÆÊ†ºÂºèÁ§∫‰æã:\nshareKey\n:    ÂÖ®Â±ÄÂàÜ‰∫´key\ndirectLink\n:  ‰∏ãËΩΩÈìæÊé•\ncacheHit\n:    ÊòØÂê¶‰∏∫ÁºìÂ≠òÈìæÊé•\nexpires\n:     ÁºìÂ≠òÂà∞ÊúüÊó∂Èó¥\n{\n\"code\"\n:\n200\n,\n\"msg\"\n:\n\"\nsuccess\n\"\n,\n\"success\"\n:\ntrue\n,\n\"count\"\n:\n0\n,\n\"data\"\n: {\n\"shareKey\"\n:\n\"\nlz:xxx\n\"\n,\n\"directLink\"\n:\n\"\n‰∏ãËΩΩÁõ¥Èìæ\n\"\n,\n\"cacheHit\"\n:\ntrue\n,\n\"expires\"\n:\n\"\n2024-09-18 01:48:02\n\"\n,\n\"expiration\"\n:\n1726638482825\n},\n\"timestamp\"\n:\n1726637151902\n}\n2. ÂàÜ‰∫´ÈìæÊé•ËØ¶ÊÉÖÊé•Âè£ /v2/linkInfo?url=ÂàÜ‰∫´ÈìæÊé•\n{\n\"code\"\n:\n200\n,\n\"msg\"\n:\n\"\nsuccess\n\"\n,\n\"success\"\n:\ntrue\n,\n\"count\"\n:\n0\n,\n\"data\"\n: {\n\"downLink\"\n:\n\"\nhttps://lz.qaiu.top/d/fj/xx\n\"\n,\n\"apiLink\"\n:\n\"\nhttps://lz.qaiu.top/json/fj/xx\n\"\n,\n\"cacheHitTotal\"\n:\n5\n,\n\"parserTotal\"\n:\n2\n,\n\"sumTotal\"\n:\n7\n,\n\"shareLinkInfo\"\n: {\n\"shareKey\"\n:\n\"\nxx\n\"\n,\n\"panName\"\n:\n\"\nÂ∞èÈ£ûÊú∫ÁΩëÁõò\n\"\n,\n\"type\"\n:\n\"\nfj\n\"\n,\n\"sharePassword\"\n:\n\"\n\"\n,\n\"shareUrl\"\n:\n\"\nhttps://share.feijipan.com/s/xx\n\"\n,\n\"standardUrl\"\n:\n\"\nhttps://www.feijix.com/s/xx\n\"\n,\n\"otherParam\"\n: {\n\"UA\"\n:\n\"\nMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\n\"\n},\n\"cacheKey\"\n:\n\"\nfj:xx\n\"\n}\n    },\n\"timestamp\"\n:\n1736489219402\n}\n3. Êñá‰ª∂Â§πËß£Êûê(‰ªÖÊîØÊåÅËìùÂ•è‰∫ë/ËìùÂ•è‰ºò‰∫´/Â∞èÈ£ûÊú∫ÁΩëÁõò)\n/v2/getFileList?url=ÂàÜ‰∫´ÈìæÊé•&pwd=ÂàÜ‰∫´ÂØÜÁ†Å\n{\n\"code\"\n:\n200\n,\n\"msg\"\n:\n\"\nsuccess\n\"\n,\n\"success\"\n:\ntrue\n,\n\"data\"\n: [\n    {\n\"fileName\"\n:\n\"\nxxx\n\"\n,\n\"fileId\"\n:\n\"\nxxx\n\"\n,\n\"fileIcon\"\n:\nnull\n,\n\"size\"\n:\n999\n,\n\"sizeStr\"\n:\n\"\n999 M\n\"\n,\n\"fileType\"\n:\n\"\nfile/folder\n\"\n,\n\"filePath\"\n:\nnull\n,\n\"createTime\"\n:\n\"\n17 Â∞èÊó∂Ââç\n\"\n,\n\"updateTime\"\n:\nnull\n,\n\"createBy\"\n:\nnull\n,\n\"description\"\n:\nnull\n,\n\"downloadCount\"\n:\n\"\n‰∏ãËΩΩÊ¨°Êï∞\n\"\n,\n\"panType\"\n:\n\"\nlz\n\"\n,\n\"parserUrl\"\n:\n\"\n‰∏ãËΩΩÈìæÊé•/Êñá‰ª∂Â§πÈìæÊé•\n\"\n,\n\"extParameters\"\n:\nnull\n}\n  ]\n}\n4. Ëß£ÊûêÊ¨°Êï∞ÁªüËÆ°Êé•Âè£ /v2/statisticsInfo\n{\n\"code\"\n:\n200\n,\n\"msg\"\n:\n\"\nsuccess\n\"\n,\n\"success\"\n:\ntrue\n,\n\"count\"\n:\n0\n,\n\"data\"\n: {\n\"parserTotal\"\n:\n320508\n,\n\"cacheTotal\"\n:\n5957910\n,\n\"total\"\n:\n6278418\n},\n\"timestamp\"\n:\n1736489378770\n}\nIDEA HttpClientÁ§∫‰æã:\n# Ëß£ÊûêÂπ∂ÈáçÂÆöÂêëÂà∞Áõ¥Èìæ\n### ËìùÂ•è‰∫ëÊôÆÈÄöÂàÜ‰∫´\n# @no-redirect\nGET http://127.0.0.1:6400/parser?url=https://lanzoux.com/ia2cntg\n### Â•∂ÁâõÂø´‰º†ÊôÆÈÄöÂàÜ‰∫´\n# @no-redirect\nGET http://127.0.0.1:6400/parser?url=https://cowtransfer.com/s/9a644fe3e3a748\n### 360‰∫øÊñπ‰∫ëÂä†ÂØÜÂàÜ‰∫´\n# @no-redirect\nGET http://127.0.0.1:6400/parser?url=https://v2.fangcloud.com/sharing/e5079007dc31226096628870c7&pwd=QAIU\n\n# RestËØ∑Ê±ÇËá™Âä®302Ë∑≥ËΩ¨(Âè™Êèê‰æõÂÖ±‰∫´Êñá‰ª∂Id):\n### ËìùÂ•è‰∫ëÊôÆÈÄöÂàÜ‰∫´\n# @no-redirect\nGET http://127.0.0.1:6400/lz/ia2cntg\n### Â•∂ÁâõÂø´‰º†ÊôÆÈÄöÂàÜ‰∫´\n# @no-redirect\nGET http://127.0.0.1:6400/cow/9a644fe3e3a748\n### 360‰∫øÊñπ‰∫ëÂä†ÂØÜÂàÜ‰∫´\nGET http://127.0.0.1:6400/json/fc/e5079007dc31226096628870c7@QAIU\n\n\n# Ëß£ÊûêËøîÂõûjsonÁõ¥Èìæ\n### ËìùÂ•è‰∫ëÊôÆÈÄöÂàÜ‰∫´\nGET http://127.0.0.1:6400/json/lz/ia2cntg\n### Â•∂ÁâõÂø´‰º†ÊôÆÈÄöÂàÜ‰∫´\nGET http://127.0.0.1:6400/json/cow/9a644fe3e3a748\n### 360‰∫øÊñπ‰∫ëÂä†ÂØÜÂàÜ‰∫´\nGET http://127.0.0.1:6400/json/fc/e5079007dc31226096628870c7@QAIU\nÁΩëÁõòÂØπÊØî\nÁΩëÁõòÂêçÁß∞\nÂÖçÁôªÈôÜ‰∏ãËΩΩÂàÜ‰∫´\nÂä†ÂØÜÂàÜ‰∫´\nÂàùÂßãÁΩëÁõòÁ©∫Èó¥\nÂçïÊñá‰ª∂Â§ßÂ∞èÈôêÂà∂\nËìùÂ•è‰∫ë\n‚àö\n‚àö\n‰∏çÈôêÁ©∫Èó¥\n100M\nÂ•∂ÁâõÂø´‰º†\n‚àö\nX\n10G\n‰∏çÈôêÂ§ßÂ∞è\nÁßªÂä®‰∫ë‰∫ëÁ©∫Èó¥(‰∏™‰∫∫Áâà)\n‚àö\n‚àö(ÂØÜÁ†ÅÂèØÂøΩÁï•)\n5G(‰∏™‰∫∫)\n‰∏çÈôêÂ§ßÂ∞è\nÂ∞èÈ£ûÊú∫ÁΩëÁõò\n‚àö\n‚àö(ÂØÜÁ†ÅÂèØÂøΩÁï•)\n10G\n‰∏çÈôêÂ§ßÂ∞è\n360‰∫øÊñπ‰∫ë\n‚àö\n‚àö(ÂØÜÁ†ÅÂèØÂøΩÁï•)\n100G(È°ªÂÆûÂêç)\n‰∏çÈôêÂ§ßÂ∞è\n123‰∫ëÁõò\n‚àö\n‚àö\n2T\n100GÔºà>100MÈúÄË¶ÅÁôªÂΩïÔºâ\nÊñáÂèîÂèî\n‚àö\n‚àö\n10G\n5GB\nÂ§∏ÂÖãÁΩëÁõò\nx\n‚àö\n10G\n‰∏çÈôêÂ§ßÂ∞è\nUCÁΩëÁõò\nx\n‚àö\n10G\n‰∏çÈôêÂ§ßÂ∞è\nÊâìÂåÖÈÉ®ÁΩ≤\nJDK‰∏ãËΩΩÔºàlz.qaiu.topÊèê‰æõÁõ¥Èìæ‰∫ëËß£ÊûêÊúçÂä°Ôºâ\nÈòøÈáåjdk17(Dragonwell17-windows-x86)\nÈòøÈáåjdk17(Dragonwell17-linux-x86)\nÈòøÈáåjdk17(Dragonwell17-linux-aarch64)\nËß£ÊûêÊúâÊïàÊÄßÊµãËØï-ÁßªÂä®‰∫ë‰∫ëÁ©∫Èó¥-ÈòøÈáåjdk17-linux-x86\nÂºÄÂèëÂíåÊâìÂåÖ\n#\nÁéØÂ¢ÉË¶ÅÊ±Ç: Jdk17 + maven;\nmvn clean\nmvn package\nÊâìÂåÖÂ•ΩÁöÑÊñá‰ª∂‰Ωç‰∫é web-service/target/netdisk-fast-download-bin.zip\nLinuxÊúçÂä°ÈÉ®ÁΩ≤\nDocker ÈÉ®ÁΩ≤ÔºàMainÂàÜÊîØÔºâ\nÊµ∑Â§ñÊúçÂä°Âô®DockerÈÉ®ÁΩ≤\n#\nÂàõÂª∫ÁõÆÂΩï\nmkdir -p netdisk-fast-download\ncd\nnetdisk-fast-download\n#\nÊãâÂèñÈïúÂÉè\ndocker pull ghcr.io/qaiu/netdisk-fast-download:latest\n#\nÂ§çÂà∂ÈÖçÁΩÆÊñá‰ª∂ÔºàÊàñ‰∏ãËΩΩ‰ªìÂ∫ìweb-service\\src\\main\\resourcesÔºâ\ndocker create --name netdisk-fast-download ghcr.io/qaiu/netdisk-fast-download:latest\ndocker cp netdisk-fast-download:/app/resources ./resources\ndocker rm netdisk-fast-download\n#\nÂêØÂä®ÂÆπÂô®\ndocker run -d -it --name netdisk-fast-download -p 6401:6401 --restart unless-stopped -e TZ=Asia/Shanghai -v ./resources:/app/resources -v ./db:/app/db -v ./logs:/app/logs ghcr.io/qaiu/netdisk-fast-download:latest\n#\nÂèç‰ª£6401Á´ØÂè£\n#\nÂçáÁ∫ßÂÆπÂô®\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --cleanup --run-once netdisk-fast-download\nÂõΩÂÜÖDockerÈÉ®ÁΩ≤\n#\nÂàõÂª∫ÁõÆÂΩï\nmkdir -p netdisk-fast-download\ncd\nnetdisk-fast-download\n#\nÊãâÂèñÈïúÂÉè\ndocker pull ghcr.nju.edu.cn/qaiu/netdisk-fast-download:latest\n#\nÂ§çÂà∂ÈÖçÁΩÆÊñá‰ª∂ÔºàÊàñ‰∏ãËΩΩ‰ªìÂ∫ìweb-service\\src\\main\\resourcesÔºâ\ndocker create --name netdisk-fast-download ghcr.nju.edu.cn/qaiu/netdisk-fast-download:latest\ndocker cp netdisk-fast-download:/app/resources ./resources\ndocker rm netdisk-fast-download\n#\nÂêØÂä®ÂÆπÂô®\ndocker run -d -it --name netdisk-fast-download -p 6401:6401 --restart unless-stopped -e TZ=Asia/Shanghai -v ./resources:/app/resources -v ./db:/app/db -v ./logs:/app/logs ghcr.nju.edu.cn/qaiu/netdisk-fast-download:latest\n#\nÂèç‰ª£6401Á´ØÂè£\n#\nÂçáÁ∫ßÂÆπÂô®\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --cleanup --run-once netdisk-fast-download\nÂÆùÂ°îÈÉ®ÁΩ≤ÊåáÂºï ->\nÁÇπÂáªËøõÂÖ•ÂÆùÂ°îÈÉ®ÁΩ≤ÊïôÁ®ã\nLinuxÂëΩ‰ª§Ë°åÈÉ®ÁΩ≤\nÊ≥®ÊÑè: netdisk-fast-download.service‰∏≠ÁöÑExecStartÁöÑË∑ØÂæÑÊîπ‰∏∫ÂÆûÈôÖË∑ØÂæÑ\ncd\n~\nwget -O netdisk-fast-download.zip  https://github.com/qaiu/netdisk-fast-download/releases/download/0.1.8-release-fixed2/netdisk-fast-download-bin-fixed2.zip\nunzip netdisk-fast-download-bin.zip\ncd\nnetdisk-fast-download\nbash service-install.sh\nÊúçÂä°Áõ∏ÂÖ≥ÂëΩ‰ª§:\nÊü•ÁúãÊúçÂä°Áä∂ÊÄÅ\nsystemctl status netdisk-fast-download.service\nÂêØÂä®ÊúçÂä°\nsystemctl start netdisk-fast-download.service\nÈáçÂêØÊúçÂä°\nsystemctl restart netdisk-fast-download.service\nÂÅúÊ≠¢ÊúçÂä°\nsystemctl stop netdisk-fast-download.service\nÂºÄÊú∫ÂêØÂä®ÊúçÂä°\nsystemctl enable netdisk-fast-download.service\nÂÅúÊ≠¢ÂºÄÊú∫ÂêØÂä®\nsystemctl disable netdisk-fast-download.service\nWindowsÊúçÂä°ÈÉ®ÁΩ≤\n‰∏ãËΩΩÂπ∂Ëß£ÂéãreleasesÁâàÊú¨netdisk-fast-download-bin.zip\nËøõÂÖ•netdisk-fast-download‰∏ãÁöÑbinÁõÆÂΩï\n‰ΩøÁî®ÁÆ°ÁêÜÂëòÊùÉÈôêËøêË°ånfd-service-install.bat\nÂ¶ÇÊûú‰∏çÊÉ≥‰ΩøÁî®ÊúçÂä°ËøêË°åÂèØ‰ª•Áõ¥Êé•ËøêË°årun.bat\nÊ≥®ÊÑè: Â¶ÇÊûújdkÁéØÂ¢ÉÂèòÈáèÁöÑjavaÁâàÊú¨‰∏çÊòØ17ËØ∑‰øÆÊîπnfd-service-template.xml‰∏≠ÁöÑjavaÂëΩ‰ª§ÁöÑË∑ØÂæÑÊîπ‰∏∫ÂÆûÈôÖË∑ØÂæÑ\nÁõ∏ÂÖ≥ÈÖçÁΩÆËØ¥Êòé\nresourcesÁõÆÂΩï‰∏ãÂåÖÂê´ÊúçÂä°Á´ØÈÖçÁΩÆÊñá‰ª∂ ÈÖçÁΩÆÊñá‰ª∂Ëá™Â∏¶ËØ¥ÊòéÔºåÂÖ∑‰ΩìËØ∑Êü•ÁúãÈÖçÁΩÆÊñá‰ª∂ÂÜÖÂÆπÔºå\napp-dev.yml ÂèØ‰ª•ÈÖçÁΩÆËß£ÊûêÊúçÂä°Áõ∏ÂÖ≥‰ø°ÊÅØÔºå ÂåÖÊã¨Á´ØÂè£ÔºåÂüüÂêçÔºåÁºìÂ≠òÊó∂ÈïøÁ≠â\nserver-proxy.yml ÂèØ‰ª•ÈÖçÁΩÆ‰ª£ÁêÜÊúçÂä°ËøêË°åÁöÑÁõ∏ÂÖ≥‰ø°ÊÅØÔºå ÂåÖÊã¨ÂâçÁ´ØÂèçÂêë‰ª£ÁêÜÁ´ØÂè£ÔºåË∑ØÂæÑÁ≠â\nip‰ª£ÁêÜÈÖçÁΩÆËØ¥Êòé\nÊúâÊó∂ÂÄôËß£ÊûêÈáèÂæàÂ§ßÔºåIPÂÆπÊòìË¢´banÔºåËøôÊó∂ÂÄôÂèØ‰ª•‰ΩøÁî®ÂÖ∂‰ªñÊúçÂä°Âô®Êê≠Âª∫nfd-proxy‰ª£ÁêÜÊúçÂä°„ÄÇ\n‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂Ôºö\napp-dev.yml\nproxy\n:\n  -\npanTypes\n:\npgd,pdb,pod\n#\nÁΩëÁõòÊ†áËØÜ\ntype\n:\nhttp\n#\nÊîØÊåÅhttp/socks4/socks5\nhost\n:\n127.0.0.1\n#\n‰ª£ÁêÜIP\nport\n:\n7890\n#\nÁ´ØÂè£\nusername\n:\n#\nÁî®Êà∑Âêç\npassword\n:\n#\nÂØÜÁ†Å\nnfd-proxyÊê≠Âª∫http‰ª£ÁêÜÊúçÂä°Âô®\nÂèÇËÄÉ\nhttps://github.com/nfd-parser/nfd-proxy\n0.1.9 ÂºÄÂèëËÆ°Âàí\nÁõÆÂΩïËß£Êûê(‰∏ìÂ±ûÁâà)\nÂ∏¶cookie/tokenÂèÇÊï∞Ëß£ÊûêÂ§ßÊñá‰ª∂(‰∏ìÂ±ûÁâà)\nÊäÄÊúØÊ†à:\nJdk17+Vert.x4\nCoreÊ®°ÂùóÈõÜÊàêVert.xÂÆûÁé∞Á±ª‰ººspringÁöÑÊ≥®Ëß£ÂºèË∑ØÁî±API\nStar History\nÂÖçË¥£Â£∞Êòé\nÁî®Êà∑Âú®‰ΩøÁî®Êú¨È°πÁõÆÊó∂ÔºåÂ∫îËá™Ë°åÊâøÊãÖÈ£éÈô©ÔºåÂπ∂Á°Æ‰øùÂÖ∂Ë°å‰∏∫Á¨¶ÂêàÂΩìÂú∞Ê≥ïÂæãÊ≥ïËßÑÂèäÁΩëÁõòÊúçÂä°Êèê‰æõÂïÜÁöÑ‰ΩøÁî®Êù°Ê¨æ„ÄÇ\nÂºÄÂèëËÄÖ‰∏çÂØπÁî®Êà∑Âõ†‰ΩøÁî®Êú¨È°πÁõÆËÄåÂØºËá¥ÁöÑ‰ªª‰ΩïÂêéÊûúË¥üË¥£ÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÊï∞ÊçÆ‰∏¢Â§±„ÄÅÈöêÁßÅÊ≥ÑÈú≤„ÄÅË¥¶Âè∑Â∞ÅÁ¶ÅÊàñÂÖ∂‰ªñ‰ªª‰ΩïÂΩ¢ÂºèÁöÑÊçüÂÆ≥„ÄÇ\nÊîØÊåÅËØ•È°πÁõÆ\nÂºÄÊ∫ê‰∏çÊòìÔºåÁî®Áà±ÂèëÁîµÔºåÊú¨È°πÁõÆÈïøÊúüÁª¥Êä§Â¶ÇÊûúËßâÂæóÊúâÂ∏ÆÂä©, ÂèØ‰ª•ËØ∑‰ΩúËÄÖÂñùÊùØÂíñÂï°, ÊÑüË∞¢ÊîØÊåÅ\nÂÖ≥‰∫é‰∏ìÂ±ûÁâà\n99ÂÖÉ, Êèê‰æõÂØπÂ∞èÈ£ûÊú∫,ËìùÂ•è‰ºò‰∫´Â§ßÊñá‰ª∂Ëß£ÊûêÁöÑÊîØÊåÅ, Êèê‰æõÂ§©Áøº‰∫ëÁõò,ÁßªÂä®‰∫ëÁõò,ËÅîË∞É‰∫ëÁõòÁöÑËß£ÊûêÊîØÊåÅ\n199ÂÖÉ, ÂåÖÂê´ÈÉ®ÁΩ≤ÊúçÂä°ÂíåÈ¶ñÈ°µÂÆöÂà∂, ÈúÄÊèê‰æõÂÆùÂ°îÁéØÂ¢É\nÂèØ‰ª•Êèê‰æõÂäüËÉΩÂÆöÂà∂ÂºÄÂèë, Âä†v‰ª∑Ê†ºËØ¶Ë∞à:\nqq: 197575894\nwechat: imcoding_",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 41",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 2,220"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/qaiu/netdisk-fast-download"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/DioxusLabs/dioxus",
      "title": "DioxusLabs/dioxus",
      "date": null,
      "executive_summary": [
        "Fullstack app framework for web, desktop, and mobile.",
        "---",
        "Website\n|\nExamples\n|\nGuide\n|\n‰∏≠Êñá\n|\nPT-BR\n|\nÊó•Êú¨Ë™û\n|\nT√ºrk√ße\n|\nÌïúÍµ≠Ïñ¥\n‚ú® Dioxus 0.7 is in alpha - test it out! ‚ú®\nBuild for web, desktop, and mobile, and more with a single codebase. Zero-config setup, integrated hot-reloading, and signals-based state management. Add backend functionality with Server Functions and bundle with our CLI.\nfn\napp\n(\n)\n->\nElement\n{\nlet\nmut\ncount =\nuse_signal\n(\n||\n0\n)\n;\nrsx\n!\n{\nh1\n{\n\"High-Five counter: {count}\"\n}\nbutton\n{\nonclick\n:\nmove |_| count +=\n1\n,\n\"Up high!\"\n}\nbutton\n{\nonclick\n:\nmove |_| count -=\n1\n,\n\"Down low!\"\n}\n}\n}\n‚≠êÔ∏è Unique features:\nCross-platform apps in three lines of code (web, desktop, mobile, server, and more)\nErgonomic state management\ncombines the best of React, Solid, and Svelte\nBuilt-in featureful, type-safe, fullstack web framework\nIntegrated bundler for deploying to the web, macOS, Linux, and Windows\nSubsecond Rust hot-patching and asset hot-reloading\nAnd more!\nTake a tour of Dioxus\n.\nInstant hot-reloading\nWith one command,\ndx serve\nand your app is running. Edit your markup, styles, and see changes in milliseconds. Use our experimental\ndx serve --hotpatch\nto update Rust code in real time.\nBuild Beautiful Apps\nDioxus apps are styled with HTML and CSS. Use the built-in TailwindCSS support or load your favorite CSS library. Easily call into native code (objective-c, JNI, Web-Sys) for a perfect native touch.\nTruly fullstack applications\nDioxus deeply integrates with\naxum\nto provide powerful fullstack capabilities for both clients and servers. Pick from a wide array of built-in batteries like WebSockets, SSE, Streaming, File Upload/Download, Server-Side-Rendering, Forms, Middleware, and Hot-Reload, or go fully custom and integrate your existing axum backend.\nExperimental Native Renderer\nRender using web-sys, webview, server-side-rendering, liveview, or even with our experimental WGPU-based renderer. Embed Dioxus in Bevy, WGPU, or even run on embedded Linux!\nFirst-party primitive components\nGet started quickly with a complete set of primitives modeled after shadcn/ui and Radix-Primitives.\nFirst-class Android and iOS support\nDioxus is the fastest way to build native mobile apps with Rust. Simply run\ndx serve --platform android\nand your app is running in an emulator or on device in seconds. Call directly into JNI and Native APIs.\nBundle for web, desktop, and mobile\nSimply run\ndx bundle\nand your app will be built and bundled with maximization optimizations. On the web, take advantage of\n.avif\ngeneration,\n.wasm\ncompression, minification\n, and more. Build WebApps weighing\nless than 50kb\nand desktop/mobile apps less than 5mb.\nFantastic documentation\nWe've put a ton of effort into building clean, readable, and comprehensive documentation. All html elements and listeners are documented with MDN docs, and our Docs runs continuous integration with Dioxus itself to ensure that the docs are always up to date. Check out the\nDioxus website\nfor guides, references, recipes, and more. Fun fact: we use the Dioxus website as a testbed for new Dioxus features -\ncheck it out!\nModular and Customizable\nBuild your own renderer, or use a community renderer like\nFreya\n. Use our modular components like RSX, VirtualDom, Blitz, Taffy, and Subsecond.\nCommunity\nDioxus is a community-driven project, with a very active\nDiscord\nand\nGitHub\ncommunity. We're always looking for help, and we're happy to answer questions and help you get started.\nOur SDK\nis community-run and we even have a\nGitHub organization\nfor the best Dioxus crates that receive free upgrades and support.\nFull-time core team\nDioxus has grown from a side project to a small team of fulltime engineers. Thanks to the generous support of FutureWei, Satellite.im, the GitHub Accelerator program, we're able to work on Dioxus full-time. Our long term goal is for Dioxus to become self-sustaining by providing paid high-quality enterprise tools. If your company is interested in adopting Dioxus and would like to work with us, please reach out!\nSupported Platforms\nWeb\nRender directly to the DOM using WebAssembly\nPre-render with SSR and rehydrate on the client\nSimple \"hello world\" at about 50kb, comparable to React\nBuilt-in dev server and hot reloading for quick iteration\nDesktop\nRender using Webview or - experimentally - with WGPU or\nFreya\n(Skia)\nZero-config setup. Simply `cargo run` or `dx serve` to build your app\nFull support for native system access without IPC\nSupports macOS, Linux, and Windows. Portable <3mb binaries\nMobile\nRender using Webview or - experimentally - with WGPU or Skia\nBuild .ipa and .apk files for iOS and Android\nCall directly into Java and Objective-C with minimal overhead\nFrom \"hello world\" to running on device in seconds\nServer-side Rendering\nSuspense, hydration, and server-side rendering\nQuickly drop in backend functionality with server functions\nExtractors, middleware, and routing integrations\nStatic-site generation and incremental regeneration\nRunning the examples\nThe examples in the main branch of this repository target the git version of dioxus and the CLI. If you are looking for examples that work with the latest stable release of dioxus, check out the\n0.6 branch\n.\nThe examples in the top level of this repository can be run with:\ncargo run --example\n<\nexample\n>\nHowever, we encourage you to download the dioxus-cli to test out features like hot-reloading. To install the most recent binary CLI, you can use cargo binstall.\ncargo binstall dioxus-cli@0.7.0-rc.1 --force\nIf this CLI is out-of-date, you can install it directly from git\ncargo install --git https://github.com/DioxusLabs/dioxus dioxus-cli --locked\nWith the CLI, you can also run examples with the web platform. You will need to disable the default desktop feature and enable the web feature with this command:\ndx serve --example\n<\nexample\n>\n--platform web -- --no-default-features\nContributing\nCheck out the website\nsection on contributing\n.\nReport issues on our\nissue tracker\n.\nJoin\nthe discord and ask questions!\nLicense\nThis project is licensed under either the\nMIT license\nor the\nApache-2 License\n.\nUnless you explicitly state otherwise, any contribution intentionally submitted\nfor inclusion in Dioxus by you, shall be licensed as MIT or Apache-2, without any additional\nterms or conditions.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 37",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 30,886"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/DioxusLabs/dioxus"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/modelcontextprotocol/python-sdk",
      "title": "modelcontextprotocol/python-sdk",
      "date": null,
      "executive_summary": [
        "The official Python SDK for Model Context Protocol servers and clients",
        "---",
        "MCP Python SDK\nPython implementation of the Model Context Protocol (MCP)\nTable of Contents\nMCP Python SDK\nOverview\nInstallation\nAdding MCP to your python project\nRunning the standalone MCP development tools\nQuickstart\nWhat is MCP?\nCore Concepts\nServer\nResources\nTools\nStructured Output\nPrompts\nImages\nContext\nGetting Context in Functions\nContext Properties and Methods\nCompletions\nElicitation\nSampling\nLogging and Notifications\nAuthentication\nFastMCP Properties\nSession Properties and Methods\nRequest Context Properties\nRunning Your Server\nDevelopment Mode\nClaude Desktop Integration\nDirect Execution\nStreamable HTTP Transport\nCORS Configuration for Browser-Based Clients\nMounting to an Existing ASGI Server\nStreamableHTTP servers\nBasic mounting\nHost-based routing\nMultiple servers with path configuration\nPath configuration at initialization\nSSE servers\nAdvanced Usage\nLow-Level Server\nStructured Output Support\nPagination (Advanced)\nWriting MCP Clients\nClient Display Utilities\nOAuth Authentication for Clients\nParsing Tool Results\nMCP Primitives\nServer Capabilities\nDocumentation\nContributing\nLicense\nOverview\nThe Model Context Protocol allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. This Python SDK implements the full MCP specification, making it easy to:\nBuild MCP clients that can connect to any MCP server\nCreate MCP servers that expose resources, prompts and tools\nUse standard transports like stdio, SSE, and Streamable HTTP\nHandle all MCP protocol messages and lifecycle events\nInstallation\nAdding MCP to your python project\nWe recommend using\nuv\nto manage your Python projects.\nIf you haven't created a uv-managed project yet, create one:\nuv init mcp-server-demo\ncd\nmcp-server-demo\nThen add MCP to your project dependencies:\nuv add\n\"\nmcp[cli]\n\"\nAlternatively, for projects using pip for dependencies:\npip install\n\"\nmcp[cli]\n\"\nRunning the standalone MCP development tools\nTo run the mcp command with uv:\nuv run mcp\nQuickstart\nLet's create a simple MCP server that exposes a calculator tool and some data:\n\"\"\"\nFastMCP quickstart example.\ncd to the `examples/snippets/clients` directory and run:\nuv run server fastmcp_quickstart stdio\n\"\"\"\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Create an MCP server\nmcp\n=\nFastMCP\n(\n\"Demo\"\n)\n# Add an addition tool\n@\nmcp\n.\ntool\n()\ndef\nadd\n(\na\n:\nint\n,\nb\n:\nint\n)\n->\nint\n:\n\"\"\"Add two numbers\"\"\"\nreturn\na\n+\nb\n# Add a dynamic greeting resource\n@\nmcp\n.\nresource\n(\n\"greeting://{name}\"\n)\ndef\nget_greeting\n(\nname\n:\nstr\n)\n->\nstr\n:\n\"\"\"Get a personalized greeting\"\"\"\nreturn\nf\"Hello,\n{\nname\n}\n!\"\n# Add a prompt\n@\nmcp\n.\nprompt\n()\ndef\ngreet_user\n(\nname\n:\nstr\n,\nstyle\n:\nstr\n=\n\"friendly\"\n)\n->\nstr\n:\n\"\"\"Generate a greeting prompt\"\"\"\nstyles\n=\n{\n\"friendly\"\n:\n\"Please write a warm, friendly greeting\"\n,\n\"formal\"\n:\n\"Please write a formal, professional greeting\"\n,\n\"casual\"\n:\n\"Please write a casual, relaxed greeting\"\n,\n    }\nreturn\nf\"\n{\nstyles\n.\nget\n(\nstyle\n,\nstyles\n[\n'friendly'\n])\n}\nfor someone named\n{\nname\n}\n.\"\nFull example:\nexamples/snippets/servers/fastmcp_quickstart.py\nYou can install this server in\nClaude Desktop\nand interact with it right away by running:\nuv run mcp install server.py\nAlternatively, you can test it with the MCP Inspector:\nuv run mcp dev server.py\nWhat is MCP?\nThe\nModel Context Protocol (MCP)\nlets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:\nExpose data through\nResources\n(think of these sort of like GET endpoints; they are used to load information into the LLM's context)\nProvide functionality through\nTools\n(sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)\nDefine interaction patterns through\nPrompts\n(reusable templates for LLM interactions)\nAnd more!\nCore Concepts\nServer\nThe FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:\n\"\"\"Example showing lifespan support for startup/shutdown with strong typing.\"\"\"\nfrom\ncollections\n.\nabc\nimport\nAsyncIterator\nfrom\ncontextlib\nimport\nasynccontextmanager\nfrom\ndataclasses\nimport\ndataclass\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nContext\n,\nFastMCP\nfrom\nmcp\n.\nserver\n.\nsession\nimport\nServerSession\n# Mock database class for example\nclass\nDatabase\n:\n\"\"\"Mock database class for example.\"\"\"\n@\nclassmethod\nasync\ndef\nconnect\n(\ncls\n)\n->\n\"Database\"\n:\n\"\"\"Connect to database.\"\"\"\nreturn\ncls\n()\nasync\ndef\ndisconnect\n(\nself\n)\n->\nNone\n:\n\"\"\"Disconnect from database.\"\"\"\npass\ndef\nquery\n(\nself\n)\n->\nstr\n:\n\"\"\"Execute a query.\"\"\"\nreturn\n\"Query result\"\n@\ndataclass\nclass\nAppContext\n:\n\"\"\"Application context with typed dependencies.\"\"\"\ndb\n:\nDatabase\n@\nasynccontextmanager\nasync\ndef\napp_lifespan\n(\nserver\n:\nFastMCP\n)\n->\nAsyncIterator\n[\nAppContext\n]:\n\"\"\"Manage application lifecycle with type-safe context.\"\"\"\n# Initialize on startup\ndb\n=\nawait\nDatabase\n.\nconnect\n()\ntry\n:\nyield\nAppContext\n(\ndb\n=\ndb\n)\nfinally\n:\n# Cleanup on shutdown\nawait\ndb\n.\ndisconnect\n()\n# Pass lifespan to server\nmcp\n=\nFastMCP\n(\n\"My App\"\n,\nlifespan\n=\napp_lifespan\n)\n# Access type-safe lifespan context in tools\n@\nmcp\n.\ntool\n()\ndef\nquery_db\n(\nctx\n:\nContext\n[\nServerSession\n,\nAppContext\n])\n->\nstr\n:\n\"\"\"Tool that uses initialized resources.\"\"\"\ndb\n=\nctx\n.\nrequest_context\n.\nlifespan_context\n.\ndb\nreturn\ndb\n.\nquery\n()\nFull example:\nexamples/snippets/servers/lifespan_example.py\nResources\nResources are how you expose data to LLMs. They're similar to GET endpoints in a REST API - they provide data but shouldn't perform significant computation or have side effects:\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\nmcp\n=\nFastMCP\n(\nname\n=\n\"Resource Example\"\n)\n@\nmcp\n.\nresource\n(\n\"file://documents/{name}\"\n)\ndef\nread_document\n(\nname\n:\nstr\n)\n->\nstr\n:\n\"\"\"Read a document by name.\"\"\"\n# This would normally read from disk\nreturn\nf\"Content of\n{\nname\n}\n\"\n@\nmcp\n.\nresource\n(\n\"config://settings\"\n)\ndef\nget_settings\n()\n->\nstr\n:\n\"\"\"Get application settings.\"\"\"\nreturn\n\"\"\"{\n\"theme\": \"dark\",\n\"language\": \"en\",\n\"debug\": false\n}\"\"\"\nFull example:\nexamples/snippets/servers/basic_resource.py\nTools\nTools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\nmcp\n=\nFastMCP\n(\nname\n=\n\"Tool Example\"\n)\n@\nmcp\n.\ntool\n()\ndef\nsum\n(\na\n:\nint\n,\nb\n:\nint\n)\n->\nint\n:\n\"\"\"Add two numbers together.\"\"\"\nreturn\na\n+\nb\n@\nmcp\n.\ntool\n()\ndef\nget_weather\n(\ncity\n:\nstr\n,\nunit\n:\nstr\n=\n\"celsius\"\n)\n->\nstr\n:\n\"\"\"Get weather for a city.\"\"\"\n# This would normally call a weather API\nreturn\nf\"Weather in\n{\ncity\n}\n: 22degrees\n{\nunit\n[\n0\n].\nupper\n()\n}\n\"\nFull example:\nexamples/snippets/servers/basic_tool.py\nTools can optionally receive a Context object by including a parameter with the\nContext\ntype annotation. This context is automatically injected by the FastMCP framework and provides access to MCP capabilities:\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nContext\n,\nFastMCP\nfrom\nmcp\n.\nserver\n.\nsession\nimport\nServerSession\nmcp\n=\nFastMCP\n(\nname\n=\n\"Progress Example\"\n)\n@\nmcp\n.\ntool\n()\nasync\ndef\nlong_running_task\n(\ntask_name\n:\nstr\n,\nctx\n:\nContext\n[\nServerSession\n,\nNone\n],\nsteps\n:\nint\n=\n5\n)\n->\nstr\n:\n\"\"\"Execute a task with progress updates.\"\"\"\nawait\nctx\n.\ninfo\n(\nf\"Starting:\n{\ntask_name\n}\n\"\n)\nfor\ni\nin\nrange\n(\nsteps\n):\nprogress\n=\n(\ni\n+\n1\n)\n/\nsteps\nawait\nctx\n.\nreport_progress\n(\nprogress\n=\nprogress\n,\ntotal\n=\n1.0\n,\nmessage\n=\nf\"Step\n{\ni\n+\n1\n}\n/\n{\nsteps\n}\n\"\n,\n        )\nawait\nctx\n.\ndebug\n(\nf\"Completed step\n{\ni\n+\n1\n}\n\"\n)\nreturn\nf\"Task '\n{\ntask_name\n}\n' completed\"\nFull example:\nexamples/snippets/servers/tool_progress.py\nStructured Output\nTools will return structured results by default, if their return type\nannotation is compatible. Otherwise, they will return unstructured results.\nStructured output supports these return types:\nPydantic models (BaseModel subclasses)\nTypedDicts\nDataclasses and other classes with type hints\ndict[str, T]\n(where T is any JSON-serializable type)\nPrimitive types (str, int, float, bool, bytes, None) - wrapped in\n{\"result\": value}\nGeneric types (list, tuple, Union, Optional, etc.) - wrapped in\n{\"result\": value}\nClasses without type hints cannot be serialized for structured output. Only\nclasses with properly annotated attributes will be converted to Pydantic models\nfor schema generation and validation.\nStructured results are automatically validated against the output schema\ngenerated from the annotation. This ensures the tool returns well-typed,\nvalidated data that clients can easily process.\nNote:\nFor backward compatibility, unstructured results are also\nreturned. Unstructured results are provided for backward compatibility\nwith previous versions of the MCP specification, and are quirks-compatible\nwith previous versions of FastMCP in the current version of the SDK.\nNote:\nIn cases where a tool function's return type annotation\ncauses the tool to be classified as structured\nand this is undesirable\n,\nthe  classification can be suppressed by passing\nstructured_output=False\nto the\n@tool\ndecorator.\n\"\"\"Example showing structured output with tools.\"\"\"\nfrom\ntyping\nimport\nTypedDict\nfrom\npydantic\nimport\nBaseModel\n,\nField\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\nmcp\n=\nFastMCP\n(\n\"Structured Output Example\"\n)\n# Using Pydantic models for rich structured data\nclass\nWeatherData\n(\nBaseModel\n):\n\"\"\"Weather information structure.\"\"\"\ntemperature\n:\nfloat\n=\nField\n(\ndescription\n=\n\"Temperature in Celsius\"\n)\nhumidity\n:\nfloat\n=\nField\n(\ndescription\n=\n\"Humidity percentage\"\n)\ncondition\n:\nstr\nwind_speed\n:\nfloat\n@\nmcp\n.\ntool\n()\ndef\nget_weather\n(\ncity\n:\nstr\n)\n->\nWeatherData\n:\n\"\"\"Get weather for a city - returns structured data.\"\"\"\n# Simulated weather data\nreturn\nWeatherData\n(\ntemperature\n=\n22.5\n,\nhumidity\n=\n45.0\n,\ncondition\n=\n\"sunny\"\n,\nwind_speed\n=\n5.2\n,\n    )\n# Using TypedDict for simpler structures\nclass\nLocationInfo\n(\nTypedDict\n):\nlatitude\n:\nfloat\nlongitude\n:\nfloat\nname\n:\nstr\n@\nmcp\n.\ntool\n()\ndef\nget_location\n(\naddress\n:\nstr\n)\n->\nLocationInfo\n:\n\"\"\"Get location coordinates\"\"\"\nreturn\nLocationInfo\n(\nlatitude\n=\n51.5074\n,\nlongitude\n=\n-\n0.1278\n,\nname\n=\n\"London, UK\"\n)\n# Using dict[str, Any] for flexible schemas\n@\nmcp\n.\ntool\n()\ndef\nget_statistics\n(\ndata_type\n:\nstr\n)\n->\ndict\n[\nstr\n,\nfloat\n]:\n\"\"\"Get various statistics\"\"\"\nreturn\n{\n\"mean\"\n:\n42.5\n,\n\"median\"\n:\n40.0\n,\n\"std_dev\"\n:\n5.2\n}\n# Ordinary classes with type hints work for structured output\nclass\nUserProfile\n:\nname\n:\nstr\nage\n:\nint\nemail\n:\nstr\n|\nNone\n=\nNone\ndef\n__init__\n(\nself\n,\nname\n:\nstr\n,\nage\n:\nint\n,\nemail\n:\nstr\n|\nNone\n=\nNone\n):\nself\n.\nname\n=\nname\nself\n.\nage\n=\nage\nself\n.\nemail\n=\nemail\n@\nmcp\n.\ntool\n()\ndef\nget_user\n(\nuser_id\n:\nstr\n)\n->\nUserProfile\n:\n\"\"\"Get user profile - returns structured data\"\"\"\nreturn\nUserProfile\n(\nname\n=\n\"Alice\"\n,\nage\n=\n30\n,\nemail\n=\n\"alice@example.com\"\n)\n# Classes WITHOUT type hints cannot be used for structured output\nclass\nUntypedConfig\n:\ndef\n__init__\n(\nself\n,\nsetting1\n,\nsetting2\n):\n# type: ignore[reportMissingParameterType]\nself\n.\nsetting1\n=\nsetting1\nself\n.\nsetting2\n=\nsetting2\n@\nmcp\n.\ntool\n()\ndef\nget_config\n()\n->\nUntypedConfig\n:\n\"\"\"This returns unstructured output - no schema generated\"\"\"\nreturn\nUntypedConfig\n(\n\"value1\"\n,\n\"value2\"\n)\n# Lists and other types are wrapped automatically\n@\nmcp\n.\ntool\n()\ndef\nlist_cities\n()\n->\nlist\n[\nstr\n]:\n\"\"\"Get a list of cities\"\"\"\nreturn\n[\n\"London\"\n,\n\"Paris\"\n,\n\"Tokyo\"\n]\n# Returns: {\"result\": [\"London\", \"Paris\", \"Tokyo\"]}\n@\nmcp\n.\ntool\n()\ndef\nget_temperature\n(\ncity\n:\nstr\n)\n->\nfloat\n:\n\"\"\"Get temperature as a simple float\"\"\"\nreturn\n22.5\n# Returns: {\"result\": 22.5}\nFull example:\nexamples/snippets/servers/structured_output.py\nPrompts\nPrompts are reusable templates that help LLMs interact with your server effectively:\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\nfrom\nmcp\n.\nserver\n.\nfastmcp\n.\nprompts\nimport\nbase\nmcp\n=\nFastMCP\n(\nname\n=\n\"Prompt Example\"\n)\n@\nmcp\n.\nprompt\n(\ntitle\n=\n\"Code Review\"\n)\ndef\nreview_code\n(\ncode\n:\nstr\n)\n->\nstr\n:\nreturn\nf\"Please review this code:\n\\n\n\\n\n{\ncode\n}\n\"\n@\nmcp\n.\nprompt\n(\ntitle\n=\n\"Debug Assistant\"\n)\ndef\ndebug_error\n(\nerror\n:\nstr\n)\n->\nlist\n[\nbase\n.\nMessage\n]:\nreturn\n[\nbase\n.\nUserMessage\n(\n\"I'm seeing this error:\"\n),\nbase\n.\nUserMessage\n(\nerror\n),\nbase\n.\nAssistantMessage\n(\n\"I'll help debug that. What have you tried so far?\"\n),\n    ]\nFull example:\nexamples/snippets/servers/basic_prompt.py\nIcons\nMCP servers can provide icons for UI display. Icons can be added to the server implementation, tools, resources, and prompts:\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n,\nIcon\n# Create an icon from a file path or URL\nicon\n=\nIcon\n(\nsrc\n=\n\"icon.png\"\n,\nmimeType\n=\n\"image/png\"\n,\nsizes\n=\n\"64x64\"\n)\n# Add icons to server\nmcp\n=\nFastMCP\n(\n\"My Server\"\n,\nwebsite_url\n=\n\"https://example.com\"\n,\nicons\n=\n[\nicon\n]\n)\n# Add icons to tools, resources, and prompts\n@\nmcp\n.\ntool\n(\nicons\n=\n[\nicon\n])\ndef\nmy_tool\n():\n\"\"\"Tool with an icon.\"\"\"\nreturn\n\"result\"\n@\nmcp\n.\nresource\n(\n\"demo://resource\"\n,\nicons\n=\n[\nicon\n])\ndef\nmy_resource\n():\n\"\"\"Resource with an icon.\"\"\"\nreturn\n\"content\"\nFull example:\nexamples/fastmcp/icons_demo.py\nImages\nFastMCP provides an\nImage\nclass that automatically handles image data:\n\"\"\"Example showing image handling with FastMCP.\"\"\"\nfrom\nPIL\nimport\nImage\nas\nPILImage\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n,\nImage\nmcp\n=\nFastMCP\n(\n\"Image Example\"\n)\n@\nmcp\n.\ntool\n()\ndef\ncreate_thumbnail\n(\nimage_path\n:\nstr\n)\n->\nImage\n:\n\"\"\"Create a thumbnail from an image\"\"\"\nimg\n=\nPILImage\n.\nopen\n(\nimage_path\n)\nimg\n.\nthumbnail\n((\n100\n,\n100\n))\nreturn\nImage\n(\ndata\n=\nimg\n.\ntobytes\n(),\nformat\n=\n\"png\"\n)\nFull example:\nexamples/snippets/servers/images.py\nContext\nThe Context object is automatically injected into tool and resource functions that request it via type hints. It provides access to MCP capabilities like logging, progress reporting, resource reading, user interaction, and request metadata.\nGetting Context in Functions\nTo use context in a tool or resource function, add a parameter with the\nContext\ntype annotation:\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nContext\n,\nFastMCP\nmcp\n=\nFastMCP\n(\nname\n=\n\"Context Example\"\n)\n@\nmcp\n.\ntool\n()\nasync\ndef\nmy_tool\n(\nx\n:\nint\n,\nctx\n:\nContext\n)\n->\nstr\n:\n\"\"\"Tool that uses context capabilities.\"\"\"\n# The context parameter can have any name as long as it's type-annotated\nreturn\nawait\nprocess_with_context\n(\nx\n,\nctx\n)\nContext Properties and Methods\nThe Context object provides the following capabilities:\nctx.request_id\n- Unique ID for the current request\nctx.client_id\n- Client ID if available\nctx.fastmcp\n- Access to the FastMCP server instance (see\nFastMCP Properties\n)\nctx.session\n- Access to the underlying session for advanced communication (see\nSession Properties and Methods\n)\nctx.request_context\n- Access to request-specific data and lifespan resources (see\nRequest Context Properties\n)\nawait ctx.debug(message)\n- Send debug log message\nawait ctx.info(message)\n- Send info log message\nawait ctx.warning(message)\n- Send warning log message\nawait ctx.error(message)\n- Send error log message\nawait ctx.log(level, message, logger_name=None)\n- Send log with custom level\nawait ctx.report_progress(progress, total=None, message=None)\n- Report operation progress\nawait ctx.read_resource(uri)\n- Read a resource by URI\nawait ctx.elicit(message, schema)\n- Request additional information from user with validation\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nContext\n,\nFastMCP\nfrom\nmcp\n.\nserver\n.\nsession\nimport\nServerSession\nmcp\n=\nFastMCP\n(\nname\n=\n\"Progress Example\"\n)\n@\nmcp\n.\ntool\n()\nasync\ndef\nlong_running_task\n(\ntask_name\n:\nstr\n,\nctx\n:\nContext\n[\nServerSession\n,\nNone\n],\nsteps\n:\nint\n=\n5\n)\n->\nstr\n:\n\"\"\"Execute a task with progress updates.\"\"\"\nawait\nctx\n.\ninfo\n(\nf\"Starting:\n{\ntask_name\n}\n\"\n)\nfor\ni\nin\nrange\n(\nsteps\n):\nprogress\n=\n(\ni\n+\n1\n)\n/\nsteps\nawait\nctx\n.\nreport_progress\n(\nprogress\n=\nprogress\n,\ntotal\n=\n1.0\n,\nmessage\n=\nf\"Step\n{\ni\n+\n1\n}\n/\n{\nsteps\n}\n\"\n,\n        )\nawait\nctx\n.\ndebug\n(\nf\"Completed step\n{\ni\n+\n1\n}\n\"\n)\nreturn\nf\"Task '\n{\ntask_name\n}\n' completed\"\nFull example:\nexamples/snippets/servers/tool_progress.py\nCompletions\nMCP supports providing completion suggestions for prompt arguments and resource template parameters. With the context parameter, servers can provide completions based on previously resolved values:\nClient usage:\n\"\"\"\ncd to the `examples/snippets` directory and run:\nuv run completion-client\n\"\"\"\nimport\nasyncio\nimport\nos\nfrom\nmcp\nimport\nClientSession\n,\nStdioServerParameters\nfrom\nmcp\n.\nclient\n.\nstdio\nimport\nstdio_client\nfrom\nmcp\n.\ntypes\nimport\nPromptReference\n,\nResourceTemplateReference\n# Create server parameters for stdio connection\nserver_params\n=\nStdioServerParameters\n(\ncommand\n=\n\"uv\"\n,\n# Using uv to run the server\nargs\n=\n[\n\"run\"\n,\n\"server\"\n,\n\"completion\"\n,\n\"stdio\"\n],\n# Server with completion support\nenv\n=\n{\n\"UV_INDEX\"\n:\nos\n.\nenviron\n.\nget\n(\n\"UV_INDEX\"\n,\n\"\"\n)},\n)\nasync\ndef\nrun\n():\n\"\"\"Run the completion client example.\"\"\"\nasync\nwith\nstdio_client\n(\nserver_params\n)\nas\n(\nread\n,\nwrite\n):\nasync\nwith\nClientSession\n(\nread\n,\nwrite\n)\nas\nsession\n:\n# Initialize the connection\nawait\nsession\n.\ninitialize\n()\n# List available resource templates\ntemplates\n=\nawait\nsession\n.\nlist_resource_templates\n()\nprint\n(\n\"Available resource templates:\"\n)\nfor\ntemplate\nin\ntemplates\n.\nresourceTemplates\n:\nprint\n(\nf\"  -\n{\ntemplate\n.\nuriTemplate\n}\n\"\n)\n# List available prompts\nprompts\n=\nawait\nsession\n.\nlist_prompts\n()\nprint\n(\n\"\n\\n\nAvailable prompts:\"\n)\nfor\nprompt\nin\nprompts\n.\nprompts\n:\nprint\n(\nf\"  -\n{\nprompt\n.\nname\n}\n\"\n)\n# Complete resource template arguments\nif\ntemplates\n.\nresourceTemplates\n:\ntemplate\n=\ntemplates\n.\nresourceTemplates\n[\n0\n]\nprint\n(\nf\"\n\\n\nCompleting arguments for resource template:\n{\ntemplate\n.\nuriTemplate\n}\n\"\n)\n# Complete without context\nresult\n=\nawait\nsession\n.\ncomplete\n(\nref\n=\nResourceTemplateReference\n(\ntype\n=\n\"ref/resource\"\n,\nuri\n=\ntemplate\n.\nuriTemplate\n),\nargument\n=\n{\n\"name\"\n:\n\"owner\"\n,\n\"value\"\n:\n\"model\"\n},\n                )\nprint\n(\nf\"Completions for 'owner' starting with 'model':\n{\nresult\n.\ncompletion\n.\nvalues\n}\n\"\n)\n# Complete with context - repo suggestions based on owner\nresult\n=\nawait\nsession\n.\ncomplete\n(\nref\n=\nResourceTemplateReference\n(\ntype\n=\n\"ref/resource\"\n,\nuri\n=\ntemplate\n.\nuriTemplate\n),\nargument\n=\n{\n\"name\"\n:\n\"repo\"\n,\n\"value\"\n:\n\"\"\n},\ncontext_arguments\n=\n{\n\"owner\"\n:\n\"modelcontextprotocol\"\n},\n                )\nprint\n(\nf\"Completions for 'repo' with owner='modelcontextprotocol':\n{\nresult\n.\ncompletion\n.\nvalues\n}\n\"\n)\n# Complete prompt arguments\nif\nprompts\n.\nprompts\n:\nprompt_name\n=\nprompts\n.\nprompts\n[\n0\n].\nname\nprint\n(\nf\"\n\\n\nCompleting arguments for prompt:\n{\nprompt_name\n}\n\"\n)\nresult\n=\nawait\nsession\n.\ncomplete\n(\nref\n=\nPromptReference\n(\ntype\n=\n\"ref/prompt\"\n,\nname\n=\nprompt_name\n),\nargument\n=\n{\n\"name\"\n:\n\"style\"\n,\n\"value\"\n:\n\"\"\n},\n                )\nprint\n(\nf\"Completions for 'style' argument:\n{\nresult\n.\ncompletion\n.\nvalues\n}\n\"\n)\ndef\nmain\n():\n\"\"\"Entry point for the completion client.\"\"\"\nasyncio\n.\nrun\n(\nrun\n())\nif\n__name__\n==\n\"__main__\"\n:\nmain\n()\nFull example:\nexamples/snippets/clients/completion_client.py\nElicitation\nRequest additional information from users. This example shows an Elicitation during a Tool Call:\nfrom\npydantic\nimport\nBaseModel\n,\nField\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nContext\n,\nFastMCP\nfrom\nmcp\n.\nserver\n.\nsession\nimport\nServerSession\nmcp\n=\nFastMCP\n(\nname\n=\n\"Elicitation Example\"\n)\nclass\nBookingPreferences\n(\nBaseModel\n):\n\"\"\"Schema for collecting user preferences.\"\"\"\ncheckAlternative\n:\nbool\n=\nField\n(\ndescription\n=\n\"Would you like to check another date?\"\n)\nalternativeDate\n:\nstr\n=\nField\n(\ndefault\n=\n\"2024-12-26\"\n,\ndescription\n=\n\"Alternative date (YYYY-MM-DD)\"\n,\n    )\n@\nmcp\n.\ntool\n()\nasync\ndef\nbook_table\n(\ndate\n:\nstr\n,\ntime\n:\nstr\n,\nparty_size\n:\nint\n,\nctx\n:\nContext\n[\nServerSession\n,\nNone\n])\n->\nstr\n:\n\"\"\"Book a table with date availability check.\"\"\"\n# Check if date is available\nif\ndate\n==\n\"2024-12-25\"\n:\n# Date unavailable - ask user for alternative\nresult\n=\nawait\nctx\n.\nelicit\n(\nmessage\n=\n(\nf\"No tables available for\n{\nparty_size\n}\non\n{\ndate\n}\n. Would you like to try another date?\"\n),\nschema\n=\nBookingPreferences\n,\n        )\nif\nresult\n.\naction\n==\n\"accept\"\nand\nresult\n.\ndata\n:\nif\nresult\n.\ndata\n.\ncheckAlternative\n:\nreturn\nf\"[SUCCESS] Booked for\n{\nresult\n.\ndata\n.\nalternativeDate\n}\n\"\nreturn\n\"[CANCELLED] No booking made\"\nreturn\n\"[CANCELLED] Booking cancelled\"\n# Date available\nreturn\nf\"[SUCCESS] Booked for\n{\ndate\n}\nat\n{\ntime\n}\n\"\nFull example:\nexamples/snippets/servers/elicitation.py\nElicitation schemas support default values for all field types. Default values are automatically included in the JSON schema sent to clients, allowing them to pre-populate forms.\nThe\nelicit()\nmethod returns an\nElicitationResult\nwith:\naction\n: \"accept\", \"decline\", or \"cancel\"\ndata\n: The validated response (only when accepted)\nvalidation_error\n: Any validation error message\nSampling\nTools can interact with LLMs through sampling (generating text):\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nContext\n,\nFastMCP\nfrom\nmcp\n.\nserver\n.\nsession\nimport\nServerSession\nfrom\nmcp\n.\ntypes\nimport\nSamplingMessage\n,\nTextContent\nmcp\n=\nFastMCP\n(\nname\n=\n\"Sampling Example\"\n)\n@\nmcp\n.\ntool\n()\nasync\ndef\ngenerate_poem\n(\ntopic\n:\nstr\n,\nctx\n:\nContext\n[\nServerSession\n,\nNone\n])\n->\nstr\n:\n\"\"\"Generate a poem using LLM sampling.\"\"\"\nprompt\n=\nf\"Write a short poem about\n{\ntopic\n}\n\"\nresult\n=\nawait\nctx\n.\nsession\n.\ncreate_message\n(\nmessages\n=\n[\nSamplingMessage\n(\nrole\n=\n\"user\"\n,\ncontent\n=\nTextContent\n(\ntype\n=\n\"text\"\n,\ntext\n=\nprompt\n),\n            )\n        ],\nmax_tokens\n=\n100\n,\n    )\nif\nresult\n.\ncontent\n.\ntype\n==\n\"text\"\n:\nreturn\nresult\n.\ncontent\n.\ntext\nreturn\nstr\n(\nresult\n.\ncontent\n)\nFull example:\nexamples/snippets/servers/sampling.py\nLogging and Notifications\nTools can send logs and notifications through the context:\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nContext\n,\nFastMCP\nfrom\nmcp\n.\nserver\n.\nsession\nimport\nServerSession\nmcp\n=\nFastMCP\n(\nname\n=\n\"Notifications Example\"\n)\n@\nmcp\n.\ntool\n()\nasync\ndef\nprocess_data\n(\ndata\n:\nstr\n,\nctx\n:\nContext\n[\nServerSession\n,\nNone\n])\n->\nstr\n:\n\"\"\"Process data with logging.\"\"\"\n# Different log levels\nawait\nctx\n.\ndebug\n(\nf\"Debug: Processing '\n{\ndata\n}\n'\"\n)\nawait\nctx\n.\ninfo\n(\n\"Info: Starting processing\"\n)\nawait\nctx\n.\nwarning\n(\n\"Warning: This is experimental\"\n)\nawait\nctx\n.\nerror\n(\n\"Error: (This is just a demo)\"\n)\n# Notify about resource changes\nawait\nctx\n.\nsession\n.\nsend_resource_list_changed\n()\nreturn\nf\"Processed:\n{\ndata\n}\n\"\nFull example:\nexamples/snippets/servers/notifications.py\nAuthentication\nAuthentication can be used by servers that want to expose tools accessing protected resources.\nmcp.server.auth\nimplements OAuth 2.1 resource server functionality, where MCP servers act as Resource Servers (RS) that validate tokens issued by separate Authorization Servers (AS). This follows the\nMCP authorization specification\nand implements RFC 9728 (Protected Resource Metadata) for AS discovery.\nMCP servers can use authentication by providing an implementation of the\nTokenVerifier\nprotocol:\n\"\"\"\nRun from the repository root:\nuv run examples/snippets/servers/oauth_server.py\n\"\"\"\nfrom\npydantic\nimport\nAnyHttpUrl\nfrom\nmcp\n.\nserver\n.\nauth\n.\nprovider\nimport\nAccessToken\n,\nTokenVerifier\nfrom\nmcp\n.\nserver\n.\nauth\n.\nsettings\nimport\nAuthSettings\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\nclass\nSimpleTokenVerifier\n(\nTokenVerifier\n):\n\"\"\"Simple token verifier for demonstration.\"\"\"\nasync\ndef\nverify_token\n(\nself\n,\ntoken\n:\nstr\n)\n->\nAccessToken\n|\nNone\n:\npass\n# This is where you would implement actual token validation\n# Create FastMCP instance as a Resource Server\nmcp\n=\nFastMCP\n(\n\"Weather Service\"\n,\n# Token verifier for authentication\ntoken_verifier\n=\nSimpleTokenVerifier\n(),\n# Auth settings for RFC 9728 Protected Resource Metadata\nauth\n=\nAuthSettings\n(\nissuer_url\n=\nAnyHttpUrl\n(\n\"https://auth.example.com\"\n),\n# Authorization Server URL\nresource_server_url\n=\nAnyHttpUrl\n(\n\"http://localhost:3001\"\n),\n# This server's URL\nrequired_scopes\n=\n[\n\"user\"\n],\n    ),\n)\n@\nmcp\n.\ntool\n()\nasync\ndef\nget_weather\n(\ncity\n:\nstr\n=\n\"London\"\n)\n->\ndict\n[\nstr\n,\nstr\n]:\n\"\"\"Get weather data for a city\"\"\"\nreturn\n{\n\"city\"\n:\ncity\n,\n\"temperature\"\n:\n\"22\"\n,\n\"condition\"\n:\n\"Partly cloudy\"\n,\n\"humidity\"\n:\n\"65%\"\n,\n    }\nif\n__name__\n==\n\"__main__\"\n:\nmcp\n.\nrun\n(\ntransport\n=\n\"streamable-http\"\n)\nFull example:\nexamples/snippets/servers/oauth_server.py\nFor a complete example with separate Authorization Server and Resource Server implementations, see\nexamples/servers/simple-auth/\n.\nArchitecture:\nAuthorization Server (AS)\n: Handles OAuth flows, user authentication, and token issuance\nResource Server (RS)\n: Your MCP server that validates tokens and serves protected resources\nClient\n: Discovers AS through RFC 9728, obtains tokens, and uses them with the MCP server\nSee\nTokenVerifier\nfor more details on implementing token validation.\nFastMCP Properties\nThe FastMCP server instance accessible via\nctx.fastmcp\nprovides access to server configuration and metadata:\nctx.fastmcp.name\n- The server's name as defined during initialization\nctx.fastmcp.instructions\n- Server instructions/description provided to clients\nctx.fastmcp.website_url\n- Optional website URL for the server\nctx.fastmcp.icons\n- Optional list of icons for UI display\nctx.fastmcp.settings\n- Complete server configuration object containing:\ndebug\n- Debug mode flag\nlog_level\n- Current logging level\nhost\nand\nport\n- Server network configuration\nmount_path\n,\nsse_path\n,\nstreamable_http_path\n- Transport paths\nstateless_http\n- Whether the server operates in stateless mode\nAnd other configuration options\n@\nmcp\n.\ntool\n()\ndef\nserver_info\n(\nctx\n:\nContext\n)\n->\ndict\n:\n\"\"\"Get information about the current server.\"\"\"\nreturn\n{\n\"name\"\n:\nctx\n.\nfastmcp\n.\nname\n,\n\"instructions\"\n:\nctx\n.\nfastmcp\n.\ninstructions\n,\n\"debug_mode\"\n:\nctx\n.\nfastmcp\n.\nsettings\n.\ndebug\n,\n\"log_level\"\n:\nctx\n.\nfastmcp\n.\nsettings\n.\nlog_level\n,\n\"host\"\n:\nctx\n.\nfastmcp\n.\nsettings\n.\nhost\n,\n\"port\"\n:\nctx\n.\nfastmcp\n.\nsettings\n.\nport\n,\n    }\nSession Properties and Methods\nThe session object accessible via\nctx.session\nprovides advanced control over client communication:\nctx.session.client_params\n- Client initialization parameters and declared capabilities\nawait ctx.session.send_log_message(level, data, logger)\n- Send log messages with full control\nawait ctx.session.create_message(messages, max_tokens)\n- Request LLM sampling/completion\nawait ctx.session.send_progress_notification(token, progress, total, message)\n- Direct progress updates\nawait ctx.session.send_resource_updated(uri)\n- Notify clients that a specific resource changed\nawait ctx.session.send_resource_list_changed()\n- Notify clients that the resource list changed\nawait ctx.session.send_tool_list_changed()\n- Notify clients that the tool list changed\nawait ctx.session.send_prompt_list_changed()\n- Notify clients that the prompt list changed\n@\nmcp\n.\ntool\n()\nasync\ndef\nnotify_data_update\n(\nresource_uri\n:\nstr\n,\nctx\n:\nContext\n)\n->\nstr\n:\n\"\"\"Update data and notify clients of the change.\"\"\"\n# Perform data update logic here\n# Notify clients that this specific resource changed\nawait\nctx\n.\nsession\n.\nsend_resource_updated\n(\nAnyUrl\n(\nresource_uri\n))\n# If this affects the overall resource list, notify about that too\nawait\nctx\n.\nsession\n.\nsend_resource_list_changed\n()\nreturn\nf\"Updated\n{\nresource_uri\n}\nand notified clients\"\nRequest Context Properties\nThe request context accessible via\nctx.request_context\ncontains request-specific information and resources:\nctx.request_context.lifespan_context\n- Access to resources initialized during server startup\nDatabase connections, configuration objects, shared services\nType-safe access to resources defined in your server's lifespan function\nctx.request_context.meta\n- Request metadata from the client including:\nprogressToken\n- Token for progress notifications\nOther client-provided metadata\nctx.request_context.request\n- The original MCP request object for advanced processing\nctx.request_context.request_id\n- Unique identifier for this request\n# Example with typed lifespan context\n@\ndataclass\nclass\nAppContext\n:\ndb\n:\nDatabase\nconfig\n:\nAppConfig\n@\nmcp\n.\ntool\n()\ndef\nquery_with_config\n(\nquery\n:\nstr\n,\nctx\n:\nContext\n)\n->\nstr\n:\n\"\"\"Execute a query using shared database and configuration.\"\"\"\n# Access typed lifespan context\napp_ctx\n:\nAppContext\n=\nctx\n.\nrequest_context\n.\nlifespan_context\n# Use shared resources\nconnection\n=\napp_ctx\n.\ndb\nsettings\n=\napp_ctx\n.\nconfig\n# Execute query with configuration\nresult\n=\nconnection\n.\nexecute\n(\nquery\n,\ntimeout\n=\nsettings\n.\nquery_timeout\n)\nreturn\nstr\n(\nresult\n)\nFull lifespan example:\nexamples/snippets/servers/lifespan_example.py\nRunning Your Server\nDevelopment Mode\nThe fastest way to test and debug your server is with the MCP Inspector:\nuv run mcp dev server.py\n#\nAdd dependencies\nuv run mcp dev server.py --with pandas --with numpy\n#\nMount local code\nuv run mcp dev server.py --with-editable\n.\nClaude Desktop Integration\nOnce your server is ready, install it in Claude Desktop:\nuv run mcp install server.py\n#\nCustom name\nuv run mcp install server.py --name\n\"\nMy Analytics Server\n\"\n#\nEnvironment variables\nuv run mcp install server.py -v API_KEY=abc123 -v DB_URL=postgres://...\nuv run mcp install server.py -f .env\nDirect Execution\nFor advanced scenarios like custom deployments:\n\"\"\"Example showing direct execution of an MCP server.\nThis is the simplest way to run an MCP server directly.\ncd to the `examples/snippets` directory and run:\nuv run direct-execution-server\nor\npython servers/direct_execution.py\n\"\"\"\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\nmcp\n=\nFastMCP\n(\n\"My App\"\n)\n@\nmcp\n.\ntool\n()\ndef\nhello\n(\nname\n:\nstr\n=\n\"World\"\n)\n->\nstr\n:\n\"\"\"Say hello to someone.\"\"\"\nreturn\nf\"Hello,\n{\nname\n}\n!\"\ndef\nmain\n():\n\"\"\"Entry point for the direct execution server.\"\"\"\nmcp\n.\nrun\n()\nif\n__name__\n==\n\"__main__\"\n:\nmain\n()\nFull example:\nexamples/snippets/servers/direct_execution.py\nRun it with:\npython servers/direct_execution.py\n#\nor\nuv run mcp run servers/direct_execution.py\nNote that\nuv run mcp run\nor\nuv run mcp dev\nonly supports server using FastMCP and not the low-level server variant.\nStreamable HTTP Transport\nNote\n: Streamable HTTP transport is superseding SSE transport for production deployments.\n\"\"\"\nRun from the repository root:\nuv run examples/snippets/servers/streamable_config.py\n\"\"\"\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Stateful server (maintains session state)\nmcp\n=\nFastMCP\n(\n\"StatefulServer\"\n)\n# Other configuration options:\n# Stateless server (no session persistence)\n# mcp = FastMCP(\"StatelessServer\", stateless_http=True)\n# Stateless server (no session persistence, no sse stream with supported client)\n# mcp = FastMCP(\"StatelessServer\", stateless_http=True, json_response=True)\n# Add a simple tool to demonstrate the server\n@\nmcp\n.\ntool\n()\ndef\ngreet\n(\nname\n:\nstr\n=\n\"World\"\n)\n->\nstr\n:\n\"\"\"Greet someone by name.\"\"\"\nreturn\nf\"Hello,\n{\nname\n}\n!\"\n# Run server with streamable_http transport\nif\n__name__\n==\n\"__main__\"\n:\nmcp\n.\nrun\n(\ntransport\n=\n\"streamable-http\"\n)\nFull example:\nexamples/snippets/servers/streamable_config.py\nYou can mount multiple FastMCP servers in a Starlette application:\n\"\"\"\nRun from the repository root:\nuvicorn examples.snippets.servers.streamable_starlette_mount:app --reload\n\"\"\"\nimport\ncontextlib\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nrouting\nimport\nMount\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Create the Echo server\necho_mcp\n=\nFastMCP\n(\nname\n=\n\"EchoServer\"\n,\nstateless_http\n=\nTrue\n)\n@\necho_mcp\n.\ntool\n()\ndef\necho\n(\nmessage\n:\nstr\n)\n->\nstr\n:\n\"\"\"A simple echo tool\"\"\"\nreturn\nf\"Echo:\n{\nmessage\n}\n\"\n# Create the Math server\nmath_mcp\n=\nFastMCP\n(\nname\n=\n\"MathServer\"\n,\nstateless_http\n=\nTrue\n)\n@\nmath_mcp\n.\ntool\n()\ndef\nadd_two\n(\nn\n:\nint\n)\n->\nint\n:\n\"\"\"Tool to add two to the input\"\"\"\nreturn\nn\n+\n2\n# Create a combined lifespan to manage both session managers\n@\ncontextlib\n.\nasynccontextmanager\nasync\ndef\nlifespan\n(\napp\n:\nStarlette\n):\nasync\nwith\ncontextlib\n.\nAsyncExitStack\n()\nas\nstack\n:\nawait\nstack\n.\nenter_async_context\n(\necho_mcp\n.\nsession_manager\n.\nrun\n())\nawait\nstack\n.\nenter_async_context\n(\nmath_mcp\n.\nsession_manager\n.\nrun\n())\nyield\n# Create the Starlette app and mount the MCP servers\napp\n=\nStarlette\n(\nroutes\n=\n[\nMount\n(\n\"/echo\"\n,\necho_mcp\n.\nstreamable_http_app\n()),\nMount\n(\n\"/math\"\n,\nmath_mcp\n.\nstreamable_http_app\n()),\n    ],\nlifespan\n=\nlifespan\n,\n)\n# Note: Clients connect to http://localhost:8000/echo/mcp and http://localhost:8000/math/mcp\n# To mount at the root of each path (e.g., /echo instead of /echo/mcp):\n# echo_mcp.settings.streamable_http_path = \"/\"\n# math_mcp.settings.streamable_http_path = \"/\"\nFull example:\nexamples/snippets/servers/streamable_starlette_mount.py\nFor low level server with Streamable HTTP implementations, see:\nStateful server:\nexamples/servers/simple-streamablehttp/\nStateless server:\nexamples/servers/simple-streamablehttp-stateless/\nThe streamable HTTP transport supports:\nStateful and stateless operation modes\nResumability with event stores\nJSON or SSE response formats\nBetter scalability for multi-node deployments\nCORS Configuration for Browser-Based Clients\nIf you'd like your server to be accessible by browser-based MCP clients, you'll need to configure CORS headers. The\nMcp-Session-Id\nheader must be exposed for browser clients to access it:\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nmiddleware\n.\ncors\nimport\nCORSMiddleware\n# Create your Starlette app first\nstarlette_app\n=\nStarlette\n(\nroutes\n=\n[...])\n# Then wrap it with CORS middleware\nstarlette_app\n=\nCORSMiddleware\n(\nstarlette_app\n,\nallow_origins\n=\n[\n\"*\"\n],\n# Configure appropriately for production\nallow_methods\n=\n[\n\"GET\"\n,\n\"POST\"\n,\n\"DELETE\"\n],\n# MCP streamable HTTP methods\nexpose_headers\n=\n[\n\"Mcp-Session-Id\"\n],\n)\nThis configuration is necessary because:\nThe MCP streamable HTTP transport uses the\nMcp-Session-Id\nheader for session management\nBrowsers restrict access to response headers unless explicitly exposed via CORS\nWithout this configuration, browser-based clients won't be able to read the session ID from initialization responses\nMounting to an Existing ASGI Server\nBy default, SSE servers are mounted at\n/sse\nand Streamable HTTP servers are mounted at\n/mcp\n. You can customize these paths using the methods described below.\nFor more information on mounting applications in Starlette, see the\nStarlette documentation\n.\nStreamableHTTP servers\nYou can mount the StreamableHTTP server to an existing ASGI server using the\nstreamable_http_app\nmethod. This allows you to integrate the StreamableHTTP server with other ASGI applications.\nBasic mounting\n\"\"\"\nBasic example showing how to mount StreamableHTTP server in Starlette.\nRun from the repository root:\nuvicorn examples.snippets.servers.streamable_http_basic_mounting:app --reload\n\"\"\"\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nrouting\nimport\nMount\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Create MCP server\nmcp\n=\nFastMCP\n(\n\"My App\"\n)\n@\nmcp\n.\ntool\n()\ndef\nhello\n()\n->\nstr\n:\n\"\"\"A simple hello tool\"\"\"\nreturn\n\"Hello from MCP!\"\n# Mount the StreamableHTTP server to the existing ASGI server\napp\n=\nStarlette\n(\nroutes\n=\n[\nMount\n(\n\"/\"\n,\napp\n=\nmcp\n.\nstreamable_http_app\n()),\n    ]\n)\nFull example:\nexamples/snippets/servers/streamable_http_basic_mounting.py\nHost-based routing\n\"\"\"\nExample showing how to mount StreamableHTTP server using Host-based routing.\nRun from the repository root:\nuvicorn examples.snippets.servers.streamable_http_host_mounting:app --reload\n\"\"\"\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nrouting\nimport\nHost\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Create MCP server\nmcp\n=\nFastMCP\n(\n\"MCP Host App\"\n)\n@\nmcp\n.\ntool\n()\ndef\ndomain_info\n()\n->\nstr\n:\n\"\"\"Get domain-specific information\"\"\"\nreturn\n\"This is served from mcp.acme.corp\"\n# Mount using Host-based routing\napp\n=\nStarlette\n(\nroutes\n=\n[\nHost\n(\n\"mcp.acme.corp\"\n,\napp\n=\nmcp\n.\nstreamable_http_app\n()),\n    ]\n)\nFull example:\nexamples/snippets/servers/streamable_http_host_mounting.py\nMultiple servers with path configuration\n\"\"\"\nExample showing how to mount multiple StreamableHTTP servers with path configuration.\nRun from the repository root:\nuvicorn examples.snippets.servers.streamable_http_multiple_servers:app --reload\n\"\"\"\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nrouting\nimport\nMount\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Create multiple MCP servers\napi_mcp\n=\nFastMCP\n(\n\"API Server\"\n)\nchat_mcp\n=\nFastMCP\n(\n\"Chat Server\"\n)\n@\napi_mcp\n.\ntool\n()\ndef\napi_status\n()\n->\nstr\n:\n\"\"\"Get API status\"\"\"\nreturn\n\"API is running\"\n@\nchat_mcp\n.\ntool\n()\ndef\nsend_message\n(\nmessage\n:\nstr\n)\n->\nstr\n:\n\"\"\"Send a chat message\"\"\"\nreturn\nf\"Message sent:\n{\nmessage\n}\n\"\n# Configure servers to mount at the root of each path\n# This means endpoints will be at /api and /chat instead of /api/mcp and /chat/mcp\napi_mcp\n.\nsettings\n.\nstreamable_http_path\n=\n\"/\"\nchat_mcp\n.\nsettings\n.\nstreamable_http_path\n=\n\"/\"\n# Mount the servers\napp\n=\nStarlette\n(\nroutes\n=\n[\nMount\n(\n\"/api\"\n,\napp\n=\napi_mcp\n.\nstreamable_http_app\n()),\nMount\n(\n\"/chat\"\n,\napp\n=\nchat_mcp\n.\nstreamable_http_app\n()),\n    ]\n)\nFull example:\nexamples/snippets/servers/streamable_http_multiple_servers.py\nPath configuration at initialization\n\"\"\"\nExample showing path configuration during FastMCP initialization.\nRun from the repository root:\nuvicorn examples.snippets.servers.streamable_http_path_config:app --reload\n\"\"\"\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nrouting\nimport\nMount\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Configure streamable_http_path during initialization\n# This server will mount at the root of wherever it's mounted\nmcp_at_root\n=\nFastMCP\n(\n\"My Server\"\n,\nstreamable_http_path\n=\n\"/\"\n)\n@\nmcp_at_root\n.\ntool\n()\ndef\nprocess_data\n(\ndata\n:\nstr\n)\n->\nstr\n:\n\"\"\"Process some data\"\"\"\nreturn\nf\"Processed:\n{\ndata\n}\n\"\n# Mount at /process - endpoints will be at /process instead of /process/mcp\napp\n=\nStarlette\n(\nroutes\n=\n[\nMount\n(\n\"/process\"\n,\napp\n=\nmcp_at_root\n.\nstreamable_http_app\n()),\n    ]\n)\nFull example:\nexamples/snippets/servers/streamable_http_path_config.py\nSSE servers\nNote\n: SSE transport is being superseded by\nStreamable HTTP transport\n.\nYou can mount the SSE server to an existing ASGI server using the\nsse_app\nmethod. This allows you to integrate the SSE server with other ASGI applications.\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nrouting\nimport\nMount\n,\nHost\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\nmcp\n=\nFastMCP\n(\n\"My App\"\n)\n# Mount the SSE server to the existing ASGI server\napp\n=\nStarlette\n(\nroutes\n=\n[\nMount\n(\n'/'\n,\napp\n=\nmcp\n.\nsse_app\n()),\n    ]\n)\n# or dynamically mount as host\napp\n.\nrouter\n.\nroutes\n.\nappend\n(\nHost\n(\n'mcp.acme.corp'\n,\napp\n=\nmcp\n.\nsse_app\n()))\nWhen mounting multiple MCP servers under different paths, you can configure the mount path in several ways:\nfrom\nstarlette\n.\napplications\nimport\nStarlette\nfrom\nstarlette\n.\nrouting\nimport\nMount\nfrom\nmcp\n.\nserver\n.\nfastmcp\nimport\nFastMCP\n# Create multiple MCP servers\ngithub_mcp\n=\nFastMCP\n(\n\"GitHub API\"\n)\nbrowser_mcp\n=\nFastMCP\n(\n\"Browser\"\n)\ncurl_mcp\n=\nFastMCP\n(\n\"Curl\"\n)\nsearch_mcp\n=\nFastMCP\n(\n\"Search\"\n)\n# Method 1: Configure mount paths via settings (recommended for persistent configuration)\ngithub_mcp\n.\nsettings\n.\nmount_path\n=\n\"/github\"\nbrowser_mcp\n.\nsettings\n.\nmount_path\n=\n\"/browser\"\n# Method 2: Pass mount path directly to sse_app (preferred for ad-hoc mounting)\n# This approach doesn't modify the server's settings permanently\n# Create Starlette app with multiple mounted servers\napp\n=\nStarlette\n(\nroutes\n=\n[\n# Using settings-based configuration\nMount\n(\n\"/github\"\n,\napp\n=\ngithub_mcp\n.\nsse_app\n()),\nMount\n(\n\"/browser\"\n,\napp\n=\nbrowser_mcp\n.\nsse_app\n()),\n# Using direct mount path parameter\nMount\n(\n\"/curl\"\n,\napp\n=\ncurl_mcp\n.\nsse_app\n(\n\"/curl\"\n)),\nMount\n(\n\"/search\"\n,\napp\n=\nsearch_mcp\n.\nsse_app\n(\n\"/search\"\n)),\n    ]\n)\n# Method 3: For direct execution, you can also pass the mount path to run()\nif\n__name__\n==\n\"__main__\"\n:\nsearch_mcp\n.\nrun\n(\ntransport\n=\n\"sse\"\n,\nmount_path\n=\n\"/search\"\n)\nFor more information on mounting applications in Starlette, see the\nStarlette documentation\n.\nAdvanced Usage\nLow-Level Server\nFor more control, you can use the low-level server implementation directly. This gives you full access to the protocol and allows you to customize every aspect of your server, including lifecycle management through the lifespan API:\n\"\"\"\nRun from the repository root:\nuv run examples/snippets/servers/lowlevel/lifespan.py\n\"\"\"\nfrom\ncollections\n.\nabc\nimport\nAsyncIterator\nfrom\ncontextlib\nimport\nasynccontextmanager\nfrom\ntyping\nimport\nAny\nimport\nmcp\n.\nserver\n.\nstdio\nimport\nmcp\n.\ntypes\nas\ntypes\nfrom\nmcp\n.\nserver\n.\nlowlevel\nimport\nNotificationOptions\n,\nServer\nfrom\nmcp\n.\nserver\n.\nmodels\nimport\nInitializationOptions\n# Mock database class for example\nclass\nDatabase\n:\n\"\"\"Mock database class for example.\"\"\"\n@\nclassmethod\nasync\ndef\nconnect\n(\ncls\n)\n->\n\"Database\"\n:\n\"\"\"Connect to database.\"\"\"\nprint\n(\n\"Database connected\"\n)\nreturn\ncls\n()\nasync\ndef\ndisconnect\n(\nself\n)\n->\nNone\n:\n\"\"\"Disconnect from database.\"\"\"\nprint\n(\n\"Database disconnected\"\n)\nasync\ndef\nquery\n(\nself\n,\nquery_str\n:\nstr\n)\n->\nlist\n[\ndict\n[\nstr\n,\nstr\n]]:\n\"\"\"Execute a query.\"\"\"\n# Simulate database query\nreturn\n[{\n\"id\"\n:\n\"1\"\n,\n\"name\"\n:\n\"Example\"\n,\n\"query\"\n:\nquery_str\n}]\n@\nasynccontextmanager\nasync\ndef\nserver_lifespan\n(\n_server\n:\nServer\n)\n->\nAsyncIterator\n[\ndict\n[\nstr\n,\nAny\n]]:\n\"\"\"Manage server startup and shutdown lifecycle.\"\"\"\n# Initialize resources on startup\ndb\n=\nawait\nDatabase\n.\nconnect\n()\ntry\n:\nyield\n{\n\"db\"\n:\ndb\n}\nfinally\n:\n# Clean up on shutdown\nawait\ndb\n.\ndisconnect\n()\n# Pass lifespan to server\nserver\n=\nServer\n(\n\"example-server\"\n,\nlifespan\n=\nserver_lifespan\n)\n@\nserver\n.\nlist_tools\n()\nasync\ndef\nhandle_list_tools\n()\n->\nlist\n[\ntypes\n.\nTool\n]:\n\"\"\"List available tools.\"\"\"\nreturn\n[\ntypes\n.\nTool\n(\nname\n=\n\"query_db\"\n,\ndescription\n=\n\"Query the database\"\n,\ninputSchema\n=\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n: {\n\"query\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"SQL query to execute\"\n}},\n\"required\"\n: [\n\"query\"\n],\n            },\n        )\n    ]\n@\nserver\n.\ncall_tool\n()\nasync\ndef\nquery_db\n(\nname\n:\nstr\n,\narguments\n:\ndict\n[\nstr\n,\nAny\n])\n->\nlist\n[\ntypes\n.\nTextContent\n]:\n\"\"\"Handle database query tool call.\"\"\"\nif\nname\n!=\n\"query_db\"\n:\nraise\nValueError\n(\nf\"Unknown tool:\n{\nname\n}\n\"\n)\n# Access lifespan context\nctx\n=\nserver\n.\nrequest_context\ndb\n=\nctx\n.\nlifespan_context\n[\n\"db\"\n]\n# Execute query\nresults\n=\nawait\ndb\n.\nquery\n(\narguments\n[\n\"query\"\n])\nreturn\n[\ntypes\n.\nTextContent\n(\ntype\n=\n\"text\"\n,\ntext\n=\nf\"Query results:\n{\nresults\n}\n\"\n)]\nasync\ndef\nrun\n():\n\"\"\"Run the server with lifespan management.\"\"\"\nasync\nwith\nmcp\n.\nserver\n.\nstdio\n.\nstdio_server\n()\nas\n(\nread_stream\n,\nwrite_stream\n):\nawait\nserver\n.\nrun\n(\nread_stream\n,\nwrite_stream\n,\nInitializationOptions\n(\nserver_name\n=\n\"example-server\"\n,\nserver_version\n=\n\"0.1.0\"\n,\ncapabilities\n=\nserver\n.\nget_capabilities\n(\nnotification_options\n=\nNotificationOptions\n(),\nexperimental_capabilities\n=\n{},\n                ),\n            ),\n        )\nif\n__name__\n==\n\"__main__\"\n:\nimport\nasyncio\nasyncio\n.\nrun\n(\nrun\n())\nFull example:\nexamples/snippets/servers/lowlevel/lifespan.py\nThe lifespan API provides:\nA way to initialize resources when the server starts and clean them up when it stops\nAccess to initialized resources through the request context in handlers\nType-safe context passing between lifespan and request handlers\n\"\"\"\nRun from the repository root:\nuv run examples/snippets/servers/lowlevel/basic.py\n\"\"\"\nimport\nasyncio\nimport\nmcp\n.\nserver\n.\nstdio\nimport\nmcp\n.\ntypes\nas\ntypes\nfrom\nmcp\n.\nserver\n.\nlowlevel\nimport\nNotificationOptions\n,\nServer\nfrom\nmcp\n.\nserver\n.\nmodels\nimport\nInitializationOptions\n# Create a server instance\nserver\n=\nServer\n(\n\"example-server\"\n)\n@\nserver\n.\nlist_prompts\n()\nasync\ndef\nhandle_list_prompts\n()\n->\nlist\n[\ntypes\n.\nPrompt\n]:\n\"\"\"List available prompts.\"\"\"\nreturn\n[\ntypes\n.\nPrompt\n(\nname\n=\n\"example-prompt\"\n,\ndescription\n=\n\"An example prompt template\"\n,\narguments\n=\n[\ntypes\n.\nPromptArgument\n(\nname\n=\n\"arg1\"\n,\ndescription\n=\n\"Example argument\"\n,\nrequired\n=\nTrue\n)],\n        )\n    ]\n@\nserver\n.\nget_prompt\n()\nasync\ndef\nhandle_get_prompt\n(\nname\n:\nstr\n,\narguments\n:\ndict\n[\nstr\n,\nstr\n]\n|\nNone\n)\n->\ntypes\n.\nGetPromptResult\n:\n\"\"\"Get a specific prompt by name.\"\"\"\nif\nname\n!=\n\"example-prompt\"\n:\nraise\nValueError\n(\nf\"Unknown prompt:\n{\nname\n}\n\"\n)\narg1_value\n=\n(\narguments\nor\n{}).\nget\n(\n\"arg1\"\n,\n\"default\"\n)\nreturn\ntypes\n.\nGetPromptResult\n(\ndescription\n=\n\"Example prompt\"\n,\nmessages\n=\n[\ntypes\n.\nPromptMessage\n(\nrole\n=\n\"user\"\n,\ncontent\n=\ntypes\n.\nTextContent\n(\ntype\n=\n\"text\"\n,\ntext\n=\nf\"Example prompt text with argument:\n{\narg1_value\n}\n\"\n),\n            )\n        ],\n    )\nasync\ndef\nrun\n():\n\"\"\"Run the basic low-level server.\"\"\"\nasync\nwith\nmcp\n.\nserver\n.\nstdio\n.\nstdio_server\n()\nas\n(\nread_stream\n,\nwrite_stream\n):\nawait\nserver\n.\nrun\n(\nread_stream\n,\nwrite_stream\n,\nInitializationOptions\n(\nserver_name\n=\n\"example\"\n,\nserver_version\n=\n\"0.1.0\"\n,\ncapabilities\n=\nserver\n.\nget_capabilities\n(\nnotification_options\n=\nNotificationOptions\n(),\nexperimental_capabilities\n=\n{},\n                ),\n            ),\n        )\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nrun\n())\nFull example:\nexamples/snippets/servers/lowlevel/basic.py\nCaution: The\nuv run mcp run\nand\nuv run mcp dev\ntool doesn't support low-level server.\nStructured Output Support\nThe low-level server supports structured output for tools, allowing you to return both human-readable content and machine-readable structured data. Tools can define an\noutputSchema\nto validate their structured output:\n\"\"\"\nRun from the repository root:\nuv run examples/snippets/servers/lowlevel/structured_output.py\n\"\"\"\nimport\nasyncio\nfrom\ntyping\nimport\nAny\nimport\nmcp\n.\nserver\n.\nstdio\nimport\nmcp\n.\ntypes\nas\ntypes\nfrom\nmcp\n.\nserver\n.\nlowlevel\nimport\nNotificationOptions\n,\nServer\nfrom\nmcp\n.\nserver\n.\nmodels\nimport\nInitializationOptions\nserver\n=\nServer\n(\n\"example-server\"\n)\n@\nserver\n.\nlist_tools\n()\nasync\ndef\nlist_tools\n()\n->\nlist\n[\ntypes\n.\nTool\n]:\n\"\"\"List available tools with structured output schemas.\"\"\"\nreturn\n[\ntypes\n.\nTool\n(\nname\n=\n\"get_weather\"\n,\ndescription\n=\n\"Get current weather for a city\"\n,\ninputSchema\n=\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n: {\n\"city\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"City name\"\n}},\n\"required\"\n: [\n\"city\"\n],\n            },\noutputSchema\n=\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n: {\n\"temperature\"\n: {\n\"type\"\n:\n\"number\"\n,\n\"description\"\n:\n\"Temperature in Celsius\"\n},\n\"condition\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"Weather condition\"\n},\n\"humidity\"\n: {\n\"type\"\n:\n\"number\"\n,\n\"description\"\n:\n\"Humidity percentage\"\n},\n\"city\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"City name\"\n},\n                },\n\"required\"\n: [\n\"temperature\"\n,\n\"condition\"\n,\n\"humidity\"\n,\n\"city\"\n],\n            },\n        )\n    ]\n@\nserver\n.\ncall_tool\n()\nasync\ndef\ncall_tool\n(\nname\n:\nstr\n,\narguments\n:\ndict\n[\nstr\n,\nAny\n])\n->\ndict\n[\nstr\n,\nAny\n]:\n\"\"\"Handle tool calls with structured output.\"\"\"\nif\nname\n==\n\"get_weather\"\n:\ncity\n=\narguments\n[\n\"city\"\n]\n# Simulated weather data - in production, call a weather API\nweather_data\n=\n{\n\"temperature\"\n:\n22.5\n,\n\"condition\"\n:\n\"partly cloudy\"\n,\n\"humidity\"\n:\n65\n,\n\"city\"\n:\ncity\n,\n# Include the requested city\n}\n# low-level server will validate structured output against the tool's\n# output schema, and additionally serialize it into a TextContent block\n# for backwards compatibility with pre-2025-06-18 clients.\nreturn\nweather_data\nelse\n:\nraise\nValueError\n(\nf\"Unknown tool:\n{\nname\n}\n\"\n)\nasync\ndef\nrun\n():\n\"\"\"Run the structured output server.\"\"\"\nasync\nwith\nmcp\n.\nserver\n.\nstdio\n.\nstdio_server\n()\nas\n(\nread_stream\n,\nwrite_stream\n):\nawait\nserver\n.\nrun\n(\nread_stream\n,\nwrite_stream\n,\nInitializationOptions\n(\nserver_name\n=\n\"structured-output-example\"\n,\nserver_version\n=\n\"0.1.0\"\n,\ncapabilities\n=\nserver\n.\nget_capabilities\n(\nnotification_options\n=\nNotificationOptions\n(),\nexperimental_capabilities\n=\n{},\n                ),\n            ),\n        )\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nrun\n())\nFull example:\nexamples/snippets/servers/lowlevel/structured_output.py\nTools can return data in three ways:\nContent only\n: Return a list of content blocks (default behavior before spec revision 2025-06-18)\nStructured data only\n: Return a dictionary that will be serialized to JSON (Introduced in spec revision 2025-06-18)\nBoth\n: Return a tuple of (content, structured_data) preferred option to use for backwards compatibility\nWhen an\noutputSchema\nis defined, the server automatically validates the structured output against the schema. This ensures type safety and helps catch errors early.\nPagination (Advanced)\nFor servers that need to handle large datasets, the low-level server provides paginated versions of list operations. This is an optional optimization - most servers won't need pagination unless they're dealing with hundreds or thousands of items.\nServer-side Implementation\n\"\"\"\nExample of implementing pagination with MCP server decorators.\n\"\"\"\nfrom\npydantic\nimport\nAnyUrl\nimport\nmcp\n.\ntypes\nas\ntypes\nfrom\nmcp\n.\nserver\n.\nlowlevel\nimport\nServer\n# Initialize the server\nserver\n=\nServer\n(\n\"paginated-server\"\n)\n# Sample data to paginate\nITEMS\n=\n[\nf\"Item\n{\ni\n}\n\"\nfor\ni\nin\nrange\n(\n1\n,\n101\n)]\n# 100 items\n@\nserver\n.\nlist_resources\n()\nasync\ndef\nlist_resources_paginated\n(\nrequest\n:\ntypes\n.\nListResourcesRequest\n)\n->\ntypes\n.\nListResourcesResult\n:\n\"\"\"List resources with pagination support.\"\"\"\npage_size\n=\n10\n# Extract cursor from request params\ncursor\n=\nrequest\n.\nparams\n.\ncursor\nif\nrequest\n.\nparams\nis\nnot\nNone\nelse\nNone\n# Parse cursor to get offset\nstart\n=\n0\nif\ncursor\nis\nNone\nelse\nint\n(\ncursor\n)\nend\n=\nstart\n+\npage_size\n# Get page of resources\npage_items\n=\n[\ntypes\n.\nResource\n(\nuri\n=\nAnyUrl\n(\nf\"resource://items/\n{\nitem\n}\n\"\n),\nname\n=\nitem\n,\ndescription\n=\nf\"Description for\n{\nitem\n}\n\"\n)\nfor\nitem\nin\nITEMS\n[\nstart\n:\nend\n]\n    ]\n# Determine next cursor\nnext_cursor\n=\nstr\n(\nend\n)\nif\nend\n<\nlen\n(\nITEMS\n)\nelse\nNone\nreturn\ntypes\n.\nListResourcesResult\n(\nresources\n=\npage_items\n,\nnextCursor\n=\nnext_cursor\n)\nFull example:\nexamples/snippets/servers/pagination_example.py\nClient-side Consumption\n\"\"\"\nExample of consuming paginated MCP endpoints from a client.\n\"\"\"\nimport\nasyncio\nfrom\nmcp\n.\nclient\n.\nsession\nimport\nClientSession\nfrom\nmcp\n.\nclient\n.\nstdio\nimport\nStdioServerParameters\n,\nstdio_client\nfrom\nmcp\n.\ntypes\nimport\nResource\nasync\ndef\nlist_all_resources\n()\n->\nNone\n:\n\"\"\"Fetch all resources using pagination.\"\"\"\nasync\nwith\nstdio_client\n(\nStdioServerParameters\n(\ncommand\n=\n\"uv\"\n,\nargs\n=\n[\n\"run\"\n,\n\"mcp-simple-pagination\"\n]))\nas\n(\nread\n,\nwrite\n,\n    ):\nasync\nwith\nClientSession\n(\nread\n,\nwrite\n)\nas\nsession\n:\nawait\nsession\n.\ninitialize\n()\nall_resources\n:\nlist\n[\nResource\n]\n=\n[]\ncursor\n=\nNone\nwhile\nTrue\n:\n# Fetch a page of resources\nresult\n=\nawait\nsession\n.\nlist_resources\n(\ncursor\n=\ncursor\n)\nall_resources\n.\nextend\n(\nresult\n.\nresources\n)\nprint\n(\nf\"Fetched\n{\nlen\n(\nresult\n.\nresources\n)\n}\nresources\"\n)\n# Check if there are more pages\nif\nresult\n.\nnextCursor\n:\ncursor\n=\nresult\n.\nnextCursor\nelse\n:\nbreak\nprint\n(\nf\"Total resources:\n{\nlen\n(\nall_resources\n)\n}\n\"\n)\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nlist_all_resources\n())\nFull example:\nexamples/snippets/clients/pagination_client.py\nKey Points\nCursors are opaque strings\n- the server defines the format (numeric offsets, timestamps, etc.)\nReturn\nnextCursor=None\nwhen there are no more pages\nBackward compatible\n- clients that don't support pagination will still work (they'll just get the first page)\nFlexible page sizes\n- Each endpoint can define its own page size based on data characteristics\nSee the\nsimple-pagination example\nfor a complete implementation.\nWriting MCP Clients\nThe SDK provides a high-level client interface for connecting to MCP servers using various\ntransports\n:\n\"\"\"\ncd to the `examples/snippets/clients` directory and run:\nuv run client\n\"\"\"\nimport\nasyncio\nimport\nos\nfrom\npydantic\nimport\nAnyUrl\nfrom\nmcp\nimport\nClientSession\n,\nStdioServerParameters\n,\ntypes\nfrom\nmcp\n.\nclient\n.\nstdio\nimport\nstdio_client\nfrom\nmcp\n.\nshared\n.\ncontext\nimport\nRequestContext\n# Create server parameters for stdio connection\nserver_params\n=\nStdioServerParameters\n(\ncommand\n=\n\"uv\"\n,\n# Using uv to run the server\nargs\n=\n[\n\"run\"\n,\n\"server\"\n,\n\"fastmcp_quickstart\"\n,\n\"stdio\"\n],\n# We're already in snippets dir\nenv\n=\n{\n\"UV_INDEX\"\n:\nos\n.\nenviron\n.\nget\n(\n\"UV_INDEX\"\n,\n\"\"\n)},\n)\n# Optional: create a sampling callback\nasync\ndef\nhandle_sampling_message\n(\ncontext\n:\nRequestContext\n[\nClientSession\n,\nNone\n],\nparams\n:\ntypes\n.\nCreateMessageRequestParams\n)\n->\ntypes\n.\nCreateMessageResult\n:\nprint\n(\nf\"Sampling request:\n{\nparams\n.\nmessages\n}\n\"\n)\nreturn\ntypes\n.\nCreateMessageResult\n(\nrole\n=\n\"assistant\"\n,\ncontent\n=\ntypes\n.\nTextContent\n(\ntype\n=\n\"text\"\n,\ntext\n=\n\"Hello, world! from model\"\n,\n        ),\nmodel\n=\n\"gpt-3.5-turbo\"\n,\nstopReason\n=\n\"endTurn\"\n,\n    )\nasync\ndef\nrun\n():\nasync\nwith\nstdio_client\n(\nserver_params\n)\nas\n(\nread\n,\nwrite\n):\nasync\nwith\nClientSession\n(\nread\n,\nwrite\n,\nsampling_callback\n=\nhandle_sampling_message\n)\nas\nsession\n:\n# Initialize the connection\nawait\nsession\n.\ninitialize\n()\n# List available prompts\nprompts\n=\nawait\nsession\n.\nlist_prompts\n()\nprint\n(\nf\"Available prompts:\n{\n[\np\n.\nname\nfor\np\nin\nprompts\n.\nprompts\n]\n}\n\"\n)\n# Get a prompt (greet_user prompt from fastmcp_quickstart)\nif\nprompts\n.\nprompts\n:\nprompt\n=\nawait\nsession\n.\nget_prompt\n(\n\"greet_user\"\n,\narguments\n=\n{\n\"name\"\n:\n\"Alice\"\n,\n\"style\"\n:\n\"friendly\"\n})\nprint\n(\nf\"Prompt result:\n{\nprompt\n.\nmessages\n[\n0\n].\ncontent\n}\n\"\n)\n# List available resources\nresources\n=\nawait\nsession\n.\nlist_resources\n()\nprint\n(\nf\"Available resources:\n{\n[\nr\n.\nuri\nfor\nr\nin\nresources\n.\nresources\n]\n}\n\"\n)\n# List available tools\ntools\n=\nawait\nsession\n.\nlist_tools\n()\nprint\n(\nf\"Available tools:\n{\n[\nt\n.\nname\nfor\nt\nin\ntools\n.\ntools\n]\n}\n\"\n)\n# Read a resource (greeting resource from fastmcp_quickstart)\nresource_content\n=\nawait\nsession\n.\nread_resource\n(\nAnyUrl\n(\n\"greeting://World\"\n))\ncontent_block\n=\nresource_content\n.\ncontents\n[\n0\n]\nif\nisinstance\n(\ncontent_block\n,\ntypes\n.\nTextContent\n):\nprint\n(\nf\"Resource content:\n{\ncontent_block\n.\ntext\n}\n\"\n)\n# Call a tool (add tool from fastmcp_quickstart)\nresult\n=\nawait\nsession\n.\ncall_tool\n(\n\"add\"\n,\narguments\n=\n{\n\"a\"\n:\n5\n,\n\"b\"\n:\n3\n})\nresult_unstructured\n=\nresult\n.\ncontent\n[\n0\n]\nif\nisinstance\n(\nresult_unstructured\n,\ntypes\n.\nTextContent\n):\nprint\n(\nf\"Tool result:\n{\nresult_unstructured\n.\ntext\n}\n\"\n)\nresult_structured\n=\nresult\n.\nstructuredContent\nprint\n(\nf\"Structured tool result:\n{\nresult_structured\n}\n\"\n)\ndef\nmain\n():\n\"\"\"Entry point for the client script.\"\"\"\nasyncio\n.\nrun\n(\nrun\n())\nif\n__name__\n==\n\"__main__\"\n:\nmain\n()\nFull example:\nexamples/snippets/clients/stdio_client.py\nClients can also connect using\nStreamable HTTP transport\n:\n\"\"\"\nRun from the repository root:\nuv run examples/snippets/clients/streamable_basic.py\n\"\"\"\nimport\nasyncio\nfrom\nmcp\nimport\nClientSession\nfrom\nmcp\n.\nclient\n.\nstreamable_http\nimport\nstreamablehttp_client\nasync\ndef\nmain\n():\n# Connect to a streamable HTTP server\nasync\nwith\nstreamablehttp_client\n(\n\"http://localhost:8000/mcp\"\n)\nas\n(\nread_stream\n,\nwrite_stream\n,\n_\n,\n    ):\n# Create a session using the client streams\nasync\nwith\nClientSession\n(\nread_stream\n,\nwrite_stream\n)\nas\nsession\n:\n# Initialize the connection\nawait\nsession\n.\ninitialize\n()\n# List available tools\ntools\n=\nawait\nsession\n.\nlist_tools\n()\nprint\n(\nf\"Available tools:\n{\n[\ntool\n.\nname\nfor\ntool\nin\ntools\n.\ntools\n]\n}\n\"\n)\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nFull example:\nexamples/snippets/clients/streamable_basic.py\nClient Display Utilities\nWhen building MCP clients, the SDK provides utilities to help display human-readable names for tools, resources, and prompts:\n\"\"\"\ncd to the `examples/snippets` directory and run:\nuv run display-utilities-client\n\"\"\"\nimport\nasyncio\nimport\nos\nfrom\nmcp\nimport\nClientSession\n,\nStdioServerParameters\nfrom\nmcp\n.\nclient\n.\nstdio\nimport\nstdio_client\nfrom\nmcp\n.\nshared\n.\nmetadata_utils\nimport\nget_display_name\n# Create server parameters for stdio connection\nserver_params\n=\nStdioServerParameters\n(\ncommand\n=\n\"uv\"\n,\n# Using uv to run the server\nargs\n=\n[\n\"run\"\n,\n\"server\"\n,\n\"fastmcp_quickstart\"\n,\n\"stdio\"\n],\nenv\n=\n{\n\"UV_INDEX\"\n:\nos\n.\nenviron\n.\nget\n(\n\"UV_INDEX\"\n,\n\"\"\n)},\n)\nasync\ndef\ndisplay_tools\n(\nsession\n:\nClientSession\n):\n\"\"\"Display available tools with human-readable names\"\"\"\ntools_response\n=\nawait\nsession\n.\nlist_tools\n()\nfor\ntool\nin\ntools_response\n.\ntools\n:\n# get_display_name() returns the title if available, otherwise the name\ndisplay_name\n=\nget_display_name\n(\ntool\n)\nprint\n(\nf\"Tool:\n{\ndisplay_name\n}\n\"\n)\nif\ntool\n.\ndescription\n:\nprint\n(\nf\"\n{\ntool\n.\ndescription\n}\n\"\n)\nasync\ndef\ndisplay_resources\n(\nsession\n:\nClientSession\n):\n\"\"\"Display available resources with human-readable names\"\"\"\nresources_response\n=\nawait\nsession\n.\nlist_resources\n()\nfor\nresource\nin\nresources_response\n.\nresources\n:\ndisplay_name\n=\nget_display_name\n(\nresource\n)\nprint\n(\nf\"Resource:\n{\ndisplay_name\n}\n(\n{\nresource\n.\nuri\n}\n)\"\n)\ntemplates_response\n=\nawait\nsession\n.\nlist_resource_templates\n()\nfor\ntemplate\nin\ntemplates_response\n.\nresourceTemplates\n:\ndisplay_name\n=\nget_display_name\n(\ntemplate\n)\nprint\n(\nf\"Resource Template:\n{\ndisplay_name\n}\n\"\n)\nasync\ndef\nrun\n():\n\"\"\"Run the display utilities example.\"\"\"\nasync\nwith\nstdio_client\n(\nserver_params\n)\nas\n(\nread\n,\nwrite\n):\nasync\nwith\nClientSession\n(\nread\n,\nwrite\n)\nas\nsession\n:\n# Initialize the connection\nawait\nsession\n.\ninitialize\n()\nprint\n(\n\"=== Available Tools ===\"\n)\nawait\ndisplay_tools\n(\nsession\n)\nprint\n(\n\"\n\\n\n=== Available Resources ===\"\n)\nawait\ndisplay_resources\n(\nsession\n)\ndef\nmain\n():\n\"\"\"Entry point for the display utilities client.\"\"\"\nasyncio\n.\nrun\n(\nrun\n())\nif\n__name__\n==\n\"__main__\"\n:\nmain\n()\nFull example:\nexamples/snippets/clients/display_utilities.py\nThe\nget_display_name()\nfunction implements the proper precedence rules for displaying names:\nFor tools:\ntitle\n>\nannotations.title\n>\nname\nFor other objects:\ntitle\n>\nname\nThis ensures your client UI shows the most user-friendly names that servers provide.\nOAuth Authentication for Clients\nThe SDK includes\nauthorization support\nfor connecting to protected MCP servers:\n\"\"\"\nBefore running, specify running MCP RS server URL.\nTo spin up RS server locally, see\nexamples/servers/simple-auth/README.md\ncd to the `examples/snippets` directory and run:\nuv run oauth-client\n\"\"\"\nimport\nasyncio\nfrom\nurllib\n.\nparse\nimport\nparse_qs\n,\nurlparse\nfrom\npydantic\nimport\nAnyUrl\nfrom\nmcp\nimport\nClientSession\nfrom\nmcp\n.\nclient\n.\nauth\nimport\nOAuthClientProvider\n,\nTokenStorage\nfrom\nmcp\n.\nclient\n.\nstreamable_http\nimport\nstreamablehttp_client\nfrom\nmcp\n.\nshared\n.\nauth\nimport\nOAuthClientInformationFull\n,\nOAuthClientMetadata\n,\nOAuthToken\nclass\nInMemoryTokenStorage\n(\nTokenStorage\n):\n\"\"\"Demo In-memory token storage implementation.\"\"\"\ndef\n__init__\n(\nself\n):\nself\n.\ntokens\n:\nOAuthToken\n|\nNone\n=\nNone\nself\n.\nclient_info\n:\nOAuthClientInformationFull\n|\nNone\n=\nNone\nasync\ndef\nget_tokens\n(\nself\n)\n->\nOAuthToken\n|\nNone\n:\n\"\"\"Get stored tokens.\"\"\"\nreturn\nself\n.\ntokens\nasync\ndef\nset_tokens\n(\nself\n,\ntokens\n:\nOAuthToken\n)\n->\nNone\n:\n\"\"\"Store tokens.\"\"\"\nself\n.\ntokens\n=\ntokens\nasync\ndef\nget_client_info\n(\nself\n)\n->\nOAuthClientInformationFull\n|\nNone\n:\n\"\"\"Get stored client information.\"\"\"\nreturn\nself\n.\nclient_info\nasync\ndef\nset_client_info\n(\nself\n,\nclient_info\n:\nOAuthClientInformationFull\n)\n->\nNone\n:\n\"\"\"Store client information.\"\"\"\nself\n.\nclient_info\n=\nclient_info\nasync\ndef\nhandle_redirect\n(\nauth_url\n:\nstr\n)\n->\nNone\n:\nprint\n(\nf\"Visit:\n{\nauth_url\n}\n\"\n)\nasync\ndef\nhandle_callback\n()\n->\ntuple\n[\nstr\n,\nstr\n|\nNone\n]:\ncallback_url\n=\ninput\n(\n\"Paste callback URL: \"\n)\nparams\n=\nparse_qs\n(\nurlparse\n(\ncallback_url\n).\nquery\n)\nreturn\nparams\n[\n\"code\"\n][\n0\n],\nparams\n.\nget\n(\n\"state\"\n, [\nNone\n])[\n0\n]\nasync\ndef\nmain\n():\n\"\"\"Run the OAuth client example.\"\"\"\noauth_auth\n=\nOAuthClientProvider\n(\nserver_url\n=\n\"http://localhost:8001\"\n,\nclient_metadata\n=\nOAuthClientMetadata\n(\nclient_name\n=\n\"Example MCP Client\"\n,\nredirect_uris\n=\n[\nAnyUrl\n(\n\"http://localhost:3000/callback\"\n)],\ngrant_types\n=\n[\n\"authorization_code\"\n,\n\"refresh_token\"\n],\nresponse_types\n=\n[\n\"code\"\n],\nscope\n=\n\"user\"\n,\n        ),\nstorage\n=\nInMemoryTokenStorage\n(),\nredirect_handler\n=\nhandle_redirect\n,\ncallback_handler\n=\nhandle_callback\n,\n    )\nasync\nwith\nstreamablehttp_client\n(\n\"http://localhost:8001/mcp\"\n,\nauth\n=\noauth_auth\n)\nas\n(\nread\n,\nwrite\n,\n_\n):\nasync\nwith\nClientSession\n(\nread\n,\nwrite\n)\nas\nsession\n:\nawait\nsession\n.\ninitialize\n()\ntools\n=\nawait\nsession\n.\nlist_tools\n()\nprint\n(\nf\"Available tools:\n{\n[\ntool\n.\nname\nfor\ntool\nin\ntools\n.\ntools\n]\n}\n\"\n)\nresources\n=\nawait\nsession\n.\nlist_resources\n()\nprint\n(\nf\"Available resources:\n{\n[\nr\n.\nuri\nfor\nr\nin\nresources\n.\nresources\n]\n}\n\"\n)\ndef\nrun\n():\nasyncio\n.\nrun\n(\nmain\n())\nif\n__name__\n==\n\"__main__\"\n:\nrun\n()\nFull example:\nexamples/snippets/clients/oauth_client.py\nFor a complete working example, see\nexamples/clients/simple-auth-client/\n.\nParsing Tool Results\nWhen calling tools through MCP, the\nCallToolResult\nobject contains the tool's response in a structured format. Understanding how to parse this result is essential for properly handling tool outputs.\n\"\"\"examples/snippets/clients/parsing_tool_results.py\"\"\"\nimport\nasyncio\nfrom\nmcp\nimport\nClientSession\n,\nStdioServerParameters\n,\ntypes\nfrom\nmcp\n.\nclient\n.\nstdio\nimport\nstdio_client\nasync\ndef\nparse_tool_results\n():\n\"\"\"Demonstrates how to parse different types of content in CallToolResult.\"\"\"\nserver_params\n=\nStdioServerParameters\n(\ncommand\n=\n\"python\"\n,\nargs\n=\n[\n\"path/to/mcp_server.py\"\n]\n    )\nasync\nwith\nstdio_client\n(\nserver_params\n)\nas\n(\nread\n,\nwrite\n):\nasync\nwith\nClientSession\n(\nread\n,\nwrite\n)\nas\nsession\n:\nawait\nsession\n.\ninitialize\n()\n# Example 1: Parsing text content\nresult\n=\nawait\nsession\n.\ncall_tool\n(\n\"get_data\"\n, {\n\"format\"\n:\n\"text\"\n})\nfor\ncontent\nin\nresult\n.\ncontent\n:\nif\nisinstance\n(\ncontent\n,\ntypes\n.\nTextContent\n):\nprint\n(\nf\"Text:\n{\ncontent\n.\ntext\n}\n\"\n)\n# Example 2: Parsing structured content from JSON tools\nresult\n=\nawait\nsession\n.\ncall_tool\n(\n\"get_user\"\n, {\n\"id\"\n:\n\"123\"\n})\nif\nhasattr\n(\nresult\n,\n\"structuredContent\"\n)\nand\nresult\n.\nstructuredContent\n:\n# Access structured data directly\nuser_data\n=\nresult\n.\nstructuredContent\nprint\n(\nf\"User:\n{\nuser_data\n.\nget\n(\n'name'\n)\n}\n, Age:\n{\nuser_data\n.\nget\n(\n'age'\n)\n}\n\"\n)\n# Example 3: Parsing embedded resources\nresult\n=\nawait\nsession\n.\ncall_tool\n(\n\"read_config\"\n, {})\nfor\ncontent\nin\nresult\n.\ncontent\n:\nif\nisinstance\n(\ncontent\n,\ntypes\n.\nEmbeddedResource\n):\nresource\n=\ncontent\n.\nresource\nif\nisinstance\n(\nresource\n,\ntypes\n.\nTextResourceContents\n):\nprint\n(\nf\"Config from\n{\nresource\n.\nuri\n}\n:\n{\nresource\n.\ntext\n}\n\"\n)\nelif\nisinstance\n(\nresource\n,\ntypes\n.\nBlobResourceContents\n):\nprint\n(\nf\"Binary data from\n{\nresource\n.\nuri\n}\n\"\n)\n# Example 4: Parsing image content\nresult\n=\nawait\nsession\n.\ncall_tool\n(\n\"generate_chart\"\n, {\n\"data\"\n: [\n1\n,\n2\n,\n3\n]})\nfor\ncontent\nin\nresult\n.\ncontent\n:\nif\nisinstance\n(\ncontent\n,\ntypes\n.\nImageContent\n):\nprint\n(\nf\"Image (\n{\ncontent\n.\nmimeType\n}\n):\n{\nlen\n(\ncontent\n.\ndata\n)\n}\nbytes\"\n)\n# Example 5: Handling errors\nresult\n=\nawait\nsession\n.\ncall_tool\n(\n\"failing_tool\"\n, {})\nif\nresult\n.\nisError\n:\nprint\n(\n\"Tool execution failed!\"\n)\nfor\ncontent\nin\nresult\n.\ncontent\n:\nif\nisinstance\n(\ncontent\n,\ntypes\n.\nTextContent\n):\nprint\n(\nf\"Error:\n{\ncontent\n.\ntext\n}\n\"\n)\nasync\ndef\nmain\n():\nawait\nparse_tool_results\n()\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nMCP Primitives\nThe MCP protocol defines three core primitives that servers can implement:\nPrimitive\nControl\nDescription\nExample Use\nPrompts\nUser-controlled\nInteractive templates invoked by user choice\nSlash commands, menu options\nResources\nApplication-controlled\nContextual data managed by the client application\nFile contents, API responses\nTools\nModel-controlled\nFunctions exposed to the LLM to take actions\nAPI calls, data updates\nServer Capabilities\nMCP servers declare capabilities during initialization:\nCapability\nFeature Flag\nDescription\nprompts\nlistChanged\nPrompt template management\nresources\nsubscribe\nlistChanged\nResource exposure and updates\ntools\nlistChanged\nTool discovery and execution\nlogging\n-\nServer logging configuration\ncompletions\n-\nArgument completion suggestions\nDocumentation\nAPI Reference\nModel Context Protocol documentation\nModel Context Protocol specification\nOfficially supported servers\nContributing\nWe are passionate about supporting contributors of all levels of experience and would love to see you get involved in the project. See the\ncontributing guide\nto get started.\nLicense\nThis project is licensed under the MIT License - see the LICENSE file for details.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 32",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 19,129"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/modelcontextprotocol/python-sdk"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    },
    {
      "url": "https://github.com/openai/openai-agents-js",
      "title": "openai/openai-agents-js",
      "date": null,
      "executive_summary": [
        "A lightweight, powerful framework for multi-agent workflows and voice agents",
        "---",
        "OpenAI Agents SDK (JavaScript/TypeScript)\nThe OpenAI Agents SDK is a lightweight yet powerful framework for building multi-agent workflows in JavaScript/TypeScript. It is provider-agnostic, supporting OpenAI APIs and more.\nNote\nLooking for the Python version? Check out\nAgents SDK Python\n.\nCore concepts\nAgents\n: LLMs configured with instructions, tools, guardrails, and handoffs.\nHandoffs\n: Specialized tool calls for transferring control between agents.\nGuardrails\n: Configurable safety checks for input and output validation.\nTracing\n: Built-in tracking of agent runs, allowing you to view, debug, and optimize your workflows.\nExplore the\nexamples/\ndirectory to see the SDK in action.\nSupported Features\nMulti-Agent Workflows\n: Compose and orchestrate multiple agents in a single workflow.\nTool Integration\n: Seamlessly call tools/functions from within agent responses.\nHandoffs\n: Transfer control between agents dynamically during a run.\nStructured Outputs\n: Support for both plain text and schema-validated structured outputs.\nStreaming Responses\n: Stream agent outputs and events in real time.\nTracing & Debugging\n: Built-in tracing for visualizing and debugging agent runs.\nGuardrails\n: Input and output validation for safety and reliability.\nParallelization\n: Run agents or tool calls in parallel and aggregate results.\nHuman-in-the-Loop\n: Integrate human approval or intervention into workflows.\nRealtime Voice Agents\n: Build realtime voice agents using WebRTC or WebSockets\nLocal MCP Server Support\n: Give an Agent access to a locally running MCP server to provide tools\nSeparate optimized browser package\n: Dedicated package meant to run in the browser for Realtime agents.\nBroader model support\n: Use non-OpenAI models through the Vercel AI SDK adapter\nLong running functions\n: Suspend an agent loop to execute a long-running function and revive it later\nVoice pipeline\n: Chain text-based agents using speech-to-text and text-to-speech into a voice agent\nGet started\nSupported environments\nNode.js 22 or later\nDeno\nBun\nExperimental support:\nCloudflare Workers with\nnodejs_compat\nenabled\nCheck out the documentation\nfor more detailed information.\nInstallation\nnpm install @openai/agents zod@3\nHello world example\nimport\n{\nAgent\n,\nrun\n}\nfrom\n'@openai/agents'\n;\nconst\nagent\n=\nnew\nAgent\n(\n{\nname\n:\n'Assistant'\n,\ninstructions\n:\n'You are a helpful assistant'\n,\n}\n)\n;\nconst\nresult\n=\nawait\nrun\n(\nagent\n,\n'Write a haiku about recursion in programming.'\n,\n)\n;\nconsole\n.\nlog\n(\nresult\n.\nfinalOutput\n)\n;\n// Code within the code,\n// Functions calling themselves,\n// Infinite loop's dance.\n(\nIf running this, ensure you set the\nOPENAI_API_KEY\nenvironment variable\n)\nFunctions example\nimport\n{\nz\n}\nfrom\n'zod'\n;\nimport\n{\nAgent\n,\nrun\n,\ntool\n}\nfrom\n'@openai/agents'\n;\nconst\ngetWeatherTool\n=\ntool\n(\n{\nname\n:\n'get_weather'\n,\ndescription\n:\n'Get the weather for a given city'\n,\nparameters\n:\nz\n.\nobject\n(\n{\ncity\n:\nz\n.\nstring\n(\n)\n}\n)\n,\nexecute\n:\nasync\n(\ninput\n)\n=>\n{\nreturn\n`The weather in\n${\ninput\n.\ncity\n}\nis sunny`\n;\n}\n,\n}\n)\n;\nconst\nagent\n=\nnew\nAgent\n(\n{\nname\n:\n'Data agent'\n,\ninstructions\n:\n'You are a data agent'\n,\ntools\n:\n[\ngetWeatherTool\n]\n,\n}\n)\n;\nasync\nfunction\nmain\n(\n)\n{\nconst\nresult\n=\nawait\nrun\n(\nagent\n,\n'What is the weather in Tokyo?'\n)\n;\nconsole\n.\nlog\n(\nresult\n.\nfinalOutput\n)\n;\n}\nmain\n(\n)\n.\ncatch\n(\nconsole\n.\nerror\n)\n;\nHandoffs example\nimport\n{\nz\n}\nfrom\n'zod'\n;\nimport\n{\nAgent\n,\nrun\n,\ntool\n}\nfrom\n'@openai/agents'\n;\nconst\ngetWeatherTool\n=\ntool\n(\n{\nname\n:\n'get_weather'\n,\ndescription\n:\n'Get the weather for a given city'\n,\nparameters\n:\nz\n.\nobject\n(\n{\ncity\n:\nz\n.\nstring\n(\n)\n}\n)\n,\nexecute\n:\nasync\n(\ninput\n)\n=>\n{\nreturn\n`The weather in\n${\ninput\n.\ncity\n}\nis sunny`\n;\n}\n,\n}\n)\n;\nconst\ndataAgent\n=\nnew\nAgent\n(\n{\nname\n:\n'Data agent'\n,\ninstructions\n:\n'You are a data agent'\n,\nhandoffDescription\n:\n'You know everything about the weather'\n,\ntools\n:\n[\ngetWeatherTool\n]\n,\n}\n)\n;\n// Use Agent.create method to ensure the finalOutput type considers handoffs\nconst\nagent\n=\nAgent\n.\ncreate\n(\n{\nname\n:\n'Basic test agent'\n,\ninstructions\n:\n'You are a basic agent'\n,\nhandoffs\n:\n[\ndataAgent\n]\n,\n}\n)\n;\nasync\nfunction\nmain\n(\n)\n{\nconst\nresult\n=\nawait\nrun\n(\nagent\n,\n'What is the weather in San Francisco?'\n)\n;\nconsole\n.\nlog\n(\nresult\n.\nfinalOutput\n)\n;\n}\nmain\n(\n)\n.\ncatch\n(\nconsole\n.\nerror\n)\n;\nVoice Agent\nimport\n{\nz\n}\nfrom\n'zod'\n;\nimport\n{\nRealtimeAgent\n,\nRealtimeSession\n,\ntool\n}\nfrom\n'@openai/agents-realtime'\n;\nconst\ngetWeatherTool\n=\ntool\n(\n{\nname\n:\n'get_weather'\n,\ndescription\n:\n'Get the weather for a given city'\n,\nparameters\n:\nz\n.\nobject\n(\n{\ncity\n:\nz\n.\nstring\n(\n)\n}\n)\n,\nexecute\n:\nasync\n(\ninput\n)\n=>\n{\nreturn\n`The weather in\n${\ninput\n.\ncity\n}\nis sunny`\n;\n}\n,\n}\n)\n;\nconst\nagent\n=\nnew\nRealtimeAgent\n(\n{\nname\n:\n'Data agent'\n,\ninstructions\n:\n'You are a data agent. When you are asked to check weather, you must use the available tools.'\n,\ntools\n:\n[\ngetWeatherTool\n]\n,\n}\n)\n;\n// Intended to run in the browser\nconst\n{\napiKey\n}\n=\nawait\nfetch\n(\n'/path/to/ephemeral/key/generation'\n)\n.\nthen\n(\n(\nresp\n)\n=>\nresp\n.\njson\n(\n)\n,\n)\n;\n// Automatically configures audio input/output ‚Äî start talking\nconst\nsession\n=\nnew\nRealtimeSession\n(\nagent\n)\n;\nawait\nsession\n.\nconnect\n(\n{\napiKey\n}\n)\n;\nRunning Complete Examples\nThe\nexamples/\ndirectory contains a series of examples to get started:\npnpm examples:basic\n- Basic example with handoffs and tool calling\npnpm examples:agents-as-tools\n- Using agents as tools for translation\npnpm examples:tools-web-search\n- Using the web search tool\npnpm examples:tools-file-search\n- Using the file search tool\npnpm examples:deterministic\n- Deterministic multi-agent workflow\npnpm examples:parallelization\n- Running agents in parallel and picking the best result\npnpm examples:human-in-the-loop\n- Human approval for certain tool calls\npnpm examples:streamed\n- Streaming agent output and events in real time\npnpm examples:streamed:human-in-the-loop\n- Streaming output with human-in-the-loop approval\npnpm examples:routing\n- Routing between agents based on language or context\npnpm examples:realtime-demo\n- Framework agnostic Voice Agent example\npnpm examples:realtime-next\n- Next.js Voice Agent example application\nThe agent loop\nWhen you call\nRunner.run()\n, the SDK executes a loop until a final output is produced.\nThe agent is invoked with the given input, using the model and settings configured on the agent (or globally).\nThe LLM returns a response, which may include tool calls or handoff requests.\nIf the response contains a final output (see below), the loop ends and the result is returned.\nIf the response contains a handoff, the agent is switched to the new agent and the loop continues.\nIf there are tool calls, the tools are executed, their results are appended to the message history, and the loop continues.\nYou can control the maximum number of iterations with the\nmaxTurns\nparameter.\nFinal output\nThe final output is the last thing the agent produces in the loop.\nIf the agent has an\noutputType\n(structured output), the loop ends when the LLM returns a response matching that type.\nIf there is no\noutputType\n(plain text), the first LLM response without tool calls or handoffs is considered the final output.\nSummary of the agent loop:\nIf the current agent has an\noutputType\n, the loop runs until structured output of that type is produced.\nIf not, the loop runs until a message is produced with no tool calls or handoffs.\nError handling\nIf the maximum number of turns is exceeded, a\nMaxTurnsExceededError\nis thrown.\nIf a guardrail is triggered, a\nGuardrailTripwireTriggered\nexception is raised.\nDocumentation\nTo view the documentation locally:\npnpm docs:dev\nThen visit\nhttp://localhost:4321\nin your browser.\nDevelopment\nIf you want to contribute or edit the SDK/examples:\nInstall dependencies\npnpm install\nBuild the project\npnpm build\n&&\npnpm -r build-check\nRun tests and linter\npnpm\ntest\n&&\npnpm lint\nSee\nAGENTS.md\nand\nCONTRIBUTING.md\nfor the full contributor guide.\nAcknowledgements\nWe'd like to acknowledge the excellent work of the open-source community, especially:\nzod\n(schema validation)\nStarlight\nvite\nand\nvitest\npnpm\nNext.js\nWe're committed to building the Agents SDK as an open source framework so others in the community can expand on our approach.\nFor more details, see the\ndocumentation\nor explore the\nexamples/\ndirectory.",
        "‰ªäÊó•„ÅÆÁç≤Âæó„Çπ„Çø„ÉºÊï∞: 31",
        "Á¥ØÁ©ç„Çπ„Çø„ÉºÊï∞: 1,610"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/openai/openai-agents-js"
      ],
      "retrieved_at": "2025-10-09T02:06:04Z"
    }
  ]
}