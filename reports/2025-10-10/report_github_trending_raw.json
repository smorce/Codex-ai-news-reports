{
  "generated_at": "2025-10-10T02:09:56Z",
  "site": "github-trending",
  "num_articles": 39,
  "articles": [
    {
      "url": "https://github.com/Stremio/stremio-web",
      "title": "Stremio/stremio-web",
      "date": null,
      "executive_summary": [
        "Stremio - Freedom to Stream",
        "---",
        "Stremio - Freedom to Stream\nStremio is a modern media center that's a one-stop solution for your video entertainment. You discover, watch and organize video content from easy to install addons.\nBuild\nPrerequisites\nNode.js 12 or higher\npnpm\n10 or higher\nInstall dependencies\npnpm install\nStart development server\npnpm start\nProduction build\npnpm run build\nRun with Docker\ndocker build -t stremio-web\n.\ndocker run -p 8080:8080 stremio-web\nScreenshots\nBoard\nDiscover\nMeta Details\nLicense\nStremio is copyright 2017-2023 Smart code and available under GPLv2 license. See the\nLICENSE\nfile in the project for more information.",
        "今日の獲得スター数: 1,576",
        "累積スター数: 6,765"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/Stremio/stremio-web"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/TibixDev/winboat",
      "title": "TibixDev/winboat",
      "date": null,
      "executive_summary": [
        "Run Windows apps on 🐧 Linux with ✨ seamless integration",
        "---",
        "WinBoat\nWindows for Penguins.\nRun Windows apps on 🐧 Linux with ✨ seamless integration\nScreenshots\n⚠️\nWork in Progress\n⚠️\nWinBoat is currently in beta, so expect to occasionally run into hiccups and bugs. You should be comfortable with some level of troubleshooting if you decide to try it, however we encourage you to give it a shot anyway.\nFeatures\n🎨 Elegant Interface\n: Sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience\n📦 Automated Installs\n: Simple installation process through our interface - pick your preferences & specs and let us handle the rest\n🚀 Run Any App\n: If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications as native OS-level windows in your Linux environment\n🖥️ Full Windows Desktop\n: Access the complete Windows desktop experience when you need it, or run individual apps seamlessly integrated into your Linux workflow\n📁 Filesystem Integration\n: Your home directory is mounted in Windows, allowing easy file sharing between the two systems without any hassle\n✨ And many more\n: Smartcard passthrough, resource monitoring, and more features being added regularly\nHow Does It Work?\nWinBoat is an Electron app which allows you to run Windows apps on Linux using a containerized approach. Windows runs as a VM inside a Docker container, we communicate with it using the\nWinBoat Guest Server\nto retrieve data we need from Windows. For compositing applications as native OS-level windows, we use FreeRDP together with Windows's RemoteApp protocol.\nPrerequisites\nBefore running WinBoat, ensure your system meets the following requirements:\nRAM\n: At least 4 GB of RAM\nCPU\n: At least 2 CPU threads\nStorage\n: At least 32 GB free space in\n/var\nVirtualization\n: KVM enabled in BIOS/UEFI\nHow to enable virtualization\nDocker\n: Required for containerization\nInstallation Guide\n⚠️\nNOTE:\nDocker Desktop is\nnot\nsupported, you will run into issues if you use it\nDocker Compose v2\n: Required for compatibility with docker-compose.yml files\nInstallation Guide\nDocker User Group\n: Add your user to the\ndocker\ngroup\nSetup Instructions\nFreeRDP\n: Required for remote desktop connection (Please make sure you have\nVersion 3.x.x\nwith sound support included)\nInstallation Guide\n[OPTIONAL]\nKernel Modules\n: The\niptables\n/\nnftables\nand\niptable_nat\nkernel modules can be loaded for network autodiscovery and better shared filesystem performance, but this is not obligatory in newer versions of WinBoat\nModule loading instructions\nDownloading\nYou can download the latest Linux builds under the\nReleases\ntab. We currently offer four variants:\nAppImage:\nA popular & portable app format which should run fine on most distributions\nUnpacked:\nThe raw unpacked files, simply run the executable (\nlinux-unpacked/winboat\n)\n.deb:\nThe intended format for Debian based distributions\n.rpm:\nThe intended format for Fedora based distributions\nKnown Issues About Container Runtimes\nPodman is\nunsupported\nfor now\nDocker Desktop is\nunsupported\nfor now\nDistros that emulate Docker through a Podman socket are\nunsupported\nAny rootless containerization solution is currently\nunsupported\nBuilding WinBoat\nFor building you need to have NodeJS and Go installed on your system\nClone the repo (\ngit clone https://github.com/TibixDev/WinBoat\n)\nInstall the dependencies (\nnpm i\n)\nBuild the app and the guest server using\nnpm run build:linux-gs\nYou can now find the built app under\ndist\nwith an AppImage and an Unpacked variant\nRunning WinBoat in development mode\nMake sure you meet the\nprerequisites\nAdditionally, for development you need to have NodeJS and Go installed on your system\nClone the repo (\ngit clone https://github.com/TibixDev/WinBoat\n)\nInstall the dependencies (\nnpm i\n)\nBuild the guest server (\nnpm run build-guest-server\n)\nRun the app (\nnpm run dev\n)\nContributing\nContributions are welcome! Whether it's bug fixes, feature improvements, or documentation updates, we appreciate your help making WinBoat better.\nPlease note\n: We maintain a focus on technical contributions only. Pull requests containing political/sexual content, or other sensitive/controversial topics will not be accepted. Let's keep things focused on making great software! 🚀\nFeel free to:\nReport bugs and issues\nSubmit feature requests\nContribute code improvements\nHelp with documentation\nShare feedback and suggestions\nCheck out our issues page to get started, or feel free to open a new issue if you've found something that needs attention.\nLicense\nWinBoat is licensed under the\nMIT\nlicense\nInspiration / Alternatives\nThese past few years some cool projects have surfaced with similar concepts, some of which we've also taken inspirations from.\nThey're awesome and you should check them out:\nWinApps\nCassowary\ndockur/windows\n(🌟 Also used in WinBoat)\nSocials & Contact\nStar History",
        "今日の獲得スター数: 859",
        "累積スター数: 8,356"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/TibixDev/winboat"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/TapXWorld/ChinaTextbook",
      "title": "TapXWorld/ChinaTextbook",
      "date": null,
      "executive_summary": [
        "所有小初高、大学PDF教材。",
        "---",
        "项目的由来\n虽然国内教育网站已提供免费资源，但大多数普通人获取信息的途径依然受限。有些人利用这一点，在某站上销售这些带有私人水印的资源。为了应对这种情况，我计划将这些资源集中并开源，以促进义务教育的普及和消除地区间的教育贫困。\n还有一个最重要的原因是，希望海外华人能够让自己的孩子继续了解国内教育。\n学习数学\n希望未来出现更多不是为了考学而读书的人。\n小学数学\n一年级上册\n一年级下册\n二年级上册\n二年级下册\n三年级上册\n三年级下册\n四年级上册\n四年级下册\n五年级上册\n五年级下册\n六年级上册\n六年级下册\n初中数学\n初一上册\n初一下册\n初二上册\n初二下册\n初三上册\n初三下册\n高中数学\n目录\n大学数学\n高等数学\n线性代数\n离散数学\n概率论\n更多数学资料-(大学数学网)\n问题：如何合并被拆分的文件？\n由于 GitHub 对单个文件的上传有最大限制，超过 100MB 的文件会被拒绝上传，超过 50MB 的文件上传时会收到警告。因此，文件大小超过 50MB 的文件会被拆分成每个 35MB 的多个文件。\n示例\n文件被拆分的示例：\n义务教育教科书 · 数学一年级上册.pdf.1\n义务教育教科书 · 数学一年级上册.pdf.2\n解决办法\n要合并这些被拆分的文件，您只需执行以下步骤(其他操作系统同理)：\n将合并程序\nmergePDFs-windows-amd64.exe\n下载到包含 PDF 文件的文件夹中。\n确保\nmergePDFs-windows-amd64.exe\n和被拆分的 PDF 文件在同一目录下。\n双击\nmergePDFs-windows-amd64.exe\n程序即可自动完成文件合并。\n下载方式\n您可以通过以下链接，下载文件合并程序：\n下载文件合并程序\n文件和程序示例\nmergePDFs-windows-amd64.exe\n义务教育教科书 · 数学一年级上册.pdf.1\n义务教育教科书 · 数学一年级上册.pdf.2\n重新下载\n如果您位于内地，并且网络不错，想重新下载，您可以使用\ntchMaterial-parser\n项目（鼓励开源），进行重新下载。\n如果您位于国外，和内地网络通信速度较慢，建议使用本存储库进行签出。\n教材捐献\n如果这个项目帮助您免费获取教育资源，请考虑支持我们推广开放教育的努力！您的捐献将帮助我们维护和扩展这个资源库。\n加入我们的 Telegram 社区，获取最新动态并分享您的想法：\nhttps://t.me/+1V6WjEq8WEM4MDM1\n支持我\n如果您觉得这个项目对您有帮助，您可以扫描以下二维码进行捐赠：",
        "今日の獲得スター数: 606",
        "累積スター数: 52,156"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/TapXWorld/ChinaTextbook"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/aandrew-me/ytDownloader",
      "title": "aandrew-me/ytDownloader",
      "date": null,
      "executive_summary": [
        "Desktop App for downloading Videos and Audios from hundreds of sites",
        "---",
        "ytDownloader\nA modern GUI video and audio downloader supporting\nhundreds of sites\nFeatures 🚀\n✅ Supports hundreds of sites including Youtube, Facebook, Instagram, Tiktok, Twitter and so on.\n✅ Multiple themes\n✅ Video Compressor with Hardware Acceleration\n✅ Advanced options like Range Selection, Subtitles\n✅ Download playlists\n✅ Available on Linux, Windows & macOS\n✅ Fast download speeds\n✅ And of-course no trackers or ads\nScreenshots\nInstallation\nWindows 🪟\nTraditional way\nDownload and install the exe or msi file. Exe file lets you choose custom download location, msi file doesn't ask for location. Windows defender may show a popup saying\nWindows Protected Your PC\n. Just click on\nMore info\nand click on\nRun Anyway\nChocolatey\nApp can be installed from\nChocolatey\nusing the following command\nchoco install ytdownloader\nScoop\nApp can be installed with\nScoop\nusing the following command\nscoop install https://raw.githubusercontent.com/aandrew-me/ytDownloader/main/ytdownloader.json\nWinget\nApp can be installed with\nWinget\nusing the following command\nwinget install aandrew-me.ytDownloader\nLinux 🐧\nLinux has several options available - Flatpak, AppImage and Snap.\nFlatpak is recommended. For arm processors, download from flathub.\nAppImage\nAppImage\nformat is supported on most Linux distros and has Auto-Update support.\nIt just needs to be executed after downloading. See more about\nAppImages here\n.\nAppImageLauncher\nis recommended for integrating AppImages.\nFlatpak\nflatpak install flathub io.github.aandrew_me.ytdn\nSnapcraft\nsudo snap install ytdownloader\nmacOS 🍎\nSince the app is not signed, when you will try to open the app, macOS will not allow you to open it.\nYou need to open terminal and execute:\nsudo xattr -r -d com.apple.quarantine /Applications/YTDownloader.app\nYou will also need to install\nyt-dlp\nwith\nhomebrew\nbrew install yt-dlp\nInternationalization (Localization) 🌍\nTranslations into other languages would be highly appreciated. If you want to help translating the app to other languages, you can join from\nhere\n. Open a new issue and that language will be added to Crowdin. Please don't make pull requests with json files, instead use Crowdin.\n✅ Available languages\nName\nStatus\nArabic\n✔️\nEnglish\n✔️\nSimplified Chinese\n✔️\nFinnish\n✔️\nFrench\n✔️\nGerman\n✔️\nGreek\n✔️\nHungarian\n✔️\nItalian\n✔️\nJapanese\n✔️\nPersian\n✔️\nPolish\n✔️\nPortuguese (Brazil)\n✔️\nRussian\n✔️\nSpanish\n✔️\nTurkish\n✔️\nUkrainian\n✔️\nVietnamese\n✔️\nThanks to\nnxjosephofficial\n,\nLINUX-SAUNA\n,\nProxycon\n,\nalbanobattistella\n,\nTheBlueQuasar\n,\nMrQuerter\n,\nKotoWhiskas\n,\nAndré\n,\nhaggen88\n,\nXfedeX\n,\nJok3r\n,\nTitouanReal\n,\nsoredake\n,\nyoi\n,\nHowlingWerewolf\n,\nKum\n,\nMohammed Bakry\n,\nHuang Bingfeng\nand others for helping.\nUsed technologies\nyt-dlp\nElectron\nffmpeg\nnodeJS\nflaticon\nFor building or running from source code\nNodejs\n(along with npm) needs to be installed.\nRequired commands to get started.\ngit clone https://github.com/aandrew-me/ytDownloader.git\ncd ytDownloader\nnpm i\nTo run with\nElectron\n:\nnpm start\nYou need to download ffmpeg and put it in the root directory of the project. If you don't need to build for arm processor, you can download ffmpeg by executing any of the files - linux.sh / mac.sh / windows.sh depending on the platform. Otherwise you need to download ffmpeg from\nhere\nfor windows/linux and from\nhere\nfor mac (not tested)\nTo build for Linux (It will create packages as specified in package.json). The builds are stored in\nrelease\nfolder.\nnpm run linux\nTo build for Windows\nnpm run windows\nTo build for macOS\nnpm run mac\nIf you only want to build for one format, you can do\nnpx electron-builder -l appimage\nIt will just create a linux appimage build.",
        "今日の獲得スター数: 541",
        "累積スター数: 4,281"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/aandrew-me/ytDownloader"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/browserbase/stagehand",
      "title": "browserbase/stagehand",
      "date": null,
      "executive_summary": [
        "The AI Browser Automation Framework",
        "---",
        "The AI Browser Automation Framework\nRead the Docs\nIf you're looking for the Python implementation, you can find it\nhere\nVibe code\nStagehand with\nDirector\nWhy Stagehand?\nMost existing browser automation tools either require you to write low-level code in a framework like Selenium, Playwright, or Puppeteer, or use high-level agents that can be unpredictable in production. By letting developers choose what to write in code vs. natural language, Stagehand is the natural choice for browser automations in production.\nChoose when to write code vs. natural language\n: use AI when you want to navigate unfamiliar pages, and use code (\nPlaywright\n) when you know exactly what you want to do.\nPreview and cache actions\n: Stagehand lets you preview AI actions before running them, and also helps you easily cache repeatable actions to save time and tokens.\nComputer use models with one line of code\n: Stagehand lets you integrate SOTA computer use models from OpenAI and Anthropic into the browser with one line of code.\nExample\nHere's how to build a sample browser automation with Stagehand:\n// Use Playwright functions on the page object\nconst\npage\n=\nstagehand\n.\npage\n;\nawait\npage\n.\ngoto\n(\n\"https://github.com/browserbase\"\n)\n;\n// Use act() to execute individual actions\nawait\npage\n.\nact\n(\n\"click on the stagehand repo\"\n)\n;\n// Use Computer Use agents for larger actions\nconst\nagent\n=\nstagehand\n.\nagent\n(\n{\nprovider\n:\n\"openai\"\n,\nmodel\n:\n\"computer-use-preview\"\n,\n}\n)\n;\nawait\nagent\n.\nexecute\n(\n\"Get to the latest PR\"\n)\n;\n// Use extract() to read data from the page\nconst\n{\nauthor\n,\ntitle\n}\n=\nawait\npage\n.\nextract\n(\n{\ninstruction\n:\n\"extract the author and title of the PR\"\n,\nschema\n:\nz\n.\nobject\n(\n{\nauthor\n:\nz\n.\nstring\n(\n)\n.\ndescribe\n(\n\"The username of the PR author\"\n)\n,\ntitle\n:\nz\n.\nstring\n(\n)\n.\ndescribe\n(\n\"The title of the PR\"\n)\n,\n}\n)\n,\n}\n)\n;\nDocumentation\nVisit\ndocs.stagehand.dev\nto view the full documentation.\nGetting Started\nStart with Stagehand with one line of code, or check out our\nQuickstart Guide\nfor more information:\nnpx create-browser-app\nWatch Anirudh demo create-browser-app to create a Stagehand project!\nBuild and Run from Source\ngit clone https://github.com/browserbase/stagehand.git\ncd\nstagehand\npnpm install\npnpm playwright install\npnpm run build\npnpm run example\n#\nrun the blank script at ./examples/example.ts\npnpm run example 2048\n#\nrun the 2048 example at ./examples/2048.ts\npnpm run evals -man\n#\nsee evaluation suite options\nStagehand is best when you have an API key for an LLM provider and Browserbase credentials. To add these to your project, run:\ncp .env.example .env\nnano .env\n#\nEdit the .env file to add API keys\nContributing\nNote\nWe highly value contributions to Stagehand! For questions or support, please join our\nSlack community\n.\nAt a high level, we're focused on improving reliability, speed, and cost in that order of priority. If you're interested in contributing, we strongly recommend reaching out to\nMiguel Gonzalez\nor\nPaul Klein\nin our\nSlack community\nbefore starting to ensure that your contribution aligns with our goals.\nFor more information, please see our\nContributing Guide\n.\nAcknowledgements\nThis project heavily relies on\nPlaywright\nas a resilient backbone to automate the web. It also would not be possible without the awesome techniques and discoveries made by\ntarsier\n,\ngemini-zod\n, and\nfuji-web\n.\nWe'd like to thank the following people for their major contributions to Stagehand:\nPaul Klein\nAnirudh Kamath\nSean McGuire\nMiguel Gonzalez\nSameel Arif\nFilip Michalsky\nJeremy Press\nNavid Pour\nLicense\nLicensed under the MIT License.\nCopyright 2025 Browserbase, Inc.",
        "今日の獲得スター数: 366",
        "累積スター数: 17,891"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/browserbase/stagehand"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/BeehiveInnovations/zen-mcp-server",
      "title": "BeehiveInnovations/zen-mcp-server",
      "date": null,
      "executive_summary": [
        "The power of Claude Code / GeminiCLI / CodexCLI + [Gemini / OpenAI / OpenRouter / Azure / Grok / Ollama / Custom Model / All Of The Above] working as one.",
        "---",
        "Zen MCP: Many Workflows. One Context.\nZen_CLink_web.mp4\n👉\nWatch more examples\nYour CLI + Multiple Models = Your AI Dev Team\nUse the 🤖 CLI you love:\nClaude Code\n·\nGemini CLI\n·\nCodex CLI\n·\nQwen Code CLI\n·\nCursor\n·\nand more\nWith multiple models within a single prompt:\nGemini · OpenAI · Anthropic · Grok · Azure · Ollama · OpenRouter · DIAL · On-Device Model\n🆕 Now with CLI-to-CLI Bridge\nThe new\nclink\n(CLI + Link) tool connects external AI CLIs directly into your workflow:\nConnect external CLIs\nlike\nGemini CLI\n,\nCodex CLI\n, and\nClaude Code\ndirectly into your workflow\nCLI Subagents\n- Launch isolated CLI instances from\nwithin\nyour current CLI! Claude Code can spawn Codex subagents, Codex can spawn Gemini CLI subagents, etc. Offload heavy tasks (code reviews, bug hunting) to fresh contexts while your main session's context window remains unpolluted. Each subagent returns only final results.\nContext Isolation\n- Run separate investigations without polluting your primary workspace\nRole Specialization\n- Spawn\nplanner\n,\ncodereviewer\n, or custom role agents with specialized system prompts\nFull CLI Capabilities\n- Web search, file inspection, MCP tool access, latest documentation lookups\nSeamless Continuity\n- Sub-CLIs participate as first-class members with full conversation context between tools\n#\nCodex spawns Codex subagent for isolated code review in fresh context\nclink with codex codereviewer to audit auth module\nfor\nsecurity issues\n#\nSubagent reviews in isolation, returns final report without cluttering your context as codex reads each file and walks the directory structure\n#\nConsensus from different AI models → Implementation handoff with full context preservation between tools\nUse consensus with gpt-5 and gemini-pro to decide: dark mode or offline support next\nContinue with clink gemini - implement the recommended feature\n#\nGemini receives full debate context and starts coding immediately\n👉\nLearn more about clink\nWhy Zen MCP?\nWhy rely on one AI model when you can orchestrate them all?\nA Model Context Protocol server that supercharges tools like\nClaude Code\n,\nCodex CLI\n, and IDE clients such\nas\nCursor\nor the\nClaude Dev VS Code extension\n.\nZen MCP connects your favorite AI tool\nto multiple AI models\nfor enhanced code analysis, problem-solving, and collaborative development.\nTrue AI Collaboration with Conversation Continuity\nZen supports\nconversation threading\nso your CLI can\ndiscuss ideas with multiple AI models, exchange reasoning, get second opinions, and even run collaborative debates between models\nto help you reach deeper insights and better solutions.\nYour CLI always stays in control but gets perspectives from the best AI for each subtask. Context carries forward seamlessly across tools and models, enabling complex workflows like: code reviews with multiple models → automated planning → implementation → pre-commit validation.\nYou're in control.\nYour CLI of choice orchestrates the AI team, but you decide the workflow. Craft powerful prompts that bring in Gemini Pro, GPT 5, Flash, or local offline models exactly when needed.\nReasons to Use Zen MCP\nA typical workflow with Claude Code as an example:\nMulti-Model Orchestration\n- Claude coordinates with Gemini Pro, O3, GPT-5, and 50+ other models to get the best analysis for each task\nContext Revival Magic\n- Even after Claude's context resets, continue conversations seamlessly by having other models \"remind\" Claude of the discussion\nGuided Workflows\n- Enforces systematic investigation phases that prevent rushed analysis and ensure thorough code examination\nExtended Context Windows\n- Break Claude's limits by delegating to Gemini (1M tokens) or O3 (200K tokens) for massive codebases\nTrue Conversation Continuity\n- Full context flows across tools and models - Gemini remembers what O3 said 10 steps ago\nModel-Specific Strengths\n- Extended thinking with Gemini Pro, blazing speed with Flash, strong reasoning with O3, privacy with local Ollama\nProfessional Code Reviews\n- Multi-pass analysis with severity levels, actionable feedback, and consensus from multiple AI experts\nSmart Debugging Assistant\n- Systematic root cause analysis with hypothesis tracking and confidence levels\nAutomatic Model Selection\n- Claude intelligently picks the right model for each subtask (or you can specify)\nVision Capabilities\n- Analyze screenshots, diagrams, and visual content with vision-enabled models\nLocal Model Support\n- Run Llama, Mistral, or other models locally for complete privacy and zero API costs\nBypass MCP Token Limits\n- Automatically works around MCP's 25K limit for large prompts and responses\nThe Killer Feature:\nWhen Claude's context resets, just ask to \"continue with O3\" - the other model's response magically revives Claude's understanding without re-ingesting documents!\nExample: Multi-Model Code Review Workflow\nPerform a codereview using gemini pro and o3 and use planner to generate a detailed plan, implement the fixes and do a final precommit check by continuing from the previous codereview\nThis triggers a\ncodereview\nworkflow where Claude walks the code, looking for all kinds of issues\nAfter multiple passes, collects relevant code and makes note of issues along the way\nMaintains a\nconfidence\nlevel between\nexploring\n,\nlow\n,\nmedium\n,\nhigh\nand\ncertain\nto track how confidently it's been able to find and identify issues\nGenerates a detailed list of critical -> low issues\nShares the relevant files, findings, etc with\nGemini Pro\nto perform a deep dive for a second\ncodereview\nComes back with a response and next does the same with o3, adding to the prompt if a new discovery comes to light\nWhen done, Claude takes in all the feedback and combines a single list of all critical -> low issues, including good patterns in your code. The final list includes new findings or revisions in case Claude misunderstood or missed something crucial and one of the other models pointed this out\nIt then uses the\nplanner\nworkflow to break the work down into simpler steps if a major refactor is required\nClaude then performs the actual work of fixing highlighted issues\nWhen done, Claude returns to Gemini Pro for a\nprecommit\nreview\nAll within a single conversation thread! Gemini Pro in step 11\nknows\nwhat was recommended by O3 in step 7! Taking that context\nand review into consideration to aid with its final pre-commit review.\nThink of it as Claude Code\nfor\nClaude Code.\nThis MCP isn't magic. It's just\nsuper-glue\n.\nRemember:\nClaude stays in full control — but\nYOU\ncall the shots.\nZen is designed to have Claude engage other models only when needed — and to follow through with meaningful back-and-forth.\nYou're\nthe one who crafts the powerful prompt that makes Claude bring in Gemini, Flash, O3 — or fly solo.\nYou're the guide. The prompter. The puppeteer.\nYou are the AI -\nActually Intelligent\n.\nRecommended AI Stack\nFor Claude Code Users\nFor best results when using\nClaude Code\n:\nSonnet 4.5\n- All agentic work and orchestration\nGemini 2.5 Pro\nOR\nGPT-5-Pro\n- Deep thinking, additional code reviews, debugging and validations, pre-commit analysis\nFor Codex Users\nFor best results when using\nCodex CLI\n:\nGPT-5 Codex Medium\n- All agentic work and orchestration\nGemini 2.5 Pro\nOR\nGPT-5-Pro\n- Deep thinking, additional code reviews, debugging and validations, pre-commit analysis\nQuick Start (5 minutes)\nPrerequisites:\nPython 3.10+, Git,\nuv installed\n1. Get API Keys\n(choose one or more):\nOpenRouter\n- Access multiple models with one API\nGemini\n- Google's latest models\nOpenAI\n- O3, GPT-5 series\nAzure OpenAI\n- Enterprise deployments of GPT-4o, GPT-4.1, GPT-5 family\nX.AI\n- Grok models\nDIAL\n- Vendor-agnostic model access\nOllama\n- Local models (free)\n2. Install\n(choose one):\nOption A: Clone and Automatic Setup\n(recommended)\ngit clone https://github.com/BeehiveInnovations/zen-mcp-server.git\ncd\nzen-mcp-server\n#\nHandles everything: setup, config, API keys from system environment.\n#\nAuto-configures Claude Desktop, Claude Code, Gemini CLI, Codex CLI, Qwen CLI\n#\nEnable / disable additional settings in .env\n./run-server.sh\nOption B: Instant Setup with\nuvx\n// Add to ~/.claude/settings.json or .mcp.json\n// Don't forget to add your API keys under env\n{\n\"mcpServers\"\n: {\n\"zen\"\n: {\n\"command\"\n:\n\"\nbash\n\"\n,\n\"args\"\n: [\n\"\n-c\n\"\n,\n\"\nfor p in $(which uvx 2>/dev/null) $HOME/.local/bin/uvx /opt/homebrew/bin/uvx /usr/local/bin/uvx uvx; do [ -x\n\\\"\n$p\n\\\"\n] && exec\n\\\"\n$p\n\\\"\n--from git+https://github.com/BeehiveInnovations/zen-mcp-server.git zen-mcp-server; done; echo 'uvx not found' >&2; exit 1\n\"\n],\n\"env\"\n: {\n\"PATH\"\n:\n\"\n/usr/local/bin:/usr/bin:/bin:/opt/homebrew/bin:~/.local/bin\n\"\n,\n\"GEMINI_API_KEY\"\n:\n\"\nyour-key-here\n\"\n,\n\"DISABLED_TOOLS\"\n:\n\"\nanalyze,refactor,testgen,secaudit,docgen,tracer\n\"\n,\n\"DEFAULT_MODEL\"\n:\n\"\nauto\n\"\n}\n    }\n  }\n}\n3. Start Using!\n\"Use zen to analyze this code for security issues with gemini pro\"\n\"Debug this error with o3 and then get flash to suggest optimizations\"\n\"Plan the migration strategy with zen, get consensus from multiple models\"\n\"clink with cli_name=\\\"gemini\\\" role=\\\"planner\\\" to draft a phased rollout plan\"\n👉\nComplete Setup Guide\nwith detailed installation, configuration for Gemini / Codex / Qwen, and troubleshooting\n👉\nCursor & VS Code Setup\nfor IDE integration instructions\n📺\nWatch tools in action\nto see real-world examples\nProvider Configuration\nZen activates any provider that has credentials in your\n.env\n. See\n.env.example\nfor deeper customization.\nCore Tools\nNote:\nEach tool comes with its own multi-step workflow, parameters, and descriptions that consume valuable context window space even when not in use. To optimize performance, some tools are disabled by default. See\nTool Configuration\nbelow to enable them.\nCollaboration & Planning\n(Enabled by default)\nclink\n- Bridge requests to external AI CLIs (Gemini planner, codereviewer, etc.)\nchat\n- Brainstorm ideas, get second opinions, validate approaches. With capable models (GPT-5 Pro, Gemini 2.5 Pro), generates complete code / implementation\nthinkdeep\n- Extended reasoning, edge case analysis, alternative perspectives\nplanner\n- Break down complex projects into structured, actionable plans\nconsensus\n- Get expert opinions from multiple AI models with stance steering\nCode Analysis & Quality\ndebug\n- Systematic investigation and root cause analysis\nprecommit\n- Validate changes before committing, prevent regressions\ncodereview\n- Professional reviews with severity levels and actionable feedback\nanalyze\n(disabled by default -\nenable\n)\n- Understand architecture, patterns, dependencies across entire codebases\nDevelopment Tools\n(Disabled by default -\nenable\n)\nrefactor\n- Intelligent code refactoring with decomposition focus\ntestgen\n- Comprehensive test generation with edge cases\nsecaudit\n- Security audits with OWASP Top 10 analysis\ndocgen\n- Generate documentation with complexity analysis\nUtilities\napilookup\n- Forces current-year API/SDK documentation lookups in a sub-process (saves tokens within the current context window), prevents outdated training data responses\nchallenge\n- Prevent \"You're absolutely right!\" responses with critical analysis\ntracer\n(disabled by default -\nenable\n)\n- Static analysis prompts for call-flow mapping\n👉 Tool Configuration\nDefault Configuration\nTo optimize context window usage, only essential tools are enabled by default:\nEnabled by default:\nchat\n,\nthinkdeep\n,\nplanner\n,\nconsensus\n- Core collaboration tools\ncodereview\n,\nprecommit\n,\ndebug\n- Essential code quality tools\napilookup\n- Rapid API/SDK information lookup\nchallenge\n- Critical thinking utility\nDisabled by default:\nanalyze\n,\nrefactor\n,\ntestgen\n,\nsecaudit\n,\ndocgen\n,\ntracer\nEnabling Additional Tools\nTo enable additional tools, remove them from the\nDISABLED_TOOLS\nlist:\nOption 1: Edit your .env file\n#\nDefault configuration (from .env.example)\nDISABLED_TOOLS=analyze,refactor,testgen,secaudit,docgen,tracer\n#\nTo enable specific tools, remove them from the list\n#\nExample: Enable analyze tool\nDISABLED_TOOLS=refactor,testgen,secaudit,docgen,tracer\n#\nTo enable ALL tools\nDISABLED_TOOLS=\nOption 2: Configure in MCP settings\n// In ~/.claude/settings.json or .mcp.json\n{\n\"mcpServers\"\n: {\n\"zen\"\n: {\n\"env\"\n: {\n// Tool configuration\n\"DISABLED_TOOLS\"\n:\n\"\nrefactor,testgen,secaudit,docgen,tracer\n\"\n,\n\"DEFAULT_MODEL\"\n:\n\"\npro\n\"\n,\n\"DEFAULT_THINKING_MODE_THINKDEEP\"\n:\n\"\nhigh\n\"\n,\n// API configuration\n\"GEMINI_API_KEY\"\n:\n\"\nyour-gemini-key\n\"\n,\n\"OPENAI_API_KEY\"\n:\n\"\nyour-openai-key\n\"\n,\n\"OPENROUTER_API_KEY\"\n:\n\"\nyour-openrouter-key\n\"\n,\n// Logging and performance\n\"LOG_LEVEL\"\n:\n\"\nINFO\n\"\n,\n\"CONVERSATION_TIMEOUT_HOURS\"\n:\n\"\n6\n\"\n,\n\"MAX_CONVERSATION_TURNS\"\n:\n\"\n50\n\"\n}\n    }\n  }\n}\nOption 3: Enable all tools\n// Remove or empty the DISABLED_TOOLS to enable everything\n{\n\"mcpServers\"\n: {\n\"zen\"\n: {\n\"env\"\n: {\n\"DISABLED_TOOLS\"\n:\n\"\n\"\n}\n    }\n  }\n}\nNote:\nEssential tools (\nversion\n,\nlistmodels\n) cannot be disabled\nAfter changing tool configuration, restart your Claude session for changes to take effect\nEach tool adds to context window usage, so only enable what you need\n📺 Watch Tools In Action\nChat Tool\n- Collaborative decision making and multi-turn conversations\nPicking Redis vs Memcached:\nChat.Redis.or.Memcached_web.webm\nMulti-turn conversation with continuation:\nChat.With.Gemini_web.webm\nConsensus Tool\n- Multi-model debate and decision making\nMulti-model consensus debate:\nZen.Debate_web.webm\nPreCommit Tool\n- Comprehensive change validation\nPre-commit validation workflow:\nAPI Lookup Tool\n- Current vs outdated API documentation\nWithout Zen - outdated APIs:\nAPI_without_zen_web.mp4\nWith Zen - current APIs:\nAPI_with_zen.mp4\nChallenge Tool\n- Critical thinking vs reflexive agreement\nWithout Zen:\nWith Zen:\nKey Features\nAI Orchestration\nAuto model selection\n- Claude picks the right AI for each task\nMulti-model workflows\n- Chain different models in single conversations\nConversation continuity\n- Context preserved across tools and models\nContext revival\n- Continue conversations even after context resets\nModel Support\nMultiple providers\n- Gemini, OpenAI, Azure, X.AI, OpenRouter, DIAL, Ollama\nLatest models\n- GPT-5, Gemini 2.5 Pro, O3, Grok-4, local Llama\nThinking modes\n- Control reasoning depth vs cost\nVision support\n- Analyze images, diagrams, screenshots\nDeveloper Experience\nGuided workflows\n- Systematic investigation prevents rushed analysis\nSmart file handling\n- Auto-expand directories, manage token limits\nWeb search integration\n- Access current documentation and best practices\nLarge prompt support\n- Bypass MCP's 25K token limit\nExample Workflows\nMulti-model Code Review:\n\"Perform a codereview using gemini pro and o3, then use planner to create a fix strategy\"\n→ Claude reviews code systematically → Consults Gemini Pro → Gets O3's perspective → Creates unified action plan\nCollaborative Debugging:\n\"Debug this race condition with max thinking mode, then validate the fix with precommit\"\n→ Deep investigation → Expert analysis → Solution implementation → Pre-commit validation\nArchitecture Planning:\n\"Plan our microservices migration, get consensus from pro and o3 on the approach\"\n→ Structured planning → Multiple expert opinions → Consensus building → Implementation roadmap\n👉\nAdvanced Usage Guide\nfor complex workflows, model configuration, and power-user features\nQuick Links\n📖 Documentation\nDocs Overview\n- High-level map of major guides\nGetting Started\n- Complete setup guide\nTools Reference\n- All tools with examples\nAdvanced Usage\n- Power user features\nConfiguration\n- Environment variables, restrictions\nAdding Providers\n- Provider-specific setup (OpenAI, Azure, custom gateways)\nModel Ranking Guide\n- How intelligence scores drive auto-mode suggestions\n🔧 Setup & Support\nWSL Setup\n- Windows users\nTroubleshooting\n- Common issues\nContributing\n- Code standards, PR process\nLicense\nApache 2.0 License - see\nLICENSE\nfile for details.\nAcknowledgments\nBuilt with the power of\nMulti-Model AI\ncollaboration 🤝\nA\nctual\nI\nntelligence by real Humans\nMCP (Model Context Protocol)\nCodex CLI\nClaude Code\nGemini\nOpenAI\nAzure OpenAI\nStar History",
        "今日の獲得スター数: 273",
        "累積スター数: 8,534"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/BeehiveInnovations/zen-mcp-server"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/thingsboard/thingsboard",
      "title": "thingsboard/thingsboard",
      "date": null,
      "executive_summary": [
        "Open-source IoT Platform - Device management, data collection, processing and visualization.",
        "---",
        "Open-source IoT platform for data collection, processing, visualization, and device management.\n💡\nGet started\n• 🌐\nWebsite\n• 📚\nDocumentation\n• 📔\nBlog\n•\n▶️\nLive demo\n• 🔗\nLinkedIn\n🚀 Installation options\nInstall ThingsBoard\nOn-premise\nTry\nThingsBoard Cloud\nor\nUse our Live demo\n💡 Getting started with ThingsBoard\nCheck out our\nGetting Started guide\nor\nwatch the video\nto learn the basics of ThingsBoard and create your first dashboard! You will learn to:\nConnect devices to ThingsBoard\nPush data from devices to ThingsBoard\nBuild real-time dashboards\nCreate a Customer and assign the dashboard with them.\nDefine thresholds and trigger alarms\nSet up notifications via email, SMS, mobile apps, or integrate with third-party services.\n✨ Features\nProvision and manage\ndevices and assets\nProvision, monitor and control your IoT entities in secure way using rich server-side APIs. Define relations between your devices, assets, customers or any other entities.\nRead more ➜\nCollect and visualize\nyour data\nCollect and store telemetry data in scalable and fault-tolerant way. Visualize your data with built-in or custom widgets and flexible dashboards. Share dashboards with your customers.\nRead more ➜\nSCADA Dashboards\nMonitor and control your industrial processes in real time with SCADA. Use SCADA symbols on dashboards to create and manage any workflow, offering full flexibility to design and oversee operations according to your requirements.\nRead more ➜\nProcess and React\nDefine data processing rule chains. Transform and normalize your device data. Raise alarms on incoming telemetry events, attribute updates, device inactivity and user actions.\nRead more ➜\n⚙️ Powerful IoT Rule Engine\nThingsBoard allows you to create complex\nRule Chains\nto process data from your devices and match your application specific use cases.\nRead more about Rule Engine ➜\n📦 Real-Time IoT Dashboards\nThingsBoard is a scalable, user-friendly, and device-agnostic IoT platform that speeds up time-to-market with powerful built-in solution templates. It enables data collection and analysis from any devices, saving resources on routine tasks and letting you focus on your solution’s unique aspects. See more our Use Cases\nhere\n.\nSmart energy\nSCADA swimming pool\nFleet tracking\nSmart farming\nSmart metering\nCheck more of our use cases ➜\n🫶 Support\nTo get support, please visit our\nGitHub issues page\n📄 Licenses\nThis project is released under\nApache 2.0 License",
        "今日の獲得スター数: 265",
        "累積スター数: 20,166"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/thingsboard/thingsboard"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/Zie619/n8n-workflows",
      "title": "Zie619/n8n-workflows",
      "date": null,
      "executive_summary": [
        "all of the workflows of n8n i could find (also from the site itself)",
        "---",
        "⚡ N8N Workflow Collection & Documentation\nA professionally organized collection of\n2,057 n8n workflows\nwith a lightning-fast documentation system that provides instant search, analysis, and browsing capabilities.\n⚠️\nIMPORTANT NOTICE (Aug 14, 2025):\nRepository history has been rewritten due to DMCA compliance. If you have a fork or local clone, please see\nIssue 85\nfor instructions on syncing your copy.\nSupport My Work\nIf you'd like to say thanks, consider buying me a coffee—your support helps me keep improving this project!\n🚀\nNEW: Public Search Interface & High-Performance Documentation\n🌐\nBrowse workflows online\n- No installation required!\nOr run locally for development with 100x performance improvement:\nOption 1: Online Search (Recommended for Users)\n🔗 Visit:\nzie619.github.io/n8n-workflows\n⚡\nInstant access\n- No setup required\n🔍\nSearch 2,057+ workflows\ndirectly in browser\n📱\nMobile-friendly\ninterface\n🏷️\nCategory filtering\nacross 15 categories\n📥\nDirect download\nof workflow JSON files\nOption 2: Local Development System\n#\nInstall dependencies\npip install -r requirements.txt\n#\nStart the fast API server\npython run.py\n#\nOpen in browser\nhttp://localhost:8000\nFeatures:\n⚡\nSub-100ms response times\nwith SQLite FTS5 search\n🔍\nInstant full-text search\nwith advanced filtering\n📱\nResponsive design\n- works perfectly on mobile\n🌙\nDark/light themes\nwith system preference detection\n📊\nLive statistics\n- 365 unique integrations, 29,445 total nodes\n🎯\nSmart categorization\nby trigger type and complexity\n🎯\nUse case categorization\nby service name mapped to categories\n📄\nOn-demand JSON viewing\nand download\n🔗\nMermaid diagram generation\nfor workflow visualization\n🔄\nReal-time workflow naming\nwith intelligent formatting\nPerformance Comparison\nMetric\nOld System\nNew System\nImprovement\nFile Size\n71MB HTML\n<100KB\n700x smaller\nLoad Time\n10+ seconds\n<1 second\n10x faster\nSearch\nClient-side only\nFull-text with FTS5\nInstant\nMemory Usage\n~2GB RAM\n<50MB RAM\n40x less\nMobile Support\nPoor\nExcellent\nFully responsive\n📂 Repository Organization\nWorkflow Collection\n2,057 workflows\nwith meaningful, searchable names\n365 unique integrations\nacross popular platforms\n29,445 total nodes\nwith professional categorization\nQuality assurance\n- All workflows analyzed and categorized\nAdvanced Naming System ✨\nOur intelligent naming system converts technical filenames into readable titles:\nBefore\n:\n2051_Telegram_Webhook_Automation_Webhook.json\nAfter\n:\nTelegram Webhook Automation\n100% meaningful names\nwith smart capitalization\nAutomatic integration detection\nfrom node analysis\nUse Case Category ✨\nThe search interface includes a dropdown filter that lets you browse 2,057+ workflows by category.\nThe system includes an automated categorization feature that organizes workflows by service categories to make them easier to discover and filter.\nHow Categorization Works\nRun the categorization script\npython create_categories.py\nService Name Recognition\nThe script analyzes each workflow JSON filename to identify recognized service names (e.g., \"Twilio\", \"Slack\", \"Gmail\", etc.)\nCategory Mapping\nEach recognized service name is matched to its corresponding category using the definitions in\ncontext/def_categories.json\n. For example:\nTwilio → Communication & Messaging\nGmail → Communication & Messaging\nAirtable → Data Processing & Analysis\nSalesforce → CRM & Sales\nSearch Categories Generation\nThe script produces a\nsearch_categories.json\nfile that contains the categorized workflow data\nFilter Interface\nUsers can then filter workflows by category in the search interface, making it easier to find workflows for specific use cases\nAvailable Categories\nThe categorization system includes the following main categories:\nAI Agent Development\nBusiness Process Automation\nCloud Storage & File Management\nCommunication & Messaging\nCreative Content & Video Automation\nCreative Design Automation\nCRM & Sales\nData Processing & Analysis\nE-commerce & Retail\nFinancial & Accounting\nMarketing & Advertising Automation\nProject Management\nSocial Media Management\nTechnical Infrastructure & DevOps\nWeb Scraping & Data Extraction\nContribute Categories\nYou can help expand the categorization by adding more service-to-category mappings (e.g., Twilio → Communication & Messaging) in context/defs_categories.json.\nMany workflow JSON files are conveniently named with the service name, often separated by underscores (_).\n🛠 Usage Instructions\nOption 1: Modern Fast System (Recommended)\n#\nClone repository\ngit clone\n<\nrepo-url\n>\ncd\nn8n-workflows\n#\nInstall Python dependencies\npip install -r requirements.txt\n#\nStart the documentation server\npython run.py\n#\nBrowse workflows at http://localhost:8000\n#\n- Instant search across 2,057 workflows\n#\n- Professional responsive interface\n#\n- Real-time workflow statistics\nOption 2: Development Mode\n#\nStart with auto-reload for development\npython run.py --dev\n#\nOr specify custom host/port\npython run.py --host 0.0.0.0 --port 3000\n#\nForce database reindexing\npython run.py --reindex\nImport Workflows into n8n\n#\nUse the Python importer (recommended)\npython import_workflows.py\n#\nOr manually import individual workflows:\n#\n1. Open your n8n Editor UI\n#\n2. Click menu (☰) → Import workflow\n#\n3. Choose any .json file from the workflows/ folder\n#\n4. Update credentials/webhook URLs before running\n📊 Workflow Statistics\nCurrent Collection Stats\nTotal Workflows\n: 2,057 automation workflows\nActive Workflows\n: 215 (10.5% active rate)\nTotal Nodes\n: 29,528 (avg 14.4 nodes per workflow)\nUnique Integrations\n: 367 different services and APIs\nDatabase\n: SQLite with FTS5 full-text search\nTrigger Distribution\nComplex\n: 832 workflows (40.4%) - Multi-trigger systems\nWebhook\n: 521 workflows (25.3%) - API-triggered automations\nManual\n: 478 workflows (23.2%) - User-initiated workflows\nScheduled\n: 226 workflows (11.0%) - Time-based executions\nComplexity Analysis\nLow (≤5 nodes)\n: ~35% - Simple automations\nMedium (6-15 nodes)\n: ~45% - Standard workflows\nHigh (16+ nodes)\n: ~20% - Complex enterprise systems\nPopular Integrations\nTop services by usage frequency:\nCommunication\n: Telegram, Discord, Slack, WhatsApp\nCloud Storage\n: Google Drive, Google Sheets, Dropbox\nDatabases\n: PostgreSQL, MySQL, MongoDB, Airtable\nAI/ML\n: OpenAI, Anthropic, Hugging Face\nDevelopment\n: HTTP Request, Webhook, GraphQL\n🔍 Advanced Search Features\nSmart Search Categories\nOur system automatically categorizes workflows into 15 main categories:\nAvailable Categories:\nAI Agent Development\n: OpenAI, Anthropic, Hugging Face, CalcsLive\nBusiness Process Automation\n: Workflow utilities, scheduling, data processing\nCloud Storage & File Management\n: Google Drive, Dropbox, OneDrive, Box\nCommunication & Messaging\n: Telegram, Discord, Slack, WhatsApp, Email\nCreative Content & Video Automation\n: YouTube, Vimeo, content creation\nCreative Design Automation\n: Canva, Figma, image processing\nCRM & Sales\n: Salesforce, HubSpot, Pipedrive, customer management\nData Processing & Analysis\n: Database operations, analytics, data transformation\nE-commerce & Retail\n: Shopify, Stripe, PayPal, online stores\nFinancial & Accounting\n: Financial tools, payment processing, accounting\nMarketing & Advertising Automation\n: Email marketing, campaigns, lead generation\nProject Management\n: Jira, Trello, Asana, task management\nSocial Media Management\n: LinkedIn, Twitter/X, Facebook, Instagram\nTechnical Infrastructure & DevOps\n: GitHub, deployment, monitoring\nWeb Scraping & Data Extraction\n: HTTP requests, webhooks, data collection\nAPI Usage Examples\n#\nSearch workflows by text\ncurl\n\"\nhttp://localhost:8000/api/workflows?q=telegram+automation\n\"\n#\nFilter by trigger type and complexity\ncurl\n\"\nhttp://localhost:8000/api/workflows?trigger=Webhook&complexity=high\n\"\n#\nFind all messaging workflows\ncurl\n\"\nhttp://localhost:8000/api/workflows/category/messaging\n\"\n#\nGet database statistics\ncurl\n\"\nhttp://localhost:8000/api/stats\n\"\n#\nBrowse available categories\ncurl\n\"\nhttp://localhost:8000/api/categories\n\"\n🏗 Technical Architecture\nModern Stack\nSQLite Database\n- FTS5 full-text search with 365 indexed integrations\nFastAPI Backend\n- RESTful API with automatic OpenAPI documentation\nResponsive Frontend\n- Modern HTML5 with embedded CSS/JavaScript\nSmart Analysis\n- Automatic workflow categorization and naming\nKey Features\nChange Detection\n- MD5 hashing for efficient re-indexing\nBackground Processing\n- Non-blocking workflow analysis\nCompressed Responses\n- Gzip middleware for optimal speed\nError Handling\n- Graceful degradation and comprehensive logging\nMobile Optimization\n- Touch-friendly interface design\nDatabase Performance\n--\nOptimized schema for lightning-fast queries\nCREATE\nTABLE\nworkflows\n(\n    id\nINTEGER\nPRIMARY KEY\n,\n    filename\nTEXT\nUNIQUE,\n    name\nTEXT\n,\n    active\nBOOLEAN\n,\n    trigger_type\nTEXT\n,\n    complexity\nTEXT\n,\n    node_count\nINTEGER\n,\n    integrations\nTEXT\n,\n--\nJSON array of 365 unique services\ndescription\nTEXT\n,\n    file_hash\nTEXT\n,\n--\nMD5 for change detection\nanalyzed_at\nTIMESTAMP\n);\n--\nFull-text search with ranking\nCREATE VIRTUAL TABLE workflows_fts USING fts5(\n    filename, name, description, integrations, tags,\n    content\n=\n'\nworkflows\n'\n, content_rowid\n=\n'\nid\n'\n);\n🔧 Setup & Requirements\nSystem Requirements\nPython 3.7+\n- For running the documentation system\nModern Browser\n- Chrome, Firefox, Safari, Edge\n50MB Storage\n- For SQLite database and indexes\nn8n Instance\n- For importing and running workflows\nInstallation\n#\nClone repository\ngit clone\n<\nrepo-url\n>\ncd\nn8n-workflows\n#\nInstall dependencies\npip install -r requirements.txt\n#\nStart documentation server\npython run.py\n#\nAccess at http://localhost:8000\nDevelopment Setup\n#\nCreate virtual environment\npython3 -m venv .venv\nsource\n.venv/bin/activate\n#\nLinux/Mac\n#\nor .venv\\Scripts\\activate  # Windows\n#\nInstall dependencies\npip install -r requirements.txt\n#\nRun with auto-reload for development\npython api_server.py --reload\n#\nForce database reindexing\npython workflow_db.py --index --force\n📋 Naming Convention\nIntelligent Formatting System\nOur system automatically converts technical filenames to user-friendly names:\n#\nAutomatic transformations:\n2051_Telegram_Webhook_Automation_Webhook.json →\n\"\nTelegram Webhook Automation\n\"\n0250_HTTP_Discord_Import_Scheduled.json →\n\"\nHTTP Discord Import Scheduled\n\"\n0966_OpenAI_Data_Processing_Manual.json →\n\"\nOpenAI Data Processing Manual\n\"\nTechnical Format\n[ID]_[Service1]_[Service2]_[Purpose]_[Trigger].json\nSmart Capitalization Rules\nHTTP\n→ HTTP (not Http)\nAPI\n→ API (not Api)\nwebhook\n→ Webhook\nautomation\n→ Automation\nscheduled\n→ Scheduled\n🚀 API Documentation\nCore Endpoints\nGET /\n- Main workflow browser interface\nGET /api/stats\n- Database statistics and metrics\nGET /api/workflows\n- Search with filters and pagination\nGET /api/workflows/{filename}\n- Detailed workflow information\nGET /api/workflows/{filename}/download\n- Download workflow JSON\nGET /api/workflows/{filename}/diagram\n- Generate Mermaid diagram\nAdvanced Search\nGET /api/workflows/category/{category}\n- Search by service category\nGET /api/categories\n- List all available categories\nGET /api/integrations\n- Get integration statistics\nPOST /api/reindex\n- Trigger background reindexing\nResponse Examples\n// GET /api/stats\n{\n\"total\"\n:\n2053\n,\n\"active\"\n:\n215\n,\n\"inactive\"\n:\n1838\n,\n\"triggers\"\n: {\n\"Complex\"\n:\n831\n,\n\"Webhook\"\n:\n519\n,\n\"Manual\"\n:\n477\n,\n\"Scheduled\"\n:\n226\n},\n\"total_nodes\"\n:\n29445\n,\n\"unique_integrations\"\n:\n365\n}\n🤝 Contributing\n🎉 This project solves\nIssue #84\n- providing online access to workflows without requiring local setup!\nAdding New Workflows\nExport workflow\nas JSON from n8n\nName descriptively\nfollowing the established pattern:\n[ID]_[Service]_[Purpose]_[Trigger].json\nAdd to workflows/\ndirectory (create service folder if needed)\nRemove sensitive data\n(credentials, personal URLs)\nAdd tags\nfor better searchability (calculation, automation, etc.)\nGitHub Actions automatically\nupdates the public search interface\nQuality Standards\n✅ Workflow must be functional and tested\n✅ Remove all credentials and sensitive data\n✅ Follow naming convention for consistency\n✅ Verify compatibility with recent n8n versions\n✅ Include meaningful description or comments\n✅ Add relevant tags for search optimization\nCustom Node Workflows\n✅ Include npm package links in descriptions\n✅ Document custom node requirements\n✅ Add installation instructions\n✅ Use descriptive tags (like CalcsLive example)\nReindexing (for local development)\n#\nForce database reindexing after adding workflows\npython run.py --reindex\n#\nOr update search index only\npython scripts/generate_search_index.py\n⚠️\nImportant Notes\nSecurity & Privacy\nReview before use\n- All workflows shared as-is for educational purposes\nUpdate credentials\n- Replace API keys, tokens, and webhooks\nTest safely\n- Verify in development environment first\nCheck permissions\n- Ensure proper access rights for integrations\nCompatibility\nn8n Version\n- Compatible with n8n 1.0+ (most workflows)\nCommunity Nodes\n- Some workflows may require additional node installations\nAPI Changes\n- External services may have updated their APIs since creation\nDependencies\n- Verify required integrations before importing\n📚 Resources & References\nWorkflow Sources\nThis comprehensive collection includes workflows from:\nOfficial n8n.io\n- Documentation and community examples\nGitHub repositories\n- Open source community contributions\nBlog posts & tutorials\n- Real-world automation patterns\nUser submissions\n- Tested and verified workflows\nEnterprise use cases\n- Business process automations\nLearn More\nn8n Documentation\n- Official documentation\nn8n Community\n- Community forum and support\nWorkflow Templates\n- Official template library\nIntegration Docs\n- Service-specific guides\n🏆 Project Achievements\nRepository Transformation\n2,053 workflows\nprofessionally organized and named\n365 unique integrations\nautomatically detected and categorized\n100% meaningful names\n(improved from basic filename patterns)\nZero data loss\nduring intelligent renaming process\nAdvanced search\nwith 15 service categories\nPerformance Revolution\nSub-100ms search\nwith SQLite FTS5 full-text indexing\nInstant filtering\nacross 29,445 workflow nodes\nMobile-optimized\nresponsive design for all devices\nReal-time statistics\nwith live database queries\nProfessional interface\nwith modern UX principles\nSystem Reliability\nRobust error handling\nwith graceful degradation\nChange detection\nfor efficient database updates\nBackground processing\nfor non-blocking operations\nComprehensive logging\nfor debugging and monitoring\nProduction-ready\nwith proper middleware and security\nThis repository represents the most comprehensive and well-organized collection of n8n workflows available, featuring cutting-edge search technology and professional documentation that makes workflow discovery and usage a delightful experience.\n🎯 Perfect for\n: Developers, automation engineers, business analysts, and anyone looking to streamline their workflows with proven n8n automations.\n中文",
        "今日の獲得スター数: 240",
        "累積スター数: 35,687"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/Zie619/n8n-workflows"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/MODSetter/SurfSense",
      "title": "MODSetter/SurfSense",
      "date": null,
      "executive_summary": [
        "Open Source Alternative to NotebookLM / Perplexity, connected to external sources such as Search Engines, Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more. Join our discord: https://discord.gg/ejRNvftDp9",
        "---",
        "SurfSense\nWhile tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar, Luma and more to come.\nVideo\ntemp_demo_v7.mp4\nPodcast Sample\nelon_vs_trump_podcast.mp4\nKey Features\n💡\nIdea\n:\nHave your own highly customizable private NotebookLM and Perplexity integrated with external sources.\n📁\nMultiple File Format Uploading Support\nSave content from your own personal files\n(Documents, images, videos and supports\n50+ file extensions\n)\nto your own personal knowledge base .\n🔍\nPowerful Search\nQuickly research or find anything in your saved content .\n💬\nChat with your Saved Content\nInteract in Natural Language and get cited answers.\n📄\nCited Answers\nGet Cited answers just like Perplexity.\n🔔\nPrivacy & Local LLM Support\nWorks Flawlessly with Ollama local LLMs.\n🏠\nSelf Hostable\nOpen source and easy to deploy locally.\n🎙️ Podcasts\nBlazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)\nConvert your chat conversations into engaging audio content\nSupport for local TTS providers (Kokoro TTS)\nSupport for multiple TTS providers (OpenAI, Azure, Google Vertex AI)\n📊\nAdvanced RAG Techniques\nSupports 100+ LLM's\nSupports 6000+ Embedding Models.\nSupports all major Rerankers (Pinecode, Cohere, Flashrank etc)\nUses Hierarchical Indices (2 tiered RAG setup).\nUtilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).\nRAG as a Service API Backend.\nℹ️\nExternal Sources\nSearch Engines (Tavily, LinkUp)\nSlack\nLinear\nJira\nClickUp\nConfluence\nNotion\nGmail\nYoutube Videos\nGitHub\nDiscord\nAirtable\nGoogle Calendar\nLuma\nand more to come.....\n📄\nSupported File Extensions\nNote\n: File format support depends on your ETL service configuration. LlamaCloud supports 50+ formats, Unstructured supports 34+ core formats, and Docling (core formats, local processing, privacy-focused, no API key).\nDocuments & Text\nLlamaCloud\n:\n.pdf\n,\n.doc\n,\n.docx\n,\n.docm\n,\n.dot\n,\n.dotm\n,\n.rtf\n,\n.txt\n,\n.xml\n,\n.epub\n,\n.odt\n,\n.wpd\n,\n.pages\n,\n.key\n,\n.numbers\n,\n.602\n,\n.abw\n,\n.cgm\n,\n.cwk\n,\n.hwp\n,\n.lwp\n,\n.mw\n,\n.mcw\n,\n.pbd\n,\n.sda\n,\n.sdd\n,\n.sdp\n,\n.sdw\n,\n.sgl\n,\n.sti\n,\n.sxi\n,\n.sxw\n,\n.stw\n,\n.sxg\n,\n.uof\n,\n.uop\n,\n.uot\n,\n.vor\n,\n.wps\n,\n.zabw\nUnstructured\n:\n.doc\n,\n.docx\n,\n.odt\n,\n.rtf\n,\n.pdf\n,\n.xml\n,\n.txt\n,\n.md\n,\n.markdown\n,\n.rst\n,\n.html\n,\n.org\n,\n.epub\nDocling\n:\n.pdf\n,\n.docx\n,\n.html\n,\n.htm\n,\n.xhtml\n,\n.adoc\n,\n.asciidoc\nPresentations\nLlamaCloud\n:\n.ppt\n,\n.pptx\n,\n.pptm\n,\n.pot\n,\n.potm\n,\n.potx\n,\n.odp\n,\n.key\nUnstructured\n:\n.ppt\n,\n.pptx\nDocling\n:\n.pptx\nSpreadsheets & Data\nLlamaCloud\n:\n.xlsx\n,\n.xls\n,\n.xlsm\n,\n.xlsb\n,\n.xlw\n,\n.csv\n,\n.tsv\n,\n.ods\n,\n.fods\n,\n.numbers\n,\n.dbf\n,\n.123\n,\n.dif\n,\n.sylk\n,\n.slk\n,\n.prn\n,\n.et\n,\n.uos1\n,\n.uos2\n,\n.wk1\n,\n.wk2\n,\n.wk3\n,\n.wk4\n,\n.wks\n,\n.wq1\n,\n.wq2\n,\n.wb1\n,\n.wb2\n,\n.wb3\n,\n.qpw\n,\n.xlr\n,\n.eth\nUnstructured\n:\n.xls\n,\n.xlsx\n,\n.csv\n,\n.tsv\nDocling\n:\n.xlsx\n,\n.csv\nImages\nLlamaCloud\n:\n.jpg\n,\n.jpeg\n,\n.png\n,\n.gif\n,\n.bmp\n,\n.svg\n,\n.tiff\n,\n.webp\n,\n.html\n,\n.htm\n,\n.web\nUnstructured\n:\n.jpg\n,\n.jpeg\n,\n.png\n,\n.bmp\n,\n.tiff\n,\n.heic\nDocling\n:\n.jpg\n,\n.jpeg\n,\n.png\n,\n.bmp\n,\n.tiff\n,\n.tif\n,\n.webp\nAudio & Video\n(Always Supported)\n.mp3\n,\n.mpga\n,\n.m4a\n,\n.wav\n,\n.mp4\n,\n.mpeg\n,\n.webm\nEmail & Communication\nUnstructured\n:\n.eml\n,\n.msg\n,\n.p7s\n🔖 Cross Browser Extension\nThe SurfSense extension can be used to save any webpage you like.\nIts main usecase is to save any webpages protected beyond authentication.\nFEATURE REQUESTS AND FUTURE\nSurfSense is actively being developed.\nWhile it's not yet production-ready, you can help us speed up the process.\nJoin the\nSurfSense Discord\nand help shape the future of SurfSense!\n🚀 Roadmap\nStay up to date with our development progress and upcoming features!\nCheck out our public roadmap and contribute your ideas or feedback:\nView the Roadmap:\nSurfSense Roadmap on GitHub Projects\nHow to get started?\nInstallation Options\nSurfSense provides two installation methods:\nDocker Installation\n- The easiest way to get SurfSense up and running with all dependencies containerized.\nIncludes pgAdmin for database management through a web UI\nSupports environment variable customization via\n.env\nfile\nFlexible deployment options (full stack or core services only)\nNo need to manually edit configuration files between environments\nSee\nDocker Setup Guide\nfor detailed instructions\nFor deployment scenarios and options, see\nDeployment Guide\nManual Installation (Recommended)\n- For users who prefer more control over their setup or need to customize their deployment.\nBoth installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.\nBefore installation, make sure to complete the\nprerequisite setup steps\nincluding:\nPGVector setup\nFile Processing ETL Service\n(choose one):\nUnstructured.io API key (supports 34+ formats)\nLlamaIndex API key (enhanced parsing, supports 50+ formats)\nDocling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)\nOther required API keys\nScreenshots\nResearch Agent\nSearch Spaces\nManage Documents\nPodcast Agent\nAgent Chat\nBrowser Extension\nTech Stack\nBackEnd\nFastAPI\n: Modern, fast web framework for building APIs with Python\nPostgreSQL with pgvector\n: Database with vector search capabilities for similarity searches\nSQLAlchemy\n: SQL toolkit and ORM (Object-Relational Mapping) for database interactions\nAlembic\n: A database migrations tool for SQLAlchemy.\nFastAPI Users\n: Authentication and user management with JWT and OAuth support\nLangGraph\n: Framework for developing AI-agents.\nLangChain\n: Framework for developing AI-powered applications.\nLLM Integration\n: Integration with LLM models through LiteLLM\nRerankers\n: Advanced result ranking for improved search relevance\nHybrid Search\n: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)\nVector Embeddings\n: Document and text embeddings for semantic search\npgvector\n: PostgreSQL extension for efficient vector similarity operations\nChonkie\n: Advanced document chunking and embedding library\nUses\nAutoEmbeddings\nfor flexible embedding model selection\nLateChunker\nfor optimized document chunking based on embedding model's max sequence length\nFrontEnd\nNext.js 15.2.3\n: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.\nReact 19.0.0\n: JavaScript library for building user interfaces.\nTypeScript\n: Static type-checking for JavaScript, enhancing code quality and developer experience.\nVercel AI SDK Kit UI Stream Protocol\n: To create scalable chat UI.\nTailwind CSS 4.x\n: Utility-first CSS framework for building custom UI designs.\nShadcn\n: Headless components library.\nLucide React\n: Icon set implemented as React components.\nFramer Motion\n: Animation library for React.\nSonner\n: Toast notification library.\nGeist\n: Font family from Vercel.\nReact Hook Form\n: Form state management and validation.\nZod\n: TypeScript-first schema validation with static type inference.\n@hookform/resolvers\n: Resolvers for using validation libraries with React Hook Form.\n@tanstack/react-table\n: Headless UI for building powerful tables & datagrids.\nDevOps\nDocker\n: Container platform for consistent deployment across environments\nDocker Compose\n: Tool for defining and running multi-container Docker applications\npgAdmin\n: Web-based PostgreSQL administration tool included in Docker setup\nExtension\nManifest v3 on Plasmo\nFuture Work\nAdd More Connectors.\nPatch minor bugs.\nDocument Podcasts\nContribute\nContributions are very welcome! A contribution can be as small as a ⭐ or even finding and creating issues.\nFine-tuning the Backend is always desired.\nFor detailed contribution guidelines, please see our\nCONTRIBUTING.md\nfile.\nStar History",
        "今日の獲得スター数: 236",
        "累積スター数: 8,894"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/MODSetter/SurfSense"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/timelinize/timelinize",
      "title": "timelinize/timelinize",
      "date": null,
      "executive_summary": [
        "Store your data from all your accounts and devices in a single cohesive timeline on your own computer",
        "---",
        "Organize your photos & videos, chats & messages, location history, social media content, contacts, and more into a single cohesive timeline on your own computer where you can keep them alive forever.\nTimelinize lets you import your data from practically anywhere: your computer, phone, online accounts, GPS-enabled radios, various apps and programs, contact lists, cameras, and more.\nJoin our Discord\nto discuss!\nNote\nI am looking for a better name for this project. If you have an idea for a good name that is short, relevant, unique, and available,\nI'd love to hear it!\nScreenshots\nThese were captured using a dev repository of mine filled with a subset of my real data, so I've run Timelinize in obfuscation mode: images and videos are blurred (except profile pictures---need to fix that); names, identifiers, and locations around sensitive areas are all randomized, and text has been replaced with random words so that the string is about the same length.\n(I hope to make a video tour soon.)\nPlease remember this is an early alpha preview, and the software is very much evolving and improving. And you can help!\nWIP dashboard.\nVery\nWIP. The bubble chart is particularly interesting as it shows you what kinds of data are most common at which times of day throughout the years.\nThe classic timeline view is a combination of all data grouped by types and time segments for reconstructing a day or other custom time period.\nViewing an item shows all the information about it, regardless of type: text, photo, live photo, video, location, etc.\nI had to make a custom file picker since browser APIs are too limiting. This is how you'll import most of your data into your timeline, but this flow is being revised soon.\nThe large map view is capable of 3D exploration, showing your memories right where they happened with a color-coded path that represents time.\nBecause Timelinize is entity-aware and supports multiple data sources, it can show data on a map even if it doesn't have geolocation information. That's what the gray dots or pins represent. In this example, a text message was received while at church, even though it doesn't have any geolocation info associated with it directly.\nTimelinize treats entities (people, pets/animals, organizations, etc.) as first-class data points which you can filter and organize.\nTimelinize will automatically recognize the same entity across data sources with enough information, but if it isn't possible automatically, you can manually merge entities with a click.\nConversations are aggregated across data sources that have messaging capabilities. They become emergent from the database by querying relationships between items and entities.\nIn this conversation view, you can see messages exchanged with this person across both Facebook and SMS/text message are displayed together. Reactions are also supported.\nA gallery displays photos and videos, but not just those in your photo library: it includes pictures and memes sent via messages, photos and videos uploaded to social media, and any other photos/videos in your data. You can always filter to drill down.\nHow it works\nObtain your data.\nThis usually involves exporting your data from apps, online accounts, or devices. For example, requesting an archive from Google Takeout. (Apple iCloud, Facebook, Twitter/X, Strava, Instagram, etc. all offer similar functionality for GDPR compliance.) Do this early/soon, because some services take days to provide your data.\nImport your data using Timelinize. You don't need to extract or decompress .tar or .zip archives; Timelinize will attempt to recognize your data in its original format and folder structure. All the data you import is indexed in a SQLite database and stored on disk organized by date -- no obfuscation or proprietary formats; you can simply browse your files if you wish.\nExplore and organize! Timelinize has a UI that portrays data using various projections and filters. It can recall moments from your past and help you view your life more comprehensively. (It's a great living family history tool.)\nRepeat steps 1-3 as often as desired. Timelinize will skip any existing data that is the same and only import new content. You could do this every few weeks or months for busy accounts that are most important to you.\nCaution\nTimelinize is in active development and is still considered unstable. The schema is still changing, necessitating starting over from a clean slate when updating. Always keep your original source data. Expect to delete and recreate your timelines as you upgrade during this alpha development period.\nDownload and run\nDownload the\nlatest release\nfor your platform.\nSee the website for\ninstallation instructions\n.\nDevelop\nSee our\nproject wiki\nfor instructions on\ncompiling from source\n.\nCommand line interface\nTimelinize has a symmetric HTTP API and CLI. When an HTTP API endpoint is created in the code, it automatically adds to the command line as well.\nRun\ntimelinize help\n(or\ngo run main.go help\nif you're running from source) to view the list of commands, which are also HTTP endpoints. JSON or form inputs are converted to command line args/flags that represent the JSON schema or form fields.\nSetup Development Environment\nDev Container setup is provided for easy development using GitHub Codespaces or Visual Studio Code with the DevContainers extension.\nGetting started with VSCode\nMake sure you have the following installed:\nDocker\nDevContainers for VSCode\nOpen this project in VSCode\nGo to the\nRemote Explorer\non Activity Bar\nClick on\nNew Dev Container (+)\nClick on\nOpen Current Folder in Container\nThis sets up a docker container with all the dependencies required for building this project. You can get started with contributing quickly.\nMotivation and vision\n(For roadmap, see\nissues tagged\nlong-term 🔭\n.)\nThe motivation for this project is two-fold. Both press upon me with a sense of urgency, which is why I dedicated some nights and weekends to work on this.\nConnecting with my family -- both living and deceased -- is important to me and my close relatives. But I wish we had more insights into the lives of those who came before us. What was important to them? Where did they live / travel / spend their time? What lessons did they learn? How did global and local events -- or heck, even the weather -- affect them? What hardships did they endure? What would they have wanted to remember? What would it be like to talk to them? A lot of this could not be known unless they wrote it all down. But these days, we have that data for ourselves. What better time than right now to start collecting personal histories from all available sources and develop a rich timeline of our life for our family, or even just for our own reference and nostalgia.\nOur lives are better-documented than any before us, but the record of our life is more ephemeral than any before us, too. We lose control of our data by relying on centralized, proprietary apps and cloud services which are useful today, and gone tomorrow. I wrote Timelinize because now is the time to liberate my data from corporations who don't own it, yet who have the only copy of it. This reality has made me feel uneasy for years, and it's not going away soon. Timelinize makes it bearable.\nImagine being able to pull up a single screen with your data from any and all of your online accounts and services -- while offline. And there you see so many aspects of your life at a glance: your photos and videos, social media posts, locations on a map and how you got there, emails and letters, documents, health and physical activities, mental and emotional wellness, and maybe even music you listened to, for any given day. You can \"zoom out\" and get the big picture. Machine learning algorithms could suggest major clusters based on your content to summarize your days, months, or years, and from that, even recommend printing physical memorabilia. It's like a highly-detailed, automated journal, fully in your control, which you can add to in the app: augment it with your own thoughts like a regular journal.\nThen cross-reference your own timeline with a global public timeline: see how locations you went to changed over time, or what major news events may have affected you, or what the political/social climate -- or the literal climate -- was like at the time. For example, you may wonder, \"Why did the family stay inside so much of the summer one year?\" You could then see, \"Oh, because it was 110 F (43 C) degrees for two months straight.\"\nOr translate the projection sideways, and instead of looking at time cross-sections, look at cross-sections of your timeline by media type: photos, posts, location, sentiment. Look at plots, charts, graphs, of your physical activity.\nOr view projections by space instead of time: view interrelations between items on a map, even items that don't have location data, because the database is entity-aware. So if a person receives a text message and the same person has location information at about the same time from a photo or GPS device, then the text message can appear on a map too, reminding you where you first got the text with the news about your nephew's birth.\nAnd all of this runs on your own computer: no one else has access to it, no one else owns it, but you.\nAnd if everyone had their own timeline, in theory they could be merged into a global supertimeline to become a thorough record of the human race, all without the need for centralizing our data on cloud services that are controlled by greedy corporations.\nHistory\nI've been working on this project since about 2013, even before I conceptualized\nCaddy\n. My initial vision was to create an automated backup of my Picasa albums that I could store on my own hard drive. This project was called Photobak. Picasa eventually became Google Photos, and about the same time I realized I wanted to backup my photos posted to Facebook, Instagram, and Twitter, too. And while I was at it, why not include my Google Location History to augment the location data from the photos. The vision continued to expand as I realized that my family could use this too, so the schema was upgraded to support multiple people/entities as well. This could allow us to merge databases, or timelines, as family members pass, or as they share parts of their timeline around with each other. Timelinize is the mature evolution of the original project that is now designed to be a comprehensive, highly detailed archive of one's life through digital (or\ndigitized\n) content. An authoritative, unified record that is easy to preserve and organize.\nLicense\nThis project is licensed with AGPL. I chose this license because I do not want others to make proprietary or commercial software using this package. The point of this project is liberation of and control over one's own, personal data, and I want to ensure that this project won't be used in anything that would perpetuate the walled garden dilemma we already face today. Even if the future of this project ever has proprietary source code, I can ensure it will stay aligned with my values and the project's original goals.",
        "今日の獲得スター数: 225",
        "累積スター数: 2,204"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/timelinize/timelinize"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/openai/codex",
      "title": "openai/codex",
      "date": null,
      "executive_summary": [
        "Lightweight coding agent that runs in your terminal",
        "---",
        "npm i -g @openai/codex\nor\nbrew install codex\nCodex CLI\nis a coding agent from OpenAI that runs locally on your computer.\nIf you want Codex in your code editor (VS Code, Cursor, Windsurf),\ninstall in your IDE\nIf you are looking for the\ncloud-based agent\nfrom OpenAI,\nCodex Web\n, go to\nchatgpt.com/codex\nQuickstart\nInstalling and running Codex CLI\nInstall globally with your preferred package manager. If you use npm:\nnpm install -g @openai/codex\nAlternatively, if you use Homebrew:\nbrew install codex\nThen simply run\ncodex\nto get started:\ncodex\nYou can also go to the\nlatest GitHub Release\nand download the appropriate binary for your platform.\nEach GitHub Release contains many executables, but in practice, you likely want one of these:\nmacOS\nApple Silicon/arm64:\ncodex-aarch64-apple-darwin.tar.gz\nx86_64 (older Mac hardware):\ncodex-x86_64-apple-darwin.tar.gz\nLinux\nx86_64:\ncodex-x86_64-unknown-linux-musl.tar.gz\narm64:\ncodex-aarch64-unknown-linux-musl.tar.gz\nEach archive contains a single entry with the platform baked into the name (e.g.,\ncodex-x86_64-unknown-linux-musl\n), so you likely want to rename it to\ncodex\nafter extracting it.\nUsing Codex with your ChatGPT plan\nRun\ncodex\nand select\nSign in with ChatGPT\n. We recommend signing into your ChatGPT account to use Codex as part of your Plus, Pro, Team, Edu, or Enterprise plan.\nLearn more about what's included in your ChatGPT plan\n.\nYou can also use Codex with an API key, but this requires\nadditional setup\n. If you previously used an API key for usage-based billing, see the\nmigration steps\n. If you're having trouble with login, please comment on\nthis issue\n.\nModel Context Protocol (MCP)\nCodex can access MCP servers. To configure them, refer to the\nconfig docs\n.\nConfiguration\nCodex CLI supports a rich set of configuration options, with preferences stored in\n~/.codex/config.toml\n. For full configuration options, see\nConfiguration\n.\nDocs & FAQ\nGetting started\nCLI usage\nRunning with a prompt as input\nExample prompts\nMemory with AGENTS.md\nConfiguration\nSandbox & approvals\nAuthentication\nAuth methods\nLogin on a \"Headless\" machine\nAutomating Codex\nGitHub Action\nTypeScript SDK\nNon-interactive mode (\ncodex exec\n)\nAdvanced\nTracing / verbose logging\nModel Context Protocol (MCP)\nZero data retention (ZDR)\nContributing\nInstall & build\nSystem Requirements\nDotSlash\nBuild from source\nFAQ\nOpen source fund\nLicense\nThis repository is licensed under the\nApache-2.0 License\n.",
        "今日の獲得スター数: 219",
        "累積スター数: 46,727"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/openai/codex"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/google/computer-use-preview",
      "title": "google/computer-use-preview",
      "date": null,
      "executive_summary": [
        "---",
        "Computer Use Preview\nQuick Start\nThis section will guide you through setting up and running the Computer Use Preview model. Follow these steps to get started.\n1. Installation\nClone the Repository\ngit clone https://github.com/google/computer-use-preview.git\ncd\ncomputer-use-preview\nSet up Python Virtual Environment and Install Dependencies\npython3 -m venv .venv\nsource\n.venv/bin/activate\npip install -r requirements.txt\nInstall Playwright and Browser Dependencies\n#\nInstall system dependencies required by Playwright for Chrome\nplaywright install-deps chrome\n#\nInstall the Chrome browser for Playwright\nplaywright install chrome\n2. Configuration\nYou can get started using either the Gemini Developer API or Vertex AI.\nA. If using the Gemini Developer API:\nYou need a Gemini API key to use the agent:\nexport\nGEMINI_API_KEY=\n\"\nYOUR_GEMINI_API_KEY\n\"\nOr to add this to your virtual environment:\necho\n'\nexport GEMINI_API_KEY=\"YOUR_GEMINI_API_KEY\"\n'\n>>\n.venv/bin/activate\n#\nAfter editing, you'll need to deactivate and reactivate your virtual\n#\nenvironment if it's already active:\ndeactivate\nsource\n.venv/bin/activate\nReplace\nYOUR_GEMINI_API_KEY\nwith your actual key.\nB. If using the Vertex AI Client:\nYou need to explicitly use Vertex AI, then provide project and location to use the agent:\nexport\nUSE_VERTEXAI=true\nexport\nVERTEXAI_PROJECT=\n\"\nYOUR_PROJECT_ID\n\"\nexport\nVERTEXAI_LOCATION=\n\"\nYOUR_LOCATION\n\"\nOr to add this to your virtual environment:\necho\n'\nexport USE_VERTEXAI=true\n'\n>>\n.venv/bin/activate\necho\n'\nexport VERTEXAI_PROJECT=\"your-project-id\"\n'\n>>\n.venv/bin/activate\necho\n'\nexport VERTEXAI_LOCATION=\"your-location\"\n'\n>>\n.venv/bin/activate\n#\nAfter editing, you'll need to deactivate and reactivate your virtual\n#\nenvironment if it's already active:\ndeactivate\nsource\n.venv/bin/activate\nReplace\nYOUR_PROJECT_ID\nand\nYOUR_LOCATION\nwith your actual project and location.\n3. Running the Tool\nThe primary way to use the tool is via the\nmain.py\nscript.\nGeneral Command Structure:\npython main.py --query\n\"\nGo to Google and type 'Hello World' into the search bar\n\"\nAvailable Environments:\nYou can specify a particular environment with the\n--env <environment>\nflag.  Available options:\nplaywright\n: Runs the browser locally using Playwright.\nbrowserbase\n: Connects to a Browserbase instance.\nLocal Playwright\nRuns the agent using a Chrome browser instance controlled locally by Playwright.\npython main.py --query=\n\"\nGo to Google and type 'Hello World' into the search bar\n\"\n--env=\n\"\nplaywright\n\"\nYou can also specify an initial URL for the Playwright environment:\npython main.py --query=\n\"\nGo to Google and type 'Hello World' into the search bar\n\"\n--env=\n\"\nplaywright\n\"\n--initial_url=\n\"\nhttps://www.google.com/search?q=latest+AI+news\n\"\nBrowserbase\nRuns the agent using Browserbase as the browser backend. Ensure the proper Browserbase environment variables are set:\nBROWSERBASE_API_KEY\nand\nBROWSERBASE_PROJECT_ID\n.\npython main.py --query=\n\"\nGo to Google and type 'Hello World' into the search bar\n\"\n--env=\n\"\nbrowserbase\n\"\nAgent CLI\nThe\nmain.py\nscript is the command-line interface (CLI) for running the browser agent.\nCommand-Line Arguments\nArgument\nDescription\nRequired\nDefault\nSupported Environment(s)\n--query\nThe natural language query for the browser agent to execute.\nYes\nN/A\nAll\n--env\nThe computer use environment to use. Must be one of the following:\nplaywright\n, or\nbrowserbase\nNo\nN/A\nAll\n--initial_url\nThe initial URL to load when the browser starts.\nNo\nhttps://www.google.com\nAll\n--highlight_mouse\nIf specified, the agent will attempt to highlight the mouse cursor's position in the screenshots. This is useful for visual debugging.\nNo\nFalse (not highlighted)\nplaywright\nEnvironment Variables\nVariable\nDescription\nRequired\nGEMINI_API_KEY\nYour API key for the Gemini model.\nYes\nBROWSERBASE_API_KEY\nYour API key for Browserbase.\nYes (when using the browserbase environment)\nBROWSERBASE_PROJECT_ID\nYour Project ID for Browserbase.\nYes (when using the browserbase environment)",
        "今日の獲得スター数: 203",
        "累積スター数: 688"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/google/computer-use-preview"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/LukeGus/Termix",
      "title": "LukeGus/Termix",
      "date": null,
      "executive_summary": [
        "Termix is a web-based server management platform with SSH terminal, tunneling, and file editing capabilities.",
        "---",
        "Repo Stats\nEnglish |\n中文\nAchieved on September 1st, 2025\nTop Technologies\nIf you would like, you can support the project here!\nOverview\nTermix is an open-source, forever-free, self-hosted all-in-one server management platform. It provides a web-based\nsolution for managing your servers and infrastructure through a single, intuitive interface. Termix offers SSH terminal\naccess, SSH tunneling capabilities, and remote file management, with many more tools to come.\nFeatures\nSSH Terminal Access\n- Full-featured terminal with split-screen support (up to 4 panels) and tab system\nSSH Tunnel Management\n- Create and manage SSH tunnels with automatic reconnection and health monitoring\nRemote File Manager\n- Manage files directly on remote servers with support for viewing and editing code, images, audio, and video. Upload, download, rename, delete, and move files seamlessly.\nSSH Host Manager\n- Save, organize, and manage your SSH connections with tags and folders and easily save reusable login info while being able to automate the deploying of SSH keys\nServer Stats\n- View CPU, memory, and HDD usage on any SSH server\nUser Authentication\n- Secure user management with admin controls and OIDC and 2FA (TOTP) support\nDatabase Encryption\n- SQLite database files encrypted at rest with automatic encryption/decryption\nData Export/Import\n- Export and import SSH hosts, credentials, and file manager data with incremental sync\nAutomatic SSL Setup\n- Built-in SSL certificate generation and management with HTTPS redirects\nModern UI\n- Clean desktop/mobile-friendly interface built with React, Tailwind CSS, and Shadcn\nLanguages\n- Built-in support for English, Chinese, and German\nPlatform Support\n- Available as a web app, desktop application (Windows & Linux), and dedicated mobile app for iOS and Android. macOS and iPadOS support is planned.\nPlanned Features\nSee\nProjects\nfor all planned features. If you are looking to contribute, see\nContributing\n.\nInstallation\nSupported Devices:\nWebsite (any modern browser like Google, Safari, and Firefox)\nWindows (app)\nLinux (app)\niOS (app)\nAndroid (app)\niPadOS and macOS are in progress\nVisit the Termix\nDocs\nfor more information on how to install Termix on all platforms. Otherwise, view\na sample Docker Compose file here:\nservices\n:\ntermix\n:\nimage\n:\nghcr.io/lukegus/termix:latest\ncontainer_name\n:\ntermix\nrestart\n:\nunless-stopped\nports\n:\n      -\n\"\n8080:8080\n\"\nvolumes\n:\n      -\ntermix-data:/app/data\nenvironment\n:\nPORT\n:\n\"\n8080\n\"\nvolumes\n:\ntermix-data\n:\ndriver\n:\nlocal\nSupport\nIf you need help with Termix, you can join the\nDiscord\nserver and visit the support\nchannel. You can also open an issue or open a pull request on the\nGitHub\nrepo.\nShow-off\n2025-09-30.23-13-19.mp4\nLicense\nDistributed under the Apache License Version 2.0. See LICENSE for more information.",
        "今日の獲得スター数: 119",
        "累積スター数: 5,131"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/LukeGus/Termix"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/openai/openai-agents-python",
      "title": "openai/openai-agents-python",
      "date": null,
      "executive_summary": [
        "A lightweight, powerful framework for multi-agent workflows",
        "---",
        "OpenAI Agents SDK\nThe OpenAI Agents SDK is a lightweight yet powerful framework for building multi-agent workflows. It is provider-agnostic, supporting the OpenAI Responses and Chat Completions APIs, as well as 100+ other LLMs.\nNote\nLooking for the JavaScript/TypeScript version? Check out\nAgents SDK JS/TS\n.\nCore concepts:\nAgents\n: LLMs configured with instructions, tools, guardrails, and handoffs\nHandoffs\n: A specialized tool call used by the Agents SDK for transferring control between agents\nGuardrails\n: Configurable safety checks for input and output validation\nSessions\n: Automatic conversation history management across agent runs\nTracing\n: Built-in tracking of agent runs, allowing you to view, debug and optimize your workflows\nExplore the\nexamples\ndirectory to see the SDK in action, and read our\ndocumentation\nfor more details.\nGet started\nTo get started, set up your Python environment (Python 3.9 or newer required), and then install OpenAI Agents SDK package.\nvenv\npython -m venv .venv\nsource\n.venv/bin/activate\n#\nOn Windows: .venv\\Scripts\\activate\npip install openai-agents\nFor voice support, install with the optional\nvoice\ngroup:\npip install 'openai-agents[voice]'\n.\nFor Redis session support, install with the optional\nredis\ngroup:\npip install 'openai-agents[redis]'\n.\nuv\nIf you're familiar with\nuv\n, using the tool would be even similar:\nuv init\nuv add openai-agents\nFor voice support, install with the optional\nvoice\ngroup:\nuv add 'openai-agents[voice]'\n.\nFor Redis session support, install with the optional\nredis\ngroup:\nuv add 'openai-agents[redis]'\n.\nHello world example\nfrom\nagents\nimport\nAgent\n,\nRunner\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n,\ninstructions\n=\n\"You are a helpful assistant\"\n)\nresult\n=\nRunner\n.\nrun_sync\n(\nagent\n,\n\"Write a haiku about recursion in programming.\"\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# Code within the code,\n# Functions calling themselves,\n# Infinite loop's dance.\n(\nIf running this, ensure you set the\nOPENAI_API_KEY\nenvironment variable\n)\n(\nFor Jupyter notebook users, see\nhello_world_jupyter.ipynb\n)\nHandoffs example\nfrom\nagents\nimport\nAgent\n,\nRunner\nimport\nasyncio\nspanish_agent\n=\nAgent\n(\nname\n=\n\"Spanish agent\"\n,\ninstructions\n=\n\"You only speak Spanish.\"\n,\n)\nenglish_agent\n=\nAgent\n(\nname\n=\n\"English agent\"\n,\ninstructions\n=\n\"You only speak English\"\n,\n)\ntriage_agent\n=\nAgent\n(\nname\n=\n\"Triage agent\"\n,\ninstructions\n=\n\"Handoff to the appropriate agent based on the language of the request.\"\n,\nhandoffs\n=\n[\nspanish_agent\n,\nenglish_agent\n],\n)\nasync\ndef\nmain\n():\nresult\n=\nawait\nRunner\n.\nrun\n(\ntriage_agent\n,\ninput\n=\n\"Hola, ¿cómo estás?\"\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# ¡Hola! Estoy bien, gracias por preguntar. ¿Y tú, cómo estás?\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nFunctions example\nimport\nasyncio\nfrom\nagents\nimport\nAgent\n,\nRunner\n,\nfunction_tool\n@\nfunction_tool\ndef\nget_weather\n(\ncity\n:\nstr\n)\n->\nstr\n:\nreturn\nf\"The weather in\n{\ncity\n}\nis sunny.\"\nagent\n=\nAgent\n(\nname\n=\n\"Hello world\"\n,\ninstructions\n=\n\"You are a helpful agent.\"\n,\ntools\n=\n[\nget_weather\n],\n)\nasync\ndef\nmain\n():\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\ninput\n=\n\"What's the weather in Tokyo?\"\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# The weather in Tokyo is sunny.\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nThe agent loop\nWhen you call\nRunner.run()\n, we run a loop until we get a final output.\nWe call the LLM, using the model and settings on the agent, and the message history.\nThe LLM returns a response, which may include tool calls.\nIf the response has a final output (see below for more on this), we return it and end the loop.\nIf the response has a handoff, we set the agent to the new agent and go back to step 1.\nWe process the tool calls (if any) and append the tool responses messages. Then we go to step 1.\nThere is a\nmax_turns\nparameter that you can use to limit the number of times the loop executes.\nFinal output\nFinal output is the last thing the agent produces in the loop.\nIf you set an\noutput_type\non the agent, the final output is when the LLM returns something of that type. We use\nstructured outputs\nfor this.\nIf there's no\noutput_type\n(i.e. plain text responses), then the first LLM response without any tool calls or handoffs is considered as the final output.\nAs a result, the mental model for the agent loop is:\nIf the current agent has an\noutput_type\n, the loop runs until the agent produces structured output matching that type.\nIf the current agent does not have an\noutput_type\n, the loop runs until the current agent produces a message without any tool calls/handoffs.\nCommon agent patterns\nThe Agents SDK is designed to be highly flexible, allowing you to model a wide range of LLM workflows including deterministic flows, iterative loops, and more. See examples in\nexamples/agent_patterns\n.\nTracing\nThe Agents SDK automatically traces your agent runs, making it easy to track and debug the behavior of your agents. Tracing is extensible by design, supporting custom spans and a wide variety of external destinations, including\nLogfire\n,\nAgentOps\n,\nBraintrust\n,\nScorecard\n, and\nKeywords AI\n. For more details about how to customize or disable tracing, see\nTracing\n, which also includes a larger list of\nexternal tracing processors\n.\nLong running agents & human-in-the-loop\nYou can use the Agents SDK\nTemporal\nintegration to run durable, long-running workflows, including human-in-the-loop tasks. View a demo of Temporal and the Agents SDK working in action to complete long-running tasks\nin this video\n, and\nview docs here\n.\nSessions\nThe Agents SDK provides built-in session memory to automatically maintain conversation history across multiple agent runs, eliminating the need to manually handle\n.to_input_list()\nbetween turns.\nQuick start\nfrom\nagents\nimport\nAgent\n,\nRunner\n,\nSQLiteSession\n# Create agent\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n,\ninstructions\n=\n\"Reply very concisely.\"\n,\n)\n# Create a session instance\nsession\n=\nSQLiteSession\n(\n\"conversation_123\"\n)\n# First turn\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"What city is the Golden Gate Bridge in?\"\n,\nsession\n=\nsession\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# \"San Francisco\"\n# Second turn - agent automatically remembers previous context\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"What state is it in?\"\n,\nsession\n=\nsession\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# \"California\"\n# Also works with synchronous runner\nresult\n=\nRunner\n.\nrun_sync\n(\nagent\n,\n\"What's the population?\"\n,\nsession\n=\nsession\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# \"Approximately 39 million\"\nSession options\nNo memory\n(default): No session memory when session parameter is omitted\nsession: Session = DatabaseSession(...)\n: Use a Session instance to manage conversation history\nfrom\nagents\nimport\nAgent\n,\nRunner\n,\nSQLiteSession\n# SQLite - file-based or in-memory database\nsession\n=\nSQLiteSession\n(\n\"user_123\"\n,\n\"conversations.db\"\n)\n# Redis - for scalable, distributed deployments\n# from agents.extensions.memory import RedisSession\n# session = RedisSession.from_url(\"user_123\", url=\"redis://localhost:6379/0\")\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n)\n# Different session IDs maintain separate conversation histories\nresult1\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"Hello\"\n,\nsession\n=\nsession\n)\nresult2\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"Hello\"\n,\nsession\n=\nSQLiteSession\n(\n\"user_456\"\n,\n\"conversations.db\"\n)\n)\nCustom session implementations\nYou can implement your own session memory by creating a class that follows the\nSession\nprotocol:\nfrom\nagents\n.\nmemory\nimport\nSession\nfrom\ntyping\nimport\nList\nclass\nMyCustomSession\n:\n\"\"\"Custom session implementation following the Session protocol.\"\"\"\ndef\n__init__\n(\nself\n,\nsession_id\n:\nstr\n):\nself\n.\nsession_id\n=\nsession_id\n# Your initialization here\nasync\ndef\nget_items\n(\nself\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n)\n->\nList\n[\ndict\n]:\n# Retrieve conversation history for the session\npass\nasync\ndef\nadd_items\n(\nself\n,\nitems\n:\nList\n[\ndict\n])\n->\nNone\n:\n# Store new items for the session\npass\nasync\ndef\npop_item\n(\nself\n)\n->\ndict\n|\nNone\n:\n# Remove and return the most recent item from the session\npass\nasync\ndef\nclear_session\n(\nself\n)\n->\nNone\n:\n# Clear all items for the session\npass\n# Use your custom session\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n)\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"Hello\"\n,\nsession\n=\nMyCustomSession\n(\n\"my_session\"\n)\n)\nDevelopment (only needed if you need to edit the SDK/examples)\nEnsure you have\nuv\ninstalled.\nuv --version\nInstall dependencies\nmake sync\n(After making changes) lint/test\nmake check # run tests linter and typechecker\nOr to run them individually:\nmake tests  # run tests\nmake mypy   # run typechecker\nmake lint   # run linter\nmake format-check # run style checker\nAcknowledgements\nWe'd like to acknowledge the excellent work of the open-source community, especially:\nPydantic\n(data validation) and\nPydanticAI\n(advanced agent framework)\nLiteLLM\n(unified interface for 100+ LLMs)\nMkDocs\nGriffe\nuv\nand\nruff\nWe're committed to continuing to build the Agents SDK as an open source framework so others in the community can expand on our approach.",
        "今日の獲得スター数: 116",
        "累積スター数: 16,004"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/openai/openai-agents-python"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/openemr/openemr",
      "title": "openemr/openemr",
      "date": null,
      "executive_summary": [
        "The most popular open source electronic health records and medical practice management solution.",
        "---",
        "OpenEMR\nOpenEMR\nis a Free and Open Source electronic health records and medical practice management application. It features fully integrated electronic health records, practice management, scheduling, electronic billing, internationalization, free support, a vibrant community, and a whole lot more. It runs on Windows, Linux, Mac OS X, and many other platforms.\nContributing\nOpenEMR is a leader in healthcare open source software and comprises a large and diverse community of software developers, medical providers and educators with a very healthy mix of both volunteers and professionals.\nJoin us and learn how to start contributing today!\nAlready comfortable with git? Check out\nCONTRIBUTING.md\nfor quick setup instructions and requirements for contributing to OpenEMR by resolving a bug or adding an awesome feature 😊.\nSupport\nCommunity and Professional support can be found\nhere\n.\nExtensive documentation and forums can be found on the\nOpenEMR website\nthat can help you to become more familiar about the project 📖.\nReporting Issues and Bugs\nReport these on the\nIssue Tracker\n. If you are unsure if it is an issue/bug, then always feel free to use the\nForum\nand\nChat\nto discuss about the issue 🪲.\nReporting Security Vulnerabilities\nCheck out\nSECURITY.md\nAPI\nCheck out\nAPI_README.md\nDocker\nCheck out\nDOCKER_README.md\nFHIR\nCheck out\nFHIR_README.md\nFor Developers\nIf using OpenEMR directly from the code repository, then the following commands will build OpenEMR (Node.js version 22.* is required) :\ncomposer install --no-dev\nnpm install\nnpm run build\ncomposer dump-autoload -o\nContributors\nThis project exists thanks to all the people who have contributed.\n[Contribute]\n.\nSponsors\nThanks to our\nONC Certification Major Sponsors\n!\nLicense\nGNU GPL",
        "今日の獲得スター数: 115",
        "累積スター数: 4,282"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/openemr/openemr"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/DioxusLabs/dioxus",
      "title": "DioxusLabs/dioxus",
      "date": null,
      "executive_summary": [
        "Fullstack app framework for web, desktop, and mobile.",
        "---",
        "Website\n|\nExamples\n|\nGuide\n|\n中文\n|\nPT-BR\n|\n日本語\n|\nTürkçe\n|\n한국어\n✨ Dioxus 0.7 is in alpha - test it out! ✨\nBuild for web, desktop, and mobile, and more with a single codebase. Zero-config setup, integrated hot-reloading, and signals-based state management. Add backend functionality with Server Functions and bundle with our CLI.\nfn\napp\n(\n)\n->\nElement\n{\nlet\nmut\ncount =\nuse_signal\n(\n||\n0\n)\n;\nrsx\n!\n{\nh1\n{\n\"High-Five counter: {count}\"\n}\nbutton\n{\nonclick\n:\nmove |_| count +=\n1\n,\n\"Up high!\"\n}\nbutton\n{\nonclick\n:\nmove |_| count -=\n1\n,\n\"Down low!\"\n}\n}\n}\n⭐️ Unique features:\nCross-platform apps in three lines of code (web, desktop, mobile, server, and more)\nErgonomic state management\ncombines the best of React, Solid, and Svelte\nBuilt-in featureful, type-safe, fullstack web framework\nIntegrated bundler for deploying to the web, macOS, Linux, and Windows\nSubsecond Rust hot-patching and asset hot-reloading\nAnd more!\nTake a tour of Dioxus\n.\nInstant hot-reloading\nWith one command,\ndx serve\nand your app is running. Edit your markup, styles, and see changes in milliseconds. Use our experimental\ndx serve --hotpatch\nto update Rust code in real time.\nBuild Beautiful Apps\nDioxus apps are styled with HTML and CSS. Use the built-in TailwindCSS support or load your favorite CSS library. Easily call into native code (objective-c, JNI, Web-Sys) for a perfect native touch.\nTruly fullstack applications\nDioxus deeply integrates with\naxum\nto provide powerful fullstack capabilities for both clients and servers. Pick from a wide array of built-in batteries like WebSockets, SSE, Streaming, File Upload/Download, Server-Side-Rendering, Forms, Middleware, and Hot-Reload, or go fully custom and integrate your existing axum backend.\nExperimental Native Renderer\nRender using web-sys, webview, server-side-rendering, liveview, or even with our experimental WGPU-based renderer. Embed Dioxus in Bevy, WGPU, or even run on embedded Linux!\nFirst-party primitive components\nGet started quickly with a complete set of primitives modeled after shadcn/ui and Radix-Primitives.\nFirst-class Android and iOS support\nDioxus is the fastest way to build native mobile apps with Rust. Simply run\ndx serve --platform android\nand your app is running in an emulator or on device in seconds. Call directly into JNI and Native APIs.\nBundle for web, desktop, and mobile\nSimply run\ndx bundle\nand your app will be built and bundled with maximization optimizations. On the web, take advantage of\n.avif\ngeneration,\n.wasm\ncompression, minification\n, and more. Build WebApps weighing\nless than 50kb\nand desktop/mobile apps less than 5mb.\nFantastic documentation\nWe've put a ton of effort into building clean, readable, and comprehensive documentation. All html elements and listeners are documented with MDN docs, and our Docs runs continuous integration with Dioxus itself to ensure that the docs are always up to date. Check out the\nDioxus website\nfor guides, references, recipes, and more. Fun fact: we use the Dioxus website as a testbed for new Dioxus features -\ncheck it out!\nModular and Customizable\nBuild your own renderer, or use a community renderer like\nFreya\n. Use our modular components like RSX, VirtualDom, Blitz, Taffy, and Subsecond.\nCommunity\nDioxus is a community-driven project, with a very active\nDiscord\nand\nGitHub\ncommunity. We're always looking for help, and we're happy to answer questions and help you get started.\nOur SDK\nis community-run and we even have a\nGitHub organization\nfor the best Dioxus crates that receive free upgrades and support.\nFull-time core team\nDioxus has grown from a side project to a small team of fulltime engineers. Thanks to the generous support of FutureWei, Satellite.im, the GitHub Accelerator program, we're able to work on Dioxus full-time. Our long term goal is for Dioxus to become self-sustaining by providing paid high-quality enterprise tools. If your company is interested in adopting Dioxus and would like to work with us, please reach out!\nSupported Platforms\nWeb\nRender directly to the DOM using WebAssembly\nPre-render with SSR and rehydrate on the client\nSimple \"hello world\" at about 50kb, comparable to React\nBuilt-in dev server and hot reloading for quick iteration\nDesktop\nRender using Webview or - experimentally - with WGPU or\nFreya\n(Skia)\nZero-config setup. Simply `cargo run` or `dx serve` to build your app\nFull support for native system access without IPC\nSupports macOS, Linux, and Windows. Portable <3mb binaries\nMobile\nRender using Webview or - experimentally - with WGPU or Skia\nBuild .ipa and .apk files for iOS and Android\nCall directly into Java and Objective-C with minimal overhead\nFrom \"hello world\" to running on device in seconds\nServer-side Rendering\nSuspense, hydration, and server-side rendering\nQuickly drop in backend functionality with server functions\nExtractors, middleware, and routing integrations\nStatic-site generation and incremental regeneration\nRunning the examples\nThe examples in the main branch of this repository target the git version of dioxus and the CLI. If you are looking for examples that work with the latest stable release of dioxus, check out the\n0.6 branch\n.\nThe examples in the top level of this repository can be run with:\ncargo run --example\n<\nexample\n>\nHowever, we encourage you to download the dioxus-cli to test out features like hot-reloading. To install the most recent binary CLI, you can use cargo binstall.\ncargo binstall dioxus-cli@0.7.0-rc.1 --force\nIf this CLI is out-of-date, you can install it directly from git\ncargo install --git https://github.com/DioxusLabs/dioxus dioxus-cli --locked\nWith the CLI, you can also run examples with the web platform. You will need to disable the default desktop feature and enable the web feature with this command:\ndx serve --example\n<\nexample\n>\n--platform web -- --no-default-features\nContributing\nCheck out the website\nsection on contributing\n.\nReport issues on our\nissue tracker\n.\nJoin\nthe discord and ask questions!\nLicense\nThis project is licensed under either the\nMIT license\nor the\nApache-2 License\n.\nUnless you explicitly state otherwise, any contribution intentionally submitted\nfor inclusion in Dioxus by you, shall be licensed as MIT or Apache-2, without any additional\nterms or conditions.",
        "今日の獲得スター数: 110",
        "累積スター数: 30,962"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/DioxusLabs/dioxus"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/open-webui/open-webui",
      "title": "open-webui/open-webui",
      "date": null,
      "executive_summary": [
        "User-friendly AI Interface (Supports Ollama, OpenAI API, ...)",
        "---",
        "Open WebUI 👋\nOpen WebUI is an\nextensible\n, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.\nIt supports various LLM runners like\nOllama\nand\nOpenAI-compatible APIs\n, with\nbuilt-in inference engine\nfor RAG, making it a\npowerful AI deployment solution\n.\nPassionate about open-source AI?\nJoin our team →\nTip\nLooking for an\nEnterprise Plan\n?\n–\nSpeak with Our Sales Team Today!\nGet\nenhanced capabilities\n, including\ncustom theming and branding\n,\nService Level Agreement (SLA) support\n,\nLong-Term Support (LTS) versions\n, and\nmore!\nFor more information, be sure to check out our\nOpen WebUI Documentation\n.\nKey Features of Open WebUI ⭐\n🚀\nEffortless Setup\n: Install seamlessly using Docker or Kubernetes (kubectl, kustomize or helm) for a hassle-free experience with support for both\n:ollama\nand\n:cuda\ntagged images.\n🤝\nOllama/OpenAI API Integration\n: Effortlessly integrate OpenAI-compatible APIs for versatile conversations alongside Ollama models. Customize the OpenAI API URL to link with\nLMStudio, GroqCloud, Mistral, OpenRouter, and more\n.\n🛡️\nGranular Permissions and User Groups\n: By allowing administrators to create detailed user roles and permissions, we ensure a secure user environment. This granularity not only enhances security but also allows for customized user experiences, fostering a sense of ownership and responsibility amongst users.\n🔄\nSCIM 2.0 Support\n: Enterprise-grade user and group provisioning through SCIM 2.0 protocol, enabling seamless integration with identity providers like Okta, Azure AD, and Google Workspace for automated user lifecycle management.\n📱\nResponsive Design\n: Enjoy a seamless experience across Desktop PC, Laptop, and Mobile devices.\n📱\nProgressive Web App (PWA) for Mobile\n: Enjoy a native app-like experience on your mobile device with our PWA, providing offline access on localhost and a seamless user interface.\n✒️🔢\nFull Markdown and LaTeX Support\n: Elevate your LLM experience with comprehensive Markdown and LaTeX capabilities for enriched interaction.\n🎤📹\nHands-Free Voice/Video Call\n: Experience seamless communication with integrated hands-free voice and video call features, allowing for a more dynamic and interactive chat environment.\n🛠️\nModel Builder\n: Easily create Ollama models via the Web UI. Create and add custom characters/agents, customize chat elements, and import models effortlessly through\nOpen WebUI Community\nintegration.\n🐍\nNative Python Function Calling Tool\n: Enhance your LLMs with built-in code editor support in the tools workspace. Bring Your Own Function (BYOF) by simply adding your pure Python functions, enabling seamless integration with LLMs.\n📚\nLocal RAG Integration\n: Dive into the future of chat interactions with groundbreaking Retrieval Augmented Generation (RAG) support. This feature seamlessly integrates document interactions into your chat experience. You can load documents directly into the chat or add files to your document library, effortlessly accessing them using the\n#\ncommand before a query.\n🔍\nWeb Search for RAG\n: Perform web searches using providers like\nSearXNG\n,\nGoogle PSE\n,\nBrave Search\n,\nserpstack\n,\nserper\n,\nSerply\n,\nDuckDuckGo\n,\nTavilySearch\n,\nSearchApi\nand\nBing\nand inject the results directly into your chat experience.\n🌐\nWeb Browsing Capability\n: Seamlessly integrate websites into your chat experience using the\n#\ncommand followed by a URL. This feature allows you to incorporate web content directly into your conversations, enhancing the richness and depth of your interactions.\n🎨\nImage Generation Integration\n: Seamlessly incorporate image generation capabilities using options such as AUTOMATIC1111 API or ComfyUI (local), and OpenAI's DALL-E (external), enriching your chat experience with dynamic visual content.\n⚙️\nMany Models Conversations\n: Effortlessly engage with various models simultaneously, harnessing their unique strengths for optimal responses. Enhance your experience by leveraging a diverse set of models in parallel.\n🔐\nRole-Based Access Control (RBAC)\n: Ensure secure access with restricted permissions; only authorized individuals can access your Ollama, and exclusive model creation/pulling rights are reserved for administrators.\n🌐🌍\nMultilingual Support\n: Experience Open WebUI in your preferred language with our internationalization (i18n) support. Join us in expanding our supported languages! We're actively seeking contributors!\n🧩\nPipelines, Open WebUI Plugin Support\n: Seamlessly integrate custom logic and Python libraries into Open WebUI using\nPipelines Plugin Framework\n. Launch your Pipelines instance, set the OpenAI URL to the Pipelines URL, and explore endless possibilities.\nExamples\ninclude\nFunction Calling\n, User\nRate Limiting\nto control access,\nUsage Monitoring\nwith tools like Langfuse,\nLive Translation with LibreTranslate\nfor multilingual support,\nToxic Message Filtering\nand much more.\n🌟\nContinuous Updates\n: We are committed to improving Open WebUI with regular updates, fixes, and new features.\nWant to learn more about Open WebUI's features? Check out our\nOpen WebUI documentation\nfor a comprehensive overview!\nSponsors 🙌\nEmerald\nTailscale\n• Connect self-hosted AI to any device with Tailscale\nWarp\n• The intelligent terminal for developers\nWe are incredibly grateful for the generous support of our sponsors. Their contributions help us to maintain and improve our project, ensuring we can continue to deliver quality work to our community. Thank you!\nHow to Install 🚀\nInstallation via Python pip 🐍\nOpen WebUI can be installed using pip, the Python package installer. Before proceeding, ensure you're using\nPython 3.11\nto avoid compatibility issues.\nInstall Open WebUI\n:\nOpen your terminal and run the following command to install Open WebUI:\npip install open-webui\nRunning Open WebUI\n:\nAfter installation, you can start Open WebUI by executing:\nopen-webui serve\nThis will start the Open WebUI server, which you can access at\nhttp://localhost:8080\nQuick Start with Docker 🐳\nNote\nPlease note that for certain Docker environments, additional configurations might be needed. If you encounter any connection issues, our detailed guide on\nOpen WebUI Documentation\nis ready to assist you.\nWarning\nWhen using Docker to install Open WebUI, make sure to include the\n-v open-webui:/app/backend/data\nin your Docker command. This step is crucial as it ensures your database is properly mounted and prevents any loss of data.\nTip\nIf you wish to utilize Open WebUI with Ollama included or CUDA acceleration, we recommend utilizing our official images tagged with either\n:cuda\nor\n:ollama\n. To enable CUDA, you must install the\nNvidia CUDA container toolkit\non your Linux/WSL system.\nInstallation with Default Configuration\nIf Ollama is on your computer\n, use this command:\ndocker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\nIf Ollama is on a Different Server\n, use this command:\nTo connect to Ollama on another server, change the\nOLLAMA_BASE_URL\nto the server's URL:\ndocker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\nTo run Open WebUI with Nvidia GPU support\n, use this command:\ndocker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda\nInstallation for OpenAI API Usage Only\nIf you're only using OpenAI API\n, use this command:\ndocker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\nInstalling Open WebUI with Bundled Ollama Support\nThis installation method uses a single container image that bundles Open WebUI with Ollama, allowing for a streamlined setup via a single command. Choose the appropriate command based on your hardware setup:\nWith GPU Support\n:\nUtilize GPU resources by running the following command:\ndocker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\nFor CPU Only\n:\nIf you're not using a GPU, use this command instead:\ndocker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\nBoth commands facilitate a built-in, hassle-free installation of both Open WebUI and Ollama, ensuring that you can get everything up and running swiftly.\nAfter installation, you can access Open WebUI at\nhttp://localhost:3000\n. Enjoy! 😄\nOther Installation Methods\nWe offer various installation alternatives, including non-Docker native installation methods, Docker Compose, Kustomize, and Helm. Visit our\nOpen WebUI Documentation\nor join our\nDiscord community\nfor comprehensive guidance.\nLook at the\nLocal Development Guide\nfor instructions on setting up a local development environment.\nTroubleshooting\nEncountering connection issues? Our\nOpen WebUI Documentation\nhas got you covered. For further assistance and to join our vibrant community, visit the\nOpen WebUI Discord\n.\nOpen WebUI: Server Connection Error\nIf you're experiencing connection issues, it’s often due to the WebUI docker container not being able to reach the Ollama server at 127.0.0.1:11434 (host.docker.internal:11434) inside the container . Use the\n--network=host\nflag in your docker command to resolve this. Note that the port changes from 3000 to 8080, resulting in the link:\nhttp://localhost:8080\n.\nExample Docker Command\n:\ndocker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main\nKeeping Your Docker Installation Up-to-Date\nIn case you want to update your local Docker installation to the latest version, you can do it with\nWatchtower\n:\ndocker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui\nIn the last part of the command, replace\nopen-webui\nwith your container name if it is different.\nCheck our Updating Guide available in our\nOpen WebUI Documentation\n.\nUsing the Dev Branch 🌙\nWarning\nThe\n:dev\nbranch contains the latest unstable features and changes. Use it at your own risk as it may have bugs or incomplete features.\nIf you want to try out the latest bleeding-edge features and are okay with occasional instability, you can use the\n:dev\ntag like this:\ndocker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev\nOffline Mode\nIf you are running Open WebUI in an offline environment, you can set the\nHF_HUB_OFFLINE\nenvironment variable to\n1\nto prevent attempts to download models from the internet.\nexport\nHF_HUB_OFFLINE=1\nWhat's Next? 🌟\nDiscover upcoming features on our roadmap in the\nOpen WebUI Documentation\n.\nLicense 📜\nThis project contains code under multiple licenses. The current codebase includes components licensed under the Open WebUI License with an additional requirement to preserve the \"Open WebUI\" branding, as well as prior contributions under their respective original licenses. For a detailed record of license changes and the applicable terms for each section of the code, please refer to\nLICENSE_HISTORY\n. For complete and updated licensing details, please see the\nLICENSE\nand\nLICENSE_HISTORY\nfiles.\nSupport 💬\nIf you have any questions, suggestions, or need assistance, please open an issue or join our\nOpen WebUI Discord community\nto connect with us! 🤝\nStar History\nCreated by\nTimothy Jaeryang Baek\n- Let's make Open WebUI even more amazing together! 💪",
        "今日の獲得スター数: 106",
        "累積スター数: 111,984"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/open-webui/open-webui"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/78/xiaozhi-esp32",
      "title": "78/xiaozhi-esp32",
      "date": null,
      "executive_summary": [
        "An MCP-based chatbot | 一个基于MCP的聊天机器人",
        "---",
        "An MCP-based Chatbot\n（中文 |\nEnglish\n|\n日本語\n）\n介绍\n👉\n人类：给 AI 装摄像头 vs AI：当场发现主人三天没洗头【bilibili】\n👉\n手工打造你的 AI 女友，新手入门教程【bilibili】\n小智 AI 聊天机器人作为一个语音交互入口，利用 Qwen / DeepSeek 等大模型的 AI 能力，通过 MCP 协议实现多端控制。\n版本说明\n当前 v2 版本与 v1 版本分区表不兼容，所以无法从 v1 版本通过 OTA 升级到 v2 版本。分区表说明参见\npartitions/v2/README.md\n。\n使用 v1 版本的所有硬件，可以通过手动烧录固件来升级到 v2 版本。\nv1 的稳定版本为 1.9.2，可以通过\ngit checkout v1\n来切换到 v1 版本，该分支会持续维护到 2026 年 2 月。\n已实现功能\nWi-Fi / ML307 Cat.1 4G\n离线语音唤醒\nESP-SR\n支持两种通信协议（\nWebsocket\n或 MQTT+UDP）\n采用 OPUS 音频编解码\n基于流式 ASR + LLM + TTS 架构的语音交互\n声纹识别，识别当前说话人的身份\n3D Speaker\nOLED / LCD 显示屏，支持表情显示\n电量显示与电源管理\n支持多语言（中文、英文、日文）\n支持 ESP32-C3、ESP32-S3、ESP32-P4 芯片平台\n通过设备端 MCP 实现设备控制（音量、灯光、电机、GPIO 等）\n通过云端 MCP 扩展大模型能力（智能家居控制、PC桌面操作、知识搜索、邮件收发等）\n自定义唤醒词、字体、表情与聊天背景，支持网页端在线修改 (\n自定义Assets生成器\n)\n硬件\n面包板手工制作实践\n详见飞书文档教程：\n👉\n《小智 AI 聊天机器人百科全书》\n面包板效果图如下：\n支持 70 多个开源硬件（仅展示部分）\n立创·实战派 ESP32-S3 开发板\n乐鑫 ESP32-S3-BOX3\nM5Stack CoreS3\nM5Stack AtomS3R + Echo Base\n神奇按钮 2.4\n微雪电子 ESP32-S3-Touch-AMOLED-1.8\nLILYGO T-Circle-S3\n虾哥 Mini C3\n璀璨·AI 吊坠\n无名科技 Nologo-星智-1.54TFT\nSenseCAP Watcher\nESP-HI 超低成本机器狗\n软件\n固件烧录\n新手第一次操作建议先不要搭建开发环境，直接使用免开发环境烧录的固件。\n固件默认接入\nxiaozhi.me\n官方服务器，个人用户注册账号可以免费使用 Qwen 实时模型。\n👉\n新手烧录固件教程\n开发环境\nCursor 或 VSCode\n安装 ESP-IDF 插件，选择 SDK 版本 5.4 或以上\nLinux 比 Windows 更好，编译速度快，也免去驱动问题的困扰\n本项目使用 Google C++ 代码风格，提交代码时请确保符合规范\n开发者文档\n自定义开发板指南\n- 学习如何为小智 AI 创建自定义开发板\nMCP 协议物联网控制用法说明\n- 了解如何通过 MCP 协议控制物联网设备\nMCP 协议交互流程\n- 设备端 MCP 协议的实现方式\nMQTT + UDP 混合通信协议文档\n一份详细的 WebSocket 通信协议文档\n大模型配置\n如果你已经拥有一个小智 AI 聊天机器人设备，并且已接入官方服务器，可以登录\nxiaozhi.me\n控制台进行配置。\n👉\n后台操作视频教程（旧版界面）\n相关开源项目\n在个人电脑上部署服务器，可以参考以下第三方开源的项目：\nxinnan-tech/xiaozhi-esp32-server\nPython 服务器\njoey-zhou/xiaozhi-esp32-server-java\nJava 服务器\nAnimeAIChat/xiaozhi-server-go\nGolang 服务器\n使用小智通信协议的第三方客户端项目：\nhuangjunsen0406/py-xiaozhi\nPython 客户端\nTOM88812/xiaozhi-android-client\nAndroid 客户端\n100askTeam/xiaozhi-linux\n百问科技提供的 Linux 客户端\n78/xiaozhi-sf32\n思澈科技的蓝牙芯片固件\nQuecPython/solution-xiaozhiAI\n移远提供的 QuecPython 固件\n关于项目\n这是一个由虾哥开源的 ESP32 项目，以 MIT 许可证发布，允许任何人免费使用，修改或用于商业用途。\n我们希望通过这个项目，能够帮助大家了解 AI 硬件开发，将当下飞速发展的大语言模型应用到实际的硬件设备中。\n如果你有任何想法或建议，请随时提出 Issues 或加入 QQ 群：1011329060\nStar History",
        "今日の獲得スター数: 102",
        "累積スター数: 19,237"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/78/xiaozhi-esp32"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/FlowiseAI/Flowise",
      "title": "FlowiseAI/Flowise",
      "date": null,
      "executive_summary": [
        "Build AI Agents, Visually",
        "---",
        "English |\n繁體中文\n|\n简体中文\n|\n日本語\n|\n한국어\nBuild AI Agents, Visually\n📚 Table of Contents\n⚡ Quick Start\n🐳 Docker\n👨‍💻 Developers\n🌱 Env Variables\n📖 Documentation\n🌐 Self Host\n☁️ Flowise Cloud\n🙋 Support\n🙌 Contributing\n📄 License\n⚡Quick Start\nDownload and Install\nNodeJS\n>= 18.15.0\nInstall Flowise\nnpm install -g flowise\nStart Flowise\nnpx flowise start\nOpen\nhttp://localhost:3000\n🐳 Docker\nDocker Compose\nClone the Flowise project\nGo to\ndocker\nfolder at the root of the project\nCopy\n.env.example\nfile, paste it into the same location, and rename to\n.env\nfile\ndocker compose up -d\nOpen\nhttp://localhost:3000\nYou can bring the containers down by\ndocker compose stop\nDocker Image\nBuild the image locally:\ndocker build --no-cache -t flowise\n.\nRun image:\ndocker run -d --name flowise -p 3000:3000 flowise\nStop image:\ndocker stop flowise\n👨‍💻 Developers\nFlowise has 3 different modules in a single mono repository.\nserver\n: Node backend to serve API logics\nui\n: React frontend\ncomponents\n: Third-party nodes integrations\napi-documentation\n: Auto-generated swagger-ui API docs from express\nPrerequisite\nInstall\nPNPM\nnpm i -g pnpm\nSetup\nClone the repository:\ngit clone https://github.com/FlowiseAI/Flowise.git\nGo into repository folder:\ncd\nFlowise\nInstall all dependencies of all modules:\npnpm install\nBuild all the code:\npnpm build\nExit code 134 (JavaScript heap out of memory)\nIf you get this error when running the above `build` script, try increasing the Node.js heap size and run the script again:\n#\nmacOS / Linux / Git Bash\nexport\nNODE_OPTIONS=\n\"\n--max-old-space-size=4096\n\"\n#\nWindows PowerShell\n$env\n:NODE_OPTIONS=\n\"\n--max-old-space-size=4096\n\"\n#\nWindows CMD\nset\nNODE_OPTIONS=--max-old-space-size=4096\nThen run:\npnpm build\nStart the app:\npnpm start\nYou can now access the app on\nhttp://localhost:3000\nFor development build:\nCreate\n.env\nfile and specify the\nVITE_PORT\n(refer to\n.env.example\n) in\npackages/ui\nCreate\n.env\nfile and specify the\nPORT\n(refer to\n.env.example\n) in\npackages/server\nRun:\npnpm dev\nAny code changes will reload the app automatically on\nhttp://localhost:8080\n🌱 Env Variables\nFlowise supports different environment variables to configure your instance. You can specify the following variables in the\n.env\nfile inside\npackages/server\nfolder. Read\nmore\n📖 Documentation\nYou can view the Flowise Docs\nhere\n🌐 Self Host\nDeploy Flowise self-hosted in your existing infrastructure, we support various\ndeployments\nAWS\nAzure\nDigital Ocean\nGCP\nAlibaba Cloud\nOthers\nRailway\nRender\nHuggingFace Spaces\nElestio\nSealos\nRepoCloud\n☁️ Flowise Cloud\nGet Started with\nFlowise Cloud\n.\n🙋 Support\nFeel free to ask any questions, raise problems, and request new features in\nDiscussion\n.\n🙌 Contributing\nThanks go to these awesome contributors\nSee\nContributing Guide\n. Reach out to us at\nDiscord\nif you have any questions or issues.\n📄 License\nSource code in this repository is made available under the\nApache License Version 2.0\n.",
        "今日の獲得スター数: 98",
        "累積スター数: 45,125"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/FlowiseAI/Flowise"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/PixelGuys/Cubyz",
      "title": "PixelGuys/Cubyz",
      "date": null,
      "executive_summary": [
        "Voxel sandbox game with a large render distance, procedurally generated content and some cool graphical effects.",
        "---",
        "Cubyz\nCubyz is a 3D voxel sandbox game (inspired by Minecraft).\nCubyz has a bunch of interesting/unique features such as:\nLevel of Detail (→ This enables far view distances.)\n3D Chunks (→ There is no height or depth limit.)\nProcedural Crafting (→ You can craft anything you want, and the game will figure out what kind of tool you tried to make.)\nAbout\nCubyz is written in\nZig\n, a rather small language with some cool features and a focus on readability.\nWindows and Linux are supported. Mac is not supported, as it does not have OpenGL 4.3.\nCheck out the\nDiscord server\nfor more information and announcements.\nThere are also some devlogs on\nYouTube\n.\nHistory\nUntil recently (the Zig rewrite was started in August 2022) Cubyz was written in Java. You can still see the code in the\nCubyz-Java\nrepository and play it using the\nJava Launcher\n.\n// TODO: Move this over to a separate repository\nOriginally Cubyz was created on August 22, 2018 by\nzenith391\nand\nZaUserA\n. Back then, it was called \"Cubz\".\nHowever, both of them lost interest at some point, and now Cubyz is maintained by\nIntegratedQuantum\n.\nRun Cubyz\nThis section is about compiling a dev version, if you just want a precompiled version, go to\nreleases\nThe Easy Way (no tools needed)\nDownload the latest\nsource code\nExtract the zip file\nGo into the extraced folder and double click the\nrun_linux.sh\nor\nrun_windows.bat\ndepending on your operating system.\nCongratulations: You just compiled your first program!\nIt doesn't work?\nIf it doesn't work and keeps running for more than 10 minutes without doing anything it can help to kill and restart the process. A few people seem to experience this, and I have not found the cause. It might also help to delete the\nzig-cache\nfolder.\nIf you see an error message in the terminal, please report it in the\nIssues\ntab or on the\nDiscord server\n.\nOtherwise you can always ask for help on the Discord server. If you are unable to get it compiling on your machine, you can also ask on the Discord server and we may compile a release for you.\nNote for Linux Users:\nI also had to install a few\n-dev\npackages for the compilation to work:\nsudo apt install libgl-dev libasound2-dev libx11-dev libxcursor-dev libxrandr-dev libxinerama-dev libxext-dev libxi-dev\nThe Better Way\nInstall Git\nClone this repository\ngit clone https://github.com/pixelguys/Cubyz\nRun\nrun_linux.sh\nor\nrun_windows.bat\n, if you already have Zig installed on your computer (it must be a compatible version) you can also just use\nzig build run\nWhen you want to update your local version you can use\ngit pull\n. This keeps everything in one place, avoiding repeatedly downloading the compiler on every update.\nContributing\nCode\nCheck out the\nContributing Guidelines\nGameplay Additions\nCheck out the\nGame Design Principles\nTextures\nIf you want to add new textures, make sure they fit the style of the game. It's recommended that you have baseline skills in pixel art before attempting to make textures. A great collection of tutorials can be found\nhere\nIf any of the following points are ignored, your texture will be rejected:\nResolution is 16 x 16\nLighting direction is top-left for items and blocks.\nKeep colour palettes small. Do not use near-duplicate colours, do not use noise, filters, or brushes that create unnecessary amounts of colours. Most blocks can be textured with ~4-6 colours.\nReference other block textures to see how colours & contrast is used. Test your textures ingame alongside other blocks.\nBlocks should tile smoothly. Avoid creating seams or repetitive patterns.\nUse hue shifting conservatively. Take the material into account when choosing colours.\nItems have full, coloured, 1-pixel outlines. It should be shaded so that the side in light (top left) is brighter, while the side in shadow (bottom right) is darker.\nItems should have higher contrast than their block counterparts.\nYour texture may be edited or replaced to ensure a consistent art style throughout the game.\nFor further information, ask\ncareeoki\non\nDiscord\n. She has made a majority of the art for Cubyz.",
        "今日の獲得スター数: 89",
        "累積スター数: 1,150"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/PixelGuys/Cubyz"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/chen08209/FlClash",
      "title": "chen08209/FlClash",
      "date": null,
      "executive_summary": [
        "A multi-platform proxy client based on ClashMeta,simple and easy to use, open-source and ad-free.",
        "---",
        "简体中文\nFlClash\nA multi-platform proxy client based on ClashMeta, simple and easy to use, open-source and ad-free.\non Desktop:\non Mobile:\nFeatures\n✈️\nMulti-platform: Android, Windows, macOS and Linux\n💻 Adaptive multiple screen sizes, Multiple color themes available\n💡 Based on Material You Design,\nSurfboard\n-like UI\n☁️ Supports data sync via WebDAV\n✨ Support subscription link, Dark mode\nUse\nLinux\n⚠️\nMake sure to install the following dependencies before using them\nsudo apt-get install libayatana-appindicator3-dev\n sudo apt-get install libkeybinder-3.0-dev\nAndroid\nSupport the following actions\ncom.follow.clash.action.START\n \n com.follow.clash.action.STOP\n \n com.follow.clash.action.TOGGLE\nDownload\nBuild\nUpdate submodules\ngit submodule update --init --recursive\nInstall\nFlutter\nand\nGolang\nenvironment\nBuild Application\nandroid\nInstall\nAndroid SDK\n,\nAndroid NDK\nSet\nANDROID_NDK\nenvironment variables\nRun Build script\ndart .\n\\s\netup.dart android\nwindows\nYou need a windows client\nInstall\nGcc\n，\nInno Setup\nRun build script\ndart .\n\\s\netup.dart windows --arch\n<\narm64\n|\namd\n64>\nlinux\nYou need a linux client\nRun build script\ndart .\n\\s\netup.dart linux --arch\n<\narm64\n|\namd\n64>\nmacOS\nYou need a macOS client\nRun build script\ndart .\n\\s\netup.dart macos --arch\n<\narm64\n|\namd\n64>\nStar\nThe easiest way to support developers is to click on the star (⭐) at the top of the page.",
        "今日の獲得スター数: 72",
        "累積スター数: 22,984"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/chen08209/FlClash"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/WECENG/ticket-purchase",
      "title": "WECENG/ticket-purchase",
      "date": null,
      "executive_summary": [
        "大麦自动抢票，支持人员、城市、日期场次、价格选择",
        "---",
        "大麦抢票脚本 V1.0\n特征\n自动无延时抢票\n支持人员、城市、日期场次、价格选择\n功能介绍\n通过selenium打开页面进行登录，模拟用户购票流程自动购票\n其流程图如下:\n准备工作\n1. 配置环境\n1.1安装python3环境\nWindows\n访问Python官方网站：\nhttps://www.python.org/downloads/windows/\n下载最新的Python 3.9+版本的安装程序。\n运行安装程序。\n在安装程序中，确保勾选 \"Add Python X.X to PATH\" 选项，这将自动将Python添加到系统环境变量中，方便在命令行中使用Python。\n完成安装后，你可以在命令提示符或PowerShell中输入\npython3\n来启动Python解释器。\nmacOS\n你可以使用Homebrew来安装Python 3。\n安装Homebrew（如果未安装）：打开终端并运行以下命令：\n/bin/bash -c\n\"\n$(\ncurl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh\n)\n\"\n安装Python 3：运行以下命令来安装Python 3：\nbrew install python@3\n1.2 安装所需要的环境\n在命令窗口输入如下指令\npip3 install selenium\n1.3 下载google chrome浏览器\n下载地址:\nhttps://www.google.cn/intl/zh-CN/chrome/?brand=YTUH&gclid=Cj0KCQjwj5mpBhDJARIsAOVjBdoV_1sBwdqKGHV3rUU1vJmNKZdy5QNzbRT8F5O0-_jq1WHXurE8a7MaAkWrEALw_wcB&gclsrc=aw.ds\n2. 修改配置文件\n在运行程序之前，需要先修改\nconfig.json\n文件。该文件用于指定用户需要抢票的相关信息，包括演唱会的场次、观演的人员、城市、日期、价格等。文件结果如下图所示：\n2.1 文件内容说明\nindex_url\n为大麦网的地址，\n无需修改\nlogin_url\n为大麦网的登录地址，\n无需修改\ntarget_url\n为用户需要抢的演唱会票的目标地址，\n待修改\nusers\n为观演人的姓名，\n观演人需要用户在手机大麦APP中先填写好，然后再填入该配置文件中\n，\n待修改\ncity\n为城市，\n如果用户需要抢的演唱会票需要选择城市，请把城市填入此处。如无需选择，则不填\ndate\n为场次日期，\n待修改，可多选\nprice\n为票档的价格，\n待修改，可多选\nif_commit_order\n为是否要自动提交订单，\n改成 true\nif_listen为是否回流监听，\n改成true\n2.2 示例说明\n进入大麦网\nhttps://www.damai.cn/，选择你需要抢票的演唱会。假设如下图所示：\n接下来按照下图的标注对配置文件进行修改：\n最终\nconfig.json\n的文件内容如下：\n{\n\"index_url\"\n:\n\"\nhttps://www.damai.cn/\n\"\n,\n\"login_url\"\n:\n\"\nhttps://passport.damai.cn/login?ru=https%3A%2F%2Fwww.damai.cn%2F\n\"\n,\n\"target_url\"\n:\n\"\nhttps://detail.damai.cn/item.htm?spm=a2oeg.home.card_0.ditem_1.591b23e1JQGWHg&id=740680932762\n\"\n,\n\"users\"\n: [\n\"\n名字1\n\"\n,\n\"\n名字2\n\"\n],\n\"city\"\n:\n\"\n广州\n\"\n,\n\"date\"\n:\n\"\n2023-10-28\n\"\n,\n\"price\"\n:\n\"\n1039\n\"\n,\n\"if_listen\"\n:\ntrue\n,\n\"if_commit_order\"\n:\ntrue\n}\n3.运行程序\n运行程序开始抢票，进入命令窗口，执行如下命令：\ncd\ndamai\npython3 damai.py\n大麦app抢票\n大麦app抢票脚本需要依赖appium，因此需要现在安装appium server&client环境，步骤如下：\nappium server\n下载\n先安装好node环境（具备npm）node版本号18.0.0\n先下载并安装好android sdk，并配置环境变量（appium server运行需依赖android sdk)\n下载appium\nnpm install -g appium\n查看appium是否安装成功\nappium -v\n下载UiAutomator2驱动\nnpm install appium-uiautomator2-driver\n​\t\t可能会遇到如下错误：\n➜  xcode git:(master) ✗ npm install appium-uiautomator2-driver\n\nnpm ERR! code 1\nnpm ERR! path /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/appium-chromedriver\nnpm ERR! command failed\nnpm ERR! command sh -c node install-npm.js\nnpm ERR! [11:57:54] Error installing Chromedriver: Request failed with status code 404\nnpm ERR! [11:57:54] AxiosError: Request failed with status code 404\nnpm ERR!     at settle (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/core/settle.js:19:12)\nnpm ERR!     at IncomingMessage.handleStreamEnd (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/adapters/http.js:572:11)\nnpm ERR!     at IncomingMessage.emit (node:events:539:35)\nnpm ERR!     at endReadableNT (node:internal/streams/readable:1344:12)\nnpm ERR!     at processTicksAndRejections (node:internal/process/task_queues:82:21)\nnpm ERR! [11:57:54] Downloading Chromedriver can be skipped by setting the'APPIUM_SKIP_CHROMEDRIVER_INSTALL' environment variable.\n\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     /Users/chenweicheng/.npm/_logs/2023-10-26T03_57_35_950Z-debug-0.log\n​\t\t解决办法（添加环境变量，错误原因是没有找到chrome浏览器驱动，忽略即可）\nexport\nAPPIUM_SKIP_CHROMEDRIVER_INSTALL=true\n启动\n启动appium server并使用uiautomator2驱动\nappium --use-plugins uiautomator2\n启动成功将出现如下信息：\n[Appium] Welcome to Appium v2.2.1 (REV 2176894a5be5da17a362bf3f20678641a78f4b69)\n[Appium] Non-default server args:\n[Appium] {\n[Appium]   usePlugins: [\n[Appium]     'uiautomator2'\n[Appium]   ]\n[Appium] }\n[Appium] Attempting to load driver uiautomator2...\n[Appium] Requiring driver at /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver\n[Appium] Appium REST http interface listener started on http://0.0.0.0:4723\n[Appium] You can provide the following URLs in your client code to connect to this server:\n[Appium] \thttp://127.0.0.1:4723/ (only accessible from the same host)\n[Appium] \thttp://172.31.102.45:4723/\n[Appium] \thttp://198.18.0.1:4723/\n[Appium] Available drivers:\n[Appium]   - uiautomator2@2.32.3 (automationName 'UiAutomator2')\n[Appium] No plugins have been installed. Use the \"appium plugin\" command to install the one(s) you want to use.\n其中\n[Appium] \thttp://127.0.0.1:4723/ (only accessible from the same host) [Appium] \thttp://172.31.102.45:4723/ [Appium] \thttp://198.18.0.1:4723/\n为appium server连接地址\nappium client\n先下载并安装好python3和pip3\n安装\npip3 install appium-python-client\n在代码中引入并使用appium\nfrom\nappium\nimport\nwebdriver\nfrom\nappium\n.\noptions\n.\ncommon\n.\nbase\nimport\nAppiumOptions\ndevice_app_info\n=\nAppiumOptions\n()\ndevice_app_info\n.\nset_capability\n(\n'platformName'\n,\n'Android'\n)\ndevice_app_info\n.\nset_capability\n(\n'platformVersion'\n,\n'10'\n)\ndevice_app_info\n.\nset_capability\n(\n'deviceName'\n,\n'YourDeviceName'\n)\ndevice_app_info\n.\nset_capability\n(\n'appPackage'\n,\n'cn.damai'\n)\ndevice_app_info\n.\nset_capability\n(\n'appActivity'\n,\n'.launcher.splash.SplashMainActivity'\n)\ndevice_app_info\n.\nset_capability\n(\n'unicodeKeyboard'\n,\nTrue\n)\ndevice_app_info\n.\nset_capability\n(\n'resetKeyboard'\n,\nTrue\n)\ndevice_app_info\n.\nset_capability\n(\n'noReset'\n,\nTrue\n)\ndevice_app_info\n.\nset_capability\n(\n'newCommandTimeout'\n,\n6000\n)\ndevice_app_info\n.\nset_capability\n(\n'automationName'\n,\n'UiAutomator2'\n)\n# 连接appium server，server地址查看appium启动信息\ndriver\n=\nwebdriver\n.\nRemote\n(\n'http://127.0.0.1:4723'\n,\noptions\n=\ndevice_app_info\n)\n启动脚本程序\ncd\ndamai_appium\npython3 damai_appium.py",
        "今日の獲得スター数: 70",
        "累積スター数: 4,347"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/WECENG/ticket-purchase"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/zed-industries/zed",
      "title": "zed-industries/zed",
      "date": null,
      "executive_summary": [
        "Code at the speed of thought – Zed is a high-performance, multiplayer code editor from the creators of Atom and Tree-sitter.",
        "---",
        "Zed\nWelcome to Zed, a high-performance, multiplayer code editor from the creators of\nAtom\nand\nTree-sitter\n.\nInstallation\nOn macOS and Linux you can\ndownload Zed directly\nor\ninstall Zed via your local package manager\n.\nOther platforms are not yet available:\nWindows (\ntracking issue\n)\nWeb (\ntracking issue\n)\nDeveloping Zed\nBuilding Zed for macOS\nBuilding Zed for Linux\nBuilding Zed for Windows\nRunning Collaboration Locally\nContributing\nSee\nCONTRIBUTING.md\nfor ways you can contribute to Zed.\nAlso... we're hiring! Check out our\njobs\npage for open roles.\nLicensing\nLicense information for third party dependencies must be correctly provided for CI to pass.\nWe use\ncargo-about\nto automatically comply with open source licenses. If CI is failing, check the following:\nIs it showing a\nno license specified\nerror for a crate you've created? If so, add\npublish = false\nunder\n[package]\nin your crate's Cargo.toml.\nIs the error\nfailed to satisfy license requirements\nfor a dependency? If so, first determine what license the project has and whether this system is sufficient to comply with this license's requirements. If you're unsure, ask a lawyer. Once you've verified that this system is acceptable add the license's SPDX identifier to the\naccepted\narray in\nscript/licenses/zed-licenses.toml\n.\nIs\ncargo-about\nunable to find the license for a dependency? If so, add a clarification field at the end of\nscript/licenses/zed-licenses.toml\n, as specified in the\ncargo-about book\n.",
        "今日の獲得スター数: 68",
        "累積スター数: 66,897"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/zed-industries/zed"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/rustdesk/rustdesk",
      "title": "rustdesk/rustdesk",
      "date": null,
      "executive_summary": [
        "An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.",
        "---",
        "Build\n•\nDocker\n•\nStructure\n•\nSnapshot\n[\nУкраїнська\n] | [\nčesky\n] | [\n中文\n] | [\nMagyar\n] | [\nEspañol\n] | [\nفارسی\n] | [\nFrançais\n] | [\nDeutsch\n] | [\nPolski\n] | [\nIndonesian\n] | [\nSuomi\n] | [\nമലയാളം\n] | [\n日本語\n] | [\nNederlands\n] | [\nItaliano\n] | [\nРусский\n] | [\nPortuguês (Brasil)\n] | [\nEsperanto\n] | [\n한국어\n] | [\nالعربي\n] | [\nTiếng Việt\n] | [\nDansk\n] | [\nΕλληνικά\n] | [\nTürkçe\n] | [\nNorsk\n]\nWe need your help to translate this README,\nRustDesk UI\nand\nRustDesk Doc\nto your native language\nCaution\nMisuse Disclaimer:\nThe developers of RustDesk do not condone or support any unethical or illegal use of this software. Misuse, such as unauthorized access, control or invasion of privacy, is strictly against our guidelines. The authors are not responsible for any misuse of the application.\nChat with us:\nDiscord\n|\nTwitter\n|\nReddit\n|\nYouTube\nYet another remote desktop solution, written in Rust. Works out of the box with no configuration required. You have full control of your data, with no concerns about security. You can use our rendezvous/relay server,\nset up your own\n, or\nwrite your own rendezvous/relay server\n.\nRustDesk welcomes contribution from everyone. See\nCONTRIBUTING.md\nfor help getting started.\nFAQ\nBINARY DOWNLOAD\nNIGHTLY BUILD\nDependencies\nDesktop versions use Flutter or Sciter (deprecated) for GUI, this tutorial is for Sciter only, since it is easier and more friendly to start. Check out our\nCI\nfor building Flutter version.\nPlease download Sciter dynamic library yourself.\nWindows\n|\nLinux\n|\nmacOS\nRaw Steps to build\nPrepare your Rust development env and C++ build env\nInstall\nvcpkg\n, and set\nVCPKG_ROOT\nenv variable correctly\nWindows: vcpkg install libvpx:x64-windows-static libyuv:x64-windows-static opus:x64-windows-static aom:x64-windows-static\nLinux/macOS: vcpkg install libvpx libyuv opus aom\nrun\ncargo run\nBuild\nHow to Build on Linux\nUbuntu 18 (Debian 10)\nsudo apt install -y zip g++ gcc git curl wget nasm yasm libgtk-3-dev clang libxcb-randr0-dev libxdo-dev \\\n        libxfixes-dev libxcb-shape0-dev libxcb-xfixes0-dev libasound2-dev libpulse-dev cmake make \\\n        libclang-dev ninja-build libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libpam0g-dev\nopenSUSE Tumbleweed\nsudo zypper install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libXfixes-devel cmake alsa-lib-devel gstreamer-devel gstreamer-plugins-base-devel xdotool-devel pam-devel\nFedora 28 (CentOS 8)\nsudo yum -y install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libxdo-devel libXfixes-devel pulseaudio-libs-devel cmake alsa-lib-devel gstreamer1-devel gstreamer1-plugins-base-devel pam-devel\nArch (Manjaro)\nsudo pacman -Syu --needed unzip git cmake gcc curl wget yasm nasm zip make pkg-config clang gtk3 xdotool libxcb libxfixes alsa-lib pipewire\nInstall vcpkg\ngit clone https://github.com/microsoft/vcpkg\ncd\nvcpkg\ngit checkout 2023.04.15\ncd\n..\nvcpkg/bootstrap-vcpkg.sh\nexport\nVCPKG_ROOT=\n$HOME\n/vcpkg\nvcpkg/vcpkg install libvpx libyuv opus aom\nFix libvpx (For Fedora)\ncd\nvcpkg/buildtrees/libvpx/src\ncd\n*\n./configure\nsed -i\n'\ns/CFLAGS+=-I/CFLAGS+=-fPIC -I/g\n'\nMakefile\nsed -i\n'\ns/CXXFLAGS+=-I/CXXFLAGS+=-fPIC -I/g\n'\nMakefile\nmake\ncp libvpx.a\n$HOME\n/vcpkg/installed/x64-linux/lib/\ncd\nBuild\ncurl --proto\n'\n=https\n'\n--tlsv1.2 -sSf https://sh.rustup.rs\n|\nsh\nsource\n$HOME\n/.cargo/env\ngit clone --recurse-submodules https://github.com/rustdesk/rustdesk\ncd\nrustdesk\nmkdir -p target/debug\nwget https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so\nmv libsciter-gtk.so target/debug\nVCPKG_ROOT=\n$HOME\n/vcpkg cargo run\nHow to build with Docker\nBegin by cloning the repository and building the Docker container:\ngit clone https://github.com/rustdesk/rustdesk\ncd\nrustdesk\ngit submodule update --init --recursive\ndocker build -t\n\"\nrustdesk-builder\n\"\n.\nThen, each time you need to build the application, run the following command:\ndocker run --rm -it -v\n$PWD\n:/home/user/rustdesk -v rustdesk-git-cache:/home/user/.cargo/git -v rustdesk-registry-cache:/home/user/.cargo/registry -e PUID=\n\"\n$(\nid -u\n)\n\"\n-e PGID=\n\"\n$(\nid -g\n)\n\"\nrustdesk-builder\nNote that the first build may take longer before dependencies are cached, subsequent builds will be faster. Additionally, if you need to specify different arguments to the build command, you may do so at the end of the command in the\n<OPTIONAL-ARGS>\nposition. For instance, if you wanted to build an optimized release version, you would run the command above followed by\n--release\n. The resulting executable will be available in the target folder on your system, and can be run with:\ntarget/debug/rustdesk\nOr, if you're running a release executable:\ntarget/release/rustdesk\nPlease ensure that you run these commands from the root of the RustDesk repository, or the application may not find the required resources. Also note that other cargo subcommands such as\ninstall\nor\nrun\nare not currently supported via this method as they would install or run the program inside the container instead of the host.\nFile Structure\nlibs/hbb_common\n: video codec, config, tcp/udp wrapper, protobuf, fs functions for file transfer, and some other utility functions\nlibs/scrap\n: screen capture\nlibs/enigo\n: platform specific keyboard/mouse control\nlibs/clipboard\n: file copy and paste implementation for Windows, Linux, macOS.\nsrc/ui\n: obsolete Sciter UI (deprecated)\nsrc/server\n: audio/clipboard/input/video services, and network connections\nsrc/client.rs\n: start a peer connection\nsrc/rendezvous_mediator.rs\n: Communicate with\nrustdesk-server\n, wait for remote direct (TCP hole punching) or relayed connection\nsrc/platform\n: platform specific code\nflutter\n: Flutter code for desktop and mobile\nflutter/web/js\n: JavaScript for Flutter web client\nScreenshots",
        "今日の獲得スター数: 67",
        "累積スター数: 99,567"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/rustdesk/rustdesk"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/samber/lo",
      "title": "samber/lo",
      "date": null,
      "executive_summary": [
        "💥 A Lodash-style Go library based on Go 1.18+ Generics (map, filter, contains, find...)",
        "---",
        "lo - Iterate over slices, maps, channels...\n✨\nsamber/lo\nis a Lodash-style Go library based on Go 1.18+ Generics.\nA utility library based on Go 1.18+ generics that makes it easier to work with slices, maps, strings, channels, and functions. It provides dozens of handy methods to simplify common coding tasks and improve code readability. It may look like\nLodash\nin some aspects.\n5 to 10 helpers may overlap with those from the Go standard library, in packages\nslices\nand\nmaps\n. I feel this library is legitimate and offers many more valuable abstractions.\nSee also:\nsamber/do\n: A dependency injection toolkit based on Go 1.18+ Generics\nsamber/mo\n: Monads based on Go 1.18+ Generics (Option, Result, Either...)\n💖 Support This Project\nI’m going all-in on open-source for the coming months.\nHelp sustain development: Become an\nindividual sponsor\nor join as a\ncorporate sponsor\n.\nWhy this name?\nI wanted a\nshort name\n, similar to \"Lodash\", and no Go package uses this name.\n🚀 Install\ngo get github.com/samber/lo@v1\nThis library is v1 and follows SemVer strictly.\nNo breaking changes will be made to exported APIs before v2.0.0.\nThis library has no dependencies outside the Go standard library.\n💡 Usage\nYou can import\nlo\nusing:\nimport\n(\n\"github.com/samber/lo\"\nlop\n\"github.com/samber/lo/parallel\"\nlom\n\"github.com/samber/lo/mutable\"\nloi\n\"github.com/samber/lo/it\"\n)\nThen use one of the helpers below:\nnames\n:=\nlo\n.\nUniq\n([]\nstring\n{\n\"Samuel\"\n,\n\"John\"\n,\n\"Samuel\"\n})\n// []string{\"Samuel\", \"John\"}\nTips for lazy developers\nI cannot recommend it, but in case you are too lazy for repeating\nlo.\neverywhere, you can import the entire library into the namespace.\nimport\n(\n    .\n\"github.com/samber/lo\"\n)\nI take no responsibility for this junk. 😁 💩\n🤠 Spec\nGoDoc:\ngodoc.org/github.com/samber/lo\nDocumentation:\nlo.samber.dev\nSupported helpers for slices:\nFilter\nMap\nUniqMap\nFilterMap\nFlatMap\nReduce\nReduceRight\nForEach\nForEachWhile\nTimes\nUniq\nUniqBy\nGroupBy\nGroupByMap\nChunk\nPartitionBy\nFlatten\nInterleave\nShuffle\nReverse\nFill\nRepeat\nRepeatBy\nKeyBy\nSliceToMap / Associate\nFilterSliceToMap\nKeyify\nDrop\nDropRight\nDropWhile\nDropRightWhile\nDropByIndex\nReject\nRejectMap\nFilterReject\nCount\nCountBy\nCountValues\nCountValuesBy\nSubset\nSlice\nReplace\nReplaceAll\nCompact\nIsSorted\nIsSortedByKey\nSplice\nCut\nCutPrefix\nCutSuffix\nTrim\nTrimLeft\nTrimPrefix\nTrimRight\nTrimSuffix\nSupported helpers for maps:\nKeys\nUniqKeys\nHasKey\nValueOr\nValues\nUniqValues\nPickBy\nPickByKeys\nPickByValues\nOmitBy\nOmitByKeys\nOmitByValues\nEntries / ToPairs\nFromEntries / FromPairs\nInvert\nAssign (merge of maps)\nChunkEntries\nMapKeys\nMapValues\nMapEntries\nMapToSlice\nFilterMapToSlice\nFilterKeys\nFilterValues\nSupported math helpers:\nRange / RangeFrom / RangeWithSteps\nClamp\nSum\nSumBy\nProduct\nProductBy\nMean\nMeanBy\nMode\nSupported helpers for strings:\nRandomString\nSubstring\nChunkString\nRuneLength\nPascalCase\nCamelCase\nKebabCase\nSnakeCase\nWords\nCapitalize\nEllipsis\nSupported helpers for tuples:\nT2 -> T9\nUnpack2 -> Unpack9\nZip2 -> Zip9\nZipBy2 -> ZipBy9\nUnzip2 -> Unzip9\nUnzipBy2 -> UnzipBy9\nCrossJoin2 -> CrossJoin2\nCrossJoinBy2 -> CrossJoinBy2\nSupported helpers for time and duration:\nDuration\nDuration0 -> Duration10\nSupported helpers for channels:\nChannelDispatcher\nSliceToChannel\nChannelToSlice\nGenerator\nBuffer\nBufferWithContext\nBufferWithTimeout\nFanIn\nFanOut\nSupported intersection helpers:\nContains\nContainsBy\nEvery\nEveryBy\nSome\nSomeBy\nNone\nNoneBy\nIntersect\nDifference\nUnion\nWithout\nWithoutBy\nWithoutEmpty\nWithoutNth\nElementsMatch\nElementsMatchBy\nSupported search helpers:\nIndexOf\nLastIndexOf\nHasPrefix\nHasSuffix\nFind\nFindIndexOf\nFindLastIndexOf\nFindOrElse\nFindKey\nFindKeyBy\nFindUniques\nFindUniquesBy\nFindDuplicates\nFindDuplicatesBy\nMin\nMinIndex\nMinBy\nMinIndexBy\nEarliest\nEarliestBy\nMax\nMaxIndex\nMaxBy\nMaxIndexBy\nLatest\nLatestBy\nFirst\nFirstOrEmpty\nFirstOr\nLast\nLastOrEmpty\nLastOr\nNth\nNthOr\nNthOrEmpty\nSample\nSampleBy\nSamples\nSamplesBy\nConditional helpers:\nTernary\nTernaryF\nIf / ElseIf / Else\nSwitch / Case / Default\nType manipulation helpers:\nIsNil\nIsNotNil\nToPtr\nNil\nEmptyableToPtr\nFromPtr\nFromPtrOr\nToSlicePtr\nFromSlicePtr\nFromSlicePtrOr\nToAnySlice\nFromAnySlice\nEmpty\nIsEmpty\nIsNotEmpty\nCoalesce\nCoalesceOrEmpty\nCoalesceSlice\nCoalesceSliceOrEmpty\nCoalesceMap\nCoalesceMapOrEmpty\nFunction helpers:\nPartial\nPartial2 -> Partial5\nConcurrency helpers:\nAttempt\nAttemptWhile\nAttemptWithDelay\nAttemptWhileWithDelay\nDebounce\nDebounceBy\nThrottle\nThrottleWithCount\nThrottleBy\nThrottleByWithCount\nSynchronize\nAsync\nAsync{0->6}\nTransaction\nWaitFor\nWaitForWithContext\nError handling:\nValidate\nMust\nTry\nTry1 -> Try6\nTryOr\nTryOr1 -> TryOr6\nTryCatch\nTryWithErrorValue\nTryCatchWithErrorValue\nErrorsAs\nAssert\nAssertf\nConstraints:\nClonable\nFilter\nIterates over a collection and returns a slice of all the elements the predicate function returns\ntrue\nfor.\neven\n:=\nlo\n.\nFilter\n([]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n},\nfunc\n(\nx\nint\n,\nindex\nint\n)\nbool\n{\nreturn\nx\n%\n2\n==\n0\n})\n// []int{2, 4}\n[\nplay\n]\nMutable: like\nlo.Filter()\n, but the slice is updated in place.\nimport\nlom\n\"github.com/samber/lo/mutable\"\nlist\n:=\n[]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n}\nnewList\n:=\nlom\n.\nFilter\n(\nlist\n,\nfunc\n(\nx\nint\n)\nbool\n{\nreturn\nx\n%\n2\n==\n0\n})\nlist\n// []int{2, 4, 3, 4}\nnewList\n// []int{2, 4}\nMap\nManipulates a slice of one type and transforms it into a slice of another type:\nimport\n\"github.com/samber/lo\"\nlo\n.\nMap\n([]\nint64\n{\n1\n,\n2\n,\n3\n,\n4\n},\nfunc\n(\nx\nint64\n,\nindex\nint\n)\nstring\n{\nreturn\nstrconv\n.\nFormatInt\n(\nx\n,\n10\n)\n})\n// []string{\"1\", \"2\", \"3\", \"4\"}\n[\nplay\n]\nParallel processing: like\nlo.Map()\n, but the mapper function is called in a goroutine. Results are returned in the same order.\nimport\nlop\n\"github.com/samber/lo/parallel\"\nlop\n.\nMap\n([]\nint64\n{\n1\n,\n2\n,\n3\n,\n4\n},\nfunc\n(\nx\nint64\n,\n_\nint\n)\nstring\n{\nreturn\nstrconv\n.\nFormatInt\n(\nx\n,\n10\n)\n})\n// []string{\"1\", \"2\", \"3\", \"4\"}\n[\nplay\n]\nMutable: like\nlo.Map()\n, but the slice is updated in place.\nimport\nlom\n\"github.com/samber/lo/mutable\"\nlist\n:=\n[]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n}\nlom\n.\nMap\n(\nlist\n,\nfunc\n(\nx\nint\n)\nint\n{\nreturn\nx\n*\n2\n})\n// []int{2, 4, 6, 8}\n[\nplay\n]\nUniqMap\nManipulates a slice and transforms it to a slice of another type with unique values.\ntype\nUser\nstruct\n{\nName\nstring\nAge\nint\n}\nusers\n:=\n[]\nUser\n{{\nName\n:\n\"Alex\"\n,\nAge\n:\n10\n}, {\nName\n:\n\"Alex\"\n,\nAge\n:\n12\n}, {\nName\n:\n\"Bob\"\n,\nAge\n:\n11\n}, {\nName\n:\n\"Alice\"\n,\nAge\n:\n20\n}}\nnames\n:=\nlo\n.\nUniqMap\n(\nusers\n,\nfunc\n(\nu\nUser\n,\nindex\nint\n)\nstring\n{\nreturn\nu\n.\nName\n})\n// []string{\"Alex\", \"Bob\", \"Alice\"}\n[\nplay\n]\nFilterMap\nReturns a slice obtained after both filtering and mapping using the given callback function.\nThe callback function should return two values: the result of the mapping operation and whether the result element should be included or not.\nmatching\n:=\nlo\n.\nFilterMap\n([]\nstring\n{\n\"cpu\"\n,\n\"gpu\"\n,\n\"mouse\"\n,\n\"keyboard\"\n},\nfunc\n(\nx\nstring\n,\n_\nint\n) (\nstring\n,\nbool\n) {\nif\nstrings\n.\nHasSuffix\n(\nx\n,\n\"pu\"\n) {\nreturn\n\"xpu\"\n,\ntrue\n}\nreturn\n\"\"\n,\nfalse\n})\n// []string{\"xpu\", \"xpu\"}\n[\nplay\n]\nFlatMap\nManipulates a slice and transforms and flattens it to a slice of another type. The transform function can either return a slice or a\nnil\n, and in the\nnil\ncase no value is added to the final slice.\nlo\n.\nFlatMap\n([]\nint64\n{\n0\n,\n1\n,\n2\n},\nfunc\n(\nx\nint64\n,\n_\nint\n) []\nstring\n{\nreturn\n[]\nstring\n{\nstrconv\n.\nFormatInt\n(\nx\n,\n10\n),\nstrconv\n.\nFormatInt\n(\nx\n,\n10\n),\n    }\n})\n// []string{\"0\", \"0\", \"1\", \"1\", \"2\", \"2\"}\n[\nplay\n]\nReduce\nReduces a collection to a single value. The value is calculated by accumulating the result of running each element in the collection through an accumulator function. Each successive invocation is supplied with the return value returned by the previous call.\nsum\n:=\nlo\n.\nReduce\n([]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n},\nfunc\n(\nagg\nint\n,\nitem\nint\n,\n_\nint\n)\nint\n{\nreturn\nagg\n+\nitem\n},\n0\n)\n// 10\n[\nplay\n]\nReduceRight\nLike\nlo.Reduce\nexcept that it iterates over elements of collection from right to left.\nresult\n:=\nlo\n.\nReduceRight\n([][]\nint\n{{\n0\n,\n1\n}, {\n2\n,\n3\n}, {\n4\n,\n5\n}},\nfunc\n(\nagg\n[]\nint\n,\nitem\n[]\nint\n,\n_\nint\n) []\nint\n{\nreturn\nappend\n(\nagg\n,\nitem\n...\n)\n}, []\nint\n{})\n// []int{4, 5, 2, 3, 0, 1}\n[\nplay\n]\nForEach\nIterates over elements of a collection and invokes the function over each element.\nimport\n\"github.com/samber/lo\"\nlo\n.\nForEach\n([]\nstring\n{\n\"hello\"\n,\n\"world\"\n},\nfunc\n(\nx\nstring\n,\n_\nint\n) {\nprintln\n(\nx\n)\n})\n// prints \"hello\\nworld\\n\"\n[\nplay\n]\nParallel processing: like\nlo.ForEach()\n, but the callback is called as a goroutine.\nimport\nlop\n\"github.com/samber/lo/parallel\"\nlop\n.\nForEach\n([]\nstring\n{\n\"hello\"\n,\n\"world\"\n},\nfunc\n(\nx\nstring\n,\n_\nint\n) {\nprintln\n(\nx\n)\n})\n// prints \"hello\\nworld\\n\" or \"world\\nhello\\n\"\nForEachWhile\nIterates over collection elements and invokes iteratee for each element collection return value decide to continue or break, like do while().\nlist\n:=\n[]\nint64\n{\n1\n,\n2\n,\n-\n42\n,\n4\n}\nlo\n.\nForEachWhile\n(\nlist\n,\nfunc\n(\nx\nint64\n,\n_\nint\n)\nbool\n{\nif\nx\n<\n0\n{\nreturn\nfalse\n}\nfmt\n.\nPrintln\n(\nx\n)\nreturn\ntrue\n})\n// 1\n// 2\n[\nplay\n]\nTimes\nTimes invokes the iteratee n times, returning a slice of the results of each invocation. The iteratee is invoked with index as argument.\nimport\n\"github.com/samber/lo\"\nlo\n.\nTimes\n(\n3\n,\nfunc\n(\ni\nint\n)\nstring\n{\nreturn\nstrconv\n.\nFormatInt\n(\nint64\n(\ni\n),\n10\n)\n})\n// []string{\"0\", \"1\", \"2\"}\n[\nplay\n]\nParallel processing: like\nlo.Times()\n, but callback is called in goroutine.\nimport\nlop\n\"github.com/samber/lo/parallel\"\nlop\n.\nTimes\n(\n3\n,\nfunc\n(\ni\nint\n)\nstring\n{\nreturn\nstrconv\n.\nFormatInt\n(\nint64\n(\ni\n),\n10\n)\n})\n// []string{\"0\", \"1\", \"2\"}\nUniq\nReturns a duplicate-free version of a slice, in which only the first occurrence of each element is kept. The order of result values is determined by the order they occur in the slice.\nuniqValues\n:=\nlo\n.\nUniq\n([]\nint\n{\n1\n,\n2\n,\n2\n,\n1\n})\n// []int{1, 2}\n[\nplay\n]\nUniqBy\nReturns a duplicate-free version of a slice, in which only the first occurrence of each element is kept. The order of result values is determined by the order they occur in the slice. It accepts\niteratee\nwhich is invoked for each element in the slice to generate the criterion by which uniqueness is computed.\nuniqValues\n:=\nlo\n.\nUniqBy\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n},\nfunc\n(\ni\nint\n)\nint\n{\nreturn\ni\n%\n3\n})\n// []int{0, 1, 2}\n[\nplay\n]\nGroupBy\nReturns an object composed of keys generated from the results of running each element of collection through iteratee.\nimport\nlo\n\"github.com/samber/lo\"\ngroups\n:=\nlo\n.\nGroupBy\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n},\nfunc\n(\ni\nint\n)\nint\n{\nreturn\ni\n%\n3\n})\n// map[int][]int{0: []int{0, 3}, 1: []int{1, 4}, 2: []int{2, 5}}\n[\nplay\n]\nParallel processing: like\nlo.GroupBy()\n, but callback is called in goroutine.\nimport\nlop\n\"github.com/samber/lo/parallel\"\nlop\n.\nGroupBy\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n},\nfunc\n(\ni\nint\n)\nint\n{\nreturn\ni\n%\n3\n})\n// map[int][]int{0: []int{0, 3}, 1: []int{1, 4}, 2: []int{2, 5}}\nGroupByMap\nReturns an object composed of keys generated from the results of running each element of collection through iteratee.\nimport\nlo\n\"github.com/samber/lo\"\ngroups\n:=\nlo\n.\nGroupByMap\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n},\nfunc\n(\ni\nint\n) (\nint\n,\nint\n) {\nreturn\ni\n%\n3\n,\ni\n*\n2\n})\n// map[int][]int{0: []int{0, 6}, 1: []int{2, 8}, 2: []int{4, 10}}\n[\nplay\n]\nChunk\nReturns a slice of elements split into groups of length size. If the slice can't be split evenly, the final chunk will be the remaining elements.\nlo\n.\nChunk\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n},\n2\n)\n// [][]int{{0, 1}, {2, 3}, {4, 5}}\nlo\n.\nChunk\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n,\n6\n},\n2\n)\n// [][]int{{0, 1}, {2, 3}, {4, 5}, {6}}\nlo\n.\nChunk\n([]\nint\n{},\n2\n)\n// [][]int{}\nlo\n.\nChunk\n([]\nint\n{\n0\n},\n2\n)\n// [][]int{{0}}\n[\nplay\n]\nPartitionBy\nReturns a slice of elements split into groups. The order of grouped values is determined by the order they occur in collection. The grouping is generated from the results of running each element of collection through iteratee.\nimport\nlo\n\"github.com/samber/lo\"\npartitions\n:=\nlo\n.\nPartitionBy\n([]\nint\n{\n-\n2\n,\n-\n1\n,\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n},\nfunc\n(\nx\nint\n)\nstring\n{\nif\nx\n<\n0\n{\nreturn\n\"negative\"\n}\nelse\nif\nx\n%\n2\n==\n0\n{\nreturn\n\"even\"\n}\nreturn\n\"odd\"\n})\n// [][]int{{-2, -1}, {0, 2, 4}, {1, 3, 5}}\n[\nplay\n]\nParallel processing: like\nlo.PartitionBy()\n, but callback is called in goroutine. Results are returned in the same order.\nimport\nlop\n\"github.com/samber/lo/parallel\"\npartitions\n:=\nlop\n.\nPartitionBy\n([]\nint\n{\n-\n2\n,\n-\n1\n,\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n},\nfunc\n(\nx\nint\n)\nstring\n{\nif\nx\n<\n0\n{\nreturn\n\"negative\"\n}\nelse\nif\nx\n%\n2\n==\n0\n{\nreturn\n\"even\"\n}\nreturn\n\"odd\"\n})\n// [][]int{{-2, -1}, {0, 2, 4}, {1, 3, 5}}\nFlatten\nReturns a slice a single level deep.\nflat\n:=\nlo\n.\nFlatten\n([][]\nint\n{{\n0\n,\n1\n}, {\n2\n,\n3\n,\n4\n,\n5\n}})\n// []int{0, 1, 2, 3, 4, 5}\n[\nplay\n]\nInterleave\nRound-robin alternating input slices and sequentially appending value at index into result.\ninterleaved\n:=\nlo\n.\nInterleave\n([]\nint\n{\n1\n,\n4\n,\n7\n}, []\nint\n{\n2\n,\n5\n,\n8\n}, []\nint\n{\n3\n,\n6\n,\n9\n})\n// []int{1, 2, 3, 4, 5, 6, 7, 8, 9}\ninterleaved\n:=\nlo\n.\nInterleave\n([]\nint\n{\n1\n}, []\nint\n{\n2\n,\n5\n,\n8\n}, []\nint\n{\n3\n,\n6\n}, []\nint\n{\n4\n,\n7\n,\n9\n,\n10\n})\n// []int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n[\nplay\n]\nShuffle\nReturns a slice of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n⚠️\nThis helper is\nmutable\n.\nimport\nlom\n\"github.com/samber/lo/mutable\"\nlist\n:=\n[]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}\nlom\n.\nShuffle\n(\nlist\n)\nlist\n// []int{1, 4, 0, 3, 5, 2}\n[\nplay\n]\nReverse\nReverses a slice so that the first element becomes the last, the second element becomes the second to last, and so on.\n⚠️\nThis helper is\nmutable\n.\nimport\nlom\n\"github.com/samber/lo/mutable\"\nlist\n:=\n[]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}\nlom\n.\nReverse\n(\nlist\n)\nlist\n// []int{5, 4, 3, 2, 1, 0}\n[\nplay\n]\nFill\nFills elements of a slice with\ninitial\nvalue.\ntype\nfoo\nstruct\n{\nbar\nstring\n}\nfunc\n(\nf\nfoo\n)\nClone\n()\nfoo\n{\nreturn\nfoo\n{\nf\n.\nbar\n}\n}\ninitializedSlice\n:=\nlo\n.\nFill\n([]\nfoo\n{\nfoo\n{\n\"a\"\n},\nfoo\n{\n\"a\"\n}},\nfoo\n{\n\"b\"\n})\n// []foo{foo{\"b\"}, foo{\"b\"}}\n[\nplay\n]\nRepeat\nBuilds a slice with N copies of initial value.\ntype\nfoo\nstruct\n{\nbar\nstring\n}\nfunc\n(\nf\nfoo\n)\nClone\n()\nfoo\n{\nreturn\nfoo\n{\nf\n.\nbar\n}\n}\nslice\n:=\nlo\n.\nRepeat\n(\n2\n,\nfoo\n{\n\"a\"\n})\n// []foo{foo{\"a\"}, foo{\"a\"}}\n[\nplay\n]\nRepeatBy\nBuilds a slice with values returned by N calls of callback.\nslice\n:=\nlo\n.\nRepeatBy\n(\n0\n,\nfunc\n(\ni\nint\n)\nstring\n{\nreturn\nstrconv\n.\nFormatInt\n(\nint64\n(\nmath\n.\nPow\n(\nfloat64\n(\ni\n),\n2\n)),\n10\n)\n})\n// []string{}\nslice\n:=\nlo\n.\nRepeatBy\n(\n5\n,\nfunc\n(\ni\nint\n)\nstring\n{\nreturn\nstrconv\n.\nFormatInt\n(\nint64\n(\nmath\n.\nPow\n(\nfloat64\n(\ni\n),\n2\n)),\n10\n)\n})\n// []string{\"0\", \"1\", \"4\", \"9\", \"16\"}\n[\nplay\n]\nKeyBy\nTransforms a slice or a slice of structs to a map based on a pivot callback.\nm\n:=\nlo\n.\nKeyBy\n([]\nstring\n{\n\"a\"\n,\n\"aa\"\n,\n\"aaa\"\n},\nfunc\n(\nstr\nstring\n)\nint\n{\nreturn\nlen\n(\nstr\n)\n})\n// map[int]string{1: \"a\", 2: \"aa\", 3: \"aaa\"}\ntype\nCharacter\nstruct\n{\ndir\nstring\ncode\nint\n}\ncharacters\n:=\n[]\nCharacter\n{\n    {\ndir\n:\n\"left\"\n,\ncode\n:\n97\n},\n    {\ndir\n:\n\"right\"\n,\ncode\n:\n100\n},\n}\nresult\n:=\nlo\n.\nKeyBy\n(\ncharacters\n,\nfunc\n(\nchar\nCharacter\n)\nstring\n{\nreturn\nstring\n(\nrune\n(\nchar\n.\ncode\n))\n})\n//map[a:{dir:left code:97} d:{dir:right code:100}]\n[\nplay\n]\nSliceToMap (alias: Associate)\nReturns a map containing key-value pairs provided by transform function applied to elements of the given slice.\nIf any of two pairs have the same key the last one gets added to the map.\nThe order of keys in returned map is not specified and is not guaranteed to be the same from the original slice.\nin\n:=\n[]\n*\nfoo\n{{\nbaz\n:\n\"apple\"\n,\nbar\n:\n1\n}, {\nbaz\n:\n\"banana\"\n,\nbar\n:\n2\n}}\naMap\n:=\nlo\n.\nSliceToMap\n(\nin\n,\nfunc\n(\nf\n*\nfoo\n) (\nstring\n,\nint\n) {\nreturn\nf\n.\nbaz\n,\nf\n.\nbar\n})\n// map[string][int]{ \"apple\":1, \"banana\":2 }\n[\nplay\n]\nFilterSliceToMap\nReturns a map containing key-value pairs provided by transform function applied to elements of the given slice.\nIf any of two pairs have the same key the last one gets added to the map.\nThe order of keys in returned map is not specified and is not guaranteed to be the same from the original slice.\nThe third return value of the transform function is a boolean that indicates whether the key-value pair should be included in the map.\nlist\n:=\n[]\nstring\n{\n\"a\"\n,\n\"aa\"\n,\n\"aaa\"\n}\nresult\n:=\nlo\n.\nFilterSliceToMap\n(\nlist\n,\nfunc\n(\nstr\nstring\n) (\nstring\n,\nint\n,\nbool\n) {\nreturn\nstr\n,\nlen\n(\nstr\n),\nlen\n(\nstr\n)\n>\n1\n})\n// map[string][int]{\"aa\":2 \"aaa\":3}\n[\nplay\n]\nKeyify\nReturns a map with each unique element of the slice as a key.\nset\n:=\nlo\n.\nKeyify\n([]\nint\n{\n1\n,\n1\n,\n2\n,\n3\n,\n4\n})\n// map[int]struct{}{1:{}, 2:{}, 3:{}, 4:{}}\n[\nplay\n]\nDrop\nDrops n elements from the beginning of a slice.\nl\n:=\nlo\n.\nDrop\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n},\n2\n)\n// []int{2, 3, 4, 5}\n[\nplay\n]\nDropRight\nDrops n elements from the end of a slice.\nl\n:=\nlo\n.\nDropRight\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n},\n2\n)\n// []int{0, 1, 2, 3}\n[\nplay\n]\nDropWhile\nDrop elements from the beginning of a slice while the predicate returns true.\nl\n:=\nlo\n.\nDropWhile\n([]\nstring\n{\n\"a\"\n,\n\"aa\"\n,\n\"aaa\"\n,\n\"aa\"\n,\n\"aa\"\n},\nfunc\n(\nval\nstring\n)\nbool\n{\nreturn\nlen\n(\nval\n)\n<=\n2\n})\n// []string{\"aaa\", \"aa\", \"aa\"}\n[\nplay\n]\nDropRightWhile\nDrop elements from the end of a slice while the predicate returns true.\nl\n:=\nlo\n.\nDropRightWhile\n([]\nstring\n{\n\"a\"\n,\n\"aa\"\n,\n\"aaa\"\n,\n\"aa\"\n,\n\"aa\"\n},\nfunc\n(\nval\nstring\n)\nbool\n{\nreturn\nlen\n(\nval\n)\n<=\n2\n})\n// []string{\"a\", \"aa\", \"aaa\"}\n[\nplay\n]\nDropByIndex\nDrops elements from a slice by the index. A negative index will drop elements from the end of the slice.\nl\n:=\nlo\n.\nDropByIndex\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n},\n2\n,\n4\n,\n-\n1\n)\n// []int{0, 1, 3}\n[\nplay\n]\nReject\nThe opposite of Filter, this method returns the elements of collection that predicate does not return true for.\nodd\n:=\nlo\n.\nReject\n([]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n},\nfunc\n(\nx\nint\n,\n_\nint\n)\nbool\n{\nreturn\nx\n%\n2\n==\n0\n})\n// []int{1, 3}\n[\nplay\n]\nRejectMap\nThe opposite of FilterMap, this method returns a slice obtained after both filtering and mapping using the given callback function.\nThe callback function should return two values:\nthe result of the mapping operation and\nwhether the result element should be included or not.\nitems\n:=\nlo\n.\nRejectMap\n([]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n},\nfunc\n(\nx\nint\n,\n_\nint\n) (\nint\n,\nbool\n) {\nreturn\nx\n*\n10\n,\nx\n%\n2\n==\n0\n})\n// []int{10, 30}\nFilterReject\nMixes Filter and Reject, this method returns two slices, one for the elements of collection that predicate returns true for and one for the elements that predicate does not return true for.\nkept\n,\nrejected\n:=\nlo\n.\nFilterReject\n([]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n},\nfunc\n(\nx\nint\n,\n_\nint\n)\nbool\n{\nreturn\nx\n%\n2\n==\n0\n})\n// []int{2, 4}\n// []int{1, 3}\nCount\nCounts the number of elements in the collection that equal value.\ncount\n:=\nlo\n.\nCount\n([]\nint\n{\n1\n,\n5\n,\n1\n},\n1\n)\n// 2\n[\nplay\n]\nCountBy\nCounts the number of elements in the collection for which predicate is true.\ncount\n:=\nlo\n.\nCountBy\n([]\nint\n{\n1\n,\n5\n,\n1\n},\nfunc\n(\ni\nint\n)\nbool\n{\nreturn\ni\n<\n4\n})\n// 2\n[\nplay\n]\nCountValues\nCounts the number of each element in the collection.\nlo\n.\nCountValues\n([]\nint\n{})\n// map[int]int{}\nlo\n.\nCountValues\n([]\nint\n{\n1\n,\n2\n})\n// map[int]int{1: 1, 2: 1}\nlo\n.\nCountValues\n([]\nint\n{\n1\n,\n2\n,\n2\n})\n// map[int]int{1: 1, 2: 2}\nlo\n.\nCountValues\n([]\nstring\n{\n\"foo\"\n,\n\"bar\"\n,\n\"\"\n})\n// map[string]int{\"\": 1, \"foo\": 1, \"bar\": 1}\nlo\n.\nCountValues\n([]\nstring\n{\n\"foo\"\n,\n\"bar\"\n,\n\"bar\"\n})\n// map[string]int{\"foo\": 1, \"bar\": 2}\n[\nplay\n]\nCountValuesBy\nCounts the number of each element in the collection. It is equivalent to chaining lo.Map and lo.CountValues.\nisEven\n:=\nfunc\n(\nv\nint\n)\nbool\n{\nreturn\nv\n%\n2\n==\n0\n}\nlo\n.\nCountValuesBy\n([]\nint\n{},\nisEven\n)\n// map[bool]int{}\nlo\n.\nCountValuesBy\n([]\nint\n{\n1\n,\n2\n},\nisEven\n)\n// map[bool]int{false: 1, true: 1}\nlo\n.\nCountValuesBy\n([]\nint\n{\n1\n,\n2\n,\n2\n},\nisEven\n)\n// map[bool]int{false: 1, true: 2}\nlength\n:=\nfunc\n(\nv\nstring\n)\nint\n{\nreturn\nlen\n(\nv\n)\n}\nlo\n.\nCountValuesBy\n([]\nstring\n{\n\"foo\"\n,\n\"bar\"\n,\n\"\"\n},\nlength\n)\n// map[int]int{0: 1, 3: 2}\nlo\n.\nCountValuesBy\n([]\nstring\n{\n\"foo\"\n,\n\"bar\"\n,\n\"bar\"\n},\nlength\n)\n// map[int]int{3: 3}\n[\nplay\n]\nSubset\nReturns a copy of a slice from\noffset\nup to\nlength\nelements. Like\nslice[start:start+length]\n, but does not panic on overflow.\nin\n:=\n[]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n}\nsub\n:=\nlo\n.\nSubset\n(\nin\n,\n2\n,\n3\n)\n// []int{2, 3, 4}\nsub\n:=\nlo\n.\nSubset\n(\nin\n,\n-\n4\n,\n3\n)\n// []int{1, 2, 3}\nsub\n:=\nlo\n.\nSubset\n(\nin\n,\n-\n2\n,\nmath\n.\nMaxUint\n)\n// []int{3, 4}\n[\nplay\n]\nSlice\nReturns a copy of a slice from\nstart\nup to, but not including\nend\n. Like\nslice[start:end]\n, but does not panic on overflow.\nin\n:=\n[]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n}\nslice\n:=\nlo\n.\nSlice\n(\nin\n,\n0\n,\n5\n)\n// []int{0, 1, 2, 3, 4}\nslice\n:=\nlo\n.\nSlice\n(\nin\n,\n2\n,\n3\n)\n// []int{2}\nslice\n:=\nlo\n.\nSlice\n(\nin\n,\n2\n,\n6\n)\n// []int{2, 3, 4}\nslice\n:=\nlo\n.\nSlice\n(\nin\n,\n4\n,\n3\n)\n// []int{}\n[\nplay\n]\nReplace\nReturns a copy of the slice with the first n non-overlapping instances of old replaced by new.\nin\n:=\n[]\nint\n{\n0\n,\n1\n,\n0\n,\n1\n,\n2\n,\n3\n,\n0\n}\nslice\n:=\nlo\n.\nReplace\n(\nin\n,\n0\n,\n42\n,\n1\n)\n// []int{42, 1, 0, 1, 2, 3, 0}\nslice\n:=\nlo\n.\nReplace\n(\nin\n,\n-\n1\n,\n42\n,\n1\n)\n// []int{0, 1, 0, 1, 2, 3, 0}\nslice\n:=\nlo\n.\nReplace\n(\nin\n,\n0\n,\n42\n,\n2\n)\n// []int{42, 1, 42, 1, 2, 3, 0}\nslice\n:=\nlo\n.\nReplace\n(\nin\n,\n0\n,\n42\n,\n-\n1\n)\n// []int{42, 1, 42, 1, 2, 3, 42}\n[\nplay\n]\nReplaceAll\nReturns a copy of the slice with all non-overlapping instances of old replaced by new.\nin\n:=\n[]\nint\n{\n0\n,\n1\n,\n0\n,\n1\n,\n2\n,\n3\n,\n0\n}\nslice\n:=\nlo\n.\nReplaceAll\n(\nin\n,\n0\n,\n42\n)\n// []int{42, 1, 42, 1, 2, 3, 42}\nslice\n:=\nlo\n.\nReplaceAll\n(\nin\n,\n-\n1\n,\n42\n)\n// []int{0, 1, 0, 1, 2, 3, 0}\n[\nplay\n]\nCompact\nReturns a slice of all non-zero elements.\nin\n:=\n[]\nstring\n{\n\"\"\n,\n\"foo\"\n,\n\"\"\n,\n\"bar\"\n,\n\"\"\n}\nslice\n:=\nlo\n.\nCompact\n(\nin\n)\n// []string{\"foo\", \"bar\"}\n[\nplay\n]\nIsSorted\nChecks if a slice is sorted.\nslice\n:=\nlo\n.\nIsSorted\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n,\n6\n,\n7\n,\n8\n,\n9\n})\n// true\n[\nplay\n]\nIsSortedByKey\nChecks if a slice is sorted by iteratee.\nslice\n:=\nlo\n.\nIsSortedByKey\n([]\nstring\n{\n\"a\"\n,\n\"bb\"\n,\n\"ccc\"\n},\nfunc\n(\ns\nstring\n)\nint\n{\nreturn\nlen\n(\ns\n)\n})\n// true\n[\nplay\n]\nSplice\nSplice inserts multiple elements at index i. A negative index counts back from the end of the slice. The helper is protected against overflow errors.\nresult\n:=\nlo\n.\nSplice\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n},\n1\n,\n\"1\"\n,\n\"2\"\n)\n// []string{\"a\", \"1\", \"2\", \"b\"}\n// negative\nresult\n=\nlo\n.\nSplice\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n},\n-\n1\n,\n\"1\"\n,\n\"2\"\n)\n// []string{\"a\", \"1\", \"2\", \"b\"}\n// overflow\nresult\n=\nlo\n.\nSplice\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n},\n42\n,\n\"1\"\n,\n\"2\"\n)\n// []string{\"a\", \"b\", \"1\", \"2\"}\n[\nplay\n]\nCut\nSlices collection around the first instance of separator, returning the part of collection before and after separator. The found result reports whether separator appears in collection. If separator does not appear in s, cut returns collection, empty slice of []T, false.\nactualLeft\n,\nactualRight\n,\nresult\n=\nlo\n.\nCut\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n,\n\"d\"\n,\n\"e\"\n,\n\"f\"\n,\n\"g\"\n}, []\nstring\n{\n\"b\"\n,\n\"c\"\n,\n\"d\"\n})\n// actualLeft: []string{\"a\"}\n// actualRight: []string{\"e\", \"f\", \"g\"}\n// result: true\nresult\n=\nlo\n.\nCut\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n,\n\"d\"\n,\n\"e\"\n,\n\"f\"\n,\n\"g\"\n}, []\nstring\n{\n\"z\"\n})\n// actualLeft: []string{\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"}\n// actualRight: []string{}\n// result: false\nresult\n=\nlo\n.\nCut\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n,\n\"d\"\n,\n\"e\"\n,\n\"f\"\n,\n\"g\"\n}, []\nstring\n{\n\"a\"\n,\n\"b\"\n})\n// actualLeft: []string{}\n// actualRight: []string{\"c\", \"d\", \"e\", \"f\", \"g\"}\n// result: true\n[\nplay\n]\nCutPrefix\nReturns collection without the provided leading prefix []T and reports whether it found the prefix. If s doesn't start with prefix, CutPrefix returns collection, false. If prefix is the empty []T, CutPrefix returns collection, true.\nactualRight\n,\nresult\n=\nlo\n.\nCutPrefix\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n,\n\"d\"\n,\n\"e\"\n,\n\"f\"\n,\n\"g\"\n}, []\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n})\n// actualRight: []string{\"d\", \"e\", \"f\", \"g\"}\n// result: true\nresult\n=\nlo\n.\nCutPrefix\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n,\n\"d\"\n,\n\"e\"\n,\n\"f\"\n,\n\"g\"\n}, []\nstring\n{\n\"b\"\n})\n// actualRight: []string{\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"}\n// result: false\nresult\n=\nlo\n.\nCutPrefix\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n,\n\"d\"\n,\n\"e\"\n,\n\"f\"\n,\n\"g\"\n}, []\nstring\n{})\n// actualRight: []string{\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"}\n// result: true\n[\nplay\n]\nCutSuffix\nReturns collection without the provided ending suffix []T and reports whether it found the suffix. If it doesn't end with suffix, CutSuffix returns collection, false. If suffix is the empty []T, CutSuffix returns collection, true.\nactualLeft\n,\nresult\n=\nlo\n.\nCutSuffix\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n,\n\"d\"\n,\n\"e\"\n,\n\"f\"\n,\n\"g\"\n}, []\nstring\n{\n\"f\"\n,\n\"g\"\n})\n// actualLeft: []string{\"a\", \"b\", \"c\", \"d\", \"e\"}\n// result: true\nactualLeft\n,\nresult\n=\nlo\n.\nCutSuffix\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n,\n\"d\"\n,\n\"e\"\n,\n\"f\"\n,\n\"g\"\n}, []\nstring\n{\n\"b\"\n})\n// actualLeft: []string{\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"}\n// result: false\nactualLeft\n,\nresult\n=\nlo\n.\nCutSuffix\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n,\n\"d\"\n,\n\"e\"\n,\n\"f\"\n,\n\"g\"\n}, []\nstring\n{})\n// actualLeft: []string{\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"}\n// result: true\n[\nplay\n]\nTrim\nRemoves all the leading and trailing cutset from the collection.\nresult\n:=\nlo\n.\nTrim\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n0\n,\n3\n,\n0\n}, []\nint\n{\n1\n,\n0\n})\n// []int{2, 0, 3}\nresult\n:=\nlo\n.\nTrim\n([]\nstring\n{\n\"hello\"\n,\n\"world\"\n,\n\" \"\n}, []\nstring\n{\n\" \"\n,\n\"\"\n})\n// []string{\"hello\", \"world\"}\n[\nplay\n]\nTrimLeft\nRemoves all the leading cutset from the collection.\nresult\n:=\nlo\n.\nTrimLeft\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n0\n,\n3\n,\n0\n}, []\nint\n{\n1\n,\n0\n})\n// []int{2, 0, 3, 0}\nresult\n:=\nlo\n.\nTrimLeft\n([]\nstring\n{\n\"hello\"\n,\n\"world\"\n,\n\" \"\n}, []\nstring\n{\n\" \"\n,\n\"\"\n})\n// []string{\"hello\", \"world\", \" \"}\n[\nplay\n]\nTrimPrefix\nRemoves all the leading prefix from the collection.\nresult\n:=\nlo\n.\nTrimPrefix\n([]\nint\n{\n1\n,\n2\n,\n1\n,\n2\n,\n3\n,\n1\n,\n2\n,\n4\n}, []\nint\n{\n1\n,\n2\n})\n// []int{3, 1, 2, 4}\nresult\n:=\nlo\n.\nTrimPrefix\n([]\nstring\n{\n\"hello\"\n,\n\"world\"\n,\n\"hello\"\n,\n\"test\"\n}, []\nstring\n{\n\"hello\"\n})\n// []string{\"world\", \"hello\", \"test\"}\n[\nplay\n]\nTrimRight\nRemoves all the trailing cutset from the collection.\nresult\n:=\nlo\n.\nTrimRight\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n0\n,\n3\n,\n0\n}, []\nint\n{\n0\n,\n3\n})\n// []int{0, 1, 2}\nresult\n:=\nlo\n.\nTrimRight\n([]\nstring\n{\n\"hello\"\n,\n\"world\"\n,\n\"  \"\n}, []\nstring\n{\n\" \"\n,\n\"\"\n})\n// []string{\"hello\", \"world\", \"\"}\n[\nplay\n]\nTrimSuffix\nRemoves all the trailing suffix from the collection.\nresult\n:=\nlo\n.\nTrimSuffix\n([]\nint\n{\n1\n,\n2\n,\n3\n,\n1\n,\n2\n,\n4\n,\n2\n,\n4\n,\n2\n,\n4\n}, []\nint\n{\n2\n,\n4\n})\n// []int{1, 2, 3, 1}\nresult\n:=\nlo\n.\nTrimSuffix\n([]\nstring\n{\n\"hello\"\n,\n\"world\"\n,\n\"hello\"\n,\n\"test\"\n}, []\nstring\n{\n\"test\"\n})\n// []string{\"hello\", \"world\", \"hello\"}\n[\nplay\n]\nKeys\nCreates a slice of the map keys.\nUse the UniqKeys variant to deduplicate common keys.\nkeys\n:=\nlo\n.\nKeys\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n})\n// []string{\"foo\", \"bar\"}\nkeys\n:=\nlo\n.\nKeys\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n},\nmap\n[\nstring\n]\nint\n{\n\"baz\"\n:\n3\n})\n// []string{\"foo\", \"bar\", \"baz\"}\nkeys\n:=\nlo\n.\nKeys\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n},\nmap\n[\nstring\n]\nint\n{\n\"bar\"\n:\n3\n})\n// []string{\"foo\", \"bar\", \"bar\"}\n[\nplay\n]\nUniqKeys\nCreates a slice of unique map keys.\nkeys\n:=\nlo\n.\nUniqKeys\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n},\nmap\n[\nstring\n]\nint\n{\n\"baz\"\n:\n3\n})\n// []string{\"foo\", \"bar\", \"baz\"}\nkeys\n:=\nlo\n.\nUniqKeys\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n},\nmap\n[\nstring\n]\nint\n{\n\"bar\"\n:\n3\n})\n// []string{\"foo\", \"bar\"}\n[\nplay\n]\nHasKey\nReturns whether the given key exists.\nexists\n:=\nlo\n.\nHasKey\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n},\n\"foo\"\n)\n// true\nexists\n:=\nlo\n.\nHasKey\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n},\n\"baz\"\n)\n// false\n[\nplay\n]\nValues\nCreates a slice of the map values.\nUse the UniqValues variant to deduplicate common values.\nvalues\n:=\nlo\n.\nValues\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n})\n// []int{1, 2}\nvalues\n:=\nlo\n.\nValues\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n},\nmap\n[\nstring\n]\nint\n{\n\"baz\"\n:\n3\n})\n// []int{1, 2, 3}\nvalues\n:=\nlo\n.\nValues\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n},\nmap\n[\nstring\n]\nint\n{\n\"bar\"\n:\n2\n})\n// []int{1, 2, 2}\n[\nplay\n]\nUniqValues\nCreates a slice of unique map values.\nvalues\n:=\nlo\n.\nUniqValues\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n})\n// []int{1, 2}\nvalues\n:=\nlo\n.\nUniqValues\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n},\nmap\n[\nstring\n]\nint\n{\n\"baz\"\n:\n3\n})\n// []int{1, 2, 3}\nvalues\n:=\nlo\n.\nUniqValues\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n},\nmap\n[\nstring\n]\nint\n{\n\"bar\"\n:\n2\n})\n// []int{1, 2}\n[\nplay\n]\nValueOr\nReturns the value of the given key or the fallback value if the key is not present.\nvalue\n:=\nlo\n.\nValueOr\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n},\n\"foo\"\n,\n42\n)\n// 1\nvalue\n:=\nlo\n.\nValueOr\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n},\n\"baz\"\n,\n42\n)\n// 42\n[\nplay\n]\nPickBy\nReturns same map type filtered by given predicate.\nm\n:=\nlo\n.\nPickBy\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n,\n\"baz\"\n:\n3\n},\nfunc\n(\nkey\nstring\n,\nvalue\nint\n)\nbool\n{\nreturn\nvalue\n%\n2\n==\n1\n})\n// map[string]int{\"foo\": 1, \"baz\": 3}\n[\nplay\n]\nPickByKeys\nReturns same map type filtered by given keys.\nm\n:=\nlo\n.\nPickByKeys\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n,\n\"baz\"\n:\n3\n}, []\nstring\n{\n\"foo\"\n,\n\"baz\"\n})\n// map[string]int{\"foo\": 1, \"baz\": 3}\n[\nplay\n]\nPickByValues\nReturns same map type filtered by given values.\nm\n:=\nlo\n.\nPickByValues\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n,\n\"baz\"\n:\n3\n}, []\nint\n{\n1\n,\n3\n})\n// map[string]int{\"foo\": 1, \"baz\": 3}\n[\nplay\n]\nOmitBy\nReturns same map type filtered by given predicate.\nm\n:=\nlo\n.\nOmitBy\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n,\n\"baz\"\n:\n3\n},\nfunc\n(\nkey\nstring\n,\nvalue\nint\n)\nbool\n{\nreturn\nvalue\n%\n2\n==\n1\n})\n// map[string]int{\"bar\": 2}\n[\nplay\n]\nOmitByKeys\nReturns same map type filtered by given keys.\nm\n:=\nlo\n.\nOmitByKeys\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n,\n\"baz\"\n:\n3\n}, []\nstring\n{\n\"foo\"\n,\n\"baz\"\n})\n// map[string]int{\"bar\": 2}\n[\nplay\n]\nOmitByValues\nReturns same map type filtered by given values.\nm\n:=\nlo\n.\nOmitByValues\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n,\n\"baz\"\n:\n3\n}, []\nint\n{\n1\n,\n3\n})\n// map[string]int{\"bar\": 2}\n[\nplay\n]\nEntries (alias: ToPairs)\nTransforms a map into a slice of key/value pairs.\nentries\n:=\nlo\n.\nEntries\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n})\n// []lo.Entry[string, int]{\n//     {\n//         Key: \"foo\",\n//         Value: 1,\n//     },\n//     {\n//         Key: \"bar\",\n//         Value: 2,\n//     },\n// }\n[\nplay\n]\nFromEntries (alias: FromPairs)\nTransforms a slice of key/value pairs into a map.\nm\n:=\nlo\n.\nFromEntries\n([]lo.\nEntry\n[\nstring\n,\nint\n]{\n    {\nKey\n:\n\"foo\"\n,\nValue\n:\n1\n,\n    },\n    {\nKey\n:\n\"bar\"\n,\nValue\n:\n2\n,\n    },\n})\n// map[string]int{\"foo\": 1, \"bar\": 2}\n[\nplay\n]\nInvert\nCreates a map composed of the inverted keys and values. If map contains duplicate values, subsequent values overwrite property assignments of previous values.\nm1\n:=\nlo\n.\nInvert\n(\nmap\n[\nstring\n]\nint\n{\n\"a\"\n:\n1\n,\n\"b\"\n:\n2\n})\n// map[int]string{1: \"a\", 2: \"b\"}\nm2\n:=\nlo\n.\nInvert\n(\nmap\n[\nstring\n]\nint\n{\n\"a\"\n:\n1\n,\n\"b\"\n:\n2\n,\n\"c\"\n:\n1\n})\n// map[int]string{1: \"c\", 2: \"b\"}\n[\nplay\n]\nAssign\nMerges multiple maps from left to right.\nmergedMaps\n:=\nlo\n.\nAssign\n(\nmap\n[\nstring\n]\nint\n{\n\"a\"\n:\n1\n,\n\"b\"\n:\n2\n},\nmap\n[\nstring\n]\nint\n{\n\"b\"\n:\n3\n,\n\"c\"\n:\n4\n},\n)\n// map[string]int{\"a\": 1, \"b\": 3, \"c\": 4}\n[\nplay\n]\nChunkEntries\nSplits a map into a slice of elements in groups of length equal to its size. If the map cannot be split evenly, the final chunk will contain the remaining elements.\nmaps\n:=\nlo\n.\nChunkEntries\n(\nmap\n[\nstring\n]\nint\n{\n\"a\"\n:\n1\n,\n\"b\"\n:\n2\n,\n\"c\"\n:\n3\n,\n\"d\"\n:\n4\n,\n\"e\"\n:\n5\n,\n    },\n3\n,\n)\n// []map[string]int{\n//    {\"a\": 1, \"b\": 2, \"c\": 3},\n//    {\"d\": 4, \"e\": 5},\n// }\n[\nplay\n]\nMapKeys\nManipulates map keys and transforms it to a map of another type.\nm2\n:=\nlo\n.\nMapKeys\n(\nmap\n[\nint\n]\nint\n{\n1\n:\n1\n,\n2\n:\n2\n,\n3\n:\n3\n,\n4\n:\n4\n},\nfunc\n(\n_\nint\n,\nv\nint\n)\nstring\n{\nreturn\nstrconv\n.\nFormatInt\n(\nint64\n(\nv\n),\n10\n)\n})\n// map[string]int{\"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4}\n[\nplay\n]\nMapValues\nManipulates map values and transforms it to a map of another type.\nm1\n:=\nmap\n[\nint\n]\nint64\n{\n1\n:\n1\n,\n2\n:\n2\n,\n3\n:\n3\n}\nm2\n:=\nlo\n.\nMapValues\n(\nm1\n,\nfunc\n(\nx\nint64\n,\n_\nint\n)\nstring\n{\nreturn\nstrconv\n.\nFormatInt\n(\nx\n,\n10\n)\n})\n// map[int]string{1: \"1\", 2: \"2\", 3: \"3\"}\n[\nplay\n]\nMapEntries\nManipulates map entries and transforms it to a map of another type.\nin\n:=\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n}\nout\n:=\nlo\n.\nMapEntries\n(\nin\n,\nfunc\n(\nk\nstring\n,\nv\nint\n) (\nint\n,\nstring\n) {\nreturn\nv\n,\nk\n})\n// map[int]string{1: \"foo\", 2: \"bar\"}\n[\nplay\n]\nMapToSlice\nTransforms a map into a slice based on specified iteratee.\nm\n:=\nmap\n[\nint\n]\nint64\n{\n1\n:\n4\n,\n2\n:\n5\n,\n3\n:\n6\n}\ns\n:=\nlo\n.\nMapToSlice\n(\nm\n,\nfunc\n(\nk\nint\n,\nv\nint64\n)\nstring\n{\nreturn\nfmt\n.\nSprintf\n(\n\"%d_%d\"\n,\nk\n,\nv\n)\n})\n// []string{\"1_4\", \"2_5\", \"3_6\"}\n[\nplay\n]\nFilterMapToSlice\nTransforms a map into a slice based on specified iteratee. The iteratee returns a value and a boolean. If the boolean is true, the value is added to the result slice.\nIf the boolean is false, the value is not added to the result slice. The order of the keys in the input map is not specified and the order of the keys in the output slice is not guaranteed.\nkv\n:=\nmap\n[\nint\n]\nint64\n{\n1\n:\n1\n,\n2\n:\n2\n,\n3\n:\n3\n,\n4\n:\n4\n}\nresult\n:=\nlo\n.\nFilterMapToSlice\n(\nkv\n,\nfunc\n(\nk\nint\n,\nv\nint64\n) (\nstring\n,\nbool\n) {\nreturn\nfmt\n.\nSprintf\n(\n\"%d_%d\"\n,\nk\n,\nv\n),\nk\n%\n2\n==\n0\n})\n// []{\"2_2\", \"4_4\"}\nFilterKeys\nTransforms a map into a slice based on predicate returns true for specific elements. It is a mix of\nlo.Filter()\nand\nlo.Keys()\n.\nkv\n:=\nmap\n[\nint\n]\nstring\n{\n1\n:\n\"foo\"\n,\n2\n:\n\"bar\"\n,\n3\n:\n\"baz\"\n}\nresult\n:=\nFilterKeys\n(\nkv\n,\nfunc\n(\nk\nint\n,\nv\nstring\n)\nbool\n{\nreturn\nv\n==\n\"foo\"\n})\n// [1]\n[\nplay\n]\nFilterValues\nTransforms a map into a slice based on predicate returns true for specific elements. It is a mix of\nlo.Filter()\nand\nlo.Values()\n.\nkv\n:=\nmap\n[\nint\n]\nstring\n{\n1\n:\n\"foo\"\n,\n2\n:\n\"bar\"\n,\n3\n:\n\"baz\"\n}\nresult\n:=\nFilterValues\n(\nkv\n,\nfunc\n(\nk\nint\n,\nv\nstring\n)\nbool\n{\nreturn\nv\n==\n\"foo\"\n})\n// [\"foo\"]\n[\nplay\n]\nRange / RangeFrom / RangeWithSteps\nCreates a slice of numbers (positive and/or negative) progressing from start up to, but not including end.\nresult\n:=\nlo\n.\nRange\n(\n4\n)\n// [0, 1, 2, 3]\nresult\n:=\nlo\n.\nRange\n(\n-\n4\n)\n// [0, -1, -2, -3]\nresult\n:=\nlo\n.\nRangeFrom\n(\n1\n,\n5\n)\n// [1, 2, 3, 4, 5]\nresult\n:=\nlo\n.\nRangeFrom\n[\nfloat64\n](\n1.0\n,\n5\n)\n// [1.0, 2.0, 3.0, 4.0, 5.0]\nresult\n:=\nlo\n.\nRangeWithSteps\n(\n0\n,\n20\n,\n5\n)\n// [0, 5, 10, 15]\nresult\n:=\nlo\n.\nRangeWithSteps\n[\nfloat32\n](\n-\n1.0\n,\n-\n4.0\n,\n-\n1.0\n)\n// [-1.0, -2.0, -3.0]\nresult\n:=\nlo\n.\nRangeWithSteps\n(\n1\n,\n4\n,\n-\n1\n)\n// []\nresult\n:=\nlo\n.\nRange\n(\n0\n)\n// []\n[\nplay\n]\nClamp\nClamps number within the inclusive lower and upper bounds.\nr1\n:=\nlo\n.\nClamp\n(\n0\n,\n-\n10\n,\n10\n)\n// 0\nr2\n:=\nlo\n.\nClamp\n(\n-\n42\n,\n-\n10\n,\n10\n)\n// -10\nr3\n:=\nlo\n.\nClamp\n(\n42\n,\n-\n10\n,\n10\n)\n// 10\n[\nplay\n]\nSum\nSums the values in a collection.\nIf collection is empty 0 is returned.\nlist\n:=\n[]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}\nsum\n:=\nlo\n.\nSum\n(\nlist\n)\n// 15\n[\nplay\n]\nSumBy\nSummarizes the values in a collection using the given return value from the iteration function.\nIf collection is empty 0 is returned.\nstrings\n:=\n[]\nstring\n{\n\"foo\"\n,\n\"bar\"\n}\nsum\n:=\nlo\n.\nSumBy\n(\nstrings\n,\nfunc\n(\nitem\nstring\n)\nint\n{\nreturn\nlen\n(\nitem\n)\n})\n// 6\nProduct\nCalculates the product of the values in a collection.\nIf collection is empty 0 is returned.\nlist\n:=\n[]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}\nproduct\n:=\nlo\n.\nProduct\n(\nlist\n)\n// 120\n[\nplay\n]\nProductBy\nCalculates the product of the values in a collection using the given return value from the iteration function.\nIf collection is empty 0 is returned.\nstrings\n:=\n[]\nstring\n{\n\"foo\"\n,\n\"bar\"\n}\nproduct\n:=\nlo\n.\nProductBy\n(\nstrings\n,\nfunc\n(\nitem\nstring\n)\nint\n{\nreturn\nlen\n(\nitem\n)\n})\n// 9\n[\nplay\n]\nMean\nCalculates the mean of a collection of numbers.\nIf collection is empty 0 is returned.\nmean\n:=\nlo\n.\nMean\n([]\nint\n{\n2\n,\n3\n,\n4\n,\n5\n})\n// 3\nmean\n:=\nlo\n.\nMean\n([]\nfloat64\n{\n2\n,\n3\n,\n4\n,\n5\n})\n// 3.5\nmean\n:=\nlo\n.\nMean\n([]\nfloat64\n{})\n// 0\nMeanBy\nCalculates the mean of a collection of numbers using the given return value from the iteration function.\nIf collection is empty 0 is returned.\nlist\n:=\n[]\nstring\n{\n\"aa\"\n,\n\"bbb\"\n,\n\"cccc\"\n,\n\"ddddd\"\n}\nmapper\n:=\nfunc\n(\nitem\nstring\n)\nfloat64\n{\nreturn\nfloat64\n(\nlen\n(\nitem\n))\n}\nmean\n:=\nlo\n.\nMeanBy\n(\nlist\n,\nmapper\n)\n// 3.5\nmean\n:=\nlo\n.\nMeanBy\n([]\nfloat64\n{},\nmapper\n)\n// 0\n[\nplay\n]\nMode\nCalculates the mode (most frequent value) of a collection of numbers.\nIf multiple values have the same highest frequency, then multiple values are returned.\nIf the collection is empty, the zero value of\nT[]\nis returned.\nmode\n:=\nlo\n.\nMode\n([]\nint\n{\n2\n,\n2\n,\n3\n,\n4\n})\n// [2]\nmode\n:=\nlo\n.\nMode\n([]\nfloat64\n{\n2\n,\n2\n,\n3\n,\n3\n})\n// [2, 3]\nmode\n:=\nlo\n.\nMode\n([]\nfloat64\n{})\n// []\nmode\n:=\nlo\n.\nMode\n([]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n,\n5\n,\n6\n,\n7\n,\n8\n,\n9\n})\n// [1, 2, 3, 4, 5, 6, 7, 8, 9]\nRandomString\nReturns a random string of the specified length and made of the specified charset.\nstr\n:=\nlo\n.\nRandomString\n(\n5\n,\nlo\n.\nLettersCharset\n)\n// example: \"eIGbt\"\n[\nplay\n]\nSubstring\nReturn part of a string.\nsub\n:=\nlo\n.\nSubstring\n(\n\"hello\"\n,\n2\n,\n3\n)\n// \"llo\"\nsub\n:=\nlo\n.\nSubstring\n(\n\"hello\"\n,\n-\n4\n,\n3\n)\n// \"ell\"\nsub\n:=\nlo\n.\nSubstring\n(\n\"hello\"\n,\n-\n2\n,\nmath\n.\nMaxUint\n)\n// \"lo\"\n[\nplay\n]\nChunkString\nReturns a slice of strings split into groups of length size. If the string can't be split evenly, the final chunk will be the remaining characters.\nlo\n.\nChunkString\n(\n\"123456\"\n,\n2\n)\n// []string{\"12\", \"34\", \"56\"}\nlo\n.\nChunkString\n(\n\"1234567\"\n,\n2\n)\n// []string{\"12\", \"34\", \"56\", \"7\"}\nlo\n.\nChunkString\n(\n\"\"\n,\n2\n)\n// []string{\"\"}\nlo\n.\nChunkString\n(\n\"1\"\n,\n2\n)\n// []string{\"1\"}\n[\nplay\n]\nRuneLength\nAn alias to utf8.RuneCountInString which returns the number of runes in string.\nsub\n:=\nlo\n.\nRuneLength\n(\n\"hellô\"\n)\n// 5\nsub\n:=\nlen\n(\n\"hellô\"\n)\n// 6\n[\nplay\n]\nPascalCase\nConverts string to pascal case.\nstr\n:=\nlo\n.\nPascalCase\n(\n\"hello_world\"\n)\n// HelloWorld\n[\nplay\n]\nCamelCase\nConverts string to camel case.\nstr\n:=\nlo\n.\nCamelCase\n(\n\"hello_world\"\n)\n// helloWorld\n[\nplay\n]\nKebabCase\nConverts string to kebab case.\nstr\n:=\nlo\n.\nKebabCase\n(\n\"helloWorld\"\n)\n// hello-world\n[\nplay\n]\nSnakeCase\nConverts string to snake case.\nstr\n:=\nlo\n.\nSnakeCase\n(\n\"HelloWorld\"\n)\n// hello_world\n[\nplay\n]\nWords\nSplits string into a slice of its words.\nstr\n:=\nlo\n.\nWords\n(\n\"helloWorld\"\n)\n// []string{\"hello\", \"world\"}\n[\nplay\n]\nCapitalize\nConverts the first character of string to upper case and the remaining to lower case.\nstr\n:=\nlo\n.\nCapitalize\n(\n\"heLLO\"\n)\n// Hello\n[\nplay\n]\nEllipsis\nTrims and truncates a string to a specified length in\nbytes\nand appends an ellipsis if truncated. If the string contains non-ASCII characters (which may occupy multiple bytes in UTF-8), truncating by byte length may split a character in the middle, potentially resulting in garbled output.\nstr\n:=\nlo\n.\nEllipsis\n(\n\"  Lorem Ipsum  \"\n,\n5\n)\n// Lo...\nstr\n:=\nlo\n.\nEllipsis\n(\n\"Lorem Ipsum\"\n,\n100\n)\n// Lorem Ipsum\nstr\n:=\nlo\n.\nEllipsis\n(\n\"Lorem Ipsum\"\n,\n3\n)\n// ...\n[\nplay\n]\nT2 -> T9\nCreates a tuple from a list of values.\ntuple1\n:=\nlo\n.\nT2\n(\n\"x\"\n,\n1\n)\n// Tuple2[string, int]{A: \"x\", B: 1}\nfunc\nexample\n() (\nstring\n,\nint\n) {\nreturn\n\"y\"\n,\n2\n}\ntuple2\n:=\nlo\n.\nT2\n(\nexample\n())\n// Tuple2[string, int]{A: \"y\", B: 2}\n[\nplay\n]\nUnpack2 -> Unpack9\nReturns values contained in a tuple.\nr1\n,\nr2\n:=\nlo\n.\nUnpack2\n(lo.\nTuple2\n[\nstring\n,\nint\n]{\n\"a\"\n,\n1\n})\n// \"a\", 1\nUnpack is also available as a method of TupleX.\ntuple2\n:=\nlo\n.\nT2\n(\n\"a\"\n,\n1\n)\na\n,\nb\n:=\ntuple2\n.\nUnpack\n()\n// \"a\", 1\n[\nplay\n]\nZip2 -> Zip9\nZip creates a slice of grouped elements, the first of which contains the first elements of the given slices, the second of which contains the second elements of the given slices, and so on.\nWhen collections are different sizes, the Tuple attributes are filled with zero value.\ntuples\n:=\nlo\n.\nZip2\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n}, []\nint\n{\n1\n,\n2\n})\n// []Tuple2[string, int]{{A: \"a\", B: 1}, {A: \"b\", B: 2}}\n[\nplay\n]\nZipBy2 -> ZipBy9\nZipBy creates a slice of transformed elements, the first of which contains the first elements of the given slices, the second of which contains the second elements of the given slices, and so on.\nWhen collections are different sizes, the Tuple attributes are filled with zero value.\nitems\n:=\nlo\n.\nZipBy2\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n}, []\nint\n{\n1\n,\n2\n},\nfunc\n(\na\nstring\n,\nb\nint\n)\nstring\n{\nreturn\nfmt\n.\nSprintf\n(\n\"%s-%d\"\n,\na\n,\nb\n)\n})\n// []string{\"a-1\", \"b-2\"}\nUnzip2 -> Unzip9\nUnzip accepts a slice of grouped elements and creates a slice regrouping the elements to their pre-zip configuration.\na\n,\nb\n:=\nlo\n.\nUnzip2\n([]\nTuple2\n[\nstring\n,\nint\n]{{\nA\n:\n\"a\"\n,\nB\n:\n1\n}, {\nA\n:\n\"b\"\n,\nB\n:\n2\n}})\n// []string{\"a\", \"b\"}\n// []int{1, 2}\n[\nplay\n]\nUnzipBy2 -> UnzipBy9\nUnzipBy2 iterates over a collection and creates a slice regrouping the elements to their pre-zip configuration.\na\n,\nb\n:=\nlo\n.\nUnzipBy2\n([]\nstring\n{\n\"hello\"\n,\n\"john\"\n,\n\"doe\"\n},\nfunc\n(\nstr\nstring\n) (\nstring\n,\nint\n) {\nreturn\nstr\n,\nlen\n(\nstr\n)\n})\n// []string{\"hello\", \"john\", \"doe\"}\n// []int{5, 4, 3}\nCrossJoin2 -> CrossJoin9\nCombines every item from one list with every item from others. It is the cartesian product of lists received as arguments. Returns an empty list if a list is empty.\nresult\n:=\nlo\n.\nCrossJoin2\n([]\nstring\n{\n\"hello\"\n,\n\"john\"\n,\n\"doe\"\n}, []\nint\n{\n1\n,\n2\n})\n// lo.Tuple2{\"hello\", 1}\n// lo.Tuple2{\"hello\", 2}\n// lo.Tuple2{\"john\", 1}\n// lo.Tuple2{\"john\", 2}\n// lo.Tuple2{\"doe\", 1}\n// lo.Tuple2{\"doe\", 2}\nCrossJoinBy2 -> CrossJoinBy9\nCombines every item from one list with every item from others. It is the cartesian product of lists received as arguments. The project function is used to create the output values. Returns an empty list if a list is empty.\nresult\n:=\nlo\n.\nCrossJoinBy2\n([]\nstring\n{\n\"hello\"\n,\n\"john\"\n,\n\"doe\"\n}, []\nint\n{\n1\n,\n2\n},\nfunc\n(\na\nA\n,\nb\nB\n)\nstring\n{\nreturn\nfmt\n.\nSprintf\n(\n\"%s - %d\"\n,\na\n,\nb\n)\n})\n// \"hello - 1\"\n// \"hello - 2\"\n// \"john - 1\"\n// \"john - 2\"\n// \"doe - 1\"\n// \"doe - 2\"\nDuration\nReturns the time taken to execute a function.\nduration\n:=\nlo\n.\nDuration\n(\nfunc\n() {\n// very long job\n})\n// 3s\n[\nplay\n]\nDuration0 -> Duration10\nReturns the time taken to execute a function.\nduration\n:=\nlo\n.\nDuration0\n(\nfunc\n() {\n// very long job\n})\n// 3s\nerr\n,\nduration\n:=\nlo\n.\nDuration1\n(\nfunc\n()\nerror\n{\n// very long job\nreturn\nerrors\n.\nNew\n(\n\"an error\"\n)\n})\n// an error\n// 3s\nstr\n,\nnbr\n,\nerr\n,\nduration\n:=\nlo\n.\nDuration3\n(\nfunc\n() (\nstring\n,\nint\n,\nerror\n) {\n// very long job\nreturn\n\"hello\"\n,\n42\n,\nnil\n})\n// hello\n// 42\n// nil\n// 3s\nChannelDispatcher\nDistributes messages from input channels into N child channels. Close events are propagated to children.\nUnderlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\nch\n:=\nmake\n(\nchan\nint\n,\n42\n)\nfor\ni\n:=\n0\n;\ni\n<=\n10\n;\ni\n++\n{\nch\n<-\ni\n}\nchildren\n:=\nlo\n.\nChannelDispatcher\n(\nch\n,\n5\n,\n10\n,\nDispatchingStrategyRoundRobin\n[\nint\n])\n// []<-chan int{...}\nconsumer\n:=\nfunc\n(\nc\n<-\nchan\nint\n) {\nfor\n{\nmsg\n,\nok\n:=\n<-\nc\nif\n!\nok\n{\nprintln\n(\n\"closed\"\n)\nbreak\n}\nprintln\n(\nmsg\n)\n    }\n}\nfor\ni\n:=\nrange\nchildren\n{\ngo\nconsumer\n(\nchildren\n[\ni\n])\n}\n[\nplay\n]\nMany distributions strategies are available:\nlo.DispatchingStrategyRoundRobin\n: Distributes messages in a rotating sequential manner.\nlo.DispatchingStrategyRandom\n: Distributes messages in a random manner.\nlo.DispatchingStrategyWeightedRandom\n: Distributes messages in a weighted manner.\nlo.DispatchingStrategyFirst\n: Distributes messages in the first non-full channel.\nlo.DispatchingStrategyLeast\n: Distributes messages in the emptiest channel.\nlo.DispatchingStrategyMost\n: Distributes to the fullest channel.\nSome strategies bring fallback, in order to favor non-blocking behaviors. See implementations.\nFor custom strategies, just implement the\nlo.DispatchingStrategy\nprototype:\ntype\nDispatchingStrategy\n[\nT\nany\n]\nfunc\n(\nmessage\nT\n,\nmessageIndex\nuint64\n,\nchannels\n[]\n<-\nchan\nT\n)\nint\nEg:\ntype\nMessage\nstruct\n{\nTenantID\nuuid.\nUUID\n}\nfunc\nhash\n(\nid\nuuid.\nUUID\n)\nint\n{\nh\n:=\nfnv\n.\nNew32a\n()\nh\n.\nWrite\n([]\nbyte\n(\nid\n.\nString\n()))\nreturn\nint\n(\nh\n.\nSum32\n())\n}\n// Routes messages per TenantID.\ncustomStrategy\n:=\nfunc\n(\nmessage\nstring\n,\nmessageIndex\nuint64\n,\nchannels\n[]\n<-\nchan\nstring\n)\nint\n{\ndestination\n:=\nhash\n(\nmessage\n)\n%\nlen\n(\nchannels\n)\n// check if channel is full\nif\nlen\n(\nchannels\n[\ndestination\n])\n<\ncap\n(\nchannels\n[\ndestination\n]) {\nreturn\ndestination\n}\n// fallback when child channel is full\nreturn\nutils\n.\nDispatchingStrategyRoundRobin\n(\nmessage\n,\nuint64\n(\ndestination\n),\nchannels\n)\n}\nchildren\n:=\nlo\n.\nChannelDispatcher\n(\nch\n,\n5\n,\n10\n,\ncustomStrategy\n)\n...\nSliceToChannel\nReturns a read-only channel of collection elements. Channel is closed after last element. Channel capacity can be customized.\nlist\n:=\n[]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}\nfor\nv\n:=\nrange\nlo\n.\nSliceToChannel\n(\n2\n,\nlist\n) {\nprintln\n(\nv\n)\n}\n// prints 1, then 2, then 3, then 4, then 5\n[\nplay\n]\nChannelToSlice\nReturns a slice built from channel items. Blocks until channel closes.\nlist\n:=\n[]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}\nch\n:=\nlo\n.\nSliceToChannel\n(\n2\n,\nlist\n)\nitems\n:=\nChannelToSlice\n(\nch\n)\n// []int{1, 2, 3, 4, 5}\nGenerator\nImplements the generator design pattern. Channel is closed after last element. Channel capacity can be customized.\ngenerator\n:=\nfunc\n(\nyield\nfunc\n(\nint\n)) {\nyield\n(\n1\n)\nyield\n(\n2\n)\nyield\n(\n3\n)\n}\nfor\nv\n:=\nrange\nlo\n.\nGenerator\n(\n2\n,\ngenerator\n) {\nprintln\n(\nv\n)\n}\n// prints 1, then 2, then 3\nBuffer\nCreates a slice of n elements from a channel. Returns the slice, the slice length, the read time and the channel status (opened/closed).\nch\n:=\nlo\n.\nSliceToChannel\n(\n2\n, []\nint\n{\n1\n,\n2\n,\n3\n,\n4\n,\n5\n})\nitems1\n,\nlength1\n,\nduration1\n,\nok1\n:=\nlo\n.\nBuffer\n(\nch\n,\n3\n)\n// []int{1, 2, 3}, 3, 0s, true\nitems2\n,\nlength2\n,\nduration2\n,\nok2\n:=\nlo\n.\nBuffer\n(\nch\n,\n3\n)\n// []int{4, 5}, 2, 0s, false\nExample: RabbitMQ consumer 👇\nch\n:=\nreadFromQueue\n()\nfor\n{\n// read 1k items\nitems\n,\nlength\n,\n_\n,\nok\n:=\nlo\n.\nBuffer\n(\nch\n,\n1000\n)\n// do batching stuff\nif\n!\nok\n{\nbreak\n}\n}\nBufferWithContext\nCreates a slice of n elements from a channel, with timeout. Returns the slice, the slice length, the read time and the channel status (opened/closed).\nctx\n,\ncancel\n:=\ncontext\n.\nWithCancel\n(\ncontext\n.\nTODO\n())\ngo\nfunc\n() {\nch\n<-\n0\ntime\n.\nSleep\n(\n10\n*\ntime\n.\nMillisecond\n)\nch\n<-\n1\ntime\n.\nSleep\n(\n10\n*\ntime\n.\nMillisecond\n)\nch\n<-\n2\ntime\n.\nSleep\n(\n10\n*\ntime\n.\nMillisecond\n)\nch\n<-\n3\ntime\n.\nSleep\n(\n10\n*\ntime\n.\nMillisecond\n)\nch\n<-\n4\ntime\n.\nSleep\n(\n10\n*\ntime\n.\nMillisecond\n)\ncancel\n()\n}()\nitems1\n,\nlength1\n,\nduration1\n,\nok1\n:=\nlo\n.\nBufferWithContext\n(\nctx\n,\nch\n,\n3\n)\n// []int{0, 1, 2}, 3, 20ms, true\nitems2\n,\nlength2\n,\nduration2\n,\nok2\n:=\nlo\n.\nBufferWithContext\n(\nctx\n,\nch\n,\n3\n)\n// []int{3, 4}, 2, 30ms, false\nBufferWithTimeout\nCreates a slice of n elements from a channel, with timeout. Returns the slice, the slice length, the read time and the channel status (opened/closed).\ngenerator\n:=\nfunc\n(\nyield\nfunc\n(\nint\n)) {\nfor\ni\n:=\n0\n;\ni\n<\n5\n;\ni\n++\n{\nyield\n(\ni\n)\ntime\n.\nSleep\n(\n35\n*\ntime\n.\nMillisecond\n)\n    }\n}\nch\n:=\nlo\n.\nGenerator\n(\n0\n,\ngenerator\n)\nitems1\n,\nlength1\n,\nduration1\n,\nok1\n:=\nlo\n.\nBufferWithTimeout\n(\nch\n,\n3\n,\n100\n*\ntime\n.\nMillisecond\n)\n// []int{1, 2}, 2, 100ms, true\nitems2\n,\nlength2\n,\nduration2\n,\nok2\n:=\nlo\n.\nBufferWithTimeout\n(\nch\n,\n3\n,\n100\n*\ntime\n.\nMillisecond\n)\n// []int{3, 4, 5}, 3, 75ms, true\nitems3\n,\nlength3\n,\nduration2\n,\nok3\n:=\nlo\n.\nBufferWithTimeout\n(\nch\n,\n3\n,\n100\n*\ntime\n.\nMillisecond\n)\n// []int{}, 0, 10ms, false\nExample: RabbitMQ consumer 👇\nch\n:=\nreadFromQueue\n()\nfor\n{\n// read 1k items\n// wait up to 1 second\nitems\n,\nlength\n,\n_\n,\nok\n:=\nlo\n.\nBufferWithTimeout\n(\nch\n,\n1000\n,\n1\n*\ntime\n.\nSecond\n)\n// do batching stuff\nif\n!\nok\n{\nbreak\n}\n}\nExample: Multithreaded RabbitMQ consumer 👇\nch\n:=\nreadFromQueue\n()\n// 5 workers\n// prefetch 1k messages per worker\nchildren\n:=\nlo\n.\nChannelDispatcher\n(\nch\n,\n5\n,\n1000\n,\nlo\n.\nDispatchingStrategyFirst\n[\nint\n])\nconsumer\n:=\nfunc\n(\nc\n<-\nchan\nint\n) {\nfor\n{\n// read 1k items\n// wait up to 1 second\nitems\n,\nlength\n,\n_\n,\nok\n:=\nlo\n.\nBufferWithTimeout\n(\nch\n,\n1000\n,\n1\n*\ntime\n.\nSecond\n)\n// do batching stuff\nif\n!\nok\n{\nbreak\n}\n    }\n}\nfor\ni\n:=\nrange\nchildren\n{\ngo\nconsumer\n(\nchildren\n[\ni\n])\n}\nFanIn\nMerge messages from multiple input channels into a single buffered channel. Output messages have no priority. When all upstream channels reach EOF, downstream channel closes.\nstream1\n:=\nmake\n(\nchan\nint\n,\n42\n)\nstream2\n:=\nmake\n(\nchan\nint\n,\n42\n)\nstream3\n:=\nmake\n(\nchan\nint\n,\n42\n)\nall\n:=\nlo\n.\nFanIn\n(\n100\n,\nstream1\n,\nstream2\n,\nstream3\n)\n// <-chan int\nFanOut\nBroadcasts all the upstream messages to multiple downstream channels. When upstream channel reaches EOF, downstream channels close. If any downstream channels is full, broadcasting is paused.\nstream\n:=\nmake\n(\nchan\nint\n,\n42\n)\nall\n:=\nlo\n.\nFanOut\n(\n5\n,\n100\n,\nstream\n)\n// [5]<-chan int\nContains\nReturns true if an element is present in a collection.\npresent\n:=\nlo\n.\nContains\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n},\n5\n)\n// true\n[\nplay\n]\nContainsBy\nReturns true if the predicate function returns\ntrue\n.\npresent\n:=\nlo\n.\nContainsBy\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n},\nfunc\n(\nx\nint\n)\nbool\n{\nreturn\nx\n==\n3\n})\n// true\nEvery\nReturns true if all elements of a subset are contained in a collection or if the subset is empty.\nok\n:=\nlo\n.\nEvery\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}, []\nint\n{\n0\n,\n2\n})\n// true\nok\n:=\nlo\n.\nEvery\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}, []\nint\n{\n0\n,\n6\n})\n// false\nEveryBy\nReturns true if the predicate returns true for all elements in the collection or if the collection is empty.\nb\n:=\nEveryBy\n([]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n},\nfunc\n(\nx\nint\n)\nbool\n{\nreturn\nx\n<\n5\n})\n// true\n[\nplay\n]\nSome\nReturns true if at least 1 element of a subset is contained in a collection.\nIf the subset is empty Some returns false.\nok\n:=\nlo\n.\nSome\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}, []\nint\n{\n0\n,\n6\n})\n// true\n[\nplay\n]\nok := lo.Some([]int{0, 1, 2, 3, 4, 5}, []int{-1, 6})\n// false\n### SomeBy\n\nReturns true if the predicate returns true for any of the elements in the collection.\nIf the collection is empty SomeBy returns false.\n\n```go\nb := SomeBy([]int{1, 2, 3, 4}, func(x int) bool {\n    return x < 3\n})\n// true\nNone\nReturns true if no element of a subset is contained in a collection or if the subset is empty.\nb\n:=\nNone\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}, []\nint\n{\n0\n,\n2\n})\n// false\nb\n:=\nNone\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}, []\nint\n{\n-\n1\n,\n6\n})\n// true\n[\nplay\n]\nNoneBy\nReturns true if the predicate returns true for none of the elements in the collection or if the collection is empty.\nb\n:=\nNoneBy\n([]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n},\nfunc\n(\nx\nint\n)\nbool\n{\nreturn\nx\n<\n0\n})\n// true\n[\nplay\n]\nIntersect\nReturns the intersection between two collections.\nresult1\n:=\nlo\n.\nIntersect\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}, []\nint\n{\n0\n,\n2\n})\n// []int{0, 2}\nresult2\n:=\nlo\n.\nIntersect\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}, []\nint\n{\n0\n,\n6\n})\n// []int{0}\nresult3\n:=\nlo\n.\nIntersect\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}, []\nint\n{\n-\n1\n,\n6\n})\n// []int{}\nDifference\nReturns the difference between two collections.\nThe first value is the collection of elements absent from list2.\nThe second value is the collection of elements absent from list1.\nleft\n,\nright\n:=\nlo\n.\nDifference\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}, []\nint\n{\n0\n,\n2\n,\n6\n})\n// []int{1, 3, 4, 5}, []int{6}\nleft\n,\nright\n:=\nlo\n.\nDifference\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}, []\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n})\n// []int{}, []int{}\n[\nplay\n]\nUnion\nReturns all distinct elements from given collections. Result will not change the order of elements relatively.\nunion\n:=\nlo\n.\nUnion\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}, []\nint\n{\n0\n,\n2\n}, []\nint\n{\n0\n,\n10\n})\n// []int{0, 1, 2, 3, 4, 5, 10}\nWithout\nReturns a slice excluding all given values.\nsubset\n:=\nlo\n.\nWithout\n([]\nint\n{\n0\n,\n2\n,\n10\n},\n2\n)\n// []int{0, 10}\nsubset\n:=\nlo\n.\nWithout\n([]\nint\n{\n0\n,\n2\n,\n10\n},\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n)\n// []int{10}\nWithoutBy\nFilters a slice by excluding elements whose extracted keys match any in the exclude list.\nReturns a new slice containing only the elements whose keys are not in the exclude list.\ntype\nstruct\nUser\n{\nID\nint\nName\nstring\n}\n// original users\nusers\n:=\n[]\nUser\n{\n    {\nID\n:\n1\n,\nName\n:\n\"Alice\"\n},\n    {\nID\n:\n2\n,\nName\n:\n\"Bob\"\n},\n    {\nID\n:\n3\n,\nName\n:\n\"Charlie\"\n},\n}\n// extract function to get the user ID\ngetID\n:=\nfunc\n(\nuser\nUser\n)\nint\n{\nreturn\nuser\n.\nID\n}\n// exclude users with IDs 2 and 3\nexcludedIDs\n:=\n[]\nint\n{\n2\n,\n3\n}\n// filtering users\nfilteredUsers\n:=\nlo\n.\nWithoutBy\n(\nusers\n,\ngetID\n,\nexcludedIDs\n...\n)\n// []User[{ID: 1, Name: \"Alice\"}]\nWithoutEmpty\nReturns a slice excluding zero values.\nsubset\n:=\nlo\n.\nWithoutEmpty\n([]\nint\n{\n0\n,\n2\n,\n10\n})\n// []int{2, 10}\nWithoutNth\nReturns a slice excluding the nth value.\nsubset\n:=\nlo\n.\nWithoutNth\n([]\nint\n{\n-\n2\n,\n-\n1\n,\n0\n,\n1\n,\n2\n},\n3\n,\n-\n42\n,\n1\n)\n// []int{-2, 0, 2}\nElementsMatch\nReturns true if lists contain the same set of elements (including empty set).\nIf there are duplicate elements, the number of occurrences in each list should match.\nThe order of elements is not checked.\nb\n:=\nlo\n.\nElementsMatch\n([]\nint\n{\n1\n,\n1\n,\n2\n}, []\nint\n{\n2\n,\n1\n,\n1\n})\n// true\nElementsMatchBy\nReturns true if lists contain the same set of elements' keys (including empty set).\nIf there are duplicate keys, the number of occurrences in each list should match.\nThe order of elements is not checked.\nb\n:=\nlo\n.\nElementsMatchBy\n(\n    []\nsomeType\n{\na\n,\nb\n},\n    []\nsomeType\n{\nb\n,\na\n},\nfunc\n(\nitem\nsomeType\n)\nstring\n{\nreturn\nitem\n.\nID\n() },\n)\n// true\nIndexOf\nReturns the index at which the first occurrence of a value is found in a slice or -1 if the value cannot be found.\nfound\n:=\nlo\n.\nIndexOf\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n1\n,\n2\n,\n3\n},\n2\n)\n// 2\nnotFound\n:=\nlo\n.\nIndexOf\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n1\n,\n2\n,\n3\n},\n6\n)\n// -1\n[\nplay\n]\nLastIndexOf\nReturns the index at which the last occurrence of a value is found in a slice or -1 if the value cannot be found.\nfound\n:=\nlo\n.\nLastIndexOf\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n1\n,\n2\n,\n3\n},\n2\n)\n// 4\nnotFound\n:=\nlo\n.\nLastIndexOf\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n1\n,\n2\n,\n3\n},\n6\n)\n// -1\nHasPrefix\nReturns true if the collection has the prefix.\nok\n:=\nlo\n.\nHasPrefix\n([]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n}, []\nint\n{\n42\n})\n// false\nok\n:=\nlo\n.\nHasPrefix\n([]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n}, []\nint\n{\n1\n,\n2\n})\n// true\n[\nplay\n]\nHasSuffix\nReturns true if the collection has the suffix.\nok\n:=\nlo\n.\nHasSuffix\n([]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n}, []\nint\n{\n42\n})\n// false\nok\n:=\nlo\n.\nHasSuffix\n([]\nint\n{\n1\n,\n2\n,\n3\n,\n4\n}, []\nint\n{\n3\n,\n4\n})\n// true\n[\nplay\n]\nFind\nSearches for an element in a slice based on a predicate. Returns element and true if element was found.\nstr\n,\nok\n:=\nlo\n.\nFind\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n,\n\"d\"\n},\nfunc\n(\ni\nstring\n)\nbool\n{\nreturn\ni\n==\n\"b\"\n})\n// \"b\", true\nstr\n,\nok\n:=\nlo\n.\nFind\n([]\nstring\n{\n\"foobar\"\n},\nfunc\n(\ni\nstring\n)\nbool\n{\nreturn\ni\n==\n\"b\"\n})\n// \"\", false\n[\nplay\n]\nFindIndexOf\nFindIndexOf searches for an element in a slice based on a predicate and returns the index and true. Returns -1 and false if the element is not found.\nstr\n,\nindex\n,\nok\n:=\nlo\n.\nFindIndexOf\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"a\"\n,\n\"b\"\n},\nfunc\n(\ni\nstring\n)\nbool\n{\nreturn\ni\n==\n\"b\"\n})\n// \"b\", 1, true\nstr\n,\nindex\n,\nok\n:=\nlo\n.\nFindIndexOf\n([]\nstring\n{\n\"foobar\"\n},\nfunc\n(\ni\nstring\n)\nbool\n{\nreturn\ni\n==\n\"b\"\n})\n// \"\", -1, false\n[\nplay\n]\nFindLastIndexOf\nFindLastIndexOf searches for the last element in a slice based on a predicate and returns the index and true. Returns -1 and false if the element is not found.\nstr\n,\nindex\n,\nok\n:=\nlo\n.\nFindLastIndexOf\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"a\"\n,\n\"b\"\n},\nfunc\n(\ni\nstring\n)\nbool\n{\nreturn\ni\n==\n\"b\"\n})\n// \"b\", 4, true\nstr\n,\nindex\n,\nok\n:=\nlo\n.\nFindLastIndexOf\n([]\nstring\n{\n\"foobar\"\n},\nfunc\n(\ni\nstring\n)\nbool\n{\nreturn\ni\n==\n\"b\"\n})\n// \"\", -1, false\n[\nplay\n]\nFindOrElse\nSearches for an element in a slice based on a predicate. Returns the element if found or a given fallback value otherwise.\nstr\n:=\nlo\n.\nFindOrElse\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n,\n\"d\"\n},\n\"x\"\n,\nfunc\n(\ni\nstring\n)\nbool\n{\nreturn\ni\n==\n\"b\"\n})\n// \"b\"\nstr\n:=\nlo\n.\nFindOrElse\n([]\nstring\n{\n\"foobar\"\n},\n\"x\"\n,\nfunc\n(\ni\nstring\n)\nbool\n{\nreturn\ni\n==\n\"b\"\n})\n// \"x\"\nFindKey\nReturns the key of the first value matching.\nresult1\n,\nok1\n:=\nlo\n.\nFindKey\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n,\n\"baz\"\n:\n3\n},\n2\n)\n// \"bar\", true\nresult2\n,\nok2\n:=\nlo\n.\nFindKey\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n,\n\"baz\"\n:\n3\n},\n42\n)\n// \"\", false\ntype\ntest\nstruct\n{\nfoobar\nstring\n}\nresult3\n,\nok3\n:=\nlo\n.\nFindKey\n(\nmap\n[\nstring\n]\ntest\n{\n\"foo\"\n:\ntest\n{\n\"foo\"\n},\n\"bar\"\n:\ntest\n{\n\"bar\"\n},\n\"baz\"\n:\ntest\n{\n\"baz\"\n}},\ntest\n{\n\"foo\"\n})\n// \"foo\", true\nFindKeyBy\nReturns the key of the first element predicate returns true for.\nresult1\n,\nok1\n:=\nlo\n.\nFindKeyBy\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n,\n\"baz\"\n:\n3\n},\nfunc\n(\nk\nstring\n,\nv\nint\n)\nbool\n{\nreturn\nk\n==\n\"foo\"\n})\n// \"foo\", true\nresult2\n,\nok2\n:=\nlo\n.\nFindKeyBy\n(\nmap\n[\nstring\n]\nint\n{\n\"foo\"\n:\n1\n,\n\"bar\"\n:\n2\n,\n\"baz\"\n:\n3\n},\nfunc\n(\nk\nstring\n,\nv\nint\n)\nbool\n{\nreturn\nfalse\n})\n// \"\", false\nFindUniques\nReturns a slice with all the elements that appear in the collection only once. The order of result values is determined by the order they occur in the slice.\nuniqueValues\n:=\nlo\n.\nFindUniques\n([]\nint\n{\n1\n,\n2\n,\n2\n,\n1\n,\n2\n,\n3\n})\n// []int{3}\nFindUniquesBy\nReturns a slice with all the elements that appear in the collection only once. The order of result values is determined by the order they occur in the slice. It accepts\niteratee\nwhich is invoked for each element in the slice to generate the criterion by which uniqueness is computed.\nuniqueValues\n:=\nlo\n.\nFindUniquesBy\n([]\nint\n{\n3\n,\n4\n,\n5\n,\n6\n,\n7\n},\nfunc\n(\ni\nint\n)\nint\n{\nreturn\ni\n%\n3\n})\n// []int{5}\nFindDuplicates\nReturns a slice with the first occurrence of each duplicated element in the collection. The order of result values is determined by the order they occur in the slice.\nduplicatedValues\n:=\nlo\n.\nFindDuplicates\n([]\nint\n{\n1\n,\n2\n,\n2\n,\n1\n,\n2\n,\n3\n})\n// []int{1, 2}\nFindDuplicatesBy\nReturns a slice with the first occurrence of each duplicated element in the collection. The order of result values is determined by the order they occur in the slice. It accepts\niteratee\nwhich is invoked for each element in the slice to generate the criterion by which uniqueness is computed.\nduplicatedValues\n:=\nlo\n.\nFindDuplicatesBy\n([]\nint\n{\n3\n,\n4\n,\n5\n,\n6\n,\n7\n},\nfunc\n(\ni\nint\n)\nint\n{\nreturn\ni\n%\n3\n})\n// []int{3, 4}\nMin\nSearch the minimum value of a collection.\nReturns zero value when the collection is empty.\nmin\n:=\nlo\n.\nMin\n([]\nint\n{\n1\n,\n2\n,\n3\n})\n// 1\nmin\n:=\nlo\n.\nMin\n([]\nint\n{})\n// 0\nmin\n:=\nlo\n.\nMin\n([]time.\nDuration\n{\ntime\n.\nSecond\n,\ntime\n.\nHour\n})\n// 1s\n[\nplay\n]\nMinIndex\nSearch the minimum value of a collection and the index of the minimum value.\nReturns (zero value, -1) when the collection is empty.\nmin\n,\nindex\n:=\nlo\n.\nMinIndex\n([]\nint\n{\n1\n,\n2\n,\n3\n})\n// 1, 0\nmin\n,\nindex\n:=\nlo\n.\nMinIndex\n([]\nint\n{})\n// 0, -1\nmin\n,\nindex\n:=\nlo\n.\nMinIndex\n([]time.\nDuration\n{\ntime\n.\nSecond\n,\ntime\n.\nHour\n})\n// 1s, 0\nMinBy\nSearch the minimum value of a collection using the given comparison function.\nIf several values of the collection are equal to the smallest value, returns the first such value.\nReturns zero value when the collection is empty.\nmin\n:=\nlo\n.\nMinBy\n([]\nstring\n{\n\"s1\"\n,\n\"string2\"\n,\n\"s3\"\n},\nfunc\n(\nitem\nstring\n,\nmin\nstring\n)\nbool\n{\nreturn\nlen\n(\nitem\n)\n<\nlen\n(\nmin\n)\n})\n// \"s1\"\nmin\n:=\nlo\n.\nMinBy\n([]\nstring\n{},\nfunc\n(\nitem\nstring\n,\nmin\nstring\n)\nbool\n{\nreturn\nlen\n(\nitem\n)\n<\nlen\n(\nmin\n)\n})\n// \"\"\nMinIndexBy\nSearch the minimum value of a collection using the given comparison function and the index of the minimum value.\nIf several values of the collection are equal to the smallest value, returns the first such value.\nReturns (zero value, -1) when the collection is empty.\nmin\n,\nindex\n:=\nlo\n.\nMinIndexBy\n([]\nstring\n{\n\"s1\"\n,\n\"string2\"\n,\n\"s3\"\n},\nfunc\n(\nitem\nstring\n,\nmin\nstring\n)\nbool\n{\nreturn\nlen\n(\nitem\n)\n<\nlen\n(\nmin\n)\n})\n// \"s1\", 0\nmin\n,\nindex\n:=\nlo\n.\nMinIndexBy\n([]\nstring\n{},\nfunc\n(\nitem\nstring\n,\nmin\nstring\n)\nbool\n{\nreturn\nlen\n(\nitem\n)\n<\nlen\n(\nmin\n)\n})\n// \"\", -1\nEarliest\nSearch the minimum time.Time of a collection.\nReturns zero value when the collection is empty.\nearliest\n:=\nlo\n.\nEarliest\n(\ntime\n.\nNow\n(), time.\nTime\n{})\n// 0001-01-01 00:00:00 +0000 UTC\nEarliestBy\nSearch the minimum time.Time of a collection using the given iteratee function.\nReturns zero value when the collection is empty.\ntype\nfoo\nstruct\n{\nbar\ntime.\nTime\n}\nearliest\n:=\nlo\n.\nEarliestBy\n([]\nfoo\n{{\ntime\n.\nNow\n()}, {}},\nfunc\n(\ni\nfoo\n) time.\nTime\n{\nreturn\ni\n.\nbar\n})\n// {bar:{2023-04-01 01:02:03 +0000 UTC}}\nMax\nSearch the maximum value of a collection.\nReturns zero value when the collection is empty.\nmax\n:=\nlo\n.\nMax\n([]\nint\n{\n1\n,\n2\n,\n3\n})\n// 3\nmax\n:=\nlo\n.\nMax\n([]\nint\n{})\n// 0\nmax\n:=\nlo\n.\nMax\n([]time.\nDuration\n{\ntime\n.\nSecond\n,\ntime\n.\nHour\n})\n// 1h\nMaxIndex\nSearch the maximum value of a collection and the index of the maximum value.\nReturns (zero value, -1) when the collection is empty.\nmax\n,\nindex\n:=\nlo\n.\nMaxIndex\n([]\nint\n{\n1\n,\n2\n,\n3\n})\n// 3, 2\nmax\n,\nindex\n:=\nlo\n.\nMaxIndex\n([]\nint\n{})\n// 0, -1\nmax\n,\nindex\n:=\nlo\n.\nMaxIndex\n([]time.\nDuration\n{\ntime\n.\nSecond\n,\ntime\n.\nHour\n})\n// 1h, 1\nMaxBy\nSearch the maximum value of a collection using the given comparison function.\nIf several values of the collection are equal to the greatest value, returns the first such value.\nReturns zero value when the collection is empty.\nmax\n:=\nlo\n.\nMaxBy\n([]\nstring\n{\n\"string1\"\n,\n\"s2\"\n,\n\"string3\"\n},\nfunc\n(\nitem\nstring\n,\nmax\nstring\n)\nbool\n{\nreturn\nlen\n(\nitem\n)\n>\nlen\n(\nmax\n)\n})\n// \"string1\"\nmax\n:=\nlo\n.\nMaxBy\n([]\nstring\n{},\nfunc\n(\nitem\nstring\n,\nmax\nstring\n)\nbool\n{\nreturn\nlen\n(\nitem\n)\n>\nlen\n(\nmax\n)\n})\n// \"\"\nMaxIndexBy\nSearch the maximum value of a collection using the given comparison function and the index of the maximum value.\nIf several values of the collection are equal to the greatest value, returns the first such value.\nReturns (zero value, -1) when the collection is empty.\nmax\n,\nindex\n:=\nlo\n.\nMaxIndexBy\n([]\nstring\n{\n\"string1\"\n,\n\"s2\"\n,\n\"string3\"\n},\nfunc\n(\nitem\nstring\n,\nmax\nstring\n)\nbool\n{\nreturn\nlen\n(\nitem\n)\n>\nlen\n(\nmax\n)\n})\n// \"string1\", 0\nmax\n,\nindex\n:=\nlo\n.\nMaxIndexBy\n([]\nstring\n{},\nfunc\n(\nitem\nstring\n,\nmax\nstring\n)\nbool\n{\nreturn\nlen\n(\nitem\n)\n>\nlen\n(\nmax\n)\n})\n// \"\", -1\nLatest\nSearch the maximum time.Time of a collection.\nReturns zero value when the collection is empty.\nlatest\n:=\nlo\n.\nLatest\n(\ntime\n.\nNow\n(), time.\nTime\n{})\n// 2023-04-01 01:02:03 +0000 UTC\nLatestBy\nSearch the maximum time.Time of a collection using the given iteratee function.\nReturns zero value when the collection is empty.\ntype\nfoo\nstruct\n{\nbar\ntime.\nTime\n}\nlatest\n:=\nlo\n.\nLatestBy\n([]\nfoo\n{{\ntime\n.\nNow\n()}, {}},\nfunc\n(\ni\nfoo\n) time.\nTime\n{\nreturn\ni\n.\nbar\n})\n// {bar:{2023-04-01 01:02:03 +0000 UTC}}\nFirst\nReturns the first element of a collection and check for availability of the first element.\nfirst\n,\nok\n:=\nlo\n.\nFirst\n([]\nint\n{\n1\n,\n2\n,\n3\n})\n// 1, true\nfirst\n,\nok\n:=\nlo\n.\nFirst\n([]\nint\n{})\n// 0, false\nFirstOrEmpty\nReturns the first element of a collection or zero value if empty.\nfirst\n:=\nlo\n.\nFirstOrEmpty\n([]\nint\n{\n1\n,\n2\n,\n3\n})\n// 1\nfirst\n:=\nlo\n.\nFirstOrEmpty\n([]\nint\n{})\n// 0\nFirstOr\nReturns the first element of a collection or the fallback value if empty.\nfirst\n:=\nlo\n.\nFirstOr\n([]\nint\n{\n1\n,\n2\n,\n3\n},\n245\n)\n// 1\nfirst\n:=\nlo\n.\nFirstOr\n([]\nint\n{},\n31\n)\n// 31\nLast\nReturns the last element of a collection or error if empty.\nlast\n,\nok\n:=\nlo\n.\nLast\n([]\nint\n{\n1\n,\n2\n,\n3\n})\n// 3\n// true\nlast\n,\nok\n:=\nlo\n.\nLast\n([]\nint\n{})\n// 0\n// false\nLastOrEmpty\nReturns the last element of a collection or zero value if empty.\nlast\n:=\nlo\n.\nLastOrEmpty\n([]\nint\n{\n1\n,\n2\n,\n3\n})\n// 3\nlast\n:=\nlo\n.\nLastOrEmpty\n([]\nint\n{})\n// 0\nLastOr\nReturns the last element of a collection or the fallback value if empty.\nlast\n:=\nlo\n.\nLastOr\n([]\nint\n{\n1\n,\n2\n,\n3\n},\n245\n)\n// 3\nlast\n:=\nlo\n.\nLastOr\n([]\nint\n{},\n31\n)\n// 31\nNth\nReturns the element at index\nnth\nof collection. If\nnth\nis negative, the nth element from the end is returned. An error is returned when nth is out of slice bounds.\nnth\n,\nerr\n:=\nlo\n.\nNth\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n},\n2\n)\n// 2\nnth\n,\nerr\n:=\nlo\n.\nNth\n([]\nint\n{\n0\n,\n1\n,\n2\n,\n3\n},\n-\n2\n)\n// 2\nNthOr\nReturns the element at index\nnth\nof the collection. If\nnth\nis negative, it returns the\nnth\nelement from the end. If\nnth\nis out of slice bounds, it returns the provided fallback value\nnth\n:=\nlo\n.\nNthOr\n([]\nint\n{\n10\n,\n20\n,\n30\n,\n40\n,\n50\n},\n2\n,\n-\n1\n)\n// 30\nnth\n:=\nlo\n.\nNthOr\n([]\nint\n{\n10\n,\n20\n,\n30\n,\n40\n,\n50\n},\n-\n1\n,\n-\n1\n)\n// 50\nnth\n:=\nlo\n.\nNthOr\n([]\nint\n{\n10\n,\n20\n,\n30\n,\n40\n,\n50\n},\n5\n,\n-\n1\n)\n// -1 (fallback value)\nNthOrEmpty\nReturns the element at index\nnth\nof the collection. If\nnth\nis negative, it returns the\nnth\nelement from the end. If\nnth\nis out of slice bounds, it returns the zero value for the element type (e.g., 0 for integers, \"\" for strings, etc).\nnth\n:=\nlo\n.\nNthOrEmpty\n([]\nint\n{\n10\n,\n20\n,\n30\n,\n40\n,\n50\n},\n2\n)\n// 30\nnth\n:=\nlo\n.\nNthOrEmpty\n([]\nint\n{\n10\n,\n20\n,\n30\n,\n40\n,\n50\n},\n-\n1\n)\n// 50\nnth\n:=\nlo\n.\nNthOrEmpty\n([]\nint\n{\n10\n,\n20\n,\n30\n,\n40\n,\n50\n},\n5\n)\n// 0 (zero value for int)\nnth\n:=\nlo\n.\nNthOrEmpty\n([]\nstring\n{\n\"apple\"\n,\n\"banana\"\n,\n\"cherry\"\n},\n2\n)\n// \"cherry\"\nnth\n:=\nlo\n.\nNthOrEmpty\n([]\nstring\n{\n\"apple\"\n,\n\"banana\"\n,\n\"cherry\"\n},\n5\n)\n// \"\" (zero value for string)\nSample\nReturns a random item from collection.\nlo\n.\nSample\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n})\n// a random string from []string{\"a\", \"b\", \"c\"}\nlo\n.\nSample\n([]\nstring\n{})\n// \"\"\n[\nplay\n]\nSampleBy\nReturns a random item from collection, using a given random integer generator.\nimport\n\"math/rand\"\nr\n:=\nrand\n.\nNew\n(\nrand\n.\nNewSource\n(\n42\n))\nlo\n.\nSampleBy\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n},\nr\n.\nIntn\n)\n// a random string from []string{\"a\", \"b\", \"c\"}, using a seeded random generator\nlo\n.\nSampleBy\n([]\nstring\n{},\nr\n.\nIntn\n)\n// \"\"\nSamples\nReturns N random unique items from collection.\nlo\n.\nSamples\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n},\n3\n)\n// []string{\"a\", \"b\", \"c\"} in random order\nSamplesBy\nReturns N random unique items from collection, using a given random integer generator.\nr\n:=\nrand\n.\nNew\n(\nrand\n.\nNewSource\n(\n42\n))\nlo\n.\nSamplesBy\n([]\nstring\n{\n\"a\"\n,\n\"b\"\n,\n\"c\"\n},\n3\n,\nr\n.\nIntn\n)\n// []string{\"a\", \"b\", \"c\"} in random order, using a seeded random generator\nTernary\nA single line if/else statement.\nresult\n:=\nlo\n.\nTernary\n(\ntrue\n,\n\"a\"\n,\n\"b\"\n)\n// \"a\"\nresult\n:=\nlo\n.\nTernary\n(\nfalse\n,\n\"a\"\n,\n\"b\"\n)\n// \"b\"\nTake care to avoid dereferencing potentially nil pointers in your A/B expressions, because they are both evaluated. See TernaryF to avoid this problem.\n[\nplay\n]\nTernaryF\nA single line if/else statement whose options are functions.\nresult\n:=\nlo\n.\nTernaryF\n(\ntrue\n,\nfunc\n()\nstring\n{\nreturn\n\"a\"\n},\nfunc\n()\nstring\n{\nreturn\n\"b\"\n})\n// \"a\"\nresult\n:=\nlo\n.\nTernaryF\n(\nfalse\n,\nfunc\n()\nstring\n{\nreturn\n\"a\"\n},\nfunc\n()\nstring\n{\nreturn\n\"b\"\n})\n// \"b\"\nUseful to avoid nil-pointer dereferencing in initializations, or avoid running unnecessary code\nvar\ns\n*\nstring\nsomeStr\n:=\nTernaryF\n(\ns\n==\nnil\n,\nfunc\n()\nstring\n{\nreturn\nuuid\n.\nNew\n().\nString\n() },\nfunc\n()\nstring\n{\nreturn\n*\ns\n})\n// ef782193-c30c-4e2e-a7ae-f8ab5e125e02\n[\nplay\n]\nIf / ElseIf / Else\nresult\n:=\nlo\n.\nIf\n(\ntrue\n,\n1\n).\nElseIf\n(\nfalse\n,\n2\n).\nElse\n(\n3\n)\n// 1\nresult\n:=\nlo\n.\nIf\n(\nfalse\n,\n1\n).\nElseIf\n(\ntrue\n,\n2\n).\nElse\n(\n3\n)\n// 2\nresult\n:=\nlo\n.\nIf\n(\nfalse\n,\n1\n).\nElseIf\n(\nfalse\n,\n2\n).\nElse\n(\n3\n)\n// 3\nUsing callbacks:\nresult\n:=\nlo\n.\nIfF\n(\ntrue\n,\nfunc\n()\nint\n{\nreturn\n1\n}).\nElseIfF\n(\nfalse\n,\nfunc\n()\nint\n{\nreturn\n2\n}).\nElseF\n(\nfunc\n()\nint\n{\nreturn\n3\n})\n// 1\nMixed:\nresult\n:=\nlo\n.\nIfF\n(\ntrue\n,\nfunc\n()\nint\n{\nreturn\n1\n}).\nElse\n(\n42\n)\n// 1\n[\nplay\n]\nSwitch / Case / Default\nresult\n:=\nlo\n.\nSwitch\n(\n1\n).\nCase\n(\n1\n,\n\"1\"\n).\nCase\n(\n2\n,\n\"2\"\n).\nDefault\n(\n\"3\"\n)\n// \"1\"\nresult\n:=\nlo\n.\nSwitch\n(\n2\n).\nCase\n(\n1\n,\n\"1\"\n).\nCase\n(\n2\n,\n\"2\"\n).\nDefault\n(\n\"3\"\n)\n// \"2\"\nresult\n:=\nlo\n.\nSwitch\n(\n42\n).\nCase\n(\n1\n,\n\"1\"\n).\nCase\n(\n2\n,\n\"2\"\n).\nDefault\n(\n\"3\"\n)\n// \"3\"\nUsing callbacks:\nresult\n:=\nlo\n.\nSwitch\n(\n1\n).\nCaseF\n(\n1\n,\nfunc\n()\nstring\n{\nreturn\n\"1\"\n}).\nCaseF\n(\n2\n,\nfunc\n()\nstring\n{\nreturn\n\"2\"\n}).\nDefaultF\n(\nfunc\n()\nstring\n{\nreturn\n\"3\"\n})\n// \"1\"\nMixed:\nresult\n:=\nlo\n.\nSwitch\n(\n1\n).\nCaseF\n(\n1\n,\nfunc\n()\nstring\n{\nreturn\n\"1\"\n}).\nDefault\n(\n\"42\"\n)\n// \"1\"\n[\nplay\n]\nIsNil\nChecks if a value is nil or if it's a reference type with a nil underlying value.\nvar\nx\nint\nlo\n.\nIsNil\n(\nx\n)\n// false\nvar\nk\nstruct\n{}\nlo\n.\nIsNil\n(\nk\n)\n// false\nvar\ni\n*\nint\nlo\n.\nIsNil\n(\ni\n)\n// true\nvar\nifaceWithNilValue\nany\n=\n(\n*\nstring\n)(\nnil\n)\nlo\n.\nIsNil\n(\nifaceWithNilValue\n)\n// true\nifaceWithNilValue\n==\nnil\n// false\nIsNotNil\nChecks if a value is not nil or if it's not a reference type with a nil underlying value.\nvar\nx\nint\nlo\n.\nIsNotNil\n(\nx\n)\n// true\nvar\nk\nstruct\n{}\nlo\n.\nIsNotNil\n(\nk\n)\n// true\nvar\ni\n*\nint\nlo\n.\nIsNotNil\n(\ni\n)\n// false\nvar\nifaceWithNilValue\nany\n=\n(\n*\nstring\n)(\nnil\n)\nlo\n.\nIsNotNil\n(\nifaceWithNilValue\n)\n// false\nifaceWithNilValue\n==\nnil\n// true\nToPtr\nReturns a pointer copy of the value.\nptr\n:=\nlo\n.\nToPtr\n(\n\"hello world\"\n)\n// *string{\"hello world\"}\n[\nplay\n]\nNil\nReturns a nil pointer of type.\nptr\n:=\nlo\n.\nNil\n[\nfloat64\n]()\n// nil\nEmptyableToPtr\nReturns a pointer copy of value if it's nonzero.\nOtherwise, returns nil pointer.\nptr\n:=\nlo\n.\nEmptyableToPtr\n(\nnil\n)\n// nil\nptr\n:=\nlo\n.\nEmptyableToPtr\n(\n\"\"\n)\n// nil\nptr\n:=\nlo\n.\nEmptyableToPtr\n([]\nint\n{})\n// *[]int{}\nptr\n:=\nlo\n.\nEmptyableToPtr\n(\n\"hello world\"\n)\n// *string{\"hello world\"}\nFromPtr\nReturns the pointer value or empty.\nstr\n:=\n\"hello world\"\nvalue\n:=\nlo\n.\nFromPtr\n(\n&\nstr\n)\n// \"hello world\"\nvalue\n:=\nlo\n.\nFromPtr\n(\nnil\n)\n// \"\"\nFromPtrOr\nReturns the pointer value or the fallback value.\nstr\n:=\n\"hello world\"\nvalue\n:=\nlo\n.\nFromPtrOr\n(\n&\nstr\n,\n\"empty\"\n)\n// \"hello world\"\nvalue\n:=\nlo\n.\nFromPtrOr\n(\nnil\n,\n\"empty\"\n)\n// \"empty\"\nToSlicePtr\nReturns a slice of pointers to each value.\nptr\n:=\nlo\n.\nToSlicePtr\n([]\nstring\n{\n\"hello\"\n,\n\"world\"\n})\n// []*string{\"hello\", \"world\"}\nFromSlicePtr\nReturns a slice with the pointer values.\nReturns a zero value in case of a nil pointer element.\nstr1\n:=\n\"hello\"\nstr2\n:=\n\"world\"\nptr\n:=\nlo.\nFromSlicePtr\n[\nstring\n]([]\n*\nstring\n{\n&\nstr1\n,\n&\nstr2\n,\nnil\n})\n// []string{\"hello\", \"world\", \"\"}\nptr\n:=\nlo\n.\nCompact\n(\n    lo.\nFromSlicePtr\n[\nstring\n]([]\n*\nstring\n{\n&\nstr1\n,\n&\nstr2\n,\nnil\n}),\n)\n// []string{\"hello\", \"world\"}\nFromSlicePtrOr\nReturns a slice with the pointer values or the fallback value.\nstr1\n:=\n\"hello\"\nstr2\n:=\n\"world\"\nptr\n:=\nlo\n.\nFromSlicePtrOr\n([]\n*\nstring\n{\n&\nstr1\n,\nnil\n,\n&\nstr2\n},\n\"fallback value\"\n)\n// []string{\"hello\", \"fallback value\", \"world\"}\n[\nplay\n]\nToAnySlice\nReturns a slice with all elements mapped to\nany\ntype.\nelements\n:=\nlo\n.\nToAnySlice\n([]\nint\n{\n1\n,\n5\n,\n1\n})\n// []any{1, 5, 1}\nFromAnySlice\nReturns a slice with all elements mapped to a type. Returns false in case of type conversion failure.\nelements\n,\nok\n:=\nlo\n.\nFromAnySlice\n([]\nany\n{\n\"foobar\"\n,\n42\n})\n// []string{}, false\nelements\n,\nok\n:=\nlo\n.\nFromAnySlice\n([]\nany\n{\n\"foobar\"\n,\n\"42\"\n})\n// []string{\"foobar\", \"42\"}, true\nEmpty\nReturns the\nzero value\n.\nlo\n.\nEmpty\n[\nint\n]()\n// 0\nlo\n.\nEmpty\n[\nstring\n]()\n// \"\"\nlo\n.\nEmpty\n[\nbool\n]()\n// false\nIsEmpty\nReturns true if argument is a zero value.\nlo\n.\nIsEmpty\n(\n0\n)\n// true\nlo\n.\nIsEmpty\n(\n42\n)\n// false\nlo\n.\nIsEmpty\n(\n\"\"\n)\n// true\nlo\n.\nIsEmpty\n(\n\"foobar\"\n)\n// false\ntype\ntest\nstruct\n{\nfoobar\nstring\n}\nlo\n.\nIsEmpty\n(\ntest\n{\nfoobar\n:\n\"\"\n})\n// true\nlo\n.\nIsEmpty\n(\ntest\n{\nfoobar\n:\n\"foobar\"\n})\n// false\nIsNotEmpty\nReturns true if argument is a zero value.\nlo\n.\nIsNotEmpty\n(\n0\n)\n// false\nlo\n.\nIsNotEmpty\n(\n42\n)\n// true\nlo\n.\nIsNotEmpty\n(\n\"\"\n)\n// false\nlo\n.\nIsNotEmpty\n(\n\"foobar\"\n)\n// true\ntype\ntest\nstruct\n{\nfoobar\nstring\n}\nlo\n.\nIsNotEmpty\n(\ntest\n{\nfoobar\n:\n\"\"\n})\n// false\nlo\n.\nIsNotEmpty\n(\ntest\n{\nfoobar\n:\n\"foobar\"\n})\n// true\nCoalesce\nReturns the first non-empty arguments. Arguments must be comparable.\nresult\n,\nok\n:=\nlo\n.\nCoalesce\n(\n0\n,\n1\n,\n2\n,\n3\n)\n// 1 true\nresult\n,\nok\n:=\nlo\n.\nCoalesce\n(\n\"\"\n)\n// \"\" false\nvar\nnilStr\n*\nstring\nstr\n:=\n\"foobar\"\nresult\n,\nok\n:=\nlo\n.\nCoalesce\n(\nnil\n,\nnilStr\n,\n&\nstr\n)\n// &\"foobar\" true\nCoalesceOrEmpty\nReturns the first non-empty arguments. Arguments must be comparable.\nresult\n:=\nlo\n.\nCoalesceOrEmpty\n(\n0\n,\n1\n,\n2\n,\n3\n)\n// 1\nresult\n:=\nlo\n.\nCoalesceOrEmpty\n(\n\"\"\n)\n// \"\"\nvar\nnilStr\n*\nstring\nstr\n:=\n\"foobar\"\nresult\n:=\nlo\n.\nCoalesceOrEmpty\n(\nnil\n,\nnilStr\n,\n&\nstr\n)\n// &\"foobar\"\nCoalesceSlice\nReturns the first non-zero slice.\nresult\n,\nok\n:=\nlo\n.\nCoalesceSlice\n([]\nint\n{\n1\n,\n2\n,\n3\n}, []\nint\n{\n4\n,\n5\n,\n6\n})\n// [1, 2, 3]\n// true\nresult\n,\nok\n:=\nlo\n.\nCoalesceSlice\n(\nnil\n, []\nint\n{})\n// []\n// true\nresult\n,\nok\n:=\nlo\n.\nCoalesceSlice\n([]\nint\n(\nnil\n))\n// []\n// false\nCoalesceSliceOrEmpty\nReturns the first non-zero slice.\nresult\n:=\nlo\n.\nCoalesceSliceOrEmpty\n([]\nint\n{\n1\n,\n2\n,\n3\n}, []\nint\n{\n4\n,\n5\n,\n6\n})\n// [1, 2, 3]\nresult\n:=\nlo\n.\nCoalesceSliceOrEmpty\n(\nnil\n, []\nint\n{})\n// []\nCoalesceMap\nReturns the first non-zero map.\nresult\n,\nok\n:=\nlo\n.\nCoalesceMap\n(\nmap\n[\nstring\n]\nint\n{\n\"1\"\n:\n1\n,\n\"2\"\n:\n2\n,\n\"3\"\n:\n3\n},\nmap\n[\nstring\n]\nint\n{\n\"4\"\n:\n4\n,\n\"5\"\n:\n5\n,\n\"6\"\n:\n6\n})\n// {\"1\": 1, \"2\": 2, \"3\": 3}\n// true\nresult\n,\nok\n:=\nlo\n.\nCoalesceMap\n(\nnil\n,\nmap\n[\nstring\n]\nint\n{})\n// {}\n// true\nresult\n,\nok\n:=\nlo\n.\nCoalesceMap\n(\nmap\n[\nstring\n]\nint\n(\nnil\n))\n// {}\n// false\nCoalesceMapOrEmpty\nReturns the first non-zero map.\nresult\n:=\nlo\n.\nCoalesceMapOrEmpty\n(\nmap\n[\nstring\n]\nint\n{\n\"1\"\n:\n1\n,\n\"2\"\n:\n2\n,\n\"3\"\n:\n3\n},\nmap\n[\nstring\n]\nint\n{\n\"4\"\n:\n4\n,\n\"5\"\n:\n5\n,\n\"6\"\n:\n6\n})\n// {\"1\": 1, \"2\": 2, \"3\": 3}\nresult\n:=\nlo\n.\nCoalesceMapOrEmpty\n(\nnil\n,\nmap\n[\nstring\n]\nint\n{})\n// {}\nPartial\nReturns new function that, when called, has its first argument set to the provided value.\nadd\n:=\nfunc\n(\nx\n,\ny\nint\n)\nint\n{\nreturn\nx\n+\ny\n}\nf\n:=\nlo\n.\nPartial\n(\nadd\n,\n5\n)\nf\n(\n10\n)\n// 15\nf\n(\n42\n)\n// 47\n[\nplay\n]\nPartial2 -> Partial5\nReturns new function that, when called, has its first argument set to the provided value.\nadd\n:=\nfunc\n(\nx\n,\ny\n,\nz\nint\n)\nint\n{\nreturn\nx\n+\ny\n+\nz\n}\nf\n:=\nlo\n.\nPartial2\n(\nadd\n,\n42\n)\nf\n(\n10\n,\n5\n)\n// 57\nf\n(\n42\n,\n-\n4\n)\n// 80\n[\nplay\n]\nAttempt\nInvokes a function N times until it returns valid output. Returns either the caught error or nil.\nWhen the first argument is less than\n1\n, the function runs until a successful response is returned.\niter\n,\nerr\n:=\nlo\n.\nAttempt\n(\n42\n,\nfunc\n(\ni\nint\n)\nerror\n{\nif\ni\n==\n5\n{\nreturn\nnil\n}\nreturn\nerrors\n.\nNew\n(\n\"failed\"\n)\n})\n// 6\n// nil\niter\n,\nerr\n:=\nlo\n.\nAttempt\n(\n2\n,\nfunc\n(\ni\nint\n)\nerror\n{\nif\ni\n==\n5\n{\nreturn\nnil\n}\nreturn\nerrors\n.\nNew\n(\n\"failed\"\n)\n})\n// 2\n// error \"failed\"\niter\n,\nerr\n:=\nlo\n.\nAttempt\n(\n0\n,\nfunc\n(\ni\nint\n)\nerror\n{\nif\ni\n<\n42\n{\nreturn\nerrors\n.\nNew\n(\n\"failed\"\n)\n    }\nreturn\nnil\n})\n// 43\n// nil\nFor more advanced retry strategies (delay, exponential backoff...), please take a look at\ncenkalti/backoff\n.\n[\nplay\n]\nAttemptWithDelay\nInvokes a function N times until it returns valid output, with a pause between each call. Returns either the caught error or nil.\nWhen the first argument is less than\n1\n, the function runs until a successful response is returned.\niter\n,\nduration\n,\nerr\n:=\nlo\n.\nAttemptWithDelay\n(\n5\n,\n2\n*\ntime\n.\nSecond\n,\nfunc\n(\ni\nint\n,\nduration\ntime.\nDuration\n)\nerror\n{\nif\ni\n==\n2\n{\nreturn\nnil\n}\nreturn\nerrors\n.\nNew\n(\n\"failed\"\n)\n})\n// 3\n// ~ 4 seconds\n// nil\nFor more advanced retry strategies (delay, exponential backoff...), please take a look at\ncenkalti/backoff\n.\n[\nplay\n]\nAttemptWhile\nInvokes a function N times until it returns valid output. Returns either the caught error or nil, along with a bool value to determine whether the function should be invoked again. It will terminate the invoke immediately if the second return value is false.\nWhen the first argument is less than\n1\n, the function runs until a successful response is returned.\ncount1\n,\nerr1\n:=\nlo\n.\nAttemptWhile\n(\n5\n,\nfunc\n(\ni\nint\n) (\nerror\n,\nbool\n) {\nerr\n:=\ndoMockedHTTPRequest\n(\ni\n)\nif\nerr\n!=\nnil\n{\nif\nerrors\n.\nIs\n(\nerr\n,\nErrBadRequest\n) {\n// let's assume ErrBadRequest is a critical error that needs to terminate the invoke\nreturn\nerr\n,\nfalse\n// flag the second return value as false to terminate the invoke\n}\nreturn\nerr\n,\ntrue\n}\nreturn\nnil\n,\nfalse\n})\nFor more advanced retry strategies (delay, exponential backoff...), please take a look at\ncenkalti/backoff\n.\n[\nplay\n]\nAttemptWhileWithDelay\nInvokes a function N times until it returns valid output, with a pause between each call. Returns either the caught error or nil, along with a bool value to determine whether the function should be invoked again. It will terminate the invoke immediately if the second return value is false.\nWhen the first argument is less than\n1\n, the function runs until a successful response is returned.\ncount1\n,\ntime1\n,\nerr1\n:=\nlo\n.\nAttemptWhileWithDelay\n(\n5\n,\ntime\n.\nMillisecond\n,\nfunc\n(\ni\nint\n,\nd\ntime.\nDuration\n) (\nerror\n,\nbool\n) {\nerr\n:=\ndoMockedHTTPRequest\n(\ni\n)\nif\nerr\n!=\nnil\n{\nif\nerrors\n.\nIs\n(\nerr\n,\nErrBadRequest\n) {\n// let's assume ErrBadRequest is a critical error that needs to terminate the invoke\nreturn\nerr\n,\nfalse\n// flag the second return value as false to terminate the invoke\n}\nreturn\nerr\n,\ntrue\n}\nreturn\nnil\n,\nfalse\n})\nFor more advanced retry strategies (delay, exponential backoff...), please take a look at\ncenkalti/backoff\n.\n[\nplay\n]\nDebounce\nNewDebounce\ncreates a debounced instance that delays invoking functions given until after wait milliseconds have elapsed, until\ncancel\nis called.\nf\n:=\nfunc\n() {\nprintln\n(\n\"Called once after 100ms when debounce stopped invoking!\"\n)\n}\ndebounce\n,\ncancel\n:=\nlo\n.\nNewDebounce\n(\n100\n*\ntime\n.\nMillisecond\n,\nf\n)\nfor\nj\n:=\n0\n;\nj\n<\n10\n;\nj\n++\n{\ndebounce\n()\n}\ntime\n.\nSleep\n(\n1\n*\ntime\n.\nSecond\n)\ncancel\n()\n[\nplay\n]\nDebounceBy\nNewDebounceBy\ncreates a debounced instance for each distinct key, that delays invoking functions given until after wait milliseconds have elapsed, until\ncancel\nis called.\nf\n:=\nfunc\n(\nkey\nstring\n,\ncount\nint\n) {\nprintln\n(\nkey\n+\n\": Called once after 100ms when debounce stopped invoking!\"\n)\n}\ndebounce\n,\ncancel\n:=\nlo\n.\nNewDebounceBy\n(\n100\n*\ntime\n.\nMillisecond\n,\nf\n)\nfor\nj\n:=\n0\n;\nj\n<\n10\n;\nj\n++\n{\ndebounce\n(\n\"first key\"\n)\ndebounce\n(\n\"second key\"\n)\n}\ntime\n.\nSleep\n(\n1\n*\ntime\n.\nSecond\n)\ncancel\n(\n\"first key\"\n)\ncancel\n(\n\"second key\"\n)\n[\nplay\n]\nThrottle\nCreates a throttled instance that invokes given functions only once in every interval.\nThis returns 2 functions, First one is throttled function and Second one is a function to reset interval.\nf\n:=\nfunc\n() {\nprintln\n(\n\"Called once in every 100ms\"\n)\n}\nthrottle\n,\nreset\n:=\nlo\n.\nNewThrottle\n(\n100\n*\ntime\n.\nMillisecond\n,\nf\n)\nfor\nj\n:=\n0\n;\nj\n<\n10\n;\nj\n++\n{\nthrottle\n()\ntime\n.\nSleep\n(\n30\n*\ntime\n.\nMillisecond\n)\n}\nreset\n()\nthrottle\n()\nNewThrottleWithCount\nis NewThrottle with count limit, throttled function will be invoked count times in every interval.\nf\n:=\nfunc\n() {\nprintln\n(\n\"Called three times in every 100ms\"\n)\n}\nthrottle\n,\nreset\n:=\nlo\n.\nNewThrottleWithCount\n(\n100\n*\ntime\n.\nMillisecond\n,\nf\n)\nfor\nj\n:=\n0\n;\nj\n<\n10\n;\nj\n++\n{\nthrottle\n()\ntime\n.\nSleep\n(\n30\n*\ntime\n.\nMillisecond\n)\n}\nreset\n()\nthrottle\n()\nNewThrottleBy\nand\nNewThrottleByWithCount\nare NewThrottle with sharding key, throttled function will be invoked count times in every interval.\nf\n:=\nfunc\n(\nkey\nstring\n) {\nprintln\n(\nkey\n,\n\"Called three times in every 100ms\"\n)\n}\nthrottle\n,\nreset\n:=\nlo\n.\nNewThrottleByWithCount\n(\n100\n*\ntime\n.\nMillisecond\n,\nf\n)\nfor\nj\n:=\n0\n;\nj\n<\n10\n;\nj\n++\n{\nthrottle\n(\n\"foo\"\n)\ntime\n.\nSleep\n(\n30\n*\ntime\n.\nMillisecond\n)\n}\nreset\n()\nthrottle\n()\nSynchronize\nWraps the underlying callback in a mutex. It receives an optional mutex.\ns\n:=\nlo\n.\nSynchronize\n()\nfor\ni\n:=\n0\n;\ni\n<\n10\n;\ni\n++\n{\ngo\ns\n.\nDo\n(\nfunc\n() {\nprintln\n(\n\"will be called sequentially\"\n)\n    })\n}\nIt is equivalent to:\nmu\n:=\nsync.\nMutex\n{}\nfunc\nfoobar\n() {\nmu\n.\nLock\n()\ndefer\nmu\n.\nUnlock\n()\n// ...\n}\nAsync\nExecutes a function in a goroutine and returns the result in a channel.\nch\n:=\nlo\n.\nAsync\n(\nfunc\n()\nerror\n{\ntime\n.\nSleep\n(\n10\n*\ntime\n.\nSecond\n);\nreturn\nnil\n})\n// chan error (nil)\nAsync{0->6}\nExecutes a function in a goroutine and returns the result in a channel.\nFor functions with multiple return values, the results will be returned as a tuple inside the channel.\nFor functions without return, struct{} will be returned in the channel.\nch\n:=\nlo\n.\nAsync0\n(\nfunc\n() {\ntime\n.\nSleep\n(\n10\n*\ntime\n.\nSecond\n) })\n// chan struct{}\nch\n:=\nlo\n.\nAsync1\n(\nfunc\n()\nint\n{\ntime\n.\nSleep\n(\n10\n*\ntime\n.\nSecond\n);\nreturn\n42\n})\n// chan int (42)\nch\n:=\nlo\n.\nAsync2\n(\nfunc\n() (\nint\n,\nstring\n) {\ntime\n.\nSleep\n(\n10\n*\ntime\n.\nSecond\n);\nreturn\n42\n,\n\"Hello\"\n})\n// chan lo.Tuple2[int, string] ({42, \"Hello\"})\nTransaction\nImplements a Saga pattern.\ntransaction\n:=\nNewTransaction\n().\nThen\n(\nfunc\n(\nstate\nint\n) (\nint\n,\nerror\n) {\nfmt\n.\nPrintln\n(\n\"step 1\"\n)\nreturn\nstate\n+\n10\n,\nnil\n},\nfunc\n(\nstate\nint\n)\nint\n{\nfmt\n.\nPrintln\n(\n\"rollback 1\"\n)\nreturn\nstate\n-\n10\n},\n    ).\nThen\n(\nfunc\n(\nstate\nint\n) (\nint\n,\nerror\n) {\nfmt\n.\nPrintln\n(\n\"step 2\"\n)\nreturn\nstate\n+\n15\n,\nnil\n},\nfunc\n(\nstate\nint\n)\nint\n{\nfmt\n.\nPrintln\n(\n\"rollback 2\"\n)\nreturn\nstate\n-\n15\n},\n    ).\nThen\n(\nfunc\n(\nstate\nint\n) (\nint\n,\nerror\n) {\nfmt\n.\nPrintln\n(\n\"step 3\"\n)\nif\ntrue\n{\nreturn\nstate\n,\nerrors\n.\nNew\n(\n\"error\"\n)\n            }\nreturn\nstate\n+\n42\n,\nnil\n},\nfunc\n(\nstate\nint\n)\nint\n{\nfmt\n.\nPrintln\n(\n\"rollback 3\"\n)\nreturn\nstate\n-\n42\n},\n    )\n_\n,\n_\n=\ntransaction\n.\nProcess\n(\n-\n5\n)\n// Output:\n// step 1\n// step 2\n// step 3\n// rollback 2\n// rollback 1\nWaitFor\nRuns periodically until a condition is validated.\nalwaysTrue\n:=\nfunc\n(\ni\nint\n)\nbool\n{\nreturn\ntrue\n}\nalwaysFalse\n:=\nfunc\n(\ni\nint\n)\nbool\n{\nreturn\nfalse\n}\nlaterTrue\n:=\nfunc\n(\ni\nint\n)\nbool\n{\nreturn\ni\n>\n5\n}\niterations\n,\nduration\n,\nok\n:=\nlo\n.\nWaitFor\n(\nalwaysTrue\n,\n10\n*\ntime\n.\nMillisecond\n,\n2\n*\ntime\n.\nMillisecond\n)\n// 1\n// 1ms\n// true\niterations\n,\nduration\n,\nok\n:=\nlo\n.\nWaitFor\n(\nalwaysFalse\n,\n10\n*\ntime\n.\nMillisecond\n,\ntime\n.\nMillisecond\n)\n// 10\n// 10ms\n// false\niterations\n,\nduration\n,\nok\n:=\nlo\n.\nWaitFor\n(\nlaterTrue\n,\n10\n*\ntime\n.\nMillisecond\n,\ntime\n.\nMillisecond\n)\n// 7\n// 7ms\n// true\niterations\n,\nduration\n,\nok\n:=\nlo\n.\nWaitFor\n(\nlaterTrue\n,\n10\n*\ntime\n.\nMillisecond\n,\n5\n*\ntime\n.\nMillisecond\n)\n// 2\n// 10ms\n// false\n[\nplay\n]\nWaitForWithContext\nRuns periodically until a condition is validated or context is invalid.\nThe condition receives also the context, so it can invalidate the process in the condition checker\nctx\n:=\ncontext\n.\nBackground\n()\nalwaysTrue\n:=\nfunc\n(\n_\ncontext.\nContext\n,\ni\nint\n)\nbool\n{\nreturn\ntrue\n}\nalwaysFalse\n:=\nfunc\n(\n_\ncontext.\nContext\n,\ni\nint\n)\nbool\n{\nreturn\nfalse\n}\nlaterTrue\n:=\nfunc\n(\n_\ncontext.\nContext\n,\ni\nint\n)\nbool\n{\nreturn\ni\n>=\n5\n}\niterations\n,\nduration\n,\nok\n:=\nlo\n.\nWaitForWithContext\n(\nctx\n,\nalwaysTrue\n,\n10\n*\ntime\n.\nMillisecond\n,\n2\n*\ntime\n.\nMillisecond\n)\n// 1\n// 1ms\n// true\niterations\n,\nduration\n,\nok\n:=\nlo\n.\nWaitForWithContext\n(\nctx\n,\nalwaysFalse\n,\n10\n*\ntime\n.\nMillisecond\n,\ntime\n.\nMillisecond\n)\n// 10\n// 10ms\n// false\niterations\n,\nduration\n,\nok\n:=\nlo\n.\nWaitForWithContext\n(\nctx\n,\nlaterTrue\n,\n10\n*\ntime\n.\nMillisecond\n,\ntime\n.\nMillisecond\n)\n// 5\n// 5ms\n// true\niterations\n,\nduration\n,\nok\n:=\nlo\n.\nWaitForWithContext\n(\nctx\n,\nlaterTrue\n,\n10\n*\ntime\n.\nMillisecond\n,\n5\n*\ntime\n.\nMillisecond\n)\n// 2\n// 10ms\n// false\nexpiringCtx\n,\ncancel\n:=\ncontext\n.\nWithTimeout\n(\nctx\n,\n5\n*\ntime\n.\nMillisecond\n)\niterations\n,\nduration\n,\nok\n:=\nlo\n.\nWaitForWithContext\n(\nexpiringCtx\n,\nalwaysFalse\n,\n100\n*\ntime\n.\nMillisecond\n,\ntime\n.\nMillisecond\n)\n// 5\n// 5.1ms\n// false\n[\nplay\n]\nValidate\nHelper function that creates an error when a condition is not met.\nslice\n:=\n[]\nstring\n{\n\"a\"\n}\nval\n:=\nlo\n.\nValidate\n(\nlen\n(\nslice\n)\n==\n0\n,\n\"Slice should be empty but contains %v\"\n,\nslice\n)\n// error(\"Slice should be empty but contains [a]\")\nslice\n:=\n[]\nstring\n{}\nval\n:=\nlo\n.\nValidate\n(\nlen\n(\nslice\n)\n==\n0\n,\n\"Slice should be empty but contains %v\"\n,\nslice\n)\n// nil\n[\nplay\n]\nMust\nWraps a function call and panics if second argument is\nerror\nor\nfalse\n, returns the value otherwise.\nval\n:=\nlo\n.\nMust\n(\ntime\n.\nParse\n(\n\"2006-01-02\"\n,\n\"2022-01-15\"\n))\n// 2022-01-15\nval\n:=\nlo\n.\nMust\n(\ntime\n.\nParse\n(\n\"2006-01-02\"\n,\n\"bad-value\"\n))\n// panics\n[\nplay\n]\nMust{0->6}\nMust* has the same behavior as Must but returns multiple values.\nfunc\nexample0\n() (\nerror\n)\nfunc\nexample1\n() (\nint\n,\nerror\n)\nfunc\nexample2\n() (\nint\n,\nstring\n,\nerror\n)\nfunc\nexample3\n() (\nint\n,\nstring\n, time.\nDate\n,\nerror\n)\nfunc\nexample4\n() (\nint\n,\nstring\n, time.\nDate\n,\nbool\n,\nerror\n)\nfunc\nexample5\n() (\nint\n,\nstring\n, time.\nDate\n,\nbool\n,\nfloat64\n,\nerror\n)\nfunc\nexample6\n() (\nint\n,\nstring\n, time.\nDate\n,\nbool\n,\nfloat64\n,\nbyte\n,\nerror\n)\nlo\n.\nMust0\n(\nexample0\n())\nval1\n:=\nlo\n.\nMust1\n(\nexample1\n())\n// alias to Must\nval1\n,\nval2\n:=\nlo\n.\nMust2\n(\nexample2\n())\nval1\n,\nval2\n,\nval3\n:=\nlo\n.\nMust3\n(\nexample3\n())\nval1\n,\nval2\n,\nval3\n,\nval4\n:=\nlo\n.\nMust4\n(\nexample4\n())\nval1\n,\nval2\n,\nval3\n,\nval4\n,\nval5\n:=\nlo\n.\nMust5\n(\nexample5\n())\nval1\n,\nval2\n,\nval3\n,\nval4\n,\nval5\n,\nval6\n:=\nlo\n.\nMust6\n(\nexample6\n())\nYou can wrap functions like\nfunc (...) (..., ok bool)\n.\n// math.Signbit(float64) bool\nlo\n.\nMust0\n(\nmath\n.\nSignbit\n(\nv\n))\n// bytes.Cut([]byte,[]byte) ([]byte, []byte, bool)\nbefore\n,\nafter\n:=\nlo\n.\nMust2\n(\nbytes\n.\nCut\n(\ns\n,\nsep\n))\nYou can give context to the panic message by adding some printf-like arguments.\nval\n,\nok\n:=\nlo\n.\nFind\n(\nmyString\n,\nfunc\n(\ni\nstring\n)\nbool\n{\nreturn\ni\n==\nrequiredChar\n})\nlo\n.\nMust0\n(\nok\n,\n\"'%s' must always contain '%s'\"\n,\nmyString\n,\nrequiredChar\n)\nlist\n:=\n[]\nint\n{\n0\n,\n1\n,\n2\n}\nitem\n:=\n5\nlo\n.\nMust0\n(\nlo\n.\nContains\n(\nlist\n,\nitem\n),\n\"'%s' must always contain '%s'\"\n,\nlist\n,\nitem\n)\n...\n[\nplay\n]\nTry\nCalls the function and returns false in case of error and panic.\nok\n:=\nlo\n.\nTry\n(\nfunc\n()\nerror\n{\npanic\n(\n\"error\"\n)\nreturn\nnil\n})\n// false\nok\n:=\nlo\n.\nTry\n(\nfunc\n()\nerror\n{\nreturn\nnil\n})\n// true\nok\n:=\nlo\n.\nTry\n(\nfunc\n()\nerror\n{\nreturn\nerrors\n.\nNew\n(\n\"error\"\n)\n})\n// false\n[\nplay\n]\nTry{0->6}\nThe same behavior as\nTry\n, but the callback returns 2 variables.\nok\n:=\nlo\n.\nTry2\n(\nfunc\n() (\nstring\n,\nerror\n) {\npanic\n(\n\"error\"\n)\nreturn\n\"\"\n,\nnil\n})\n// false\n[\nplay\n]\nTryOr\nCalls the function and return a default value in case of error and on panic.\nstr\n,\nok\n:=\nlo\n.\nTryOr\n(\nfunc\n() (\nstring\n,\nerror\n) {\npanic\n(\n\"error\"\n)\nreturn\n\"hello\"\n,\nnil\n},\n\"world\"\n)\n// world\n// false\nstr\n,\nok\n:=\nlo\n.\nTryOr\n(\nfunc\n()\nerror\n{\nreturn\n\"hello\"\n,\nnil\n},\n\"world\"\n)\n// hello\n// true\nstr\n,\nok\n:=\nlo\n.\nTryOr\n(\nfunc\n()\nerror\n{\nreturn\n\"hello\"\n,\nerrors\n.\nNew\n(\n\"error\"\n)\n},\n\"world\"\n)\n// world\n// false\n[\nplay\n]\nTryOr{0->6}\nThe same behavior as\nTryOr\n, but the callback returns\nX\nvariables.\nstr\n,\nnbr\n,\nok\n:=\nlo\n.\nTryOr2\n(\nfunc\n() (\nstring\n,\nint\n,\nerror\n) {\npanic\n(\n\"error\"\n)\nreturn\n\"hello\"\n,\n42\n,\nnil\n},\n\"world\"\n,\n21\n)\n// world\n// 21\n// false\n[\nplay\n]\nTryWithErrorValue\nThe same behavior as\nTry\n, but also returns the value passed to panic.\nerr\n,\nok\n:=\nlo\n.\nTryWithErrorValue\n(\nfunc\n()\nerror\n{\npanic\n(\n\"error\"\n)\nreturn\nnil\n})\n// \"error\", false\n[\nplay\n]\nTryCatch\nThe same behavior as\nTry\n, but calls the catch function in case of error.\ncaught\n:=\nfalse\nok\n:=\nlo\n.\nTryCatch\n(\nfunc\n()\nerror\n{\npanic\n(\n\"error\"\n)\nreturn\nnil\n},\nfunc\n() {\ncaught\n=\ntrue\n})\n// false\n// caught == true\n[\nplay\n]\nTryCatchWithErrorValue\nThe same behavior as\nTryWithErrorValue\n, but calls the catch function in case of error.\ncaught\n:=\nfalse\nok\n:=\nlo\n.\nTryCatchWithErrorValue\n(\nfunc\n()\nerror\n{\npanic\n(\n\"error\"\n)\nreturn\nnil\n},\nfunc\n(\nval\nany\n) {\ncaught\n=\nval\n==\n\"error\"\n})\n// false\n// caught == true\n[\nplay\n]\nErrorsAs\nA shortcut for:\nerr\n:=\ndoSomething\n()\nvar\nrateLimitErr\n*\nRateLimitError\nif\nok\n:=\nerrors\n.\nAs\n(\nerr\n,\n&\nrateLimitErr\n);\nok\n{\n// retry later\n}\nsingle line\nlo\nhelper:\nerr\n:=\ndoSomething\n()\nif\nrateLimitErr\n,\nok\n:=\nlo.\nErrorsAs\n[\n*\nRateLimitError\n](\nerr\n);\nok\n{\n// retry later\n}\n[\nplay\n]\nAssert\nDoes nothing when the condition is\ntrue\n, otherwise it panics with an optional message.\nThink twice before using it, given that\nGo intentionally omits assertions from its standard library\n.\nage\n:=\ngetUserAge\n()\nlo\n.\nAssert\n(\nage\n>=\n15\n)\nage\n:=\ngetUserAge\n()\nlo\n.\nAssert\n(\nage\n>=\n15\n,\n\"user age must be >= 15\"\n)\n[\nplay\n]\nAssertf\nLike\nAssert\n, but with\nfmt.Printf\n-like formatting.\nThink twice before using it, given that\nGo intentionally omits assertions from its standard library\n.\nage\n:=\ngetUserAge\n()\nlo\n.\nAssertf\n(\nage\n>=\n15\n,\n\"user age must be >= 15, got %d\"\n,\nage\n)\n[\nplay\n]\n🛩 Benchmark\nWe executed a simple benchmark with a dead-simple\nlo.Map\nloop:\nSee the full implementation\nhere\n.\n_\n=\nlo\n.\nMap\n[\nint64\n](\narr\n,\nfunc\n(\nx\nint64\n,\ni\nint\n)\nstring\n{\nreturn\nstrconv\n.\nFormatInt\n(\nx\n,\n10\n)\n})\nResult:\nHere is a comparison between\nlo.Map\n,\nlop.Map\n,\ngo-funk\nlibrary and a simple Go\nfor\nloop.\n$ go\ntest\n-benchmem -bench ./...\ngoos: linux\ngoarch: amd64\npkg: github.com/samber/lo\ncpu: Intel(R) Core(TM) i5-7267U CPU @ 3.10GHz\ncpu: Intel(R) Core(TM) i7 CPU         920  @ 2.67GHz\nBenchmarkMap/lo.Map-8         \t       8\t 132728237 ns/op\t39998945 B/op\t 1000002 allocs/op\nBenchmarkMap/lop.Map-8        \t       2\t 503947830 ns/op\t119999956 B/op\t 3000007 allocs/op\nBenchmarkMap/reflect-8        \t       2\t 826400560 ns/op\t170326512 B/op\t 4000042 allocs/op\nBenchmarkMap/for-8            \t       9\t 126252954 ns/op\t39998674 B/op\t 1000001 allocs/op\nPASS\nok  \tgithub.com/samber/lo\t6.657s\nlo.Map\nis way faster (x7) than\ngo-funk\n, a reflection-based Map implementation.\nlo.Map\nhas the same allocation profile as\nfor\n.\nlo.Map\nis 4% slower than\nfor\n.\nlop.Map\nis slower than\nlo.Map\nbecause it implies more memory allocation and locks.\nlop.Map\nis useful for long-running callbacks, such as i/o bound processing.\nfor\nbeats other implementations for memory and CPU.\n🤝 Contributing\nPing me on Twitter\n@samuelberthe\n(DMs, mentions, whatever :))\nFork the\nproject\nFix\nopen issues\nor request new features\nDon't hesitate ;)\nHelper naming: helpers must be self-explanatory and respect standards (other languages, libraries...). Feel free to suggest many names in your contributions.\n#\nInstall some dev dependencies\nmake tools\n#\nRun tests\nmake\ntest\n#\nor\nmake watch-test\n👤 Contributors\n💫 Show your support\nGive a ⭐️ if this project helped you!\n📝 License\nCopyright © 2022\nSamuel Berthe\n.\nThis project is under\nMIT\nlicense.",
        "今日の獲得スター数: 64",
        "累積スター数: 20,337"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/samber/lo"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/aaPanel/BillionMail",
      "title": "aaPanel/BillionMail",
      "date": null,
      "executive_summary": [
        "BillionMail gives you open-source MailServer, NewsLetter, Email Marketing — fully self-hosted, dev-friendly, and free from monthly fees. Join the discord: https://discord.gg/asfXzBUhZr",
        "---",
        "BillionMail 📧\nAn Open-Source MailServer, NewsLetter, Email Marketing Solution for Smarter Campaigns\nEnglish |\n简体中文\n|\n日本語\n|\nTürkçe\nWhat is BillionMail?\nBillionMail is a\nfuture open-source Mail server, Email marketing platform\ndesigned to help businesses and individuals manage their email campaigns with ease. Whether you're sending newsletters, promotional emails, or transactional messages, this tool will provide\nfull control\nover your email marketing efforts. With features like\nadvanced analytics\n, and\ncustomer management\n, you'll be able to create, send, and track emails like a pro.\nJust 3 steps to send a billion emails!\nBillion emails. Any business. Guaranteed.\nStep 1️⃣ Install BillionMail:\n✅ It takes\nonly 8️⃣ minutes\nfrom installation to\n✅ successful email sending\ncd\n/opt\n&&\ngit clone https://github.com/aaPanel/BillionMail\n&&\ncd\nBillionMail\n&&\nbash install.sh\nStep 2️⃣: Connect Your Domain\nAdd the sending domain\nVerify DNS records\nAuto-enable free SSL\nStep 3️⃣: Build Your Campaign\nWrite or paste your email\nChoose list & tags\nSet send time or send now\nWatch on Youtube\nOther installation methods\nOne-click installation on aaPanel\n👉\nhttps://www.aapanel.com/new/download.html\n(Log in to ✅aaPanel --> 🐳Docker --> 1️⃣OneClick install)\nDocker\ncd\n/opt\n&&\ngit clone https://github.com/aaPanel/BillionMail\n&&\ncd\nBillionMail\n&&\ncp env_init .env\n&&\ndocker compose up -d\n||\ndocker-compose up -d\nManagement script\nManagement help\nbm help\nView Login default info\nbm default\nShow domain DNS record\nbm show-record\nUpdate BillionMail\nbm update\nLive Demo\nBillionMail Demo:\nhttps://demo.billionmail.com/billionmail\nUsername:\nbillionmail\nPassword:\nbillionmail\nWebMail\nBillionMail has integrated\nRoundCube\n, you can access WebMail via\n/roundcube/\n.\nWhy BillionMail?\nMost email marketing platforms are either\nexpensive\n,\nclosed-source\n, or\nlack essential features\n. BillionMail aims to be different:\n✅\nFully Open-Source\n– No hidden costs, no vendor lock-in.\n📊\nAdvanced Analytics\n– Track email delivery, open rates, click-through rates, and more.\n📧\nUnlimited Sending\n– No restrictions on the number of emails you can send.\n🎨\nCustomizable Templates\n– Custom professional marketing templates for reuse.\n🔒\nPrivacy-First\n– Your data stays with you, no third-party tracking.\n🚀\nSelf-Hosted\n– Run it on your own server for complete control.\nHow You Can Help 🌟\nBillionMail is a\ncommunity-driven project\n, and we need your support to get started! Here's how you can help:\nStar This Repository\n: Show your interest by starring this repo.\nSpread the Word\n: Share BillionMail with your network—developers, marketers, and open-source enthusiasts.\nShare Feedback\n: Let us know what features you'd like to see in BillionMail by opening an issue or joining the discussion.\nContribute\n: Once development begins, we'll welcome contributions from the community. Stay tuned for updates!\n📧\nBillionMail – The Future of Open-Source Email Marketing.\nIssues\nIf you encounter any issues or have feature requests, please\nopen an issue\n. Be sure to include:\nA clear description of the problem or request.\nSteps to reproduce the issue (if applicable).\nScreenshots or error logs (if applicable).\nInstall Now:\n✅It takes\nonly 8 minutes\nfrom installation to\nsuccessful email sending\ncd\n/opt\n&&\ngit clone https://github.com/aaPanel/BillionMail\n&&\ncd\nBillionMail\n&&\nbash install.sh\nInstall with Docker:\n(Please install Docker and docker-compose-plugin manually, and modify .env file)\ncd\n/opt\n&&\ngit clone https://github.com/aaPanel/BillionMail\n&&\ncd\nBillionMail\n&&\ncp env_init .env\n&&\ndocker compose up -d\n||\ndocker-compose up -d\nStar History\nLicense\nBillionMail is licensed under the\nAGPLv3 License\n. This means you can:\n✅ Use the software for free.\n✅ Modify and distribute the code.\n✅ Use it privately without restrictions.\nSee the\nLICENSE\nfile for more details.",
        "今日の獲得スター数: 54",
        "累積スター数: 11,553"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/aaPanel/BillionMail"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/ruvnet/claude-flow",
      "title": "ruvnet/claude-flow",
      "date": null,
      "executive_summary": [
        "🌊 The leading agent orchestration platform for Claude. Deploy intelligent multi-agent swarms, coordinate autonomous workflows, and build conversational AI systems. Features enterprise-grade architecture, distributed swarm intelligence, RAG integration, and native Claude Code support via MCP protocol. Ranked #1 in agent-based frameworks.",
        "---",
        "🌊 Claude-Flow v2.5.0 Alpha 140: AI Orchestration Platform\n🌟\nOverview\nClaude-Flow v2 Alpha\nis an enterprise-grade AI orchestration platform that reimagines how developers build with AI. By combining\nhive-mind swarm intelligence\n,\nneural pattern recognition\n, and\n87 advanced MCP tools\n, Claude-Flow enables unprecedented AI-powered development workflows.\n🎯\nKey Features\n🐝 Hive-Mind Intelligence\n: Queen-led AI coordination with specialized worker agents\n🧠 Neural Networks\n: 27+ cognitive models with WASM SIMD acceleration\n🔧 87 MCP Tools\n: Comprehensive toolkit for swarm orchestration, memory, and automation\n🔄 Dynamic Agent Architecture (DAA)\n: Self-organizing agents with fault tolerance\n💾 SQLite Memory System\n: Persistent\n.swarm/memory.db\nwith 12 specialized tables\n🪝 Advanced Hooks System\n: Automated workflows with pre/post operation hooks\n📊 GitHub Integration\n: 6 specialized modes for repository management\n🌐 Flow Nexus Cloud Platform\n: E2B sandboxes, AI swarms, challenges, and marketplace integration\n🎯 PreToolUse Modification Hooks\n: NEW - Claude Code v2.0.10+ intelligent input modification (safety, organization, optimization)\n🔥\nRevolutionary AI Coordination\n: Build faster, smarter, and more efficiently with AI-powered development orchestration\n🎯\nNEW: PreToolUse Modification Hooks Plugin\n(v2.5.0-alpha.140)\nFirst Claude Code plugin with intelligent tool input modification\n- automatically enhances commands and files before execution.\nKey Features:\n🛡️\nSafety\n: Auto-adds\n-i\nto\nrm\ncommands, detects sensitive keywords\n📁\nOrganization\n: Auto-routes files (tests→\n/tests/\n, src→\n/src/\n)\n⚡\nProductivity\n: Alias expansion (\nll\n→\nls -lah\n), conventional commits\nQuick Start:\nOption 1: Direct Plugin Installation\n(Recommended)\n#\nIn Claude Code, run:\n/plugin ruvnet/claude-flow\nOption 2: Via NPM\nnpx claude-flow@alpha init --force\n#\nAuto-configures .claude-plugin/hooks/hooks.json\nExamples:\nrm test.txt          → rm -i test.txt\n#\nSafety\ntest.js             → src/test.js\n#\nOrganization\ngit commit -m\n\"\nfix\n\"\n→ [fix] fix + co-author\n#\nCommits\n📚\nDocs\n:\nHOOKS-V2-MODIFICATION.md\n|\nPlugin\n:\n.claude-plugin/\n|\nComposable\nwith\nagent-booster\n🌐\nFlow Nexus Cloud Platform\nNEW\n: Claude-Flow v2.0.0 now includes\nFlow Nexus integration\n- a cloud-powered AI development platform featuring:\nE2B Sandboxes\n: Secure isolated environments for Node.js, Python, React, Next.js\nAI Swarms\n: Deploy multi-agent systems in cloud infrastructure\nNeural Training\n: Distributed machine learning with custom model deployment\nChallenges & Marketplace\n: Coding challenges with rUv credit rewards and template marketplace\nWorkflow Automation\n: Event-driven automation with message queue processing\n📚\nComplete documentation\n: Visit\nflow-nexus.ruv.io\nfor comprehensive guides, tutorials, and API reference. Also see issue #\n#732\n⚡\nTry v2.0.0 Alpha in 4 Commands\n📋\nPrerequisites\nNode.js 18+\n(LTS recommended)\nnpm 9+\nor equivalent package manager\nWindows users\n: See\nWindows Installation Guide\nfor special instructions\n⚠️\nIMPORTANT\n: Claude Code must be installed first:\n#\n1. Install Claude Code globally\nnpm install -g @anthropic-ai/claude-code\n#\n2. (Optional) Skip permissions check for faster setup\n#\nOnly use if you understand the security implications\nclaude --dangerously-skip-permissions\n💡\nWindows Note\n: If you encounter SQLite errors, Claude Flow will automatically use in-memory storage. For persistent storage options, see our\nWindows guide\n.\n🎯\nInstant Alpha Testing\nMethod 1: Plugin Installation\n(Easiest - includes PreToolUse hooks!)\n#\nIn Claude Code:\n/plugin ruvnet/claude-flow\nMethod 2: NPM Installation\n(For MCP server + CLI)\n#\n1. Initialize Claude Flow with enhanced MCP setup (auto-configures permissions!)\nnpx claude-flow@alpha init --force\n#\n2. Explore all revolutionary capabilities\nnpx claude-flow@alpha --help\n#\n3a. Quick AI coordination (recommended for most tasks)\nnpx claude-flow@alpha swarm\n\"\nbuild me a REST API\n\"\n--claude\n#\n3b. OR launch the full hive-mind system (for complex projects)\nnpx claude-flow@alpha hive-mind wizard\nnpx claude-flow@alpha hive-mind spawn\n\"\nbuild enterprise system\n\"\n--claude\n🚀\nQuick Start with Flow Nexus\n#\n1. Initialize Flow Nexus only (minimal setup)\nnpx claude-flow init --flow-nexus\n#\n2. Register and login (use MCP tools in Claude Code)\nmcp__flow-nexus__user_register({ email:\n\"\nyour@email.com\n\"\n, password:\n\"\nsecure\n\"\n})\nmcp__flow-nexus__user_login({ email:\n\"\nyour@email.com\n\"\n, password:\n\"\nsecure\n\"\n})\n#\n3. Deploy your first cloud swarm\nmcp__flow-nexus__swarm_init({ topology:\n\"\nmesh\n\"\n, maxAgents: 5 })\nmcp__flow-nexus__sandbox_create({ template:\n\"\nnode\n\"\n, name:\n\"\napi-dev\n\"\n})\n🤔\nSwarm vs Hive-Mind: Which to Use?\nFeature\nswarm\nCommand\nhive-mind\nCommand\nBest For\nQuick tasks, single objectives\nComplex projects, persistent sessions\nSetup\nInstant - no configuration needed\nInteractive wizard setup\nSession\nTemporary coordination\nPersistent with resume capability\nMemory\nTask-scoped\nProject-wide with SQLite storage\nAgents\nAuto-spawned for task\nManual control with specializations\nUse When\n\"Build X\", \"Fix Y\", \"Analyze Z\"\nMulti-feature projects, team coordination\nQuick Rule:\nStart with\nswarm\nfor most tasks. Use\nhive-mind\nwhen you need persistent sessions or complex multi-agent coordination.\n🎯\nTypical Workflows - Your \"Happy Path\" Guide\nNew to Claude-Flow? Start Here!\nConfused about\n.hive-mind\nand\n.swarm\ndirectories? Not sure when to create new hives? Here are the most common workflow patterns:\n🚀 Pattern 1: Single Feature Development\n#\nInitialize once per feature/task\nnpx claude-flow@alpha init --force\nnpx claude-flow@alpha hive-mind spawn\n\"\nImplement user authentication\n\"\n--claude\n#\nContinue working on SAME feature (reuse existing hive)\nnpx claude-flow@alpha hive-mind status\nnpx claude-flow@alpha memory query\n\"\nauthentication\n\"\n--recent\nnpx claude-flow@alpha swarm\n\"\nAdd password reset functionality\n\"\n--continue-session\n🏗️ Pattern 2: Multi-Feature Project\n#\nProject-level initialization (once per project)\nnpx claude-flow@alpha init --force --project-name\n\"\nmy-app\n\"\n#\nFeature 1: Authentication (new hive)\nnpx claude-flow@alpha hive-mind spawn\n\"\nauth-system\n\"\n--namespace auth --claude\n#\nFeature 2: User management (separate hive)\nnpx claude-flow@alpha hive-mind spawn\n\"\nuser-management\n\"\n--namespace users --claude\n#\nResume Feature 1 later (use session ID from spawn output)\nnpx claude-flow@alpha hive-mind resume session-xxxxx-xxxxx\n🔍 Pattern 3: Research & Analysis\n#\nStart research session\nnpx claude-flow@alpha hive-mind spawn\n\"\nResearch microservices patterns\n\"\n--agents researcher,analyst --claude\n#\nContinue research in SAME session\nnpx claude-flow@alpha memory stats\n#\nSee what's been learned\nnpx claude-flow@alpha swarm\n\"\nDeep dive into API gateway patterns\n\"\n--continue-session\n🤔 When Should I Create a New Hive?\nSituation\nAction\nCommand\nSame objective/feature\nContinue existing hive\nnpx claude-flow@alpha hive-mind resume <session-id>\nNew feature in same project\nCreate new hive with namespace\nnpx claude-flow@alpha hive-mind spawn \"new-feature\" --namespace feature-name\nCompletely different project\nNew directory + init\nmkdir new-project && cd new-project && npx claude-flow@alpha init\nExperimenting/testing\nTemporary hive\nnpx claude-flow@alpha hive-mind spawn \"experiment\" --temp\n📁 Understanding \"Empty\" Directories\nDon't panic if directories seem empty!\nClaude-Flow uses SQLite databases that may not show files in directory listings:\n#\nCheck what's actually stored (even if directories look empty)\nnpx claude-flow@alpha memory stats\n#\nSee memory data\nnpx claude-flow@alpha memory list\n#\nList all namespaces\nnpx claude-flow@alpha hive-mind status\n#\nSee active hives\n#\nYour project structure after initialization:\n#\n.hive-mind/     <- Contains config.json + SQLite session data\n#\n.swarm/         <- Contains memory.db (SQLite database)\n#\nmemory/         <- Agent-specific memories (created when agents spawn)\n#\ncoordination/   <- Active workflow files (created during tasks)\n🔄 Continuing Previous Work\n#\nSee what you were working on\nnpx claude-flow@alpha hive-mind status\nnpx claude-flow@alpha memory query --recent --limit 5\n#\nList all sessions to find the one you want\nnpx claude-flow@alpha hive-mind sessions\n#\nResume specific session by ID\nnpx claude-flow@alpha hive-mind resume session-xxxxx-xxxxx\n🪝\nAdvanced Hooks System\nAutomated Workflow Enhancement\nClaude-Flow v2.0.0 introduces a powerful hooks system that automates coordination and enhances every operation:\n#\nHooks automatically trigger on operations\nnpx claude-flow@alpha init --force\n#\nAuto-configures MCP servers & hooks\nAvailable Hooks\nPre-Operation Hooks\npre-task\n: Auto-assigns agents based on task complexity\npre-search\n: Caches searches for improved performance\npre-edit\n: Validates files and prepares resources\npre-command\n: Security validation before execution\nPost-Operation Hooks\npost-edit\n: Auto-formats code using language-specific tools\npost-task\n: Trains neural patterns from successful operations\npost-command\n: Updates memory with operation context\nnotification\n: Real-time progress updates\nSession Hooks\nsession-start\n: Restores previous context automatically\nsession-end\n: Generates summaries and persists state\nsession-restore\n: Loads memory from previous sessions\nHook Configuration\n// .claude/settings.json (auto-configured)\n{\n\"hooks\"\n: {\n\"preEditHook\"\n: {\n\"command\"\n:\n\"\nnpx\n\"\n,\n\"args\"\n: [\n\"\nclaude-flow\n\"\n,\n\"\nhooks\n\"\n,\n\"\npre-edit\n\"\n,\n\"\n--file\n\"\n,\n\"\n${file}\n\"\n,\n\"\n--auto-assign-agents\n\"\n,\n\"\ntrue\n\"\n],\n\"alwaysRun\"\n:\nfalse\n},\n\"postEditHook\"\n: {\n\"command\"\n:\n\"\nnpx\n\"\n,\n\"args\"\n: [\n\"\nclaude-flow\n\"\n,\n\"\nhooks\n\"\n,\n\"\npost-edit\n\"\n,\n\"\n--file\n\"\n,\n\"\n${file}\n\"\n,\n\"\n--format\n\"\n,\n\"\ntrue\n\"\n],\n\"alwaysRun\"\n:\ntrue\n},\n\"sessionEndHook\"\n: {\n\"command\"\n:\n\"\nnpx\n\"\n,\n\"args\"\n: [\n\"\nclaude-flow\n\"\n,\n\"\nhooks\n\"\n,\n\"\nsession-end\n\"\n,\n\"\n--generate-summary\n\"\n,\n\"\ntrue\n\"\n],\n\"alwaysRun\"\n:\ntrue\n}\n  }\n}\n📚\nComplete Documentation\nFor detailed information about all features, advanced usage, and comprehensive guides, visit our\nGitHub Wiki\n:\n🤖\nCore Features\nNeural Module\n- SAFLA self-learning systems with 4-tier memory architecture\nGoal Module\n- GOAP intelligent planning with A* pathfinding\nAgent System Overview\n- Complete catalog of all 64 agents\nHive-Mind Intelligence\n- Queen-led AI coordination patterns\n⚡\nAdvanced Topics\nMemory System\n- SQLite-based persistent memory\nMCP Tools Reference\n- Complete guide to all 87 tools\nGitHub Integration\n- Repository management automation\nPerformance Benchmarking\n- Optimization strategies\n📋\nConfiguration & Templates\nCLAUDE.md Templates\n- Project-specific configurations\nSPARC Methodology\n- Test-driven development patterns\nDevelopment Patterns\n- Best practices\n🛠️\nSetup & Troubleshooting\nInstallation Guide\n- Detailed setup instructions\nWindows Installation\n- Windows-specific setup\nTroubleshooting\n- Common issues and solutions\nNon-Interactive Mode\n- CI/CD automation\n🤝\nCommunity & Support\nGitHub Issues\n:\nReport bugs or request features\nDiscord\n:\nJoin the Agentics Foundation community\nWiki\n:\nComprehensive documentation\nExamples\n:\nReal-world usage patterns\n📊\nPerformance & Stats\n84.8% SWE-Bench solve rate\n- Industry-leading problem-solving capability\n32.3% token reduction\n- Efficient context management\n2.8-4.4x speed improvement\n- Parallel coordination strategies\n64 specialized agents\n- Complete development ecosystem\n87 MCP tools\n- Comprehensive automation toolkit\n📊 Targets (Month 12)\n5K+ GitHub stars, 50K npm downloads/month\n$25K MRR, 15 enterprise customers\n90%+ error prevention, 30+ min saved/dev/week\nStar History\nBuilt with ❤️ by\nrUv\n| Powered by Revolutionary AI\nv2.5.0-alpha.140 - The Future of AI Orchestration with PreToolUse Modification Hooks",
        "今日の獲得スター数: 54",
        "累積スター数: 8,772"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/ruvnet/claude-flow"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/google/langextract",
      "title": "google/langextract",
      "date": null,
      "executive_summary": [
        "A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.",
        "---",
        "LangExtract\nTable of Contents\nIntroduction\nWhy LangExtract?\nQuick Start\nInstallation\nAPI Key Setup for Cloud Models\nAdding Custom Model Providers\nUsing OpenAI Models\nUsing Local LLMs with Ollama\nMore Examples\nRomeo and Juliet\nFull Text Extraction\nMedication Extraction\nRadiology Report Structuring: RadExtract\nCommunity Providers\nContributing\nTesting\nDisclaimer\nIntroduction\nLangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.\nWhy LangExtract?\nPrecise Source Grounding:\nMaps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.\nReliable Structured Outputs:\nEnforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.\nOptimized for Long Documents:\nOvercomes the \"needle-in-a-haystack\" challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.\nInteractive Visualization:\nInstantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.\nFlexible LLM Support:\nSupports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.\nAdaptable to Any Domain:\nDefine extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.\nLeverages LLM World Knowledge:\nUtilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.\nQuick Start\nNote:\nUsing cloud-hosted models like Gemini requires an API key. See the\nAPI Key Setup\nsection for instructions on how to get and configure your key.\nExtract structured information with just a few lines of code.\n1. Define Your Extraction Task\nFirst, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.\nimport\nlangextract\nas\nlx\nimport\ntextwrap\n# 1. Define the prompt and extraction rules\nprompt\n=\ntextwrap\n.\ndedent\n(\n\"\"\"\n\\\nExtract characters, emotions, and relationships in order of appearance.\nUse exact text for extractions. Do not paraphrase or overlap entities.\nProvide meaningful attributes for each entity to add context.\"\"\"\n)\n# 2. Provide a high-quality example to guide the model\nexamples\n=\n[\nlx\n.\ndata\n.\nExampleData\n(\ntext\n=\n\"ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.\"\n,\nextractions\n=\n[\nlx\n.\ndata\n.\nExtraction\n(\nextraction_class\n=\n\"character\"\n,\nextraction_text\n=\n\"ROMEO\"\n,\nattributes\n=\n{\n\"emotional_state\"\n:\n\"wonder\"\n}\n            ),\nlx\n.\ndata\n.\nExtraction\n(\nextraction_class\n=\n\"emotion\"\n,\nextraction_text\n=\n\"But soft!\"\n,\nattributes\n=\n{\n\"feeling\"\n:\n\"gentle awe\"\n}\n            ),\nlx\n.\ndata\n.\nExtraction\n(\nextraction_class\n=\n\"relationship\"\n,\nextraction_text\n=\n\"Juliet is the sun\"\n,\nattributes\n=\n{\n\"type\"\n:\n\"metaphor\"\n}\n            ),\n        ]\n    )\n]\n2. Run the Extraction\nProvide your input text and the prompt materials to the\nlx.extract\nfunction.\n# The input text to be processed\ninput_text\n=\n\"Lady Juliet gazed longingly at the stars, her heart aching for Romeo\"\n# Run the extraction\nresult\n=\nlx\n.\nextract\n(\ntext_or_documents\n=\ninput_text\n,\nprompt_description\n=\nprompt\n,\nexamples\n=\nexamples\n,\nmodel_id\n=\n\"gemini-2.5-flash\"\n,\n)\nModel Selection\n:\ngemini-2.5-flash\nis the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning,\ngemini-2.5-pro\nmay provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See the\nrate-limit documentation\nfor details.\nModel Lifecycle\n: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult the\nofficial model version documentation\nto stay informed about the latest stable and legacy versions.\n3. Visualize the Results\nThe extractions can be saved to a\n.jsonl\nfile, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.\n# Save the results to a JSONL file\nlx\n.\nio\n.\nsave_annotated_documents\n([\nresult\n],\noutput_name\n=\n\"extraction_results.jsonl\"\n,\noutput_dir\n=\n\".\"\n)\n# Generate the visualization from the file\nhtml_content\n=\nlx\n.\nvisualize\n(\n\"extraction_results.jsonl\"\n)\nwith\nopen\n(\n\"visualization.html\"\n,\n\"w\"\n)\nas\nf\n:\nif\nhasattr\n(\nhtml_content\n,\n'data'\n):\nf\n.\nwrite\n(\nhtml_content\n.\ndata\n)\n# For Jupyter/Colab\nelse\n:\nf\n.\nwrite\n(\nhtml_content\n)\nThis creates an animated and interactive HTML file:\nNote on LLM Knowledge Utilization:\nThis example demonstrates extractions that stay close to the text evidence - extracting \"longing\" for Lady Juliet's emotional state and identifying \"yearning\" from \"gazed longingly at the stars.\" The task could be modified to generate attributes that draw more heavily from the LLM's world knowledge (e.g., adding\n\"identity\": \"Capulet family daughter\"\nor\n\"literary_context\": \"tragic heroine\"\n). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.\nScaling to Longer Documents\nFor larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:\n# Process Romeo & Juliet directly from Project Gutenberg\nresult\n=\nlx\n.\nextract\n(\ntext_or_documents\n=\n\"https://www.gutenberg.org/files/1513/1513-0.txt\"\n,\nprompt_description\n=\nprompt\n,\nexamples\n=\nexamples\n,\nmodel_id\n=\n\"gemini-2.5-flash\"\n,\nextraction_passes\n=\n3\n,\n# Improves recall through multiple passes\nmax_workers\n=\n20\n,\n# Parallel processing for speed\nmax_char_buffer\n=\n1000\n# Smaller contexts for better accuracy\n)\nThis approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file.\nSee the full\nRomeo and Juliet\nextraction example →\nfor detailed results and performance insights.\nInstallation\nFrom PyPI\npip install langextract\nRecommended for most users. For isolated environments, consider using a virtual environment:\npython -m venv langextract_env\nsource\nlangextract_env/bin/activate\n#\nOn Windows: langextract_env\\Scripts\\activate\npip install langextract\nFrom Source\nLangExtract uses modern Python packaging with\npyproject.toml\nfor dependency management:\nInstalling with\n-e\nputs the package in development mode, allowing you to modify the code without reinstalling.\ngit clone https://github.com/google/langextract.git\ncd\nlangextract\n#\nFor basic installation:\npip install -e\n.\n#\nFor development (includes linting tools):\npip install -e\n\"\n.[dev]\n\"\n#\nFor testing (includes pytest):\npip install -e\n\"\n.[test]\n\"\nDocker\ndocker build -t langextract\n.\ndocker run --rm -e LANGEXTRACT_API_KEY=\n\"\nyour-api-key\n\"\nlangextract python your_script.py\nAPI Key Setup for Cloud Models\nWhen using LangExtract with cloud-hosted models (like Gemini or OpenAI), you'll need to\nset up an API key. On-device models don't require an API key. For developers\nusing local LLMs, LangExtract offers built-in support for Ollama and can be\nextended to other third-party APIs by updating the inference endpoints.\nAPI Key Sources\nGet API keys from:\nAI Studio\nfor Gemini models\nVertex AI\nfor enterprise use\nOpenAI Platform\nfor OpenAI models\nSetting up API key in your environment\nOption 1: Environment Variable\nexport\nLANGEXTRACT_API_KEY=\n\"\nyour-api-key-here\n\"\nOption 2: .env File (Recommended)\nAdd your API key to a\n.env\nfile:\n#\nAdd API key to .env file\ncat\n>>\n.env\n<<\n'\nEOF\n'\nLANGEXTRACT_API_KEY=your-api-key-here\nEOF\n#\nKeep your API key secure\necho\n'\n.env\n'\n>>\n.gitignore\nIn your Python code:\nimport\nlangextract\nas\nlx\nresult\n=\nlx\n.\nextract\n(\ntext_or_documents\n=\ninput_text\n,\nprompt_description\n=\n\"Extract information...\"\n,\nexamples\n=\n[...],\nmodel_id\n=\n\"gemini-2.5-flash\"\n)\nOption 3: Direct API Key (Not Recommended for Production)\nYou can also provide the API key directly in your code, though this is not recommended for production use:\nresult\n=\nlx\n.\nextract\n(\ntext_or_documents\n=\ninput_text\n,\nprompt_description\n=\n\"Extract information...\"\n,\nexamples\n=\n[...],\nmodel_id\n=\n\"gemini-2.5-flash\"\n,\napi_key\n=\n\"your-api-key-here\"\n# Only use this for testing/development\n)\nOption 4: Vertex AI (Service Accounts)\nUse\nVertex AI\nfor authentication with service accounts:\nresult\n=\nlx\n.\nextract\n(\ntext_or_documents\n=\ninput_text\n,\nprompt_description\n=\n\"Extract information...\"\n,\nexamples\n=\n[...],\nmodel_id\n=\n\"gemini-2.5-flash\"\n,\nlanguage_model_params\n=\n{\n\"vertexai\"\n:\nTrue\n,\n\"project\"\n:\n\"your-project-id\"\n,\n\"location\"\n:\n\"global\"\n# or regional endpoint\n}\n)\nAdding Custom Model Providers\nLangExtract supports custom LLM providers via a lightweight plugin system. You can add support for new models without changing core code.\nAdd new model support independently of the core library\nDistribute your provider as a separate Python package\nKeep custom dependencies isolated\nOverride or extend built-in providers via priority-based resolution\nSee the detailed guide in\nProvider System Documentation\nto learn how to:\nRegister a provider with\n@registry.register(...)\nPublish an entry point for discovery\nOptionally provide a schema with\nget_schema_class()\nfor structured output\nIntegrate with the factory via\ncreate_model(...)\nUsing OpenAI Models\nLangExtract supports OpenAI models (requires optional dependency:\npip install langextract[openai]\n):\nimport\nlangextract\nas\nlx\nresult\n=\nlx\n.\nextract\n(\ntext_or_documents\n=\ninput_text\n,\nprompt_description\n=\nprompt\n,\nexamples\n=\nexamples\n,\nmodel_id\n=\n\"gpt-4o\"\n,\n# Automatically selects OpenAI provider\napi_key\n=\nos\n.\nenviron\n.\nget\n(\n'OPENAI_API_KEY'\n),\nfence_output\n=\nTrue\n,\nuse_schema_constraints\n=\nFalse\n)\nNote: OpenAI models require\nfence_output=True\nand\nuse_schema_constraints=False\nbecause LangExtract doesn't implement schema constraints for OpenAI yet.\nUsing Local LLMs with Ollama\nLangExtract supports local inference using Ollama, allowing you to run models without API keys:\nimport\nlangextract\nas\nlx\nresult\n=\nlx\n.\nextract\n(\ntext_or_documents\n=\ninput_text\n,\nprompt_description\n=\nprompt\n,\nexamples\n=\nexamples\n,\nmodel_id\n=\n\"gemma2:2b\"\n,\n# Automatically selects Ollama provider\nmodel_url\n=\n\"http://localhost:11434\"\n,\nfence_output\n=\nFalse\n,\nuse_schema_constraints\n=\nFalse\n)\nQuick setup:\nInstall Ollama from\nollama.com\n, run\nollama pull gemma2:2b\n, then\nollama serve\n.\nFor detailed installation, Docker setup, and examples, see\nexamples/ollama/\n.\nMore Examples\nAdditional examples of LangExtract in action:\nRomeo and Juliet\nFull Text Extraction\nLangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text of\nRomeo and Juliet\nfrom Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.\nView\nRomeo and Juliet\nFull Text Example →\nMedication Extraction\nDisclaimer:\nThis demonstration is for illustrative purposes of LangExtract's baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.\nLangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract's effectiveness for healthcare applications.\nView Medication Examples →\nRadiology Report Structuring: RadExtract\nExplore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.\nView RadExtract Demo →\nCommunity Providers\nExtend LangExtract with custom model providers! Check out our\nCommunity Provider Plugins\nregistry to discover providers created by the community or add your own.\nFor detailed instructions on creating a provider plugin, see the\nCustom Provider Plugin Example\n.\nContributing\nContributions are welcome! See\nCONTRIBUTING.md\nto get started\nwith development, testing, and pull requests. You must sign a\nContributor License Agreement\nbefore submitting patches.\nTesting\nTo run tests locally from the source:\n#\nClone the repository\ngit clone https://github.com/google/langextract.git\ncd\nlangextract\n#\nInstall with test dependencies\npip install -e\n\"\n.[test]\n\"\n#\nRun all tests\npytest tests\nOr reproduce the full CI matrix locally with tox:\ntox\n#\nruns pylint + pytest on Python 3.10 and 3.11\nOllama Integration Testing\nIf you have Ollama installed locally, you can run integration tests:\n#\nTest Ollama integration (requires Ollama running with gemma2:2b model)\ntox -e ollama-integration\nThis test will automatically detect if Ollama is available and run real inference tests.\nDevelopment\nCode Formatting\nThis project uses automated formatting tools to maintain consistent code style:\n#\nAuto-format all code\n./autoformat.sh\n#\nOr run formatters separately\nisort langextract tests --profile google --line-length 80\npyink langextract tests --config pyproject.toml\nPre-commit Hooks\nFor automatic formatting checks:\npre-commit install\n#\nOne-time setup\npre-commit run --all-files\n#\nManual run\nLinting\nRun linting before submitting PRs:\npylint --rcfile=.pylintrc langextract tests\nSee\nCONTRIBUTING.md\nfor full development guidelines.\nDisclaimer\nThis is not an officially supported Google product. If you use\nLangExtract in production or publications, please cite accordingly and\nacknowledge usage. Use is subject to the\nApache 2.0 License\n.\nFor health-related applications, use of LangExtract is also subject to the\nHealth AI Developer Foundations Terms of Use\n.\nHappy Extracting!",
        "今日の獲得スター数: 52",
        "累積スター数: 16,257"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/google/langextract"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/vercel/ai",
      "title": "vercel/ai",
      "date": null,
      "executive_summary": [
        "The AI Toolkit for TypeScript. From the creators of Next.js, the AI SDK is a free open-source library for building AI-powered applications and agents",
        "---",
        "AI SDK\nThe\nAI SDK\nis a TypeScript toolkit designed to help you build AI-powered applications and agents using popular frameworks like Next.js, React, Svelte, Vue and runtimes like Node.js.\nTo learn more about how to use the AI SDK, check out our\nAPI Reference\nand\nDocumentation\n.\nInstallation\nYou will need Node.js 18+ and npm (or another package manager) installed on your local development machine.\nnpm install ai\nUnified Provider Architecture\nThe AI SDK provides a\nunified API\nto interact with model providers like\nOpenAI\n,\nAnthropic\n,\nGoogle\n, and\nmore\n.\nnpm install @ai-sdk/openai @ai-sdk/anthropic @ai-sdk/google\nAlternatively you can use the\nVercel AI Gateway\n.\nUsage\nGenerating Text\nimport\n{\ngenerateText\n}\nfrom\n'ai'\n;\nconst\n{\ntext\n}\n=\nawait\ngenerateText\n(\n{\nmodel\n:\n'openai/gpt-5'\n,\n// use Vercel AI Gateway\nprompt\n:\n'What is an agent?'\n,\n}\n)\n;\nimport\n{\ngenerateText\n}\nfrom\n'ai'\n;\nimport\n{\nopenai\n}\nfrom\n'@ai-sdk/openai'\n;\nconst\n{\ntext\n}\n=\nawait\ngenerateText\n(\n{\nmodel\n:\nopenai\n(\n'gpt-5'\n)\n,\n// use OpenAI Responses API\nprompt\n:\n'What is an agent?'\n,\n}\n)\n;\nGenerating Structured Data\nimport\n{\ngenerateObject\n}\nfrom\n'ai'\n;\nimport\n{\nz\n}\nfrom\n'zod'\n;\nconst\n{\nobject\n}\n=\nawait\ngenerateObject\n(\n{\nmodel\n:\n'openai/gpt-4.1'\n,\nschema\n:\nz\n.\nobject\n(\n{\nrecipe\n:\nz\n.\nobject\n(\n{\nname\n:\nz\n.\nstring\n(\n)\n,\ningredients\n:\nz\n.\narray\n(\nz\n.\nobject\n(\n{\nname\n:\nz\n.\nstring\n(\n)\n,\namount\n:\nz\n.\nstring\n(\n)\n}\n)\n)\n,\nsteps\n:\nz\n.\narray\n(\nz\n.\nstring\n(\n)\n)\n,\n}\n)\n,\n}\n)\n,\nprompt\n:\n'Generate a lasagna recipe.'\n,\n}\n)\n;\nAgents\nimport\n{\nAgent\n}\nfrom\n'ai'\n;\nconst\nsandboxAgent\n=\nnew\nAgent\n(\n{\nmodel\n:\n'openai/gpt-5-codex'\n,\nsystem\n:\n'You are an agent with access to a shell environment.'\n,\ntools\n:\n{\nlocal_shell\n:\nopenai\n.\ntools\n.\nlocalShell\n(\n{\nexecute\n:\nasync\n(\n{\naction\n}\n)\n=>\n{\nconst\n[\ncmd\n,\n...\nargs\n]\n=\naction\n.\ncommand\n;\nconst\nsandbox\n=\nawait\ngetSandbox\n(\n)\n;\n// Vercel Sandbox\nconst\ncommand\n=\nawait\nsandbox\n.\nrunCommand\n(\n{\ncmd\n,\nargs\n}\n)\n;\nreturn\n{\noutput\n:\nawait\ncommand\n.\nstdout\n(\n)\n}\n;\n}\n,\n}\n)\n,\n}\n,\n}\n)\n;\nUI Integration\nThe\nAI SDK UI\nmodule provides a set of hooks that help you build chatbots and generative user interfaces. These hooks are framework agnostic, so they can be used in Next.js, React, Svelte, and Vue.\nYou need to install the package for your framework, e.g.:\nnpm install @ai-sdk/react\nAgent @/agent/image-generation-agent.ts\nimport\n{\nopenai\n}\nfrom\n'@ai-sdk/openai'\n;\nimport\n{\nAgent\n,\nInferAgentUIMessage\n}\nfrom\n'ai'\n;\nexport\nconst\nimageGenerationAgent\n=\nnew\nAgent\n(\n{\nmodel\n:\nopenai\n(\n'gpt-5'\n)\n,\ntools\n:\n{\nimage_generation\n:\nopenai\n.\ntools\n.\nimageGeneration\n(\n{\npartialImages\n:\n3\n,\n}\n)\n,\n}\n,\n}\n)\n;\nexport\ntype\nImageGenerationAgentMessage\n=\nInferAgentUIMessage\n<\ntypeof\nimageGenerationAgent\n>\n;\nRoute (Next.js App Router) @/app/api/chat/route.ts\nimport\n{\nimageGenerationAgent\n}\nfrom\n'@/agent/image-generation-agent'\n;\nimport\n{\nvalidateUIMessages\n}\nfrom\n'ai'\n;\nexport\nasync\nfunction\nPOST\n(\nreq\n:\nRequest\n)\n{\nconst\n{\nmessages\n}\n=\nawait\nreq\n.\njson\n(\n)\n;\nreturn\nimageGenerationAgent\n.\nrespond\n(\n{\nmessages\n:\nawait\nvalidateUIMessages\n(\n{\nmessages\n}\n)\n,\n}\n)\n;\n}\nUI Component for Tool @/component/image-generation-view.tsx\nimport\n{\nopenai\n}\nfrom\n'@ai-sdk/openai'\n;\nimport\n{\nUIToolInvocation\n}\nfrom\n'ai'\n;\nexport\ndefault\nfunction\nImageGenerationView\n(\n{\ninvocation\n,\n}\n:\n{\ninvocation\n:\nUIToolInvocation\n<\nReturnType\n<\ntypeof\nopenai\n.\ntools\n.\nimageGeneration\n>\n>\n;\n}\n)\n{\nswitch\n(\ninvocation\n.\nstate\n)\n{\ncase\n'input-available'\n:\nreturn\n<\ndiv\n>\nGenerating image...\n</\ndiv\n>\n;\ncase\n'output-available'\n:\nreturn\n<\nimg\nsrc\n=\n{\n`data:image/png;base64,\n${\ninvocation\n.\noutput\n.\nresult\n}\n`\n}\n/>\n;\n}\n}\nPage @/app/page.tsx\n'use client'\n;\nimport\n{\nImageGenerationAgentMessage\n}\nfrom\n'@/agent/image-generation-agent'\n;\nimport\nImageGenerationView\nfrom\n'@/component/image-generation-view'\n;\nimport\n{\nuseChat\n}\nfrom\n'@ai-sdk/react'\n;\nexport\ndefault\nfunction\nPage\n(\n)\n{\nconst\n{\nmessages\n,\nstatus\n,\nsendMessage\n}\n=\nuseChat\n<\nImageGenerationAgentMessage\n>\n(\n)\n;\nconst\n[\ninput\n,\nsetInput\n]\n=\nuseState\n(\n''\n)\n;\nconst\nhandleSubmit\n=\ne\n=>\n{\ne\n.\npreventDefault\n(\n)\n;\nsendMessage\n(\n{\ntext\n:\ninput\n}\n)\n;\nsetInput\n(\n''\n)\n;\n}\n;\nreturn\n(\n<\ndiv\n>\n{\nmessages\n.\nmap\n(\nmessage\n=>\n(\n<\ndiv\nkey\n=\n{\nmessage\n.\nid\n}\n>\n<\nstrong\n>\n{\n`\n${\nmessage\n.\nrole\n}\n: `\n}\n</\nstrong\n>\n{\nmessage\n.\nparts\n.\nmap\n(\n(\npart\n,\nindex\n)\n=>\n{\nswitch\n(\npart\n.\ntype\n)\n{\ncase\n'text'\n:\nreturn\n<\ndiv\nkey\n=\n{\nindex\n}\n>\n{\npart\n.\ntext\n}\n</\ndiv\n>\n;\ncase\n'tool-image_generation'\n:\nreturn\n<\nImageGenerationView\nkey\n=\n{\nindex\n}\ninvocation\n=\n{\npart\n}\n/>\n;\n}\n}\n)\n}\n</\ndiv\n>\n)\n)\n}\n<\nform\nonSubmit\n=\n{\nhandleSubmit\n}\n>\n<\ninput\nvalue\n=\n{\ninput\n}\nonChange\n=\n{\ne\n=>\nsetInput\n(\ne\n.\ntarget\n.\nvalue\n)\n}\ndisabled\n=\n{\nstatus\n!==\n'ready'\n}\n/>\n</\nform\n>\n</\ndiv\n>\n)\n;\n}\nTemplates\nWe've built\ntemplates\nthat include AI SDK integrations for different use cases, providers, and frameworks. You can use these templates to get started with your AI-powered application.\nCommunity\nThe AI SDK community can be found on\nGitHub Discussions\nwhere you can ask questions, voice ideas, and share your projects with other people.\nContributing\nContributions to the AI SDK are welcome and highly appreciated. However, before you jump right into it, we would like you to review our\nContribution Guidelines\nto make sure you have smooth experience contributing to AI SDK.\nAuthors\nThis library is created by\nVercel\nand\nNext.js\nteam members, with contributions from the\nOpen Source Community\n.",
        "今日の獲得スター数: 51",
        "累積スター数: 18,328"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/vercel/ai"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/sapientinc/HRM",
      "title": "sapientinc/HRM",
      "date": null,
      "executive_summary": [
        "Hierarchical Reasoning Model Official Release",
        "---",
        "Hierarchical Reasoning Model\nReasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI.\nCurrent large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency.\nHRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes.\nFurthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities.\nThese results underscore HRM’s potential as a transformative advancement toward universal computation and general-purpose reasoning systems.\nJoin our Discord Community:\nhttps://discord.gg/sapient\nQuick Start Guide 🚀\nPrerequisites ⚙️\nEnsure PyTorch and CUDA are installed. The repo needs CUDA extensions to be built. If not present, run the following commands:\n#\nInstall CUDA 12.6\nCUDA_URL=https://developer.download.nvidia.com/compute/cuda/12.6.3/local_installers/cuda_12.6.3_560.35.05_linux.run\n\nwget -q --show-progress --progress=bar:force:noscroll -O cuda_installer.run\n$CUDA_URL\nsudo sh cuda_installer.run --silent --toolkit --override\nexport\nCUDA_HOME=/usr/local/cuda-12.6\n#\nInstall PyTorch with CUDA 12.6\nPYTORCH_INDEX_URL=https://download.pytorch.org/whl/cu126\n\npip3 install torch torchvision torchaudio --index-url\n$PYTORCH_INDEX_URL\n#\nAdditional packages for building extensions\npip3 install packaging ninja wheel setuptools setuptools-scm\nThen install FlashAttention. For Hopper GPUs, install FlashAttention 3\ngit clone git@github.com:Dao-AILab/flash-attention.git\ncd\nflash-attention/hopper\npython setup.py install\nFor Ampere or earlier GPUs, install FlashAttention 2\npip3 install flash-attn\nInstall Python Dependencies 🐍\npip install -r requirements.txt\nW&B Integration 📈\nThis project uses\nWeights & Biases\nfor experiment tracking and metric visualization. Ensure you're logged in:\nwandb login\nRun Experiments\nQuick Demo: Sudoku Solver 💻🗲\nTrain a master-level Sudoku AI capable of solving extremely difficult puzzles on a modern laptop GPU. 🧩\n#\nDownload and build Sudoku dataset\npython dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000\n#\nStart training (single GPU, smaller batch size)\nOMP_NUM_THREADS=8 python pretrain.py data_path=data/sudoku-extreme-1k-aug-1000 epochs=20000 eval_interval=2000 global_batch_size=384 lr=7e-5 puzzle_emb_lr=7e-5 weight_decay=1.0 puzzle_emb_weight_decay=1.0\nRuntime: ~10 hours on a RTX 4070 laptop GPU\nTrained Checkpoints 🚧\nARC-AGI-2\nSudoku 9x9 Extreme (1000 examples)\nMaze 30x30 Hard (1000 examples)\nTo use the checkpoints, see Evaluation section below.\nFull-scale Experiments 🔵\nExperiments below assume an 8-GPU setup.\nDataset Preparation\n#\nInitialize submodules\ngit submodule update --init --recursive\n#\nARC-1\npython dataset/build_arc_dataset.py\n#\nARC offical + ConceptARC, 960 examples\n#\nARC-2\npython dataset/build_arc_dataset.py --dataset-dirs dataset/raw-data/ARC-AGI-2/data --output-dir data/arc-2-aug-1000\n#\nARC-2 official, 1120 examples\n#\nSudoku-Extreme\npython dataset/build_sudoku_dataset.py\n#\nFull version\npython dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000\n#\n1000 examples\n#\nMaze\npython dataset/build_maze_dataset.py\n#\n1000 examples\nDataset Visualization\nExplore the puzzles visually:\nOpen\npuzzle_visualizer.html\nin your browser.\nUpload the generated dataset folder located in\ndata/...\n.\nLaunch experiments\nSmall-sample (1K)\nARC-1:\nOMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py\nRuntime:\n~24 hours\nARC-2:\nOMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/arc-2-aug-1000\nRuntime:\n~24 hours (checkpoint after 8 hours is often sufficient)\nSudoku Extreme (1k):\nOMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/sudoku-extreme-1k-aug-1000 epochs=20000 eval_interval=2000 lr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle_emb_weight_decay=1.0\nRuntime:\n~10 minutes\nMaze 30x30 Hard (1k):\nOMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/maze-30x30-hard-1k epochs=20000 eval_interval=2000 lr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle_emb_weight_decay=1.0\nRuntime:\n~1 hour\nFull Sudoku-Hard\nOMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/sudoku-hard-full epochs=100 eval_interval=10 lr_min_ratio=0.1 global_batch_size=2304 lr=3e-4 puzzle_emb_lr=3e-4 weight_decay=0.1 puzzle_emb_weight_decay=0.1 arch.loss.loss_type=softmax_cross_entropy arch.L_cycles=8 arch.halt_max_steps=8 arch.pos_encodings=learned\nRuntime:\n~2 hours\nEvaluation\nEvaluate your trained models:\nCheck\neval/exact_accuracy\nin W&B.\nFor ARC-AGI, follow these additional steps:\nOMP_NUM_THREADS=8 torchrun --nproc-per-node 8 evaluate.py checkpoint=\n<\nCHECKPOINT_PATH\n>\nThen use the provided\narc_eval.ipynb\nnotebook to finalize and inspect your results.\nNotes\nSmall-sample learning typically exhibits accuracy variance of around ±2 points.\nFor Sudoku-Extreme (1,000-example dataset), late-stage overfitting may cause numerical instability during training and Q-learning. It is advisable to use early stopping once the training accuracy approaches 100%.\nCitation 📜\n@misc\n{\nwang2025hierarchicalreasoningmodel\n,\ntitle\n=\n{\nHierarchical Reasoning Model\n}\n,\nauthor\n=\n{\nGuan Wang and Jin Li and Yuhao Sun and Xing Chen and Changling Liu and Yue Wu and Meng Lu and Sen Song and Yasin Abbasi Yadkori\n}\n,\nyear\n=\n{\n2025\n}\n,\neprint\n=\n{\n2506.21734\n}\n,\narchivePrefix\n=\n{\narXiv\n}\n,\nprimaryClass\n=\n{\ncs.AI\n}\n,\nurl\n=\n{\nhttps://arxiv.org/abs/2506.21734\n}\n, \n}",
        "今日の獲得スター数: 51",
        "累積スター数: 10,867"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/sapientinc/HRM"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/BerriAI/litellm",
      "title": "BerriAI/litellm",
      "date": null,
      "executive_summary": [
        "Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]",
        "---",
        "🚅 LiteLLM\nCall all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]\nLiteLLM Proxy Server (LLM Gateway)\n|\nHosted Proxy (Preview)\n|\nEnterprise Tier\nLiteLLM manages:\nTranslate inputs to provider's\ncompletion\n,\nembedding\n, and\nimage_generation\nendpoints\nConsistent output\n, text responses will always be available at\n['choices'][0]['message']['content']\nRetry/fallback logic across multiple deployments (e.g. Azure/OpenAI) -\nRouter\nSet Budgets & Rate limits per project, api key, model\nLiteLLM Proxy Server (LLM Gateway)\nJump to LiteLLM Proxy (LLM Gateway) Docs\nJump to Supported LLM Providers\n🚨\nStable Release:\nUse docker images with the\n-stable\ntag. These have undergone 12 hour load tests, before being published.\nMore information about the release cycle here\nSupport for more providers. Missing a provider or LLM Platform, raise a\nfeature request\n.\nUsage (\nDocs\n)\nImportant\nLiteLLM v1.0.0 now requires\nopenai>=1.0.0\n. Migration guide\nhere\nLiteLLM v1.40.14+ now requires\npydantic>=2.0.0\n. No changes required.\npip install litellm\nfrom\nlitellm\nimport\ncompletion\nimport\nos\n## set ENV variables\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\n\"your-openai-key\"\nos\n.\nenviron\n[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"your-anthropic-key\"\nmessages\n=\n[{\n\"content\"\n:\n\"Hello, how are you?\"\n,\n\"role\"\n:\n\"user\"\n}]\n# openai call\nresponse\n=\ncompletion\n(\nmodel\n=\n\"openai/gpt-4o\"\n,\nmessages\n=\nmessages\n)\n# anthropic call\nresponse\n=\ncompletion\n(\nmodel\n=\n\"anthropic/claude-sonnet-4-20250514\"\n,\nmessages\n=\nmessages\n)\nprint\n(\nresponse\n)\nResponse (OpenAI Format)\n{\n\"id\"\n:\n\"\nchatcmpl-1214900a-6cdd-4148-b663-b5e2f642b4de\n\"\n,\n\"created\"\n:\n1751494488\n,\n\"model\"\n:\n\"\nclaude-sonnet-4-20250514\n\"\n,\n\"object\"\n:\n\"\nchat.completion\n\"\n,\n\"system_fingerprint\"\n:\nnull\n,\n\"choices\"\n: [\n        {\n\"finish_reason\"\n:\n\"\nstop\n\"\n,\n\"index\"\n:\n0\n,\n\"message\"\n: {\n\"content\"\n:\n\"\nHello! I'm doing well, thank you for asking. I'm here and ready to help with whatever you'd like to discuss or work on. How are you doing today?\n\"\n,\n\"role\"\n:\n\"\nassistant\n\"\n,\n\"tool_calls\"\n:\nnull\n,\n\"function_call\"\n:\nnull\n}\n        }\n    ],\n\"usage\"\n: {\n\"completion_tokens\"\n:\n39\n,\n\"prompt_tokens\"\n:\n13\n,\n\"total_tokens\"\n:\n52\n,\n\"completion_tokens_details\"\n:\nnull\n,\n\"prompt_tokens_details\"\n: {\n\"audio_tokens\"\n:\nnull\n,\n\"cached_tokens\"\n:\n0\n},\n\"cache_creation_input_tokens\"\n:\n0\n,\n\"cache_read_input_tokens\"\n:\n0\n}\n}\nCall any model supported by a provider, with\nmodel=<provider_name>/<model_name>\n. There might be provider-specific details here, so refer to\nprovider docs for more information\nAsync (\nDocs\n)\nfrom\nlitellm\nimport\nacompletion\nimport\nasyncio\nasync\ndef\ntest_get_response\n():\nuser_message\n=\n\"Hello, how are you?\"\nmessages\n=\n[{\n\"content\"\n:\nuser_message\n,\n\"role\"\n:\n\"user\"\n}]\nresponse\n=\nawait\nacompletion\n(\nmodel\n=\n\"openai/gpt-4o\"\n,\nmessages\n=\nmessages\n)\nreturn\nresponse\nresponse\n=\nasyncio\n.\nrun\n(\ntest_get_response\n())\nprint\n(\nresponse\n)\nStreaming (\nDocs\n)\nliteLLM supports streaming the model response back, pass\nstream=True\nto get a streaming iterator in response.\nStreaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)\nfrom\nlitellm\nimport\ncompletion\nresponse\n=\ncompletion\n(\nmodel\n=\n\"openai/gpt-4o\"\n,\nmessages\n=\nmessages\n,\nstream\n=\nTrue\n)\nfor\npart\nin\nresponse\n:\nprint\n(\npart\n.\nchoices\n[\n0\n].\ndelta\n.\ncontent\nor\n\"\"\n)\n# claude sonnet 4\nresponse\n=\ncompletion\n(\n'anthropic/claude-sonnet-4-20250514'\n,\nmessages\n,\nstream\n=\nTrue\n)\nfor\npart\nin\nresponse\n:\nprint\n(\npart\n)\nResponse chunk (OpenAI Format)\n{\n\"id\"\n:\n\"\nchatcmpl-fe575c37-5004-4926-ae5e-bfbc31f356ca\n\"\n,\n\"created\"\n:\n1751494808\n,\n\"model\"\n:\n\"\nclaude-sonnet-4-20250514\n\"\n,\n\"object\"\n:\n\"\nchat.completion.chunk\n\"\n,\n\"system_fingerprint\"\n:\nnull\n,\n\"choices\"\n: [\n        {\n\"finish_reason\"\n:\nnull\n,\n\"index\"\n:\n0\n,\n\"delta\"\n: {\n\"provider_specific_fields\"\n:\nnull\n,\n\"content\"\n:\n\"\nHello\n\"\n,\n\"role\"\n:\n\"\nassistant\n\"\n,\n\"function_call\"\n:\nnull\n,\n\"tool_calls\"\n:\nnull\n,\n\"audio\"\n:\nnull\n},\n\"logprobs\"\n:\nnull\n}\n    ],\n\"provider_specific_fields\"\n:\nnull\n,\n\"stream_options\"\n:\nnull\n,\n\"citations\"\n:\nnull\n}\nLogging Observability (\nDocs\n)\nLiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack\nfrom\nlitellm\nimport\ncompletion\n## set env variables for logging tools (when using MLflow, no API key set up is required)\nos\n.\nenviron\n[\n\"LUNARY_PUBLIC_KEY\"\n]\n=\n\"your-lunary-public-key\"\nos\n.\nenviron\n[\n\"HELICONE_API_KEY\"\n]\n=\n\"your-helicone-auth-key\"\nos\n.\nenviron\n[\n\"LANGFUSE_PUBLIC_KEY\"\n]\n=\n\"\"\nos\n.\nenviron\n[\n\"LANGFUSE_SECRET_KEY\"\n]\n=\n\"\"\nos\n.\nenviron\n[\n\"ATHINA_API_KEY\"\n]\n=\n\"your-athina-api-key\"\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n=\n\"your-openai-key\"\n# set callbacks\nlitellm\n.\nsuccess_callback\n=\n[\n\"lunary\"\n,\n\"mlflow\"\n,\n\"langfuse\"\n,\n\"athina\"\n,\n\"helicone\"\n]\n# log input/output to lunary, langfuse, supabase, athina, helicone etc\n#openai call\nresponse\n=\ncompletion\n(\nmodel\n=\n\"openai/gpt-4o\"\n,\nmessages\n=\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Hi 👋 - i'm openai\"\n}])\nLiteLLM Proxy Server (LLM Gateway) - (\nDocs\n)\nTrack spend + Load Balance across multiple projects\nHosted Proxy (Preview)\nThe proxy provides:\nHooks for auth\nHooks for logging\nCost tracking\nRate Limiting\n📖 Proxy Endpoints -\nSwagger Docs\nQuick Start Proxy - CLI\npip install\n'\nlitellm[proxy]\n'\nStep 1: Start litellm proxy\n$ litellm --model huggingface/bigcode/starcoder\n#\nINFO: Proxy running on http://0.0.0.0:4000\nStep 2: Make ChatCompletions Request to Proxy\nImportant\n💡\nUse LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl\nimport\nopenai\n# openai v1.0.0+\nclient\n=\nopenai\n.\nOpenAI\n(\napi_key\n=\n\"anything\"\n,\nbase_url\n=\n\"http://0.0.0.0:4000\"\n)\n# set proxy to base_url\n# request sent to model set on litellm proxy, `litellm --model`\nresponse\n=\nclient\n.\nchat\n.\ncompletions\n.\ncreate\n(\nmodel\n=\n\"gpt-3.5-turbo\"\n,\nmessages\n=\n[\n    {\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"this is a test request, write a short poem\"\n}\n])\nprint\n(\nresponse\n)\nProxy Key Management (\nDocs\n)\nConnect the proxy with a Postgres DB to create proxy keys\n#\nGet the code\ngit clone https://github.com/BerriAI/litellm\n#\nGo to folder\ncd\nlitellm\n#\nAdd the master key - you can change this after setup\necho\n'\nLITELLM_MASTER_KEY=\"sk-1234\"\n'\n>\n.env\n#\nAdd the litellm salt key - you cannot change this after adding a model\n#\nIt is used to encrypt / decrypt your LLM API Key credentials\n#\nWe recommend - https://1password.com/password-generator/\n#\npassword generator to get a random hash for litellm salt key\necho\n'\nLITELLM_SALT_KEY=\"sk-1234\"\n'\n>>\n.env\nsource\n.env\n#\nStart\ndocker compose up\nUI on\n/ui\non your proxy server\nSet budgets and rate limits across multiple projects\nPOST /key/generate\nRequest\ncurl\n'\nhttp://0.0.0.0:4000/key/generate\n'\n\\\n--header\n'\nAuthorization: Bearer sk-1234\n'\n\\\n--header\n'\nContent-Type: application/json\n'\n\\\n--data-raw\n'\n{\"models\": [\"gpt-3.5-turbo\", \"gpt-4\", \"claude-2\"], \"duration\": \"20m\",\"metadata\": {\"user\": \"ishaan@berri.ai\", \"team\": \"core-infra\"}}\n'\nExpected Response\n{\n\"\nkey\n\"\n:\n\"\nsk-kdEXbIqZRwEeEiHwdg7sFA\n\"\n,\n#\nBearer token\n\"\nexpires\n\"\n:\n\"\n2023-11-19T01:38:25.838000+00:00\n\"\n#\ndatetime object\n}\nSupported Providers (\nDocs\n)\nProvider\nCompletion\nStreaming\nAsync Completion\nAsync Streaming\nAsync Embedding\nAsync Image Generation\nopenai\n✅\n✅\n✅\n✅\n✅\n✅\nMeta - Llama API\n✅\n✅\n✅\n✅\nazure\n✅\n✅\n✅\n✅\n✅\n✅\nAI/ML API\n✅\n✅\n✅\n✅\n✅\n✅\naws - sagemaker\n✅\n✅\n✅\n✅\n✅\naws - bedrock\n✅\n✅\n✅\n✅\n✅\ngoogle - vertex_ai\n✅\n✅\n✅\n✅\n✅\n✅\ngoogle - palm\n✅\n✅\n✅\n✅\ngoogle AI Studio - gemini\n✅\n✅\n✅\n✅\nmistral ai api\n✅\n✅\n✅\n✅\n✅\ncloudflare AI Workers\n✅\n✅\n✅\n✅\nCompactifAI\n✅\n✅\n✅\n✅\ncohere\n✅\n✅\n✅\n✅\n✅\nanthropic\n✅\n✅\n✅\n✅\nempower\n✅\n✅\n✅\n✅\nhuggingface\n✅\n✅\n✅\n✅\n✅\nreplicate\n✅\n✅\n✅\n✅\ntogether_ai\n✅\n✅\n✅\n✅\nopenrouter\n✅\n✅\n✅\n✅\nai21\n✅\n✅\n✅\n✅\nbaseten\n✅\n✅\n✅\n✅\nvllm\n✅\n✅\n✅\n✅\nnlp_cloud\n✅\n✅\n✅\n✅\naleph alpha\n✅\n✅\n✅\n✅\npetals\n✅\n✅\n✅\n✅\nollama\n✅\n✅\n✅\n✅\n✅\ndeepinfra\n✅\n✅\n✅\n✅\nperplexity-ai\n✅\n✅\n✅\n✅\nGroq AI\n✅\n✅\n✅\n✅\nDeepseek\n✅\n✅\n✅\n✅\nanyscale\n✅\n✅\n✅\n✅\nIBM - watsonx.ai\n✅\n✅\n✅\n✅\n✅\nvoyage ai\n✅\nxinference [Xorbits Inference]\n✅\nFriendliAI\n✅\n✅\n✅\n✅\nGaladriel\n✅\n✅\n✅\n✅\nGradientAI\n✅\n✅\nNovita AI\n✅\n✅\n✅\n✅\nFeatherless AI\n✅\n✅\n✅\n✅\nNebius AI Studio\n✅\n✅\n✅\n✅\n✅\nHeroku\n✅\n✅\nOVHCloud AI Endpoints\n✅\n✅\nRead the Docs\nRun in Developer mode\nServices\nSetup .env file in root\nRun dependant services\ndocker-compose up db prometheus\nBackend\n(In root) create virtual environment\npython -m venv .venv\nActivate virtual environment\nsource .venv/bin/activate\nInstall dependencies\npip install -e \".[all]\"\nStart proxy backend\npython litellm/proxy_cli.py\nFrontend\nNavigate to\nui/litellm-dashboard\nInstall dependencies\nnpm install\nRun\nnpm run dev\nto start the dashboard\nEnterprise\nFor companies that need better security, user management and professional support\nTalk to founders\nThis covers:\n✅\nFeatures under the\nLiteLLM Commercial License\n:\n✅\nFeature Prioritization\n✅\nCustom Integrations\n✅\nProfessional Support - Dedicated discord + slack\n✅\nCustom SLAs\n✅\nSecure access with Single Sign-On\nContributing\nWe welcome contributions to LiteLLM! Whether you're fixing bugs, adding features, or improving documentation, we appreciate your help.\nQuick Start for Contributors\nThis requires poetry to be installed.\ngit clone https://github.com/BerriAI/litellm.git\ncd\nlitellm\nmake install-dev\n#\nInstall development dependencies\nmake format\n#\nFormat your code\nmake lint\n#\nRun all linting checks\nmake test-unit\n#\nRun unit tests\nmake format-check\n#\nCheck formatting only\nFor detailed contributing guidelines, see\nCONTRIBUTING.md\n.\nCode Quality / Linting\nLiteLLM follows the\nGoogle Python Style Guide\n.\nOur automated checks include:\nBlack\nfor code formatting\nRuff\nfor linting and code quality\nMyPy\nfor type checking\nCircular import detection\nImport safety checks\nAll these checks must pass before your PR can be merged.\nSupport / talk with founders\nSchedule Demo 👋\nCommunity Discord 💭\nCommunity Slack 💭\nOur numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬\nOur emails ✉️\nishaan@berri.ai\n/\nkrrish@berri.ai\nWhy did we build this\nNeed for simplicity\n: Our code started to get extremely complicated managing & translating calls between Azure, OpenAI and Cohere.\nContributors",
        "今日の獲得スター数: 42",
        "累積スター数: 29,706"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/BerriAI/litellm"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/astral-sh/ruff",
      "title": "astral-sh/ruff",
      "date": null,
      "executive_summary": [
        "An extremely fast Python linter and code formatter, written in Rust.",
        "---",
        "Ruff\nDocs\n|\nPlayground\nAn extremely fast Python linter and code formatter, written in Rust.\nLinting the CPython codebase from scratch.\n⚡️ 10-100x faster than existing linters (like Flake8) and formatters (like Black)\n🐍 Installable via\npip\n🛠️\npyproject.toml\nsupport\n🤝 Python 3.13 compatibility\n⚖️ Drop-in parity with\nFlake8\n, isort, and\nBlack\n📦 Built-in caching, to avoid re-analyzing unchanged files\n🔧 Fix support, for automatic error correction (e.g., automatically remove unused imports)\n📏 Over\n800 built-in rules\n, with native re-implementations\nof popular Flake8 plugins, like flake8-bugbear\n⌨️ First-party\neditor integrations\nfor\nVS Code\nand\nmore\n🌎 Monorepo-friendly, with\nhierarchical and cascading configuration\nRuff aims to be orders of magnitude faster than alternative tools while integrating more\nfunctionality behind a single, common interface.\nRuff can be used to replace\nFlake8\n(plus dozens of plugins),\nBlack\n,\nisort\n,\npydocstyle\n,\npyupgrade\n,\nautoflake\n, and more, all while executing tens or hundreds of\ntimes faster than any individual tool.\nRuff is extremely actively developed and used in major open-source projects like:\nApache Airflow\nApache Superset\nFastAPI\nHugging Face\nPandas\nSciPy\n...and\nmany more\n.\nRuff is backed by\nAstral\n. Read the\nlaunch post\n,\nor the original\nproject announcement\n.\nTestimonials\nSebastián Ramírez\n, creator\nof\nFastAPI\n:\nRuff is so fast that sometimes I add an intentional bug in the code just to confirm it's actually\nrunning and checking the code.\nNick Schrock\n, founder of\nElementl\n,\nco-creator of\nGraphQL\n:\nWhy is Ruff a gamechanger? Primarily because it is nearly 1000x faster. Literally. Not a typo. On\nour largest module (dagster itself, 250k LOC) pylint takes about 2.5 minutes, parallelized across 4\ncores on my M1. Running ruff against our\nentire\ncodebase takes .4 seconds.\nBryan Van de Ven\n, co-creator\nof\nBokeh\n, original author\nof\nConda\n:\nRuff is ~150-200x faster than flake8 on my machine, scanning the whole repo takes ~0.2s instead of\n~20s. This is an enormous quality of life improvement for local dev. It's fast enough that I added\nit as an actual commit hook, which is terrific.\nTimothy Crosley\n,\ncreator of\nisort\n:\nJust switched my first project to Ruff. Only one downside so far: it's so fast I couldn't believe\nit was working till I intentionally introduced some errors.\nTim Abbott\n, lead\ndeveloper of\nZulip\n:\nThis is just ridiculously fast...\nruff\nis amazing.\nTable of Contents\nFor more, see the\ndocumentation\n.\nGetting Started\nConfiguration\nRules\nContributing\nSupport\nAcknowledgements\nWho's Using Ruff?\nLicense\nGetting Started\nFor more, see the\ndocumentation\n.\nInstallation\nRuff is available as\nruff\non PyPI.\nInvoke Ruff directly with\nuvx\n:\nuvx ruff check\n#\nLint all files in the current directory.\nuvx ruff format\n#\nFormat all files in the current directory.\nOr install Ruff with\nuv\n(recommended),\npip\n, or\npipx\n:\n#\nWith uv.\nuv tool install ruff@latest\n#\nInstall Ruff globally.\nuv add --dev ruff\n#\nOr add Ruff to your project.\n#\nWith pip.\npip install ruff\n#\nWith pipx.\npipx install ruff\nStarting with version\n0.5.0\n, Ruff can be installed with our standalone installers:\n#\nOn macOS and Linux.\ncurl -LsSf https://astral.sh/ruff/install.sh\n|\nsh\n#\nOn Windows.\npowershell -c\n\"\nirm https://astral.sh/ruff/install.ps1 | iex\n\"\n#\nFor a specific version.\ncurl -LsSf https://astral.sh/ruff/0.14.0/install.sh\n|\nsh\npowershell -c\n\"\nirm https://astral.sh/ruff/0.14.0/install.ps1 | iex\n\"\nYou can also install Ruff via\nHomebrew\n,\nConda\n,\nand with\na variety of other package managers\n.\nUsage\nTo run Ruff as a linter, try any of the following:\nruff check\n#\nLint all files in the current directory (and any subdirectories).\nruff check path/to/code/\n#\nLint all files in `/path/to/code` (and any subdirectories).\nruff check path/to/code/\n*\n.py\n#\nLint all `.py` files in `/path/to/code`.\nruff check path/to/code/to/file.py\n#\nLint `file.py`.\nruff check @arguments.txt\n#\nLint using an input file, treating its contents as newline-delimited command-line arguments.\nOr, to run Ruff as a formatter:\nruff format\n#\nFormat all files in the current directory (and any subdirectories).\nruff format path/to/code/\n#\nFormat all files in `/path/to/code` (and any subdirectories).\nruff format path/to/code/\n*\n.py\n#\nFormat all `.py` files in `/path/to/code`.\nruff format path/to/code/to/file.py\n#\nFormat `file.py`.\nruff format @arguments.txt\n#\nFormat using an input file, treating its contents as newline-delimited command-line arguments.\nRuff can also be used as a\npre-commit\nhook via\nruff-pre-commit\n:\n-\nrepo\n:\nhttps://github.com/astral-sh/ruff-pre-commit\n#\nRuff version.\nrev\n:\nv0.14.0\nhooks\n:\n#\nRun the linter.\n-\nid\n:\nruff-check\nargs\n:\n[ --fix ]\n#\nRun the formatter.\n-\nid\n:\nruff-format\nRuff can also be used as a\nVS Code extension\nor with\nvarious other editors\n.\nRuff can also be used as a\nGitHub Action\nvia\nruff-action\n:\nname\n:\nRuff\non\n:\n[ push, pull_request ]\njobs\n:\nruff\n:\nruns-on\n:\nubuntu-latest\nsteps\n:\n      -\nuses\n:\nactions/checkout@v4\n-\nuses\n:\nastral-sh/ruff-action@v3\nConfiguration\nRuff can be configured through a\npyproject.toml\n,\nruff.toml\n, or\n.ruff.toml\nfile (see:\nConfiguration\n, or\nSettings\nfor a complete list of all configuration options).\nIf left unspecified, Ruff's default configuration is equivalent to the following\nruff.toml\nfile:\n#\nExclude a variety of commonly ignored directories.\nexclude\n= [\n\"\n.bzr\n\"\n,\n\"\n.direnv\n\"\n,\n\"\n.eggs\n\"\n,\n\"\n.git\n\"\n,\n\"\n.git-rewrite\n\"\n,\n\"\n.hg\n\"\n,\n\"\n.ipynb_checkpoints\n\"\n,\n\"\n.mypy_cache\n\"\n,\n\"\n.nox\n\"\n,\n\"\n.pants.d\n\"\n,\n\"\n.pyenv\n\"\n,\n\"\n.pytest_cache\n\"\n,\n\"\n.pytype\n\"\n,\n\"\n.ruff_cache\n\"\n,\n\"\n.svn\n\"\n,\n\"\n.tox\n\"\n,\n\"\n.venv\n\"\n,\n\"\n.vscode\n\"\n,\n\"\n__pypackages__\n\"\n,\n\"\n_build\n\"\n,\n\"\nbuck-out\n\"\n,\n\"\nbuild\n\"\n,\n\"\ndist\n\"\n,\n\"\nnode_modules\n\"\n,\n\"\nsite-packages\n\"\n,\n\"\nvenv\n\"\n,\n]\n#\nSame as Black.\nline-length\n=\n88\nindent-width\n=\n4\n#\nAssume Python 3.9\ntarget-version\n=\n\"\npy39\n\"\n[\nlint\n]\n#\nEnable Pyflakes (`F`) and a subset of the pycodestyle (`E`) codes by default.\nselect\n= [\n\"\nE4\n\"\n,\n\"\nE7\n\"\n,\n\"\nE9\n\"\n,\n\"\nF\n\"\n]\nignore\n= []\n#\nAllow fix for all enabled rules (when `--fix`) is provided.\nfixable\n= [\n\"\nALL\n\"\n]\nunfixable\n= []\n#\nAllow unused variables when underscore-prefixed.\ndummy-variable-rgx\n=\n\"\n^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\n\"\n[\nformat\n]\n#\nLike Black, use double quotes for strings.\nquote-style\n=\n\"\ndouble\n\"\n#\nLike Black, indent with spaces, rather than tabs.\nindent-style\n=\n\"\nspace\n\"\n#\nLike Black, respect magic trailing commas.\nskip-magic-trailing-comma\n=\nfalse\n#\nLike Black, automatically detect the appropriate line ending.\nline-ending\n=\n\"\nauto\n\"\nNote that, in a\npyproject.toml\n, each section header should be prefixed with\ntool.ruff\n. For\nexample,\n[lint]\nshould be replaced with\n[tool.ruff.lint]\n.\nSome configuration options can be provided via dedicated command-line arguments, such as those\nrelated to rule enablement and disablement, file discovery, and logging level:\nruff check --select F401 --select F403 --quiet\nThe remaining configuration options can be provided through a catch-all\n--config\nargument:\nruff check --config\n\"\nlint.per-file-ignores = {'some_file.py' = ['F841']}\n\"\nTo opt in to the latest lint rules, formatter style changes, interface updates, and more, enable\npreview mode\nby setting\npreview = true\nin your configuration\nfile or passing\n--preview\non the command line. Preview mode enables a collection of unstable\nfeatures that may change prior to stabilization.\nSee\nruff help\nfor more on Ruff's top-level commands, or\nruff help check\nand\nruff help format\nfor more on the linting and formatting commands, respectively.\nRules\nRuff supports over 800 lint rules\n, many of which are inspired by popular tools like Flake8,\nisort, pyupgrade, and others. Regardless of the rule's origin, Ruff re-implements every rule in\nRust as a first-party feature.\nBy default, Ruff enables Flake8's\nF\nrules, along with a subset of the\nE\nrules, omitting any\nstylistic rules that overlap with the use of a formatter, like\nruff format\nor\nBlack\n.\nIf you're just getting started with Ruff,\nthe default rule set is a great place to start\n: it\ncatches a wide variety of common errors (like unused imports) with zero configuration.\nBeyond the defaults, Ruff re-implements some of the most popular Flake8 plugins and related code\nquality tools, including:\nautoflake\neradicate\nflake8-2020\nflake8-annotations\nflake8-async\nflake8-bandit\n(\n#1646\n)\nflake8-blind-except\nflake8-boolean-trap\nflake8-bugbear\nflake8-builtins\nflake8-commas\nflake8-comprehensions\nflake8-copyright\nflake8-datetimez\nflake8-debugger\nflake8-django\nflake8-docstrings\nflake8-eradicate\nflake8-errmsg\nflake8-executable\nflake8-future-annotations\nflake8-gettext\nflake8-implicit-str-concat\nflake8-import-conventions\nflake8-logging\nflake8-logging-format\nflake8-no-pep420\nflake8-pie\nflake8-print\nflake8-pyi\nflake8-pytest-style\nflake8-quotes\nflake8-raise\nflake8-return\nflake8-self\nflake8-simplify\nflake8-slots\nflake8-super\nflake8-tidy-imports\nflake8-todos\nflake8-type-checking\nflake8-use-pathlib\nflynt\n(\n#2102\n)\nisort\nmccabe\npandas-vet\npep8-naming\npydocstyle\npygrep-hooks\npylint-airflow\npyupgrade\ntryceratops\nyesqa\nFor a complete enumeration of the supported rules, see\nRules\n.\nContributing\nContributions are welcome and highly appreciated. To get started, check out the\ncontributing guidelines\n.\nYou can also join us on\nDiscord\n.\nSupport\nHaving trouble? Check out the existing issues on\nGitHub\n,\nor feel free to\nopen a new one\n.\nYou can also ask for help on\nDiscord\n.\nAcknowledgements\nRuff's linter draws on both the APIs and implementation details of many other\ntools in the Python ecosystem, especially\nFlake8\n,\nPyflakes\n,\npycodestyle\n,\npydocstyle\n,\npyupgrade\n, and\nisort\n.\nIn some cases, Ruff includes a \"direct\" Rust port of the corresponding tool.\nWe're grateful to the maintainers of these tools for their work, and for all\nthe value they've provided to the Python community.\nRuff's formatter is built on a fork of Rome's\nrome_formatter\n,\nand again draws on both API and implementation details from\nRome\n,\nPrettier\n, and\nBlack\n.\nRuff's import resolver is based on the import resolution algorithm from\nPyright\n.\nRuff is also influenced by a number of tools outside the Python ecosystem, like\nClippy\nand\nESLint\n.\nRuff is the beneficiary of a large number of\ncontributors\n.\nRuff is released under the MIT license.\nWho's Using Ruff?\nRuff is used by a number of major open-source projects and companies, including:\nAlbumentations\nAmazon (\nAWS SAM\n)\nAnki\nAnthropic (\nPython SDK\n)\nApache Airflow\nAstraZeneca (\nMagnus\n)\nBabel\nBenchling (\nRefac\n)\nBokeh\nCapital One (\ndatacompy\n)\nCrowdCent (\nNumerBlox\n)\nCryptography (PyCA)\nCERN (\nIndico\n)\nDVC\nDagger\nDagster\nDatabricks (\nMLflow\n)\nDify\nFastAPI\nGodot\nGradio\nGreat Expectations\nHTTPX\nHatch\nHome Assistant\nHugging Face (\nTransformers\n,\nDatasets\n,\nDiffusers\n)\nIBM (\nQiskit\n)\nING Bank (\npopmon\n,\nprobatus\n)\nIbis\nivy\nJAX\nJupyter\nKraken Tech\nLangChain\nLitestar\nLlamaIndex\nMatrix (\nSynapse\n)\nMegaLinter\nMeltano (\nMeltano CLI\n,\nSinger SDK\n)\nMicrosoft (\nSemantic Kernel\n,\nONNX Runtime\n,\nLightGBM\n)\nModern Treasury (\nPython SDK\n)\nMozilla (\nFirefox\n)\nMypy\nNautobot\nNetflix (\nDispatch\n)\nNeon\nNokia\nNoneBot\nNumPyro\nONNX\nOpenBB\nOpen Wine Components\nPDM\nPaddlePaddle\nPandas\nPillow\nPoetry\nPolars\nPostHog\nPrefect (\nPython SDK\n,\nMarvin\n)\nPyInstaller\nPyMC\nPyMC-Marketing\npytest\nPyTorch\nPydantic\nPylint\nPyVista\nReflex\nRiver\nRippling\nRobyn\nSaleor\nScale AI (\nLaunch SDK\n)\nSciPy\nSnowflake (\nSnowCLI\n)\nSphinx\nStable Baselines3\nStarlette\nStreamlit\nThe Algorithms\nVega-Altair\nWeblate\nWordPress (\nOpenverse\n)\nZenML\nZulip\nbuild (PyPA)\ncibuildwheel (PyPA)\ndelta-rs\nfeaturetools\nmeson-python\nnox\npip\nShow Your Support\nIf you're using Ruff, consider adding the Ruff badge to your project's\nREADME.md\n:\n[\n![\nRuff\n]\n(\nhttps://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json\n)]\n(\nhttps://github.com/astral-sh/ruff\n)\n...or\nREADME.rst\n:\n..\nimage\n::\nhttps://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json\n:target:\nhttps://github.com/astral-sh/ruff\n:alt:\nRuff\n...or, as HTML:\n<\na\nhref\n=\"\nhttps://github.com/astral-sh/ruff\n\"\n>\n<\nimg\nsrc\n=\"\nhttps://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json\n\"\nalt\n=\"\nRuff\n\"\nstyle\n=\"\nmax-width:100%;\n\"\n>\n</\na\n>\nLicense\nThis repository is licensed under the\nMIT License",
        "今日の獲得スター数: 41",
        "累積スター数: 42,956"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/astral-sh/ruff"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/JannisX11/blockbench",
      "title": "JannisX11/blockbench",
      "date": null,
      "executive_summary": [
        "Blockbench - A low poly 3D model editor",
        "---",
        "Blockbench\nBlockbench is a free and open source model editor for low-poly models with pixel art textures.\nModels can be exported into standardized formats, to be shared, rendered, 3D-printed, or used in game engines. There are also multiple dedicated formats for Minecraft Java and Bedrock Edition with format-specific features.\nBlockbench features a modern and beginner friendly interface, but also offers lots of customization and advanced features for experienced 3D artists. Plugins can extend the functionality of the program even further.\nWebsite and download:\nblockbench.net\nContribution\nCheck out the\nContribution Guidelines\n.\nLaunching Blockbench\nTo launch Blockbench from source, you can clone the repository, navigate to the correct branch and launch the program in development mode using the instructions below. If you just want to use the latest version, please download the app from the website.\nInstall\nNodeJS\n.\nThen install all dependencies via\nnpm install\nBundle the code via\nnpm run bundle\nFinally, launch Blockbench using\nnpm run dev\nPlugins\nBlockbench supports Javascript-based plugins. Learn more about creating plugins on\nhttps://www.blockbench.net/wiki/docs/plugin\n.\nLicense\nThe Blockbench source-code is licensed under the GPL license version 3. See\nLICENSE.MD\n.\nModifications to the source code can be made under the terms of that license.\nBlockbench plugins (external scripts) and themes (theme files to customize the design) that interact with the Blockbench API are an exception. Plugins and themes can be created and/or published as open source, proprietary or paid software.\nAll assets created with Blockbench (models, textures, animations, screenshots etc.) are your own!",
        "今日の獲得スター数: 41",
        "累積スター数: 4,413"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/JannisX11/blockbench"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/jgraph/drawio-desktop",
      "title": "jgraph/drawio-desktop",
      "date": null,
      "executive_summary": [
        "Official electron build of draw.io",
        "---",
        "About\ndrawio-desktop\nis a diagramming desktop app based on\nElectron\nthat wraps the\ncore draw.io editor\n.\nDownload built binaries from the\nreleases section\n.\nCan I use this app for free?\nYes, under the apache 2.0 license. If you don't change the code and accept it is provided \"as-is\", you can use it for any purpose.\nSecurity\ndraw.io Desktop is designed to be completely isolated from the Internet, apart from the update process. This checks github.com at startup for a newer version and downloads it from an AWS S3 bucket owned by Github. All JavaScript files are self-contained, the Content Security Policy forbids running remotely loaded JavaScript.\nNo diagram data is ever sent externally, nor do we send any analytics about app usage externally. There is a Content Security Policy in place on the web part of the interface to ensure external transmission cannot happen, even by accident.\nSecurity and isolating the app are the primarily objectives of draw.io desktop. If you ask for anything that involves external connections enabled in the app by default, the answer will be no.\nSupport\nSupport is provided on a reasonable business constraints basis, but without anything contractually binding. All support is provided via this repo. There is no private ticketing support for non-paying users.\nPurchasing draw.io for Confluence or Jira does not entitle you to commercial support for draw.io desktop.\nDeveloping\ndraw.io\nis a git submodule of\ndrawio-desktop\n. To get both you need to clone recursively:\ngit clone --recursive https://github.com/jgraph/drawio-desktop.git\nTo run this:\nnpm install\n(in the root directory of this repo)\n[internal use only] export DRAWIO_ENV=dev if you want to develop/debug in dev mode.\nnpm start\nin the root directory of this repo\nruns the app. For debugging, use\nnpm start --enable-logging\n.\nNote: If a symlink is used to refer to drawio repo (instead of the submodule), then symlink the\nnode_modules\ndirectory inside\ndrawio/src/main/webapp\nalso.\nTo release:\nUpdate the draw.io sub-module and push the change. Add version tag before pushing to origin.\nWait for the builds to complete (\nhttps://travis-ci.org/jgraph/drawio-desktop\nand\nhttps://ci.appveyor.com/project/davidjgraph/drawio-desktop\n)\nGo to\nhttps://github.com/jgraph/drawio-desktop/releases\n, edit the preview release.\nDownload the windows exe and windows portable, sign them using\nsigntool sign /a /tr http://rfc3161timestamp.globalsign.com/advanced /td SHA256 c:/path/to/your/file.exe\nRe-upload signed file as\ndraw.io-windows-installer-x.y.z.exe\nand\ndraw.io-windows-no-installer-x.y.z.exe\nAdd release notes\nPublish release\nNote\n: In Windows release, when using both x64 and is32 as arch, the result is one big file with both archs. This is why we split them.\nLocal Storage and Session Storage is stored in the AppData folder:\nmacOS:\n~/Library/Application Support/draw.io\nWindows:\nC:\\Users\\<USER-NAME>\\AppData\\Roaming\\draw.io\\\nNot open-contribution\ndraw.io is closed to contributions (unless a maintainer permits it, which is extremely rare).\nThe level of complexity of this project means that even simple changes\ncan break a\nlot\nof other moving parts. The amount of testing required\nis far more than it first seems. If we were to receive a PR, we'd have\nto basically throw it away and write it how we want it to be implemented.\nWe are grateful for community involvement, bug reports, & feature requests. We do\nnot wish to come off as anything but welcoming, however, we've\nmade the decision to keep this project closed to contributions for\nthe long term viability of the project.",
        "今日の獲得スター数: 38",
        "累積スター数: 57,185"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/jgraph/drawio-desktop"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/linshenkx/prompt-optimizer",
      "title": "linshenkx/prompt-optimizer",
      "date": null,
      "executive_summary": [
        "一款提示词优化器，助力于编写高质量的提示词",
        "---",
        "Prompt Optimizer (提示词优化器) 🚀\nEnglish\n|\n中文\n在线体验\n|\n快速开始\n|\n常见问题\n|\nChrome插件\n|\n💖赞助支持\n开发文档\n|\nVercel部署指南\n|\nMCP部署使用说明\n|\nDeepWiki文档\n|\nZRead文档\n📖 项目简介\nPrompt Optimizer是一个强大的AI提示词优化工具，帮助你编写更好的AI提示词，提升AI输出质量。支持Web应用、桌面应用、Chrome插件和Docker部署四种使用方式。\n🎥 功能演示\n1. 角色扮演对话：激发小模型潜力\n在追求成本效益的生产或注重隐私的本地化场景中，结构化的提示词能让小模型稳定地进入角色，提供沉浸式、高一致性的角色扮演体验，有效激发其潜力。\n2. 知识图谱提取：保障生产环境的稳定性\n在需要程序化处理的生产环境中，高质量的提示词能显著降低对模型智能程度的要求，使得更经济的小模型也能稳定输出可靠的指定格式。本工具旨在辅助开发者快速达到此目的，从而加速开发、保障稳定，实现降本增效。\n3. 诗歌写作：辅助创意探索与需求定制\n当面对一个强大的AI，我们的目标不只是得到一个“好”答案，而是得到一个“我们想要的”独特答案。本工具能帮助用户将一个模糊的灵感（如“写首诗”）细化为具体的需求（关于什么主题、何种意象、何种情感），辅助您探索、发掘并精确表达自己的创意，与AI共创独一无二的作品。\n✨ 核心特性\n🎯\n智能优化\n：一键优化提示词，支持多轮迭代改进，提升AI回复准确度\n📝\n双模式优化\n：支持系统提示词优化和用户提示词优化，满足不同使用场景\n🔄\n对比测试\n：支持原始提示词和优化后提示词的实时对比，直观展示优化效果\n🤖\n多模型集成\n：支持OpenAI、Gemini、DeepSeek、智谱AI、SiliconFlow等主流AI模型\n🖼️\n图像生成\n：支持文生图（T2I）和图生图（I2I），集成Gemini、Seedream等图像模型\n📊\n高级测试模式\n：上下文变量管理、多轮会话测试、工具调用（Function Calling）支持\n🔒\n安全架构\n：纯客户端处理，数据直接与AI服务商交互，不经过中间服务器\n📱\n多端支持\n：同时提供Web应用、桌面应用、Chrome插件和Docker部署四种使用方式\n🔐\n访问控制\n：支持密码保护功能，保障部署安全\n🧩\nMCP协议支持\n：支持Model Context Protocol (MCP) 协议，可与Claude Desktop等MCP兼容应用集成\n🚀 高级功能\n图像生成模式\n🖼️\n文生图（T2I）\n：通过文本提示词生成图像\n🎨\n图生图（I2I）\n：基于本地图片进行图像变换和优化\n🔌\n多模型支持\n：集成Gemini、Seedream等主流图像生成模型\n⚙️\n模型参数\n：支持各模型特有参数配置（如尺寸、风格等）\n📥\n预览与下载\n：实时预览生成结果，支持下载保存\n高级测试模式\n📊\n上下文变量管理\n：自定义变量、批量替换、变量预览\n💬\n多轮会话测试\n：模拟真实对话场景，测试提示词在多轮交互中的表现\n🛠️\n工具调用支持\n：Function Calling集成，支持OpenAI和Gemini工具调用\n🎯\n灵活调试\n：更强大的提示词测试和调试能力\n详细使用说明请查看\n图像模式文档\n快速开始\n1. 使用在线版本（推荐）\n直接访问：\nhttps://prompt.always200.com\n项目是纯前端项目，所有数据只存储在浏览器本地，不会上传至任何服务器，因此直接使用在线版本也是安全可靠的\n2. Vercel部署\n方式1：一键部署到自己的Vercel(方便，但后续无法自动更新)：\n方式2: Fork项目后在Vercel中导入（推荐，但需参考部署文档进行手动设置）：\n先Fork项目到自己的GitHub\n然后在Vercel中导入该项目\n可跟踪源项目更新，便于同步最新功能和修复\n配置环境变量：\nACCESS_PASSWORD\n：设置访问密码，启用访问限制\nVITE_OPENAI_API_KEY\n等：配置各AI服务商的API密钥\n更多详细的部署步骤和注意事项，请查看：\nVercel部署指南\n3. 下载桌面应用\n从\nGitHub Releases\n下载最新版本。我们为各平台提供\n安装程序\n和\n压缩包\n两种格式。\n安装程序 (推荐)\n: 如\n*.exe\n,\n*.dmg\n,\n*.AppImage\n等。\n强烈推荐使用此方式，因为它支持自动更新\n。\n压缩包\n: 如\n*.zip\n。解压即用，但无法自动更新。\n桌面应用核心优势\n:\n✅\n无跨域限制\n：作为原生桌面应用，它能彻底摆脱浏览器跨域（CORS）问题的困扰。这意味着您可以直接连接任何AI服务提供商的API，包括本地部署的Ollama或有严格安全策略的商业API，获得最完整、最稳定的功能体验。\n✅\n自动更新\n：通过安装程序（如\n.exe\n,\n.dmg\n）安装的版本，能够自动检查并更新到最新版。\n✅\n独立运行\n：无需依赖浏览器，提供更快的响应和更佳的性能。\n4. 安装Chrome插件\n从Chrome商店安装（由于审批较慢，可能不是最新的）：\nChrome商店地址\n点击图标即可打开提示词优化器\n5. Docker部署\n点击查看 Docker 部署命令\n#\n运行容器（默认配置）\ndocker run -d -p 8081:80 --restart unless-stopped --name prompt-optimizer linshen/prompt-optimizer\n#\n运行容器（配置API密钥和访问密码）\ndocker run -d -p 8081:80 \\\n  -e VITE_OPENAI_API_KEY=your_key \\\n  -e ACCESS_USERNAME=your_username\n\\\n#\n可选，默认为\"admin\"\n-e ACCESS_PASSWORD=your_password\n\\\n#\n设置访问密码\n--restart unless-stopped \\\n  --name prompt-optimizer \\\n  linshen/prompt-optimizer\n国内镜像\n: 如果Docker Hub访问较慢，可以将上述命令中的\nlinshen/prompt-optimizer\n替换为\nregistry.cn-guangzhou.aliyuncs.com/prompt-optimizer/prompt-optimizer\n6. Docker Compose部署\n点击查看 Docker Compose 部署步骤\n#\n1. 克隆仓库\ngit clone https://github.com/linshenkx/prompt-optimizer.git\ncd\nprompt-optimizer\n#\n2. 可选：创建.env文件配置API密钥和访问认证\ncp env.local.example .env\n#\n编辑 .env 文件，填入实际的 API 密钥和配置\n#\n3. 启动服务\ndocker compose up -d\n#\n4. 查看日志\ndocker compose logs -f\n#\n5. 访问服务\nWeb 界面：http://localhost:8081\nMCP 服务器：http://localhost:8081/mcp\n你还可以直接编辑docker-compose.yml文件，自定义配置：\n点击查看 docker-compose.yml 示例\nservices\n:\nprompt-optimizer\n:\n#\n使用Docker Hub镜像\nimage\n:\nlinshen/prompt-optimizer:latest\n#\n或使用阿里云镜像（国内用户推荐）\n#\nimage: registry.cn-guangzhou.aliyuncs.com/prompt-optimizer/prompt-optimizer:latest\ncontainer_name\n:\nprompt-optimizer\nrestart\n:\nunless-stopped\nports\n:\n      -\n\"\n8081:80\n\"\n#\nWeb应用端口（包含MCP服务器，通过/mcp路径访问）\nenvironment\n:\n#\nAPI密钥配置\n-\nVITE_OPENAI_API_KEY=your_openai_key\n-\nVITE_GEMINI_API_KEY=your_gemini_key\n#\n访问控制（可选）\n-\nACCESS_USERNAME=admin\n-\nACCESS_PASSWORD=your_password\n7. MCP Server 使用说明\n点击查看 MCP Server 使用说明\nPrompt Optimizer 现在支持 Model Context Protocol (MCP) 协议，可以与 Claude Desktop 等支持 MCP 的 AI 应用集成。\n当通过 Docker 运行时，MCP Server 会自动启动，并可通过\nhttp://ip:port/mcp\n访问。\n环境变量配置\nMCP Server 需要配置 API 密钥才能正常工作。主要的 MCP 专属配置：\n#\nMCP 服务器配置\nMCP_DEFAULT_MODEL_PROVIDER=openai\n#\n可选值：openai, gemini, deepseek, siliconflow, zhipu, custom\nMCP_LOG_LEVEL=info\n#\n日志级别\nDocker 环境下使用 MCP\n在 Docker 环境中，MCP Server 会与 Web 应用一起运行，您可以通过 Web 应用的相同端口访问 MCP 服务，路径为\n/mcp\n。\n例如，如果您将容器的 80 端口映射到主机的 8081 端口：\ndocker run -d -p 8081:80 \\\n  -e VITE_OPENAI_API_KEY=your-openai-key \\\n  -e MCP_DEFAULT_MODEL_PROVIDER=openai \\\n  --name prompt-optimizer \\\n  linshen/prompt-optimizer\n那么 MCP Server 将可以通过\nhttp://localhost:8081/mcp\n访问。\nClaude Desktop 集成示例\n要在 Claude Desktop 中使用 Prompt Optimizer，您需要在 Claude Desktop 的配置文件中添加服务配置。\n找到 Claude Desktop 的配置目录：\nWindows:\n%APPDATA%\\Claude\\services\nmacOS:\n~/Library/Application Support/Claude/services\nLinux:\n~/.config/Claude/services\n编辑或创建\nservices.json\n文件，添加以下内容：\n{\n\"services\"\n: [\n    {\n\"name\"\n:\n\"\nPrompt Optimizer\n\"\n,\n\"url\"\n:\n\"\nhttp://localhost:8081/mcp\n\"\n}\n  ]\n}\n请确保将\nlocalhost:8081\n替换为您实际部署 Prompt Optimizer 的地址和端口。\n可用工具\noptimize-user-prompt\n: 优化用户提示词以提高 LLM 性能\noptimize-system-prompt\n: 优化系统提示词以提高 LLM 性能\niterate-prompt\n: 对已经成熟/完善的提示词进行定向迭代优化\n更多详细信息，请查看\nMCP 服务器用户指南\n。\n⚙️ API密钥配置\n点击查看API密钥配置方法\n方式一：通过界面配置（推荐）\n点击界面右上角的\"⚙️设置\"按钮\n选择\"模型管理\"选项卡\n点击需要配置的模型（如OpenAI、Gemini、DeepSeek等）\n在弹出的配置框中输入对应的API密钥\n点击\"保存\"即可\n支持的模型：OpenAI、Gemini、DeepSeek、Zhipu智谱、SiliconFlow、自定义API（OpenAI兼容接口）\n除了API密钥，您还可以在模型配置界面为每个模型单独设置高级LLM参数。这些参数通过一个名为\nllmParams\n的字段进行配置，它允许您以键值对的形式指定LLM SDK支持的任何参数，从而更精细地控制模型行为。\n高级LLM参数配置示例：\nOpenAI/兼容API\n:\n{\"temperature\": 0.7, \"max_tokens\": 4096, \"timeout\": 60000}\nGemini\n:\n{\"temperature\": 0.8, \"maxOutputTokens\": 2048, \"topP\": 0.95}\nDeepSeek\n:\n{\"temperature\": 0.5, \"top_p\": 0.9, \"frequency_penalty\": 0.1}\n有关\nllmParams\n的更详细说明和配置指南，请参阅\nLLM参数配置指南\n。\n方式二：通过环境变量配置\nDocker部署时通过\n-e\n参数配置环境变量：\n-e VITE_OPENAI_API_KEY=your_key\n-e VITE_GEMINI_API_KEY=your_key\n-e VITE_DEEPSEEK_API_KEY=your_key\n-e VITE_ZHIPU_API_KEY=your_key\n-e VITE_SILICONFLOW_API_KEY=your_key\n#\n多自定义模型配置（支持无限数量）\n-e VITE_CUSTOM_API_KEY_ollama=dummy_key\n-e VITE_CUSTOM_API_BASE_URL_ollama=http://localhost:11434/v1\n-e VITE_CUSTOM_API_MODEL_ollama=qwen2.5:7b\n📖\n详细配置指南\n: 查看\n多自定义模型配置文档\n了解完整的配置方法和高级用法\n本地开发\n详细文档可查看\n开发文档\n点击查看本地开发命令\n#\n1. 克隆项目\ngit clone https://github.com/linshenkx/prompt-optimizer.git\ncd\nprompt-optimizer\n#\n2. 安装依赖\npnpm install\n#\n3. 启动开发服务\npnpm dev\n#\n主开发命令：构建core/ui并运行web应用\npnpm dev:web\n#\n仅运行web应用\npnpm dev:fresh\n#\n完整重置并重新启动开发环境\n🗺️ 开发路线\n基础功能开发\nWeb应用发布\nChrome插件发布\n国际化支持\n支持系统提示词优化和用户提示词优化\n桌面应用发布\nMCP服务发布\n高级模式：变量管理、上下文测试、工具调用\n图像生成：文生图（T2I）和图生图（I2I）支持\n支持工作区/项目管理\n支持提示词收藏和模板管理\n详细的项目状态可查看\n项目状态文档\n📖 相关文档\n文档索引\n- 所有文档的索引\n技术开发指南\n- 技术栈和开发规范\nLLM参数配置指南\n- 高级LLM参数配置详细说明\n项目结构\n- 详细的项目结构说明\n项目状态\n- 当前进度和计划\n产品需求\n- 产品需求文档\nVercel部署指南\n- Vercel部署详细说明\nStar History\n常见问题\n点击查看常见问题解答\nAPI连接问题\nQ1: 为什么配置好API密钥后仍然无法连接到模型服务？\nA\n: 大多数连接失败是由\n跨域问题\n（CORS）导致的。由于本项目是纯前端应用，浏览器出于安全考虑会阻止直接访问不同源的API服务。模型服务如未正确配置CORS策略，会拒绝来自浏览器的直接请求。\nQ2: 如何解决本地Ollama的连接问题？\nA\n: Ollama完全支持OpenAI标准接口，只需配置正确的跨域策略：\n设置环境变量\nOLLAMA_ORIGINS=*\n允许任意来源的请求\n如仍有问题，设置\nOLLAMA_HOST=0.0.0.0:11434\n监听任意IP地址\nQ3: 如何解决商业API（如Nvidia的DS API、字节跳动的火山API）的跨域问题？\nA\n: 这些平台通常有严格的跨域限制，推荐以下解决方案：\n使用桌面版应用\n（最推荐）\n桌面应用作为原生应用，完全没有跨域限制\n可以直接连接任何API服务，包括本地部署的模型\n提供最完整、最稳定的功能体验\n从\nGitHub Releases\n下载\n使用自部署的API中转服务\n（专业方案）\n部署如OneAPI、NewAPI等开源API聚合/代理工具\n在设置中配置为自定义API端点\n请求流向：浏览器→中转服务→模型服务提供商\n完全控制安全策略和访问权限\n注意\n：Web版（包括在线版、Vercel部署、Docker部署）都是纯前端应用，都会受到浏览器CORS限制。只有桌面版或使用API中转服务才能解决跨域问题。\nQ4: 我已正确配置本地模型（如Ollama）的跨域策略，为什么使用在线版依然无法连接？\nA\n: 这是由浏览器的\n混合内容（Mixed Content）安全策略\n导致的。出于安全考虑，浏览器会阻止安全的HTTPS页面（如在线版）向不安全的HTTP地址（如您的本地Ollama服务）发送请求。\n解决方案\n：\n为了绕过此限制，您需要让应用和API处于同一种协议下（例如，都是HTTP）。推荐以下方式：\n使用桌面版\n：桌面应用没有浏览器限制，是连接本地模型最稳定可靠的方式\n使用Docker部署（HTTP）\n：通过\nhttp://localhost:8081\n访问，与本地Ollama都是HTTP\n使用Chrome插件\n：插件在某些情况下也可以绕过部分安全限制\n🤝 参与贡献\n点击查看贡献指南\nFork 本仓库\n创建特性分支 (\ngit checkout -b feature/AmazingFeature\n)\n提交更改 (\ngit commit -m '添加某个特性'\n)\n推送到分支 (\ngit push origin feature/AmazingFeature\n)\n提交 Pull Request\n提示：使用cursor工具开发时，建议在提交前:\n使用\"code_review\"规则进行代码审查\n按照审查报告格式检查:\n变更的整体一致性\n代码质量和实现方式\n测试覆盖情况\n文档完善程度\n根据审查结果进行优化后再提交\n👏 贡献者名单\n感谢所有为项目做出贡献的开发者！\n📄 开源协议\n本项目采用\nMIT\n协议开源。\n如果这个项目对你有帮助，请考虑给它一个 Star ⭐️\n👥 联系我们\n提交 Issue\n发起 Pull Request\n加入讨论组",
        "今日の獲得スター数: 36",
        "累積スター数: 15,876"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/linshenkx/prompt-optimizer"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/keycloak/keycloak",
      "title": "keycloak/keycloak",
      "date": null,
      "executive_summary": [
        "Open Source Identity and Access Management For Modern Applications and Services",
        "---",
        "Open Source Identity and Access Management\nAdd authentication to applications and secure services with minimum effort. No need to deal with storing users or authenticating users.\nKeycloak provides user federation, strong authentication, user management, fine-grained authorization, and more.\nHelp and Documentation\nDocumentation\nUser Mailing List\n- Mailing list for help and general questions about Keycloak\nJoin\n#keycloak\nfor general questions, or\n#keycloak-dev\non Slack for design and development discussions, by creating an account at\nhttps://slack.cncf.io/\n.\nReporting Security Vulnerabilities\nIf you have found a security vulnerability, please look at the\ninstructions on how to properly report it\n.\nReporting an issue\nIf you believe you have discovered a defect in Keycloak, please open\nan issue\n.\nPlease remember to provide a good summary, description as well as steps to reproduce the issue.\nGetting started\nTo run Keycloak, download the distribution from our\nwebsite\n. Unzip and run:\nbin/kc.[sh|bat] start-dev\nAlternatively, you can use the Docker image by running:\ndocker run quay.io/keycloak/keycloak start-dev\nFor more details refer to the\nKeycloak Documentation\n.\nBuilding from Source\nTo build from source, refer to the\nbuilding and working with the code base\nguide.\nTesting\nTo run tests, refer to the\nrunning tests\nguide.\nWriting Tests\nTo write tests, refer to the\nwriting tests\nguide.\nContributing\nBefore contributing to Keycloak, please read our\ncontributing guidelines\n. Participation in the Keycloak project is governed by the\nCNCF Code of Conduct\n.\nJoining a\ncommunity meeting\nis a great way to get involved and help shape the future of Keycloak.\nOther Keycloak Projects\nKeycloak\n- Keycloak Server and Java adapters\nKeycloak QuickStarts\n- QuickStarts for getting started with Keycloak\nKeycloak Node.js Connect\n- Node.js adapter for Keycloak\nLicense\nApache License, Version 2.0",
        "今日の獲得スター数: 34",
        "累積スター数: 30,042"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/keycloak/keycloak"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/qaiu/netdisk-fast-download",
      "title": "qaiu/netdisk-fast-download",
      "date": null,
      "executive_summary": [
        "各类网盘直链解析服务, 已支持蓝奏云/蓝奏优享/小飞机盘/123云盘/移动联通/天翼云等. 支持文件夹分享解析. 体验地址: https://lz.qaiu.top http://www.722shop.top:6401",
        "---",
        "netdisk-fast-download 网盘分享链接云解析服务\nQQ群：1017480890\nnetdisk-fast-download网盘直链云解析(nfd云解析)能把网盘分享下载链接转化为直链，支持多款云盘，已支持蓝奏云/蓝奏云优享/奶牛快传/移动云云空间/小飞机盘/亿方云/123云盘/Cloudreve等，支持加密分享，以及部分网盘文件夹分享。\n快速开始\n命令行下载分享文件：\ncurl -LOJ\n\"\nhttps://lz.qaiu.top/parser?url=https://share.feijipan.com/s/nQOaNRPW&pwd=1234\n\"\n或者使用wget:\nwget -O bilibili.mp4\n\"\nhttps://lz.qaiu.top/parser?url=https://share.feijipan.com/s/nQOaNRPW&pwd=1234\n\"\n或者使用浏览器\n直接访问\n:\n### 调用演示站下载：\nhttps://lz.qaiu.top/parser?url=https://share.feijipan.com/s/nQOaNRPW&pwd=1234  \n### 调用演示站预览：\nhttps://nfd-parser.github.io/nfd-preview/preview.html?src=https%3A%2F%2Flz.qaiu.top%2Fparser%3Furl%3Dhttps%3A%2F%2Fshare.feijipan.com%2Fs%2FnQOaNRPW&name=bilibili.mp4&ext=mp4\n预览地址\n预览地址1\n预览地址2\n天翼云盘大文件解析限时开放\nmain分支依赖JDK17, 提供了JDK11分支\nmain-jdk11\n0.1.8及以上版本json接口格式有调整 参考json返回数据格式示例\n小飞机解析有IP限制，多数云服务商的大陆IP会被拦截（可以自行配置代理），和本程序无关\n注意: 请不要过度依赖lz.qaiu.top预览地址服务，建议本地搭建或者云服务器自行搭建。解析次数过多IP会被部分网盘厂商限制，不推荐做公共解析。\n网盘支持情况:\n20230905 奶牛云直链做了防盗链，需加入请求头：Referer:\nhttps://cowtransfer.com/\n20230824 123云盘解析大文件(>100MB)失效，需要登录\n20230722 UC网盘解析失效，需要登录\n网盘名称-网盘标识:\n蓝奏云-lz\n蓝奏云优享-iz\n奶牛快传-cow\n移动云云空间-ec\n小飞机网盘-fj\n亿方云-fc\n123云盘-ye\n115网盘(失效)-p115\n118网盘(已停服)-p118\n文叔叔-ws\n联想乐云-le\nQQ邮箱云盘-qqw\nQQ闪传-qqsc\n城通网盘-ct\n网易云音乐分享链接-mnes\n酷狗音乐分享链接-mkgs\n酷我音乐分享链接-mkws\nQQ音乐分享链接-mqqs\n咪咕音乐分享链接(开发中)\nCloudreve自建网盘-ce\n微雨云存储-pvvy\n超星云盘(需要referer: https://pan-yz.chaoxing.com)-pcx\nGoogle云盘-pgd\nOnedrive-pod\nDropbox-pdp\niCloud-pic\n仅专属版提供\n移动云盘-p139\n联通云盘-pwo\n天翼云盘-p189\nAPI接口说明\nyour_host指的是您的域名或者IP，实际使用时替换为实际域名或者IP，端口默认6400，可以使用nginx代理来做域名访问。\n解析方式分为两种类型直接跳转下载文件和获取下载链接,\n每一种都提供了两种接口形式:\n通用接口parser?url=\n和\n网盘标志/分享key拼接的短地址（标志短链）\n，所有规则参考示例。\n通用接口:\n/parser?url=分享链接&pwd=密码\n没有分享密码去掉&pwd参数;\n标志短链:\n/d/网盘标识/分享key@密码\n没有分享密码去掉@密码;\n直链JSON:\n/json/网盘标识/分享key@密码\n和\n/json/parser?url=分享链接&pwd=密码\n网盘标识参考上面网盘支持情况\n当带有分享密码时需要加上密码参数(pwd)\n移动云云空间,小飞机网盘的加密分享的密码可以忽略\n移动云空间分享key取分享链接中的data参数,比如\n&data=xxx\n的参数就是xxx\nAPI规则:\n建议使用UrlEncode编码分享链接\n解析并自动302跳转\nhttp://your_host/parser?url=分享链接&pwd=xxx\nhttp://your_host/parser?url=UrlEncode(分享链接)&pwd=xxx\nhttp://your_host/d/网盘标识/分享key@分享密码\n获取解析后的直链--JSON格式\nhttp://your_host/json/parser?url=分享链接&pwd=xxx\nhttp://your_host/json/网盘标识/分享key@分享密码\n文件夹解析v0.1.8fixed3新增\nhttp://your_host/json/getFileList?url=分享链接&pwd=xxx\njson接口说明\n1. 文件解析：/json/parser?url=分享链接&pwd=xxx\njson返回数据格式示例:\nshareKey\n:    全局分享key\ndirectLink\n:  下载链接\ncacheHit\n:    是否为缓存链接\nexpires\n:     缓存到期时间\n{\n\"code\"\n:\n200\n,\n\"msg\"\n:\n\"\nsuccess\n\"\n,\n\"success\"\n:\ntrue\n,\n\"count\"\n:\n0\n,\n\"data\"\n: {\n\"shareKey\"\n:\n\"\nlz:xxx\n\"\n,\n\"directLink\"\n:\n\"\n下载直链\n\"\n,\n\"cacheHit\"\n:\ntrue\n,\n\"expires\"\n:\n\"\n2024-09-18 01:48:02\n\"\n,\n\"expiration\"\n:\n1726638482825\n},\n\"timestamp\"\n:\n1726637151902\n}\n2. 分享链接详情接口 /v2/linkInfo?url=分享链接\n{\n\"code\"\n:\n200\n,\n\"msg\"\n:\n\"\nsuccess\n\"\n,\n\"success\"\n:\ntrue\n,\n\"count\"\n:\n0\n,\n\"data\"\n: {\n\"downLink\"\n:\n\"\nhttps://lz.qaiu.top/d/fj/xx\n\"\n,\n\"apiLink\"\n:\n\"\nhttps://lz.qaiu.top/json/fj/xx\n\"\n,\n\"cacheHitTotal\"\n:\n5\n,\n\"parserTotal\"\n:\n2\n,\n\"sumTotal\"\n:\n7\n,\n\"shareLinkInfo\"\n: {\n\"shareKey\"\n:\n\"\nxx\n\"\n,\n\"panName\"\n:\n\"\n小飞机网盘\n\"\n,\n\"type\"\n:\n\"\nfj\n\"\n,\n\"sharePassword\"\n:\n\"\n\"\n,\n\"shareUrl\"\n:\n\"\nhttps://share.feijipan.com/s/xx\n\"\n,\n\"standardUrl\"\n:\n\"\nhttps://www.feijix.com/s/xx\n\"\n,\n\"otherParam\"\n: {\n\"UA\"\n:\n\"\nMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\n\"\n},\n\"cacheKey\"\n:\n\"\nfj:xx\n\"\n}\n    },\n\"timestamp\"\n:\n1736489219402\n}\n3. 文件夹解析(仅支持蓝奏云/蓝奏优享/小飞机网盘)\n/v2/getFileList?url=分享链接&pwd=分享密码\n{\n\"code\"\n:\n200\n,\n\"msg\"\n:\n\"\nsuccess\n\"\n,\n\"success\"\n:\ntrue\n,\n\"data\"\n: [\n    {\n\"fileName\"\n:\n\"\nxxx\n\"\n,\n\"fileId\"\n:\n\"\nxxx\n\"\n,\n\"fileIcon\"\n:\nnull\n,\n\"size\"\n:\n999\n,\n\"sizeStr\"\n:\n\"\n999 M\n\"\n,\n\"fileType\"\n:\n\"\nfile/folder\n\"\n,\n\"filePath\"\n:\nnull\n,\n\"createTime\"\n:\n\"\n17 小时前\n\"\n,\n\"updateTime\"\n:\nnull\n,\n\"createBy\"\n:\nnull\n,\n\"description\"\n:\nnull\n,\n\"downloadCount\"\n:\n\"\n下载次数\n\"\n,\n\"panType\"\n:\n\"\nlz\n\"\n,\n\"parserUrl\"\n:\n\"\n下载链接/文件夹链接\n\"\n,\n\"extParameters\"\n:\nnull\n}\n  ]\n}\n4. 解析次数统计接口 /v2/statisticsInfo\n{\n\"code\"\n:\n200\n,\n\"msg\"\n:\n\"\nsuccess\n\"\n,\n\"success\"\n:\ntrue\n,\n\"count\"\n:\n0\n,\n\"data\"\n: {\n\"parserTotal\"\n:\n320508\n,\n\"cacheTotal\"\n:\n5957910\n,\n\"total\"\n:\n6278418\n},\n\"timestamp\"\n:\n1736489378770\n}\nIDEA HttpClient示例:\n# 解析并重定向到直链\n### 蓝奏云普通分享\n# @no-redirect\nGET http://127.0.0.1:6400/parser?url=https://lanzoux.com/ia2cntg\n### 奶牛快传普通分享\n# @no-redirect\nGET http://127.0.0.1:6400/parser?url=https://cowtransfer.com/s/9a644fe3e3a748\n### 360亿方云加密分享\n# @no-redirect\nGET http://127.0.0.1:6400/parser?url=https://v2.fangcloud.com/sharing/e5079007dc31226096628870c7&pwd=QAIU\n\n# Rest请求自动302跳转(只提供共享文件Id):\n### 蓝奏云普通分享\n# @no-redirect\nGET http://127.0.0.1:6400/lz/ia2cntg\n### 奶牛快传普通分享\n# @no-redirect\nGET http://127.0.0.1:6400/cow/9a644fe3e3a748\n### 360亿方云加密分享\nGET http://127.0.0.1:6400/json/fc/e5079007dc31226096628870c7@QAIU\n\n\n# 解析返回json直链\n### 蓝奏云普通分享\nGET http://127.0.0.1:6400/json/lz/ia2cntg\n### 奶牛快传普通分享\nGET http://127.0.0.1:6400/json/cow/9a644fe3e3a748\n### 360亿方云加密分享\nGET http://127.0.0.1:6400/json/fc/e5079007dc31226096628870c7@QAIU\n网盘对比\n网盘名称\n免登陆下载分享\n加密分享\n初始网盘空间\n单文件大小限制\n蓝奏云\n√\n√\n不限空间\n100M\n奶牛快传\n√\nX\n10G\n不限大小\n移动云云空间(个人版)\n√\n√(密码可忽略)\n5G(个人)\n不限大小\n小飞机网盘\n√\n√(密码可忽略)\n10G\n不限大小\n360亿方云\n√\n√(密码可忽略)\n100G(须实名)\n不限大小\n123云盘\n√\n√\n2T\n100G（>100M需要登录）\n文叔叔\n√\n√\n10G\n5GB\n夸克网盘\nx\n√\n10G\n不限大小\nUC网盘\nx\n√\n10G\n不限大小\n打包部署\nJDK下载（lz.qaiu.top提供直链云解析服务）\n阿里jdk17(Dragonwell17-windows-x86)\n阿里jdk17(Dragonwell17-linux-x86)\n阿里jdk17(Dragonwell17-linux-aarch64)\n解析有效性测试-移动云云空间-阿里jdk17-linux-x86\n开发和打包\n#\n环境要求: Jdk17 + maven;\nmvn clean\nmvn package\n打包好的文件位于 web-service/target/netdisk-fast-download-bin.zip\nLinux服务部署\nDocker 部署（Main分支）\n海外服务器Docker部署\n#\n创建目录\nmkdir -p netdisk-fast-download\ncd\nnetdisk-fast-download\n#\n拉取镜像\ndocker pull ghcr.io/qaiu/netdisk-fast-download:latest\n#\n复制配置文件（或下载仓库web-service\\src\\main\\resources）\ndocker create --name netdisk-fast-download ghcr.io/qaiu/netdisk-fast-download:latest\ndocker cp netdisk-fast-download:/app/resources ./resources\ndocker rm netdisk-fast-download\n#\n启动容器\ndocker run -d -it --name netdisk-fast-download -p 6401:6401 --restart unless-stopped -e TZ=Asia/Shanghai -v ./resources:/app/resources -v ./db:/app/db -v ./logs:/app/logs ghcr.io/qaiu/netdisk-fast-download:latest\n#\n反代6401端口\n#\n升级容器\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --cleanup --run-once netdisk-fast-download\n国内Docker部署\n#\n创建目录\nmkdir -p netdisk-fast-download\ncd\nnetdisk-fast-download\n#\n拉取镜像\ndocker pull ghcr.nju.edu.cn/qaiu/netdisk-fast-download:latest\n#\n复制配置文件（或下载仓库web-service\\src\\main\\resources）\ndocker create --name netdisk-fast-download ghcr.nju.edu.cn/qaiu/netdisk-fast-download:latest\ndocker cp netdisk-fast-download:/app/resources ./resources\ndocker rm netdisk-fast-download\n#\n启动容器\ndocker run -d -it --name netdisk-fast-download -p 6401:6401 --restart unless-stopped -e TZ=Asia/Shanghai -v ./resources:/app/resources -v ./db:/app/db -v ./logs:/app/logs ghcr.nju.edu.cn/qaiu/netdisk-fast-download:latest\n#\n反代6401端口\n#\n升级容器\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --cleanup --run-once netdisk-fast-download\n宝塔部署指引 ->\n点击进入宝塔部署教程\nLinux命令行部署\n注意: netdisk-fast-download.service中的ExecStart的路径改为实际路径\ncd\n~\nwget -O netdisk-fast-download.zip https://github.com/qaiu/netdisk-fast-download/releases/download/v0.1.9b7/netdisk-fast-download-bin.zip\nunzip netdisk-fast-download-bin.zip\ncd\nnetdisk-fast-download\nbash service-install.sh\n服务相关命令:\n查看服务状态\nsystemctl status netdisk-fast-download.service\n启动服务\nsystemctl start netdisk-fast-download.service\n重启服务\nsystemctl restart netdisk-fast-download.service\n停止服务\nsystemctl stop netdisk-fast-download.service\n开机启动服务\nsystemctl enable netdisk-fast-download.service\n停止开机启动\nsystemctl disable netdisk-fast-download.service\nWindows服务部署\n下载并解压releases版本netdisk-fast-download-bin.zip\n进入netdisk-fast-download下的bin目录\n使用管理员权限运行nfd-service-install.bat\n如果不想使用服务运行可以直接运行run.bat\n注意: 如果jdk环境变量的java版本不是17请修改nfd-service-template.xml中的java命令的路径改为实际路径\n相关配置说明\nresources目录下包含服务端配置文件 配置文件自带说明，具体请查看配置文件内容，\napp-dev.yml 可以配置解析服务相关信息， 包括端口，域名，缓存时长等\nserver-proxy.yml 可以配置代理服务运行的相关信息， 包括前端反向代理端口，路径等\nip代理配置说明\n有时候解析量很大，IP容易被ban，这时候可以使用其他服务器搭建nfd-proxy代理服务。\n修改配置文件：\napp-dev.yml\nproxy\n:\n  -\npanTypes\n:\npgd,pdb,pod\n#\n网盘标识\ntype\n:\nhttp\n#\n支持http/socks4/socks5\nhost\n:\n127.0.0.1\n#\n代理IP\nport\n:\n7890\n#\n端口\nusername\n:\n#\n用户名\npassword\n:\n#\n密码\nnfd-proxy搭建http代理服务器\n参考\nhttps://github.com/nfd-parser/nfd-proxy\n0.1.9 开发计划\n目录解析(专属版)\n带cookie/token参数解析大文件(专属版)\n技术栈:\nJdk17+Vert.x4\nCore模块集成Vert.x实现类似spring的注解式路由API\nStar History\n免责声明\n用户在使用本项目时，应自行承担风险，并确保其行为符合当地法律法规及网盘服务提供商的使用条款。\n开发者不对用户因使用本项目而导致的任何后果负责，包括但不限于数据丢失、隐私泄露、账号封禁或其他任何形式的损害。\n支持该项目\n开源不易，用爱发电，本项目长期维护如果觉得有帮助, 可以请作者喝杯咖啡, 感谢支持\n关于专属版\n99元, 提供对小飞机,蓝奏优享大文件解析的支持, 提供天翼云盘,移动云盘,联通云盘的解析支持\n199元, 包含部署服务和首页定制, 需提供宝塔环境\n可以提供功能定制开发, 加v价格详谈:\nqq: 197575894\nwechat: imcoding_",
        "今日の獲得スター数: 32",
        "累積スター数: 2,248"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/qaiu/netdisk-fast-download"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/helix-editor/helix",
      "title": "helix-editor/helix",
      "date": null,
      "executive_summary": [
        "A post-modern modal text editor.",
        "---",
        "A\nKakoune\n/\nNeovim\ninspired editor, written in Rust.\nThe editing model is very heavily based on Kakoune; during development I found\nmyself agreeing with most of Kakoune's design decisions.\nFor more information, see the\nwebsite\nor\ndocumentation\n.\nAll shortcuts/keymaps can be found\nin the documentation on the website\n.\nTroubleshooting\nFeatures\nVim-like modal editing\nMultiple selections\nBuilt-in language server support\nSmart, incremental syntax highlighting and code editing via tree-sitter\nAlthough it's primarily a terminal-based editor, I am interested in exploring\na custom renderer (similar to Emacs) using wgpu or skulpin.\nNote: Only certain languages have indentation definitions at the moment. Check\nruntime/queries/<lang>/\nfor\nindents.scm\n.\nInstallation\nInstallation documentation\n.\nContributing\nContributing guidelines can be found\nhere\n.\nGetting help\nYour question might already be answered on the\nFAQ\n.\nDiscuss the project on the community\nMatrix Space\n(make sure to join\n#helix-editor:matrix.org\nif you're on a client that doesn't support Matrix Spaces yet).\nCredits\nThanks to\n@jakenvac\nfor designing the logo!",
        "今日の獲得スター数: 31",
        "累積スター数: 40,220"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/helix-editor/helix"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    },
    {
      "url": "https://github.com/Ranchero-Software/NetNewsWire",
      "title": "Ranchero-Software/NetNewsWire",
      "date": null,
      "executive_summary": [
        "RSS reader for macOS and iOS.",
        "---",
        "NetNewsWire\nNetNewsWire is a free and open-source feed reader for macOS and iOS.\nIt supports\nRSS\n,\nAtom\n,\nJSON Feed\n, and\nRSS-in-JSON\nformats.\nMore info:\nhttps://netnewswire.com/\nYou can\nreport bugs and make feature requests\nhere on GitHub. You can also\nread change notes\nfor current and previous releases.\nHere’s\nHow to Support NetNewsWire\n. Spoiler: don’t send money. :)\n(NetNewsWire’s Help menu has these links, so you don’t have to remember to come back to this page.)\nCommunity\nJoin the Slack group\nto talk with other NetNewsWire users — and to help out, if you’d like to, by testing, coding, writing, providing feedback, or just helping us think things through. Everybody is welcome and encouraged to join.\nEvery community member is expected to abide by the\ncode of conduct\nwhich is included in the\nContributing\npage.\nPull Requests\nSee the\nContributing\npage for our process. It’s pretty straightforward.\nBuilding\nYou can build and test NetNewsWire without a paid developer account.\ngit clone https://github.com/Ranchero-Software/NetNewsWire.git\nYou can locally override the Xcode settings for code signing\nby creating a\nDeveloperSettings.xcconfig\nfile locally at the appropriate path.\nThis allows for a pristine project with code signing set up with the appropriate\ndeveloper ID and certificates, and for developer to be able to have local settings\nwithout needing to check in anything into source control.\nYou can do this in one of two ways: using the included\nsetup.sh\nscript or by creating the folder structure and file manually.\nUsing\nsetup.sh\nOpen Terminal and\ncd\ninto the NetNewsWire directory.\nRun this command to ensure you have execution rights for the script:\nchmod +x setup.sh\nExecute the script with the following command:\n./setup.sh\nand complete the answers.\nManually\nMake a directory\nSharedXcodeSettings\nnext to where you have this repository.\nThe directory structure is:\ndirectory/\n  SharedXcodeSettings/\n    DeveloperSettings.xcconfig\n  NetNewsWire/\n    NetNewsWire.xcodeproj\nExample:\nIf your NetNewsWire Xcode project file is at:\n/Users/name/projects/NetNewsWire/NetNewsWire.xcodeproj\nCreate your\nDeveloperSettings.xcconfig\nfile at\n/Users/name/projects/SharedXcodeSettings/DeveloperSettings.xcconfig\nThen create a plain text file in it:\nSharedXcodeSettings/DeveloperSettings.xcconfig\nand\ngive it the contents:\nCODE_SIGN_IDENTITY = Mac Developer\nDEVELOPMENT_TEAM = <Your Team ID>\nCODE_SIGN_STYLE = Automatic\nORGANIZATION_IDENTIFIER = <Your Domain Name Reversed>\nDEVELOPER_ENTITLEMENTS = -dev\nPROVISIONING_PROFILE_SPECIFIER =\nSet\nDEVELOPMENT_TEAM\nto your Apple supplied development team.  You can use Keychain\nAccess to\nfind your development team ID\n.\nSet\nORGANIZATION_IDENTIFIER\nto a reversed domain name that you control or have made up.\nNote that\nPROVISIONING_PROFILE_SPECIFIER\nshould not have a value associated with it.\nYou can now open the\nNetNewsWire.xccodeproj\nin Xcode.\nNow you should be able to build without code signing errors and without modifying\nthe NetNewsWire Xcode project.  This is a special build of NetNewsWire with some\nfunctionality disabled.  This is because we have API keys that can't be stored in the\nrepository or shared between developers.  Certain account types, like iCloud and Feedly, aren't\nenabled and the Reader View isn't enabled because of this.\nIf you have any problems, we will help you out in Slack (\nsee above\n).",
        "今日の獲得スター数: 30",
        "累積スター数: 9,152"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/Ranchero-Software/NetNewsWire"
      ],
      "retrieved_at": "2025-10-10T02:09:56Z"
    }
  ]
}