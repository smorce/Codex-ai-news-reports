# AI News Report (github-trending)

- Generated at: 2025-10-11T02:07:31Z
- Articles: 47

## TibixDev/winboat

### Executive Summary
- Run Windows apps on ğŸ§ Linux with âœ¨ seamless integration
- ---
- WinBoat
Windows for Penguins.
Run Windows apps on ğŸ§ Linux with âœ¨ seamless integration
Screenshots
âš ï¸
Work in Progress
âš ï¸
WinBoat is currently in beta, so expect to occasionally run into hiccups and bugs. You should be comfortable with some level of troubleshooting if you decide to try it, however we encourage you to give it a shot anyway.
Features
ğŸ¨ Elegant Interface
: Sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience
ğŸ“¦ Automated Installs
: Simple installation process through our interface - pick your preferences & specs and let us handle the rest
ğŸš€ Run Any App
: If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications as native OS-level windows in your Linux environment
ğŸ–¥ï¸ Full Windows Desktop
: Access the complete Windows desktop experience when you need it, or run individual apps seamlessly integrated into your Linux workflow
ğŸ“ Filesystem Integration
: Your home directory is mounted in Windows, allowing easy file sharing between the two systems without any hassle
âœ¨ And many more
: Smartcard passthrough, resource monitoring, and more features being added regularly
How Does It Work?
WinBoat is an Electron app which allows you to run Windows apps on Linux using a containerized approach. Windows runs as a VM inside a Docker container, we communicate with it using the
WinBoat Guest Server
to retrieve data we need from Windows. For compositing applications as native OS-level windows, we use FreeRDP together with Windows's RemoteApp protocol.
Prerequisites
Before running WinBoat, ensure your system meets the following requirements:
RAM
: At least 4 GB of RAM
CPU
: At least 2 CPU threads
Storage
: At least 32 GB free space in
/var
Virtualization
: KVM enabled in BIOS/UEFI
How to enable virtualization
Docker
: Required for containerization
Installation Guide
âš ï¸
NOTE:
Docker Desktop is
not
supported, you will run into issues if you use it
Docker Compose v2
: Required for compatibility with docker-compose.yml files
Installation Guide
Docker User Group
: Add your user to the
docker
group
Setup Instructions
FreeRDP
: Required for remote desktop connection (Please make sure you have
Version 3.x.x
with sound support included)
Installation Guide
[OPTIONAL]
Kernel Modules
: The
iptables
/
nftables
and
iptable_nat
kernel modules can be loaded for network autodiscovery and better shared filesystem performance, but this is not obligatory in newer versions of WinBoat
Module loading instructions
Downloading
You can download the latest Linux builds under the
Releases
tab. We currently offer four variants:
AppImage:
A popular & portable app format which should run fine on most distributions
Unpacked:
The raw unpacked files, simply run the executable (
linux-unpacked/winboat
)
.deb:
The intended format for Debian based distributions
.rpm:
The intended format for Fedora based distributions
Known Issues About Container Runtimes
Podman is
unsupported
for now
Docker Desktop is
unsupported
for now
Distros that emulate Docker through a Podman socket are
unsupported
Any rootless containerization solution is currently
unsupported
Building WinBoat
For building you need to have NodeJS and Go installed on your system
Clone the repo (
git clone https://github.com/TibixDev/WinBoat
)
Install the dependencies (
npm i
)
Build the app and the guest server using
npm run build:linux-gs
You can now find the built app under
dist
with an AppImage and an Unpacked variant
Running WinBoat in development mode
Make sure you meet the
prerequisites
Additionally, for development you need to have NodeJS and Go installed on your system
Clone the repo (
git clone https://github.com/TibixDev/WinBoat
)
Install the dependencies (
npm i
)
Build the guest server (
npm run build-guest-server
)
Run the app (
npm run dev
)
Contributing
Contributions are welcome! Whether it's bug fixes, feature improvements, or documentation updates, we appreciate your help making WinBoat better.
Please note
: We maintain a focus on technical contributions only. Pull requests containing political/sexual content, or other sensitive/controversial topics will not be accepted. Let's keep things focused on making great software! ğŸš€
Feel free to:
Report bugs and issues
Submit feature requests
Contribute code improvements
Help with documentation
Share feedback and suggestions
Check out our issues page to get started, or feel free to open a new issue if you've found something that needs attention.
License
WinBoat is licensed under the
MIT
license
Inspiration / Alternatives
These past few years some cool projects have surfaced with similar concepts, some of which we've also taken inspirations from.
They're awesome and you should check them out:
WinApps
Cassowary
dockur/windows
(ğŸŒŸ Also used in WinBoat)
Socials & Contact
Star History
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 1,263
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 9,280

### References
- https://github.com/TibixDev/winboat

## Stremio/stremio-web

### Executive Summary
- Stremio - Freedom to Stream
- ---
- Stremio - Freedom to Stream
Stremio is a modern media center that's a one-stop solution for your video entertainment. You discover, watch and organize video content from easy to install addons.
Build
Prerequisites
Node.js 12 or higher
pnpm
10 or higher
Install dependencies
pnpm install
Start development server
pnpm start
Production build
pnpm run build
Run with Docker
docker build -t stremio-web
.
docker run -p 8080:8080 stremio-web
Screenshots
Board
Discover
Meta Details
License
Stremio is copyright 2017-2023 Smart code and available under GPLv2 license. See the
LICENSE
file in the project for more information.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 956
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 7,250

### References
- https://github.com/Stremio/stremio-web

## TapXWorld/ChinaTextbook

### Executive Summary
- æ‰€æœ‰å°åˆé«˜ã€å¤§å­¦PDFæ•™æã€‚
- ---
- é¡¹ç›®çš„ç”±æ¥
è™½ç„¶å›½å†…æ•™è‚²ç½‘ç«™å·²æä¾›å…è´¹èµ„æºï¼Œä½†å¤§å¤šæ•°æ™®é€šäººè·å–ä¿¡æ¯çš„é€”å¾„ä¾ç„¶å—é™ã€‚æœ‰äº›äººåˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œåœ¨æŸç«™ä¸Šé”€å”®è¿™äº›å¸¦æœ‰ç§äººæ°´å°çš„èµ„æºã€‚ä¸ºäº†åº”å¯¹è¿™ç§æƒ…å†µï¼Œæˆ‘è®¡åˆ’å°†è¿™äº›èµ„æºé›†ä¸­å¹¶å¼€æºï¼Œä»¥ä¿ƒè¿›ä¹‰åŠ¡æ•™è‚²çš„æ™®åŠå’Œæ¶ˆé™¤åœ°åŒºé—´çš„æ•™è‚²è´«å›°ã€‚
è¿˜æœ‰ä¸€ä¸ªæœ€é‡è¦çš„åŸå› æ˜¯ï¼Œå¸Œæœ›æµ·å¤–åäººèƒ½å¤Ÿè®©è‡ªå·±çš„å­©å­ç»§ç»­äº†è§£å›½å†…æ•™è‚²ã€‚
å­¦ä¹ æ•°å­¦
å¸Œæœ›æœªæ¥å‡ºç°æ›´å¤šä¸æ˜¯ä¸ºäº†è€ƒå­¦è€Œè¯»ä¹¦çš„äººã€‚
å°å­¦æ•°å­¦
ä¸€å¹´çº§ä¸Šå†Œ
ä¸€å¹´çº§ä¸‹å†Œ
äºŒå¹´çº§ä¸Šå†Œ
äºŒå¹´çº§ä¸‹å†Œ
ä¸‰å¹´çº§ä¸Šå†Œ
ä¸‰å¹´çº§ä¸‹å†Œ
å››å¹´çº§ä¸Šå†Œ
å››å¹´çº§ä¸‹å†Œ
äº”å¹´çº§ä¸Šå†Œ
äº”å¹´çº§ä¸‹å†Œ
å…­å¹´çº§ä¸Šå†Œ
å…­å¹´çº§ä¸‹å†Œ
åˆä¸­æ•°å­¦
åˆä¸€ä¸Šå†Œ
åˆä¸€ä¸‹å†Œ
åˆäºŒä¸Šå†Œ
åˆäºŒä¸‹å†Œ
åˆä¸‰ä¸Šå†Œ
åˆä¸‰ä¸‹å†Œ
é«˜ä¸­æ•°å­¦
ç›®å½•
å¤§å­¦æ•°å­¦
é«˜ç­‰æ•°å­¦
çº¿æ€§ä»£æ•°
ç¦»æ•£æ•°å­¦
æ¦‚ç‡è®º
æ›´å¤šæ•°å­¦èµ„æ–™-(å¤§å­¦æ•°å­¦ç½‘)
é—®é¢˜ï¼šå¦‚ä½•åˆå¹¶è¢«æ‹†åˆ†çš„æ–‡ä»¶ï¼Ÿ
ç”±äº GitHub å¯¹å•ä¸ªæ–‡ä»¶çš„ä¸Šä¼ æœ‰æœ€å¤§é™åˆ¶ï¼Œè¶…è¿‡ 100MB çš„æ–‡ä»¶ä¼šè¢«æ‹’ç»ä¸Šä¼ ï¼Œè¶…è¿‡ 50MB çš„æ–‡ä»¶ä¸Šä¼ æ—¶ä¼šæ”¶åˆ°è­¦å‘Šã€‚å› æ­¤ï¼Œæ–‡ä»¶å¤§å°è¶…è¿‡ 50MB çš„æ–‡ä»¶ä¼šè¢«æ‹†åˆ†æˆæ¯ä¸ª 35MB çš„å¤šä¸ªæ–‡ä»¶ã€‚
ç¤ºä¾‹
æ–‡ä»¶è¢«æ‹†åˆ†çš„ç¤ºä¾‹ï¼š
ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ Â· æ•°å­¦ä¸€å¹´çº§ä¸Šå†Œ.pdf.1
ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ Â· æ•°å­¦ä¸€å¹´çº§ä¸Šå†Œ.pdf.2
è§£å†³åŠæ³•
è¦åˆå¹¶è¿™äº›è¢«æ‹†åˆ†çš„æ–‡ä»¶ï¼Œæ‚¨åªéœ€æ‰§è¡Œä»¥ä¸‹æ­¥éª¤(å…¶ä»–æ“ä½œç³»ç»ŸåŒç†)ï¼š
å°†åˆå¹¶ç¨‹åº
mergePDFs-windows-amd64.exe
ä¸‹è½½åˆ°åŒ…å« PDF æ–‡ä»¶çš„æ–‡ä»¶å¤¹ä¸­ã€‚
ç¡®ä¿
mergePDFs-windows-amd64.exe
å’Œè¢«æ‹†åˆ†çš„ PDF æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚
åŒå‡»
mergePDFs-windows-amd64.exe
ç¨‹åºå³å¯è‡ªåŠ¨å®Œæˆæ–‡ä»¶åˆå¹¶ã€‚
ä¸‹è½½æ–¹å¼
æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹é“¾æ¥ï¼Œä¸‹è½½æ–‡ä»¶åˆå¹¶ç¨‹åºï¼š
ä¸‹è½½æ–‡ä»¶åˆå¹¶ç¨‹åº
æ–‡ä»¶å’Œç¨‹åºç¤ºä¾‹
mergePDFs-windows-amd64.exe
ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ Â· æ•°å­¦ä¸€å¹´çº§ä¸Šå†Œ.pdf.1
ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ Â· æ•°å­¦ä¸€å¹´çº§ä¸Šå†Œ.pdf.2
é‡æ–°ä¸‹è½½
å¦‚æœæ‚¨ä½äºå†…åœ°ï¼Œå¹¶ä¸”ç½‘ç»œä¸é”™ï¼Œæƒ³é‡æ–°ä¸‹è½½ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨
tchMaterial-parser
é¡¹ç›®ï¼ˆé¼“åŠ±å¼€æºï¼‰ï¼Œè¿›è¡Œé‡æ–°ä¸‹è½½ã€‚
å¦‚æœæ‚¨ä½äºå›½å¤–ï¼Œå’Œå†…åœ°ç½‘ç»œé€šä¿¡é€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨æœ¬å­˜å‚¨åº“è¿›è¡Œç­¾å‡ºã€‚
æ•™ææçŒ®
å¦‚æœè¿™ä¸ªé¡¹ç›®å¸®åŠ©æ‚¨å…è´¹è·å–æ•™è‚²èµ„æºï¼Œè¯·è€ƒè™‘æ”¯æŒæˆ‘ä»¬æ¨å¹¿å¼€æ”¾æ•™è‚²çš„åŠªåŠ›ï¼æ‚¨çš„æçŒ®å°†å¸®åŠ©æˆ‘ä»¬ç»´æŠ¤å’Œæ‰©å±•è¿™ä¸ªèµ„æºåº“ã€‚
åŠ å…¥æˆ‘ä»¬çš„ Telegram ç¤¾åŒºï¼Œè·å–æœ€æ–°åŠ¨æ€å¹¶åˆ†äº«æ‚¨çš„æƒ³æ³•ï¼š
https://t.me/+1V6WjEq8WEM4MDM1
æ”¯æŒæˆ‘
å¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œæ‚¨å¯ä»¥æ‰«æä»¥ä¸‹äºŒç»´ç è¿›è¡Œæèµ ï¼š
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 441
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 52,599

### References
- https://github.com/TapXWorld/ChinaTextbook

## timelinize/timelinize

### Executive Summary
- Store your data from all your accounts and devices in a single cohesive timeline on your own computer
- ---
- Organize your photos & videos, chats & messages, location history, social media content, contacts, and more into a single cohesive timeline on your own computer where you can keep them alive forever.
Timelinize lets you import your data from practically anywhere: your computer, phone, online accounts, GPS-enabled radios, various apps and programs, contact lists, cameras, and more.
Join our Discord
to discuss!
Note
I am looking for a better name for this project. If you have an idea for a good name that is short, relevant, unique, and available,
I'd love to hear it!
Screenshots
These were captured using a dev repository of mine filled with a subset of my real data, so I've run Timelinize in obfuscation mode: images and videos are blurred (except profile pictures---need to fix that); names, identifiers, and locations around sensitive areas are all randomized, and text has been replaced with random words so that the string is about the same length.
(I hope to make a video tour soon.)
Please remember this is an early alpha preview, and the software is very much evolving and improving. And you can help!
WIP dashboard.
Very
WIP. The bubble chart is particularly interesting as it shows you what kinds of data are most common at which times of day throughout the years.
The classic timeline view is a combination of all data grouped by types and time segments for reconstructing a day or other custom time period.
Viewing an item shows all the information about it, regardless of type: text, photo, live photo, video, location, etc.
I had to make a custom file picker since browser APIs are too limiting. This is how you'll import most of your data into your timeline, but this flow is being revised soon.
The large map view is capable of 3D exploration, showing your memories right where they happened with a color-coded path that represents time.
Because Timelinize is entity-aware and supports multiple data sources, it can show data on a map even if it doesn't have geolocation information. That's what the gray dots or pins represent. In this example, a text message was received while at church, even though it doesn't have any geolocation info associated with it directly.
Timelinize treats entities (people, pets/animals, organizations, etc.) as first-class data points which you can filter and organize.
Timelinize will automatically recognize the same entity across data sources with enough information, but if it isn't possible automatically, you can manually merge entities with a click.
Conversations are aggregated across data sources that have messaging capabilities. They become emergent from the database by querying relationships between items and entities.
In this conversation view, you can see messages exchanged with this person across both Facebook and SMS/text message are displayed together. Reactions are also supported.
A gallery displays photos and videos, but not just those in your photo library: it includes pictures and memes sent via messages, photos and videos uploaded to social media, and any other photos/videos in your data. You can always filter to drill down.
How it works
Obtain your data.
This usually involves exporting your data from apps, online accounts, or devices. For example, requesting an archive from Google Takeout. (Apple iCloud, Facebook, Twitter/X, Strava, Instagram, etc. all offer similar functionality for GDPR compliance.) Do this early/soon, because some services take days to provide your data.
Import your data using Timelinize. You don't need to extract or decompress .tar or .zip archives; Timelinize will attempt to recognize your data in its original format and folder structure. All the data you import is indexed in a SQLite database and stored on disk organized by date -- no obfuscation or proprietary formats; you can simply browse your files if you wish.
Explore and organize! Timelinize has a UI that portrays data using various projections and filters. It can recall moments from your past and help you view your life more comprehensively. (It's a great living family history tool.)
Repeat steps 1-3 as often as desired. Timelinize will skip any existing data that is the same and only import new content. You could do this every few weeks or months for busy accounts that are most important to you.
Caution
Timelinize is in active development and is still considered unstable. The schema is still changing, necessitating starting over from a clean slate when updating. Always keep your original source data. Expect to delete and recreate your timelines as you upgrade during this alpha development period.
Download and run
Download the
latest release
for your platform.
See the website for
installation instructions
.
Develop
See our
project wiki
for instructions on
compiling from source
.
Command line interface
Timelinize has a symmetric HTTP API and CLI. When an HTTP API endpoint is created in the code, it automatically adds to the command line as well.
Run
timelinize help
(or
go run main.go help
if you're running from source) to view the list of commands, which are also HTTP endpoints. JSON or form inputs are converted to command line args/flags that represent the JSON schema or form fields.
Setup Development Environment
Dev Container setup is provided for easy development using GitHub Codespaces or Visual Studio Code with the DevContainers extension.
Getting started with VSCode
Make sure you have the following installed:
Docker
DevContainers for VSCode
Open this project in VSCode
Go to the
Remote Explorer
on Activity Bar
Click on
New Dev Container (+)
Click on
Open Current Folder in Container
This sets up a docker container with all the dependencies required for building this project. You can get started with contributing quickly.
Motivation and vision
(For roadmap, see
issues tagged
long-term ğŸ”­
.)
The motivation for this project is two-fold. Both press upon me with a sense of urgency, which is why I dedicated some nights and weekends to work on this.
Connecting with my family -- both living and deceased -- is important to me and my close relatives. But I wish we had more insights into the lives of those who came before us. What was important to them? Where did they live / travel / spend their time? What lessons did they learn? How did global and local events -- or heck, even the weather -- affect them? What hardships did they endure? What would they have wanted to remember? What would it be like to talk to them? A lot of this could not be known unless they wrote it all down. But these days, we have that data for ourselves. What better time than right now to start collecting personal histories from all available sources and develop a rich timeline of our life for our family, or even just for our own reference and nostalgia.
Our lives are better-documented than any before us, but the record of our life is more ephemeral than any before us, too. We lose control of our data by relying on centralized, proprietary apps and cloud services which are useful today, and gone tomorrow. I wrote Timelinize because now is the time to liberate my data from corporations who don't own it, yet who have the only copy of it. This reality has made me feel uneasy for years, and it's not going away soon. Timelinize makes it bearable.
Imagine being able to pull up a single screen with your data from any and all of your online accounts and services -- while offline. And there you see so many aspects of your life at a glance: your photos and videos, social media posts, locations on a map and how you got there, emails and letters, documents, health and physical activities, mental and emotional wellness, and maybe even music you listened to, for any given day. You can "zoom out" and get the big picture. Machine learning algorithms could suggest major clusters based on your content to summarize your days, months, or years, and from that, even recommend printing physical memorabilia. It's like a highly-detailed, automated journal, fully in your control, which you can add to in the app: augment it with your own thoughts like a regular journal.
Then cross-reference your own timeline with a global public timeline: see how locations you went to changed over time, or what major news events may have affected you, or what the political/social climate -- or the literal climate -- was like at the time. For example, you may wonder, "Why did the family stay inside so much of the summer one year?" You could then see, "Oh, because it was 110 F (43 C) degrees for two months straight."
Or translate the projection sideways, and instead of looking at time cross-sections, look at cross-sections of your timeline by media type: photos, posts, location, sentiment. Look at plots, charts, graphs, of your physical activity.
Or view projections by space instead of time: view interrelations between items on a map, even items that don't have location data, because the database is entity-aware. So if a person receives a text message and the same person has location information at about the same time from a photo or GPS device, then the text message can appear on a map too, reminding you where you first got the text with the news about your nephew's birth.
And all of this runs on your own computer: no one else has access to it, no one else owns it, but you.
And if everyone had their own timeline, in theory they could be merged into a global supertimeline to become a thorough record of the human race, all without the need for centralizing our data on cloud services that are controlled by greedy corporations.
History
I've been working on this project since about 2013, even before I conceptualized
Caddy
. My initial vision was to create an automated backup of my Picasa albums that I could store on my own hard drive. This project was called Photobak. Picasa eventually became Google Photos, and about the same time I realized I wanted to backup my photos posted to Facebook, Instagram, and Twitter, too. And while I was at it, why not include my Google Location History to augment the location data from the photos. The vision continued to expand as I realized that my family could use this too, so the schema was upgraded to support multiple people/entities as well. This could allow us to merge databases, or timelines, as family members pass, or as they share parts of their timeline around with each other. Timelinize is the mature evolution of the original project that is now designed to be a comprehensive, highly detailed archive of one's life through digital (or
digitized
) content. An authoritative, unified record that is easy to preserve and organize.
License
This project is licensed with AGPL. I chose this license because I do not want others to make proprietary or commercial software using this package. The point of this project is liberation of and control over one's own, personal data, and I want to ensure that this project won't be used in anything that would perpetuate the walled garden dilemma we already face today. Even if the future of this project ever has proprietary source code, I can ensure it will stay aligned with my values and the project's original goals.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 336
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 2,387

### References
- https://github.com/timelinize/timelinize

## MODSetter/SurfSense

### Executive Summary
- Open Source Alternative to NotebookLM / Perplexity, connected to external sources such as Search Engines, Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more. Join our discord: https://discord.gg/ejRNvftDp9
- ---
- SurfSense
While tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar, Luma and more to come.
Video
temp_demo_v7.mp4
Podcast Sample
elon_vs_trump_podcast.mp4
Key Features
ğŸ’¡
Idea
:
Have your own highly customizable private NotebookLM and Perplexity integrated with external sources.
ğŸ“
Multiple File Format Uploading Support
Save content from your own personal files
(Documents, images, videos and supports
50+ file extensions
)
to your own personal knowledge base .
ğŸ”
Powerful Search
Quickly research or find anything in your saved content .
ğŸ’¬
Chat with your Saved Content
Interact in Natural Language and get cited answers.
ğŸ“„
Cited Answers
Get Cited answers just like Perplexity.
ğŸ””
Privacy & Local LLM Support
Works Flawlessly with Ollama local LLMs.
ğŸ 
Self Hostable
Open source and easy to deploy locally.
ğŸ™ï¸ Podcasts
Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)
Convert your chat conversations into engaging audio content
Support for local TTS providers (Kokoro TTS)
Support for multiple TTS providers (OpenAI, Azure, Google Vertex AI)
ğŸ“Š
Advanced RAG Techniques
Supports 100+ LLM's
Supports 6000+ Embedding Models.
Supports all major Rerankers (Pinecode, Cohere, Flashrank etc)
Uses Hierarchical Indices (2 tiered RAG setup).
Utilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).
RAG as a Service API Backend.
â„¹ï¸
External Sources
Search Engines (Tavily, LinkUp)
Slack
Linear
Jira
ClickUp
Confluence
Notion
Gmail
Youtube Videos
GitHub
Discord
Airtable
Google Calendar
Luma
and more to come.....
ğŸ“„
Supported File Extensions
Note
: File format support depends on your ETL service configuration. LlamaCloud supports 50+ formats, Unstructured supports 34+ core formats, and Docling (core formats, local processing, privacy-focused, no API key).
Documents & Text
LlamaCloud
:
.pdf
,
.doc
,
.docx
,
.docm
,
.dot
,
.dotm
,
.rtf
,
.txt
,
.xml
,
.epub
,
.odt
,
.wpd
,
.pages
,
.key
,
.numbers
,
.602
,
.abw
,
.cgm
,
.cwk
,
.hwp
,
.lwp
,
.mw
,
.mcw
,
.pbd
,
.sda
,
.sdd
,
.sdp
,
.sdw
,
.sgl
,
.sti
,
.sxi
,
.sxw
,
.stw
,
.sxg
,
.uof
,
.uop
,
.uot
,
.vor
,
.wps
,
.zabw
Unstructured
:
.doc
,
.docx
,
.odt
,
.rtf
,
.pdf
,
.xml
,
.txt
,
.md
,
.markdown
,
.rst
,
.html
,
.org
,
.epub
Docling
:
.pdf
,
.docx
,
.html
,
.htm
,
.xhtml
,
.adoc
,
.asciidoc
Presentations
LlamaCloud
:
.ppt
,
.pptx
,
.pptm
,
.pot
,
.potm
,
.potx
,
.odp
,
.key
Unstructured
:
.ppt
,
.pptx
Docling
:
.pptx
Spreadsheets & Data
LlamaCloud
:
.xlsx
,
.xls
,
.xlsm
,
.xlsb
,
.xlw
,
.csv
,
.tsv
,
.ods
,
.fods
,
.numbers
,
.dbf
,
.123
,
.dif
,
.sylk
,
.slk
,
.prn
,
.et
,
.uos1
,
.uos2
,
.wk1
,
.wk2
,
.wk3
,
.wk4
,
.wks
,
.wq1
,
.wq2
,
.wb1
,
.wb2
,
.wb3
,
.qpw
,
.xlr
,
.eth
Unstructured
:
.xls
,
.xlsx
,
.csv
,
.tsv
Docling
:
.xlsx
,
.csv
Images
LlamaCloud
:
.jpg
,
.jpeg
,
.png
,
.gif
,
.bmp
,
.svg
,
.tiff
,
.webp
,
.html
,
.htm
,
.web
Unstructured
:
.jpg
,
.jpeg
,
.png
,
.bmp
,
.tiff
,
.heic
Docling
:
.jpg
,
.jpeg
,
.png
,
.bmp
,
.tiff
,
.tif
,
.webp
Audio & Video
(Always Supported)
.mp3
,
.mpga
,
.m4a
,
.wav
,
.mp4
,
.mpeg
,
.webm
Email & Communication
Unstructured
:
.eml
,
.msg
,
.p7s
ğŸ”– Cross Browser Extension
The SurfSense extension can be used to save any webpage you like.
Its main usecase is to save any webpages protected beyond authentication.
FEATURE REQUESTS AND FUTURE
SurfSense is actively being developed.
While it's not yet production-ready, you can help us speed up the process.
Join the
SurfSense Discord
and help shape the future of SurfSense!
ğŸš€ Roadmap
Stay up to date with our development progress and upcoming features!
Check out our public roadmap and contribute your ideas or feedback:
View the Roadmap:
SurfSense Roadmap on GitHub Projects
How to get started?
Installation Options
SurfSense provides two installation methods:
Docker Installation
- The easiest way to get SurfSense up and running with all dependencies containerized.
Includes pgAdmin for database management through a web UI
Supports environment variable customization via
.env
file
Flexible deployment options (full stack or core services only)
No need to manually edit configuration files between environments
See
Docker Setup Guide
for detailed instructions
For deployment scenarios and options, see
Deployment Guide
Manual Installation (Recommended)
- For users who prefer more control over their setup or need to customize their deployment.
Both installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.
Before installation, make sure to complete the
prerequisite setup steps
including:
PGVector setup
File Processing ETL Service
(choose one):
Unstructured.io API key (supports 34+ formats)
LlamaIndex API key (enhanced parsing, supports 50+ formats)
Docling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)
Other required API keys
Screenshots
Research Agent
Search Spaces
Manage Documents
Podcast Agent
Agent Chat
Browser Extension
Tech Stack
BackEnd
FastAPI
: Modern, fast web framework for building APIs with Python
PostgreSQL with pgvector
: Database with vector search capabilities for similarity searches
SQLAlchemy
: SQL toolkit and ORM (Object-Relational Mapping) for database interactions
Alembic
: A database migrations tool for SQLAlchemy.
FastAPI Users
: Authentication and user management with JWT and OAuth support
LangGraph
: Framework for developing AI-agents.
LangChain
: Framework for developing AI-powered applications.
LLM Integration
: Integration with LLM models through LiteLLM
Rerankers
: Advanced result ranking for improved search relevance
Hybrid Search
: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)
Vector Embeddings
: Document and text embeddings for semantic search
pgvector
: PostgreSQL extension for efficient vector similarity operations
Chonkie
: Advanced document chunking and embedding library
Uses
AutoEmbeddings
for flexible embedding model selection
LateChunker
for optimized document chunking based on embedding model's max sequence length
FrontEnd
Next.js 15.2.3
: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.
React 19.0.0
: JavaScript library for building user interfaces.
TypeScript
: Static type-checking for JavaScript, enhancing code quality and developer experience.
Vercel AI SDK Kit UI Stream Protocol
: To create scalable chat UI.
Tailwind CSS 4.x
: Utility-first CSS framework for building custom UI designs.
Shadcn
: Headless components library.
Lucide React
: Icon set implemented as React components.
Framer Motion
: Animation library for React.
Sonner
: Toast notification library.
Geist
: Font family from Vercel.
React Hook Form
: Form state management and validation.
Zod
: TypeScript-first schema validation with static type inference.
@hookform/resolvers
: Resolvers for using validation libraries with React Hook Form.
@tanstack/react-table
: Headless UI for building powerful tables & datagrids.
DevOps
Docker
: Container platform for consistent deployment across environments
Docker Compose
: Tool for defining and running multi-container Docker applications
pgAdmin
: Web-based PostgreSQL administration tool included in Docker setup
Extension
Manifest v3 on Plasmo
Future Work
Add More Connectors.
Patch minor bugs.
Document Podcasts
Contribute
Contributions are very welcome! A contribution can be as small as a â­ or even finding and creating issues.
Fine-tuning the Backend is always desired.
For detailed contribution guidelines, please see our
CONTRIBUTING.md
file.
Star History
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 334
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 9,147

### References
- https://github.com/MODSetter/SurfSense

## browserbase/stagehand

### Executive Summary
- The AI Browser Automation Framework
- ---
- The AI Browser Automation Framework
Read the Docs
If you're looking for the Python implementation, you can find it
here
Vibe code
Stagehand with
Director
Why Stagehand?
Most existing browser automation tools either require you to write low-level code in a framework like Selenium, Playwright, or Puppeteer, or use high-level agents that can be unpredictable in production. By letting developers choose what to write in code vs. natural language, Stagehand is the natural choice for browser automations in production.
Choose when to write code vs. natural language
: use AI when you want to navigate unfamiliar pages, and use code (
Playwright
) when you know exactly what you want to do.
Preview and cache actions
: Stagehand lets you preview AI actions before running them, and also helps you easily cache repeatable actions to save time and tokens.
Computer use models with one line of code
: Stagehand lets you integrate SOTA computer use models from OpenAI and Anthropic into the browser with one line of code.
Example
Here's how to build a sample browser automation with Stagehand:
// Use Playwright functions on the page object
const
page
=
stagehand
.
page
;
await
page
.
goto
(
"https://github.com/browserbase"
)
;
// Use act() to execute individual actions
await
page
.
act
(
"click on the stagehand repo"
)
;
// Use Computer Use agents for larger actions
const
agent
=
stagehand
.
agent
(
{
provider
:
"openai"
,
model
:
"computer-use-preview"
,
}
)
;
await
agent
.
execute
(
"Get to the latest PR"
)
;
// Use extract() to read data from the page
const
{
author
,
title
}
=
await
page
.
extract
(
{
instruction
:
"extract the author and title of the PR"
,
schema
:
z
.
object
(
{
author
:
z
.
string
(
)
.
describe
(
"The username of the PR author"
)
,
title
:
z
.
string
(
)
.
describe
(
"The title of the PR"
)
,
}
)
,
}
)
;
Documentation
Visit
docs.stagehand.dev
to view the full documentation.
Getting Started
Start with Stagehand with one line of code, or check out our
Quickstart Guide
for more information:
npx create-browser-app
Watch Anirudh demo create-browser-app to create a Stagehand project!
Build and Run from Source
git clone https://github.com/browserbase/stagehand.git
cd
stagehand
pnpm install
pnpm playwright install
pnpm run build
pnpm run example
#
run the blank script at ./examples/example.ts
pnpm run example 2048
#
run the 2048 example at ./examples/2048.ts
pnpm run evals -man
#
see evaluation suite options
Stagehand is best when you have an API key for an LLM provider and Browserbase credentials. To add these to your project, run:
cp .env.example .env
nano .env
#
Edit the .env file to add API keys
Contributing
Note
We highly value contributions to Stagehand! For questions or support, please join our
Slack community
.
At a high level, we're focused on improving reliability, speed, and cost in that order of priority. If you're interested in contributing, we strongly recommend reaching out to
Miguel Gonzalez
or
Paul Klein
in our
Slack community
before starting to ensure that your contribution aligns with our goals.
For more information, please see our
Contributing Guide
.
Acknowledgements
This project heavily relies on
Playwright
as a resilient backbone to automate the web. It also would not be possible without the awesome techniques and discoveries made by
tarsier
,
gemini-zod
, and
fuji-web
.
We'd like to thank the following people for their major contributions to Stagehand:
Paul Klein
Anirudh Kamath
Sean McGuire
Miguel Gonzalez
Sameel Arif
Filip Michalsky
Jeremy Press
Navid Pour
License
Licensed under the MIT License.
Copyright 2025 Browserbase, Inc.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 248
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 18,145

### References
- https://github.com/browserbase/stagehand

## rustdesk/rustdesk

### Executive Summary
- An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.
- ---
- Build
â€¢
Docker
â€¢
Structure
â€¢
Snapshot
[
Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°
] | [
Äesky
] | [
ä¸­æ–‡
] | [
Magyar
] | [
EspaÃ±ol
] | [
ÙØ§Ø±Ø³ÛŒ
] | [
FranÃ§ais
] | [
Deutsch
] | [
Polski
] | [
Indonesian
] | [
Suomi
] | [
à´®à´²à´¯à´¾à´³à´‚
] | [
æ—¥æœ¬èª
] | [
Nederlands
] | [
Italiano
] | [
Ğ ÑƒÑÑĞºĞ¸Ğ¹
] | [
PortuguÃªs (Brasil)
] | [
Esperanto
] | [
í•œêµ­ì–´
] | [
Ø§Ù„Ø¹Ø±Ø¨ÙŠ
] | [
Tiáº¿ng Viá»‡t
] | [
Dansk
] | [
Î•Î»Î»Î·Î½Î¹ÎºÎ¬
] | [
TÃ¼rkÃ§e
] | [
Norsk
]
We need your help to translate this README,
RustDesk UI
and
RustDesk Doc
to your native language
Caution
Misuse Disclaimer:
The developers of RustDesk do not condone or support any unethical or illegal use of this software. Misuse, such as unauthorized access, control or invasion of privacy, is strictly against our guidelines. The authors are not responsible for any misuse of the application.
Chat with us:
Discord
|
Twitter
|
Reddit
|
YouTube
Yet another remote desktop solution, written in Rust. Works out of the box with no configuration required. You have full control of your data, with no concerns about security. You can use our rendezvous/relay server,
set up your own
, or
write your own rendezvous/relay server
.
RustDesk welcomes contribution from everyone. See
CONTRIBUTING.md
for help getting started.
FAQ
BINARY DOWNLOAD
NIGHTLY BUILD
Dependencies
Desktop versions use Flutter or Sciter (deprecated) for GUI, this tutorial is for Sciter only, since it is easier and more friendly to start. Check out our
CI
for building Flutter version.
Please download Sciter dynamic library yourself.
Windows
|
Linux
|
macOS
Raw Steps to build
Prepare your Rust development env and C++ build env
Install
vcpkg
, and set
VCPKG_ROOT
env variable correctly
Windows: vcpkg install libvpx:x64-windows-static libyuv:x64-windows-static opus:x64-windows-static aom:x64-windows-static
Linux/macOS: vcpkg install libvpx libyuv opus aom
run
cargo run
Build
How to Build on Linux
Ubuntu 18 (Debian 10)
sudo apt install -y zip g++ gcc git curl wget nasm yasm libgtk-3-dev clang libxcb-randr0-dev libxdo-dev \
        libxfixes-dev libxcb-shape0-dev libxcb-xfixes0-dev libasound2-dev libpulse-dev cmake make \
        libclang-dev ninja-build libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libpam0g-dev
openSUSE Tumbleweed
sudo zypper install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libXfixes-devel cmake alsa-lib-devel gstreamer-devel gstreamer-plugins-base-devel xdotool-devel pam-devel
Fedora 28 (CentOS 8)
sudo yum -y install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libxdo-devel libXfixes-devel pulseaudio-libs-devel cmake alsa-lib-devel gstreamer1-devel gstreamer1-plugins-base-devel pam-devel
Arch (Manjaro)
sudo pacman -Syu --needed unzip git cmake gcc curl wget yasm nasm zip make pkg-config clang gtk3 xdotool libxcb libxfixes alsa-lib pipewire
Install vcpkg
git clone https://github.com/microsoft/vcpkg
cd
vcpkg
git checkout 2023.04.15
cd
..
vcpkg/bootstrap-vcpkg.sh
export
VCPKG_ROOT=
$HOME
/vcpkg
vcpkg/vcpkg install libvpx libyuv opus aom
Fix libvpx (For Fedora)
cd
vcpkg/buildtrees/libvpx/src
cd
*
./configure
sed -i
'
s/CFLAGS+=-I/CFLAGS+=-fPIC -I/g
'
Makefile
sed -i
'
s/CXXFLAGS+=-I/CXXFLAGS+=-fPIC -I/g
'
Makefile
make
cp libvpx.a
$HOME
/vcpkg/installed/x64-linux/lib/
cd
Build
curl --proto
'
=https
'
--tlsv1.2 -sSf https://sh.rustup.rs
|
sh
source
$HOME
/.cargo/env
git clone --recurse-submodules https://github.com/rustdesk/rustdesk
cd
rustdesk
mkdir -p target/debug
wget https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so
mv libsciter-gtk.so target/debug
VCPKG_ROOT=
$HOME
/vcpkg cargo run
How to build with Docker
Begin by cloning the repository and building the Docker container:
git clone https://github.com/rustdesk/rustdesk
cd
rustdesk
git submodule update --init --recursive
docker build -t
"
rustdesk-builder
"
.
Then, each time you need to build the application, run the following command:
docker run --rm -it -v
$PWD
:/home/user/rustdesk -v rustdesk-git-cache:/home/user/.cargo/git -v rustdesk-registry-cache:/home/user/.cargo/registry -e PUID=
"
$(
id -u
)
"
-e PGID=
"
$(
id -g
)
"
rustdesk-builder
Note that the first build may take longer before dependencies are cached, subsequent builds will be faster. Additionally, if you need to specify different arguments to the build command, you may do so at the end of the command in the
<OPTIONAL-ARGS>
position. For instance, if you wanted to build an optimized release version, you would run the command above followed by
--release
. The resulting executable will be available in the target folder on your system, and can be run with:
target/debug/rustdesk
Or, if you're running a release executable:
target/release/rustdesk
Please ensure that you run these commands from the root of the RustDesk repository, or the application may not find the required resources. Also note that other cargo subcommands such as
install
or
run
are not currently supported via this method as they would install or run the program inside the container instead of the host.
File Structure
libs/hbb_common
: video codec, config, tcp/udp wrapper, protobuf, fs functions for file transfer, and some other utility functions
libs/scrap
: screen capture
libs/enigo
: platform specific keyboard/mouse control
libs/clipboard
: file copy and paste implementation for Windows, Linux, macOS.
src/ui
: obsolete Sciter UI (deprecated)
src/server
: audio/clipboard/input/video services, and network connections
src/client.rs
: start a peer connection
src/rendezvous_mediator.rs
: Communicate with
rustdesk-server
, wait for remote direct (TCP hole punching) or relayed connection
src/platform
: platform specific code
flutter
: Flutter code for desktop and mobile
flutter/web/js
: JavaScript for Flutter web client
Screenshots
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 231
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 99,705

### References
- https://github.com/rustdesk/rustdesk

## shadcn-ui/ui

### Executive Summary
- A set of beautifully-designed, accessible components and a code distribution platform. Works with your favorite frameworks. Open Source. Open Code.
- ---
- shadcn/ui
Accessible and customizable components that you can copy and paste into your apps. Free. Open Source.
Use this to build your own component library
.
Documentation
Visit
http://ui.shadcn.com/docs
to view the documentation.
Contributing
Please read the
contributing guide
.
License
Licensed under the
MIT license
.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 231
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 97,333

### References
- https://github.com/shadcn-ui/ui

## openai/codex

### Executive Summary
- Lightweight coding agent that runs in your terminal
- ---
- npm i -g @openai/codex
or
brew install codex
Codex CLI
is a coding agent from OpenAI that runs locally on your computer.
If you want Codex in your code editor (VS Code, Cursor, Windsurf),
install in your IDE
If you are looking for the
cloud-based agent
from OpenAI,
Codex Web
, go to
chatgpt.com/codex
Quickstart
Installing and running Codex CLI
Install globally with your preferred package manager. If you use npm:
npm install -g @openai/codex
Alternatively, if you use Homebrew:
brew install codex
Then simply run
codex
to get started:
codex
You can also go to the
latest GitHub Release
and download the appropriate binary for your platform.
Each GitHub Release contains many executables, but in practice, you likely want one of these:
macOS
Apple Silicon/arm64:
codex-aarch64-apple-darwin.tar.gz
x86_64 (older Mac hardware):
codex-x86_64-apple-darwin.tar.gz
Linux
x86_64:
codex-x86_64-unknown-linux-musl.tar.gz
arm64:
codex-aarch64-unknown-linux-musl.tar.gz
Each archive contains a single entry with the platform baked into the name (e.g.,
codex-x86_64-unknown-linux-musl
), so you likely want to rename it to
codex
after extracting it.
Using Codex with your ChatGPT plan
Run
codex
and select
Sign in with ChatGPT
. We recommend signing into your ChatGPT account to use Codex as part of your Plus, Pro, Team, Edu, or Enterprise plan.
Learn more about what's included in your ChatGPT plan
.
You can also use Codex with an API key, but this requires
additional setup
. If you previously used an API key for usage-based billing, see the
migration steps
. If you're having trouble with login, please comment on
this issue
.
Model Context Protocol (MCP)
Codex can access MCP servers. To configure them, refer to the
config docs
.
Configuration
Codex CLI supports a rich set of configuration options, with preferences stored in
~/.codex/config.toml
. For full configuration options, see
Configuration
.
Docs & FAQ
Getting started
CLI usage
Running with a prompt as input
Example prompts
Memory with AGENTS.md
Configuration
Sandbox & approvals
Authentication
Auth methods
Login on a "Headless" machine
Automating Codex
GitHub Action
TypeScript SDK
Non-interactive mode (
codex exec
)
Advanced
Tracing / verbose logging
Model Context Protocol (MCP)
Zero data retention (ZDR)
Contributing
Install & build
System Requirements
DotSlash
Build from source
FAQ
Open source fund
License
This repository is licensed under the
Apache-2.0 License
.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 188
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 46,874

### References
- https://github.com/openai/codex

## Zie619/n8n-workflows

### Executive Summary
- all of the workflows of n8n i could find (also from the site itself)
- ---
- âš¡ N8N Workflow Collection & Documentation
A professionally organized collection of
2,057 n8n workflows
with a lightning-fast documentation system that provides instant search, analysis, and browsing capabilities.
âš ï¸
IMPORTANT NOTICE (Aug 14, 2025):
Repository history has been rewritten due to DMCA compliance. If you have a fork or local clone, please see
Issue 85
for instructions on syncing your copy.
Support My Work
If you'd like to say thanks, consider buying me a coffeeâ€”your support helps me keep improving this project!
ğŸš€
NEW: Public Search Interface & High-Performance Documentation
ğŸŒ
Browse workflows online
- No installation required!
Or run locally for development with 100x performance improvement:
Option 1: Online Search (Recommended for Users)
ğŸ”— Visit:
zie619.github.io/n8n-workflows
âš¡
Instant access
- No setup required
ğŸ”
Search 2,057+ workflows
directly in browser
ğŸ“±
Mobile-friendly
interface
ğŸ·ï¸
Category filtering
across 15 categories
ğŸ“¥
Direct download
of workflow JSON files
Option 2: Local Development System
#
Install dependencies
pip install -r requirements.txt
#
Start the fast API server
python run.py
#
Open in browser
http://localhost:8000
Features:
âš¡
Sub-100ms response times
with SQLite FTS5 search
ğŸ”
Instant full-text search
with advanced filtering
ğŸ“±
Responsive design
- works perfectly on mobile
ğŸŒ™
Dark/light themes
with system preference detection
ğŸ“Š
Live statistics
- 365 unique integrations, 29,445 total nodes
ğŸ¯
Smart categorization
by trigger type and complexity
ğŸ¯
Use case categorization
by service name mapped to categories
ğŸ“„
On-demand JSON viewing
and download
ğŸ”—
Mermaid diagram generation
for workflow visualization
ğŸ”„
Real-time workflow naming
with intelligent formatting
Performance Comparison
Metric
Old System
New System
Improvement
File Size
71MB HTML
<100KB
700x smaller
Load Time
10+ seconds
<1 second
10x faster
Search
Client-side only
Full-text with FTS5
Instant
Memory Usage
~2GB RAM
<50MB RAM
40x less
Mobile Support
Poor
Excellent
Fully responsive
ğŸ“‚ Repository Organization
Workflow Collection
2,057 workflows
with meaningful, searchable names
365 unique integrations
across popular platforms
29,445 total nodes
with professional categorization
Quality assurance
- All workflows analyzed and categorized
Advanced Naming System âœ¨
Our intelligent naming system converts technical filenames into readable titles:
Before
:
2051_Telegram_Webhook_Automation_Webhook.json
After
:
Telegram Webhook Automation
100% meaningful names
with smart capitalization
Automatic integration detection
from node analysis
Use Case Category âœ¨
The search interface includes a dropdown filter that lets you browse 2,057+ workflows by category.
The system includes an automated categorization feature that organizes workflows by service categories to make them easier to discover and filter.
How Categorization Works
Run the categorization script
python create_categories.py
Service Name Recognition
The script analyzes each workflow JSON filename to identify recognized service names (e.g., "Twilio", "Slack", "Gmail", etc.)
Category Mapping
Each recognized service name is matched to its corresponding category using the definitions in
context/def_categories.json
. For example:
Twilio â†’ Communication & Messaging
Gmail â†’ Communication & Messaging
Airtable â†’ Data Processing & Analysis
Salesforce â†’ CRM & Sales
Search Categories Generation
The script produces a
search_categories.json
file that contains the categorized workflow data
Filter Interface
Users can then filter workflows by category in the search interface, making it easier to find workflows for specific use cases
Available Categories
The categorization system includes the following main categories:
AI Agent Development
Business Process Automation
Cloud Storage & File Management
Communication & Messaging
Creative Content & Video Automation
Creative Design Automation
CRM & Sales
Data Processing & Analysis
E-commerce & Retail
Financial & Accounting
Marketing & Advertising Automation
Project Management
Social Media Management
Technical Infrastructure & DevOps
Web Scraping & Data Extraction
Contribute Categories
You can help expand the categorization by adding more service-to-category mappings (e.g., Twilio â†’ Communication & Messaging) in context/defs_categories.json.
Many workflow JSON files are conveniently named with the service name, often separated by underscores (_).
ğŸ›  Usage Instructions
Option 1: Modern Fast System (Recommended)
#
Clone repository
git clone
<
repo-url
>
cd
n8n-workflows
#
Install Python dependencies
pip install -r requirements.txt
#
Start the documentation server
python run.py
#
Browse workflows at http://localhost:8000
#
- Instant search across 2,057 workflows
#
- Professional responsive interface
#
- Real-time workflow statistics
Option 2: Development Mode
#
Start with auto-reload for development
python run.py --dev
#
Or specify custom host/port
python run.py --host 0.0.0.0 --port 3000
#
Force database reindexing
python run.py --reindex
Import Workflows into n8n
#
Use the Python importer (recommended)
python import_workflows.py
#
Or manually import individual workflows:
#
1. Open your n8n Editor UI
#
2. Click menu (â˜°) â†’ Import workflow
#
3. Choose any .json file from the workflows/ folder
#
4. Update credentials/webhook URLs before running
ğŸ“Š Workflow Statistics
Current Collection Stats
Total Workflows
: 2,057 automation workflows
Active Workflows
: 215 (10.5% active rate)
Total Nodes
: 29,528 (avg 14.4 nodes per workflow)
Unique Integrations
: 367 different services and APIs
Database
: SQLite with FTS5 full-text search
Trigger Distribution
Complex
: 832 workflows (40.4%) - Multi-trigger systems
Webhook
: 521 workflows (25.3%) - API-triggered automations
Manual
: 478 workflows (23.2%) - User-initiated workflows
Scheduled
: 226 workflows (11.0%) - Time-based executions
Complexity Analysis
Low (â‰¤5 nodes)
: ~35% - Simple automations
Medium (6-15 nodes)
: ~45% - Standard workflows
High (16+ nodes)
: ~20% - Complex enterprise systems
Popular Integrations
Top services by usage frequency:
Communication
: Telegram, Discord, Slack, WhatsApp
Cloud Storage
: Google Drive, Google Sheets, Dropbox
Databases
: PostgreSQL, MySQL, MongoDB, Airtable
AI/ML
: OpenAI, Anthropic, Hugging Face
Development
: HTTP Request, Webhook, GraphQL
ğŸ” Advanced Search Features
Smart Search Categories
Our system automatically categorizes workflows into 15 main categories:
Available Categories:
AI Agent Development
: OpenAI, Anthropic, Hugging Face, CalcsLive
Business Process Automation
: Workflow utilities, scheduling, data processing
Cloud Storage & File Management
: Google Drive, Dropbox, OneDrive, Box
Communication & Messaging
: Telegram, Discord, Slack, WhatsApp, Email
Creative Content & Video Automation
: YouTube, Vimeo, content creation
Creative Design Automation
: Canva, Figma, image processing
CRM & Sales
: Salesforce, HubSpot, Pipedrive, customer management
Data Processing & Analysis
: Database operations, analytics, data transformation
E-commerce & Retail
: Shopify, Stripe, PayPal, online stores
Financial & Accounting
: Financial tools, payment processing, accounting
Marketing & Advertising Automation
: Email marketing, campaigns, lead generation
Project Management
: Jira, Trello, Asana, task management
Social Media Management
: LinkedIn, Twitter/X, Facebook, Instagram
Technical Infrastructure & DevOps
: GitHub, deployment, monitoring
Web Scraping & Data Extraction
: HTTP requests, webhooks, data collection
API Usage Examples
#
Search workflows by text
curl
"
http://localhost:8000/api/workflows?q=telegram+automation
"
#
Filter by trigger type and complexity
curl
"
http://localhost:8000/api/workflows?trigger=Webhook&complexity=high
"
#
Find all messaging workflows
curl
"
http://localhost:8000/api/workflows/category/messaging
"
#
Get database statistics
curl
"
http://localhost:8000/api/stats
"
#
Browse available categories
curl
"
http://localhost:8000/api/categories
"
ğŸ— Technical Architecture
Modern Stack
SQLite Database
- FTS5 full-text search with 365 indexed integrations
FastAPI Backend
- RESTful API with automatic OpenAPI documentation
Responsive Frontend
- Modern HTML5 with embedded CSS/JavaScript
Smart Analysis
- Automatic workflow categorization and naming
Key Features
Change Detection
- MD5 hashing for efficient re-indexing
Background Processing
- Non-blocking workflow analysis
Compressed Responses
- Gzip middleware for optimal speed
Error Handling
- Graceful degradation and comprehensive logging
Mobile Optimization
- Touch-friendly interface design
Database Performance
--
Optimized schema for lightning-fast queries
CREATE
TABLE
workflows
(
    id
INTEGER
PRIMARY KEY
,
    filename
TEXT
UNIQUE,
    name
TEXT
,
    active
BOOLEAN
,
    trigger_type
TEXT
,
    complexity
TEXT
,
    node_count
INTEGER
,
    integrations
TEXT
,
--
JSON array of 365 unique services
description
TEXT
,
    file_hash
TEXT
,
--
MD5 for change detection
analyzed_at
TIMESTAMP
);
--
Full-text search with ranking
CREATE VIRTUAL TABLE workflows_fts USING fts5(
    filename, name, description, integrations, tags,
    content
=
'
workflows
'
, content_rowid
=
'
id
'
);
ğŸ”§ Setup & Requirements
System Requirements
Python 3.7+
- For running the documentation system
Modern Browser
- Chrome, Firefox, Safari, Edge
50MB Storage
- For SQLite database and indexes
n8n Instance
- For importing and running workflows
Installation
#
Clone repository
git clone
<
repo-url
>
cd
n8n-workflows
#
Install dependencies
pip install -r requirements.txt
#
Start documentation server
python run.py
#
Access at http://localhost:8000
Development Setup
#
Create virtual environment
python3 -m venv .venv
source
.venv/bin/activate
#
Linux/Mac
#
or .venv\Scripts\activate  # Windows
#
Install dependencies
pip install -r requirements.txt
#
Run with auto-reload for development
python api_server.py --reload
#
Force database reindexing
python workflow_db.py --index --force
ğŸ“‹ Naming Convention
Intelligent Formatting System
Our system automatically converts technical filenames to user-friendly names:
#
Automatic transformations:
2051_Telegram_Webhook_Automation_Webhook.json â†’
"
Telegram Webhook Automation
"
0250_HTTP_Discord_Import_Scheduled.json â†’
"
HTTP Discord Import Scheduled
"
0966_OpenAI_Data_Processing_Manual.json â†’
"
OpenAI Data Processing Manual
"
Technical Format
[ID]_[Service1]_[Service2]_[Purpose]_[Trigger].json
Smart Capitalization Rules
HTTP
â†’ HTTP (not Http)
API
â†’ API (not Api)
webhook
â†’ Webhook
automation
â†’ Automation
scheduled
â†’ Scheduled
ğŸš€ API Documentation
Core Endpoints
GET /
- Main workflow browser interface
GET /api/stats
- Database statistics and metrics
GET /api/workflows
- Search with filters and pagination
GET /api/workflows/{filename}
- Detailed workflow information
GET /api/workflows/{filename}/download
- Download workflow JSON
GET /api/workflows/{filename}/diagram
- Generate Mermaid diagram
Advanced Search
GET /api/workflows/category/{category}
- Search by service category
GET /api/categories
- List all available categories
GET /api/integrations
- Get integration statistics
POST /api/reindex
- Trigger background reindexing
Response Examples
// GET /api/stats
{
"total"
:
2053
,
"active"
:
215
,
"inactive"
:
1838
,
"triggers"
: {
"Complex"
:
831
,
"Webhook"
:
519
,
"Manual"
:
477
,
"Scheduled"
:
226
},
"total_nodes"
:
29445
,
"unique_integrations"
:
365
}
ğŸ¤ Contributing
ğŸ‰ This project solves
Issue #84
- providing online access to workflows without requiring local setup!
Adding New Workflows
Export workflow
as JSON from n8n
Name descriptively
following the established pattern:
[ID]_[Service]_[Purpose]_[Trigger].json
Add to workflows/
directory (create service folder if needed)
Remove sensitive data
(credentials, personal URLs)
Add tags
for better searchability (calculation, automation, etc.)
GitHub Actions automatically
updates the public search interface
Quality Standards
âœ… Workflow must be functional and tested
âœ… Remove all credentials and sensitive data
âœ… Follow naming convention for consistency
âœ… Verify compatibility with recent n8n versions
âœ… Include meaningful description or comments
âœ… Add relevant tags for search optimization
Custom Node Workflows
âœ… Include npm package links in descriptions
âœ… Document custom node requirements
âœ… Add installation instructions
âœ… Use descriptive tags (like CalcsLive example)
Reindexing (for local development)
#
Force database reindexing after adding workflows
python run.py --reindex
#
Or update search index only
python scripts/generate_search_index.py
âš ï¸
Important Notes
Security & Privacy
Review before use
- All workflows shared as-is for educational purposes
Update credentials
- Replace API keys, tokens, and webhooks
Test safely
- Verify in development environment first
Check permissions
- Ensure proper access rights for integrations
Compatibility
n8n Version
- Compatible with n8n 1.0+ (most workflows)
Community Nodes
- Some workflows may require additional node installations
API Changes
- External services may have updated their APIs since creation
Dependencies
- Verify required integrations before importing
ğŸ“š Resources & References
Workflow Sources
This comprehensive collection includes workflows from:
Official n8n.io
- Documentation and community examples
GitHub repositories
- Open source community contributions
Blog posts & tutorials
- Real-world automation patterns
User submissions
- Tested and verified workflows
Enterprise use cases
- Business process automations
Learn More
n8n Documentation
- Official documentation
n8n Community
- Community forum and support
Workflow Templates
- Official template library
Integration Docs
- Service-specific guides
ğŸ† Project Achievements
Repository Transformation
2,053 workflows
professionally organized and named
365 unique integrations
automatically detected and categorized
100% meaningful names
(improved from basic filename patterns)
Zero data loss
during intelligent renaming process
Advanced search
with 15 service categories
Performance Revolution
Sub-100ms search
with SQLite FTS5 full-text indexing
Instant filtering
across 29,445 workflow nodes
Mobile-optimized
responsive design for all devices
Real-time statistics
with live database queries
Professional interface
with modern UX principles
System Reliability
Robust error handling
with graceful degradation
Change detection
for efficient database updates
Background processing
for non-blocking operations
Comprehensive logging
for debugging and monitoring
Production-ready
with proper middleware and security
This repository represents the most comprehensive and well-organized collection of n8n workflows available, featuring cutting-edge search technology and professional documentation that makes workflow discovery and usage a delightful experience.
ğŸ¯ Perfect for
: Developers, automation engineers, business analysts, and anyone looking to streamline their workflows with proven n8n automations.
ä¸­æ–‡
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 186
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 35,868

### References
- https://github.com/Zie619/n8n-workflows

## davila7/claude-code-templates

### Executive Summary
- CLI tool for configuring and monitoring Claude Code
- ---
- Claude Code Templates (aitmpl.com)
Ready-to-use configurations for Anthropic's Claude Code.
A comprehensive collection of AI agents, custom commands, settings, hooks, external integrations (MCPs), and project templates to enhance your development workflow.
Browse & Install Components and Templates
Browse All Templates
- Interactive web interface to explore and install 100+ agents, commands, settings, hooks, and MCPs.
ğŸš€ Quick Installation
#
Install a complete development stack
npx claude-code-templates@latest --agent frontend-developer --command generate-tests --mcp github-integration
#
Browse and install interactively
npx claude-code-templates@latest
#
Install specific components
npx claude-code-templates@latest --agent security-auditor
npx claude-code-templates@latest --command optimize-bundle
npx claude-code-templates@latest --setting mcp-timeouts
npx claude-code-templates@latest --hook pre-commit-validation
npx claude-code-templates@latest --mcp postgresql-integration
What You Get
Component
Description
Examples
ğŸ¤– Agents
AI specialists for specific domains
Security auditor, React performance optimizer, database architect
âš¡ Commands
Custom slash commands
/generate-tests
,
/optimize-bundle
,
/check-security
ğŸ”Œ MCPs
External service integrations
GitHub, PostgreSQL, Stripe, AWS, OpenAI
âš™ï¸ Settings
Claude Code configurations
Timeouts, memory settings, output styles
ğŸª Hooks
Automation triggers
Pre-commit validation, post-completion actions
ğŸ“¦ Templates
Complete project configurations with CLAUDE.md, .claude/* files and .mcp.json
Framework-specific setups, project best practices
ğŸ› ï¸ Additional Tools
Beyond the template catalog, Claude Code Templates includes powerful development tools:
ğŸ“Š Claude Code Analytics
Monitor your AI-powered development sessions in real-time with live state detection and performance metrics.
npx claude-code-templates@latest --analytics
ğŸ’¬ Conversation Monitor
Mobile-optimized interface to view Claude responses in real-time with secure remote access.
#
Local access
npx claude-code-templates@latest --chats
#
Secure remote access via Cloudflare Tunnel
npx claude-code-templates@latest --chats --tunnel
ğŸ” Health Check
Comprehensive diagnostics to ensure your Claude Code installation is optimized.
npx claude-code-templates@latest --health-check
ğŸ“– Documentation
ğŸ“š docs.aitmpl.com
- Complete guides, examples, and API reference for all components and tools.
Contributing
We welcome contributions!
Browse existing templates
to see what's available, then check our
contributing guidelines
to add your own agents, commands, MCPs, settings, or hooks.
Please read our
Code of Conduct
before contributing.
Attribution
This collection includes components from multiple sources:
Agents Collection:
wshobson/agents Collection
by
wshobson
- Licensed under MIT License (48 agents)
Commands Collection:
awesome-claude-code Commands
by
hesreallyhim
- Licensed under CC0 1.0 Universal (21 commands)
ğŸ“„ License
This project is licensed under the MIT License - see the
LICENSE
file for details.
ğŸ”— Links
ğŸŒ Browse Templates
:
aitmpl.com
ğŸ“š Documentation
:
docs.aitmpl.com
ğŸ’¬ Community
:
GitHub Discussions
ğŸ› Issues
:
GitHub Issues
â­ Star History
â­ Found this useful? Give us a star to support the project!
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 186
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 6,984

### References
- https://github.com/davila7/claude-code-templates

## anthropics/claude-code

### Executive Summary
- Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.
- ---
- Claude Code
Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows -- all through natural language commands. Use it in your terminal, IDE, or tag @claude on Github.
Learn more in the
official documentation
.
Get started
Install Claude Code:
npm install -g @anthropic-ai/claude-code
Navigate to your project directory and run
claude
.
Reporting Bugs
We welcome your feedback. Use the
/bug
command to report issues directly within Claude Code, or file a
GitHub issue
.
Connect on Discord
Join the
Claude Developers Discord
to connect with other developers using Claude Code. Get help, share feedback, and discuss your projects with the community.
Data collection, usage, and retention
When you use Claude Code, we collect feedback, which includes usage data (such as code acceptance or rejections), associated conversation data, and user feedback submitted via the
/bug
command.
How we use your data
See our
data usage policies
.
Privacy safeguards
We have implemented several safeguards to protect your data, including limited retention periods for sensitive information, restricted access to user session data, and clear policies against using feedback for model training.
For full details, please review our
Commercial Terms of Service
and
Privacy Policy
.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 177
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 36,195

### References
- https://github.com/anthropics/claude-code

## CapSoftware/Cap

### Executive Summary
- Open source Loom alternative. Beautiful, shareable screen recordings.
- ---
- Cap
The open source Loom alternative.
Cap.so Â»
Downloads for
macOS & Windows
Cap is the open source alternative to Loom. It's a video messaging tool that allows you to record, edit and share videos in seconds.
Self Hosting
Cap Web is available to self-host using Docker or Railway, see our
self-hosting docs
to learn more.
You can also use the button below to deploy Cap Web to Railway:
Cap Desktop can connect to your self-hosted Cap Web instance regardless of if you build it yourself or
download from our website
.
Monorepo App Architecture
We use a combination of Rust, React (Next.js), TypeScript, Tauri, Drizzle (ORM), MySQL, TailwindCSS throughout this Turborepo powered monorepo.
A note about database: The codebase is currently designed to work with MySQL only. MariaDB or other compatible databases might partially work but are not officially supported.
Apps:
desktop
: A
Tauri
(Rust) app, using
SolidStart
on the frontend.
web
: A
Next.js
web app.
Packages:
ui
: A
React
Shared component library.
utils
: A
React
Shared utility library.
tsconfig
: Shared
tsconfig
configurations used throughout the monorepo.
database
: A
React
and
Drizzle ORM
Shared database library.
config
:
eslint
configurations (includes
eslint-config-next
,
eslint-config-prettier
other configs used throughout the monorepo).
License:
Portions of this software are licensed as follows:
All code residing in the
cap-camera*
and
scap-*
families of crates is licensed under the MIT License (see
licenses/LICENSE-MIT
).
All third party components are licensed under the original license provided by the owner of the applicable component
All other content not mentioned above is available under the AGPLv3 license as defined in
LICENSE
Contributing
See
CONTRIBUTING.md
for more information. This guide is a work in progress, and is updated regularly as the app matures.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 136
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 12,407

### References
- https://github.com/CapSoftware/Cap

## open-webui/open-webui

### Executive Summary
- User-friendly AI Interface (Supports Ollama, OpenAI API, ...)
- ---
- Open WebUI ğŸ‘‹
Open WebUI is an
extensible
, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.
It supports various LLM runners like
Ollama
and
OpenAI-compatible APIs
, with
built-in inference engine
for RAG, making it a
powerful AI deployment solution
.
Passionate about open-source AI?
Join our team â†’
Tip
Looking for an
Enterprise Plan
?
â€“
Speak with Our Sales Team Today!
Get
enhanced capabilities
, including
custom theming and branding
,
Service Level Agreement (SLA) support
,
Long-Term Support (LTS) versions
, and
more!
For more information, be sure to check out our
Open WebUI Documentation
.
Key Features of Open WebUI â­
ğŸš€
Effortless Setup
: Install seamlessly using Docker or Kubernetes (kubectl, kustomize or helm) for a hassle-free experience with support for both
:ollama
and
:cuda
tagged images.
ğŸ¤
Ollama/OpenAI API Integration
: Effortlessly integrate OpenAI-compatible APIs for versatile conversations alongside Ollama models. Customize the OpenAI API URL to link with
LMStudio, GroqCloud, Mistral, OpenRouter, and more
.
ğŸ›¡ï¸
Granular Permissions and User Groups
: By allowing administrators to create detailed user roles and permissions, we ensure a secure user environment. This granularity not only enhances security but also allows for customized user experiences, fostering a sense of ownership and responsibility amongst users.
ğŸ”„
SCIM 2.0 Support
: Enterprise-grade user and group provisioning through SCIM 2.0 protocol, enabling seamless integration with identity providers like Okta, Azure AD, and Google Workspace for automated user lifecycle management.
ğŸ“±
Responsive Design
: Enjoy a seamless experience across Desktop PC, Laptop, and Mobile devices.
ğŸ“±
Progressive Web App (PWA) for Mobile
: Enjoy a native app-like experience on your mobile device with our PWA, providing offline access on localhost and a seamless user interface.
âœ’ï¸ğŸ”¢
Full Markdown and LaTeX Support
: Elevate your LLM experience with comprehensive Markdown and LaTeX capabilities for enriched interaction.
ğŸ¤ğŸ“¹
Hands-Free Voice/Video Call
: Experience seamless communication with integrated hands-free voice and video call features, allowing for a more dynamic and interactive chat environment.
ğŸ› ï¸
Model Builder
: Easily create Ollama models via the Web UI. Create and add custom characters/agents, customize chat elements, and import models effortlessly through
Open WebUI Community
integration.
ğŸ
Native Python Function Calling Tool
: Enhance your LLMs with built-in code editor support in the tools workspace. Bring Your Own Function (BYOF) by simply adding your pure Python functions, enabling seamless integration with LLMs.
ğŸ“š
Local RAG Integration
: Dive into the future of chat interactions with groundbreaking Retrieval Augmented Generation (RAG) support. This feature seamlessly integrates document interactions into your chat experience. You can load documents directly into the chat or add files to your document library, effortlessly accessing them using the
#
command before a query.
ğŸ”
Web Search for RAG
: Perform web searches using providers like
SearXNG
,
Google PSE
,
Brave Search
,
serpstack
,
serper
,
Serply
,
DuckDuckGo
,
TavilySearch
,
SearchApi
and
Bing
and inject the results directly into your chat experience.
ğŸŒ
Web Browsing Capability
: Seamlessly integrate websites into your chat experience using the
#
command followed by a URL. This feature allows you to incorporate web content directly into your conversations, enhancing the richness and depth of your interactions.
ğŸ¨
Image Generation Integration
: Seamlessly incorporate image generation capabilities using options such as AUTOMATIC1111 API or ComfyUI (local), and OpenAI's DALL-E (external), enriching your chat experience with dynamic visual content.
âš™ï¸
Many Models Conversations
: Effortlessly engage with various models simultaneously, harnessing their unique strengths for optimal responses. Enhance your experience by leveraging a diverse set of models in parallel.
ğŸ”
Role-Based Access Control (RBAC)
: Ensure secure access with restricted permissions; only authorized individuals can access your Ollama, and exclusive model creation/pulling rights are reserved for administrators.
ğŸŒğŸŒ
Multilingual Support
: Experience Open WebUI in your preferred language with our internationalization (i18n) support. Join us in expanding our supported languages! We're actively seeking contributors!
ğŸ§©
Pipelines, Open WebUI Plugin Support
: Seamlessly integrate custom logic and Python libraries into Open WebUI using
Pipelines Plugin Framework
. Launch your Pipelines instance, set the OpenAI URL to the Pipelines URL, and explore endless possibilities.
Examples
include
Function Calling
, User
Rate Limiting
to control access,
Usage Monitoring
with tools like Langfuse,
Live Translation with LibreTranslate
for multilingual support,
Toxic Message Filtering
and much more.
ğŸŒŸ
Continuous Updates
: We are committed to improving Open WebUI with regular updates, fixes, and new features.
Want to learn more about Open WebUI's features? Check out our
Open WebUI documentation
for a comprehensive overview!
Sponsors ğŸ™Œ
Emerald
Tailscale
â€¢ Connect self-hosted AI to any device with Tailscale
Warp
â€¢ The intelligent terminal for developers
We are incredibly grateful for the generous support of our sponsors. Their contributions help us to maintain and improve our project, ensuring we can continue to deliver quality work to our community. Thank you!
How to Install ğŸš€
Installation via Python pip ğŸ
Open WebUI can be installed using pip, the Python package installer. Before proceeding, ensure you're using
Python 3.11
to avoid compatibility issues.
Install Open WebUI
:
Open your terminal and run the following command to install Open WebUI:
pip install open-webui
Running Open WebUI
:
After installation, you can start Open WebUI by executing:
open-webui serve
This will start the Open WebUI server, which you can access at
http://localhost:8080
Quick Start with Docker ğŸ³
Note
Please note that for certain Docker environments, additional configurations might be needed. If you encounter any connection issues, our detailed guide on
Open WebUI Documentation
is ready to assist you.
Warning
When using Docker to install Open WebUI, make sure to include the
-v open-webui:/app/backend/data
in your Docker command. This step is crucial as it ensures your database is properly mounted and prevents any loss of data.
Tip
If you wish to utilize Open WebUI with Ollama included or CUDA acceleration, we recommend utilizing our official images tagged with either
:cuda
or
:ollama
. To enable CUDA, you must install the
Nvidia CUDA container toolkit
on your Linux/WSL system.
Installation with Default Configuration
If Ollama is on your computer
, use this command:
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
If Ollama is on a Different Server
, use this command:
To connect to Ollama on another server, change the
OLLAMA_BASE_URL
to the server's URL:
docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
To run Open WebUI with Nvidia GPU support
, use this command:
docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
Installation for OpenAI API Usage Only
If you're only using OpenAI API
, use this command:
docker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
Installing Open WebUI with Bundled Ollama Support
This installation method uses a single container image that bundles Open WebUI with Ollama, allowing for a streamlined setup via a single command. Choose the appropriate command based on your hardware setup:
With GPU Support
:
Utilize GPU resources by running the following command:
docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
For CPU Only
:
If you're not using a GPU, use this command instead:
docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
Both commands facilitate a built-in, hassle-free installation of both Open WebUI and Ollama, ensuring that you can get everything up and running swiftly.
After installation, you can access Open WebUI at
http://localhost:3000
. Enjoy! ğŸ˜„
Other Installation Methods
We offer various installation alternatives, including non-Docker native installation methods, Docker Compose, Kustomize, and Helm. Visit our
Open WebUI Documentation
or join our
Discord community
for comprehensive guidance.
Look at the
Local Development Guide
for instructions on setting up a local development environment.
Troubleshooting
Encountering connection issues? Our
Open WebUI Documentation
has got you covered. For further assistance and to join our vibrant community, visit the
Open WebUI Discord
.
Open WebUI: Server Connection Error
If you're experiencing connection issues, itâ€™s often due to the WebUI docker container not being able to reach the Ollama server at 127.0.0.1:11434 (host.docker.internal:11434) inside the container . Use the
--network=host
flag in your docker command to resolve this. Note that the port changes from 3000 to 8080, resulting in the link:
http://localhost:8080
.
Example Docker Command
:
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
Keeping Your Docker Installation Up-to-Date
In case you want to update your local Docker installation to the latest version, you can do it with
Watchtower
:
docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui
In the last part of the command, replace
open-webui
with your container name if it is different.
Check our Updating Guide available in our
Open WebUI Documentation
.
Using the Dev Branch ğŸŒ™
Warning
The
:dev
branch contains the latest unstable features and changes. Use it at your own risk as it may have bugs or incomplete features.
If you want to try out the latest bleeding-edge features and are okay with occasional instability, you can use the
:dev
tag like this:
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev
Offline Mode
If you are running Open WebUI in an offline environment, you can set the
HF_HUB_OFFLINE
environment variable to
1
to prevent attempts to download models from the internet.
export
HF_HUB_OFFLINE=1
What's Next? ğŸŒŸ
Discover upcoming features on our roadmap in the
Open WebUI Documentation
.
License ğŸ“œ
This project contains code under multiple licenses. The current codebase includes components licensed under the Open WebUI License with an additional requirement to preserve the "Open WebUI" branding, as well as prior contributions under their respective original licenses. For a detailed record of license changes and the applicable terms for each section of the code, please refer to
LICENSE_HISTORY
. For complete and updated licensing details, please see the
LICENSE
and
LICENSE_HISTORY
files.
Support ğŸ’¬
If you have any questions, suggestions, or need assistance, please open an issue or join our
Open WebUI Discord community
to connect with us! ğŸ¤
Star History
Created by
Timothy Jaeryang Baek
- Let's make Open WebUI even more amazing together! ğŸ’ª
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 110
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 112,077

### References
- https://github.com/open-webui/open-webui

## 78/xiaozhi-esp32

### Executive Summary
- An MCP-based chatbot | ä¸€ä¸ªåŸºäºMCPçš„èŠå¤©æœºå™¨äºº
- ---
- An MCP-based Chatbot
ï¼ˆä¸­æ–‡ |
English
|
æ—¥æœ¬èª
ï¼‰
ä»‹ç»
ğŸ‘‰
äººç±»ï¼šç»™ AI è£…æ‘„åƒå¤´ vs AIï¼šå½“åœºå‘ç°ä¸»äººä¸‰å¤©æ²¡æ´—å¤´ã€bilibiliã€‘
ğŸ‘‰
æ‰‹å·¥æ‰“é€ ä½ çš„ AI å¥³å‹ï¼Œæ–°æ‰‹å…¥é—¨æ•™ç¨‹ã€bilibiliã€‘
å°æ™º AI èŠå¤©æœºå™¨äººä½œä¸ºä¸€ä¸ªè¯­éŸ³äº¤äº’å…¥å£ï¼Œåˆ©ç”¨ Qwen / DeepSeek ç­‰å¤§æ¨¡å‹çš„ AI èƒ½åŠ›ï¼Œé€šè¿‡ MCP åè®®å®ç°å¤šç«¯æ§åˆ¶ã€‚
ç‰ˆæœ¬è¯´æ˜
å½“å‰ v2 ç‰ˆæœ¬ä¸ v1 ç‰ˆæœ¬åˆ†åŒºè¡¨ä¸å…¼å®¹ï¼Œæ‰€ä»¥æ— æ³•ä» v1 ç‰ˆæœ¬é€šè¿‡ OTA å‡çº§åˆ° v2 ç‰ˆæœ¬ã€‚åˆ†åŒºè¡¨è¯´æ˜å‚è§
partitions/v2/README.md
ã€‚
ä½¿ç”¨ v1 ç‰ˆæœ¬çš„æ‰€æœ‰ç¡¬ä»¶ï¼Œå¯ä»¥é€šè¿‡æ‰‹åŠ¨çƒ§å½•å›ºä»¶æ¥å‡çº§åˆ° v2 ç‰ˆæœ¬ã€‚
v1 çš„ç¨³å®šç‰ˆæœ¬ä¸º 1.9.2ï¼Œå¯ä»¥é€šè¿‡
git checkout v1
æ¥åˆ‡æ¢åˆ° v1 ç‰ˆæœ¬ï¼Œè¯¥åˆ†æ”¯ä¼šæŒç»­ç»´æŠ¤åˆ° 2026 å¹´ 2 æœˆã€‚
å·²å®ç°åŠŸèƒ½
Wi-Fi / ML307 Cat.1 4G
ç¦»çº¿è¯­éŸ³å”¤é†’
ESP-SR
æ”¯æŒä¸¤ç§é€šä¿¡åè®®ï¼ˆ
Websocket
æˆ– MQTT+UDPï¼‰
é‡‡ç”¨ OPUS éŸ³é¢‘ç¼–è§£ç 
åŸºäºæµå¼ ASR + LLM + TTS æ¶æ„çš„è¯­éŸ³äº¤äº’
å£°çº¹è¯†åˆ«ï¼Œè¯†åˆ«å½“å‰è¯´è¯äººçš„èº«ä»½
3D Speaker
OLED / LCD æ˜¾ç¤ºå±ï¼Œæ”¯æŒè¡¨æƒ…æ˜¾ç¤º
ç”µé‡æ˜¾ç¤ºä¸ç”µæºç®¡ç†
æ”¯æŒå¤šè¯­è¨€ï¼ˆä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ï¼‰
æ”¯æŒ ESP32-C3ã€ESP32-S3ã€ESP32-P4 èŠ¯ç‰‡å¹³å°
é€šè¿‡è®¾å¤‡ç«¯ MCP å®ç°è®¾å¤‡æ§åˆ¶ï¼ˆéŸ³é‡ã€ç¯å…‰ã€ç”µæœºã€GPIO ç­‰ï¼‰
é€šè¿‡äº‘ç«¯ MCP æ‰©å±•å¤§æ¨¡å‹èƒ½åŠ›ï¼ˆæ™ºèƒ½å®¶å±…æ§åˆ¶ã€PCæ¡Œé¢æ“ä½œã€çŸ¥è¯†æœç´¢ã€é‚®ä»¶æ”¶å‘ç­‰ï¼‰
è‡ªå®šä¹‰å”¤é†’è¯ã€å­—ä½“ã€è¡¨æƒ…ä¸èŠå¤©èƒŒæ™¯ï¼Œæ”¯æŒç½‘é¡µç«¯åœ¨çº¿ä¿®æ”¹ (
è‡ªå®šä¹‰Assetsç”Ÿæˆå™¨
)
ç¡¬ä»¶
é¢åŒ…æ¿æ‰‹å·¥åˆ¶ä½œå®è·µ
è¯¦è§é£ä¹¦æ–‡æ¡£æ•™ç¨‹ï¼š
ğŸ‘‰
ã€Šå°æ™º AI èŠå¤©æœºå™¨äººç™¾ç§‘å…¨ä¹¦ã€‹
é¢åŒ…æ¿æ•ˆæœå›¾å¦‚ä¸‹ï¼š
æ”¯æŒ 70 å¤šä¸ªå¼€æºç¡¬ä»¶ï¼ˆä»…å±•ç¤ºéƒ¨åˆ†ï¼‰
ç«‹åˆ›Â·å®æˆ˜æ´¾ ESP32-S3 å¼€å‘æ¿
ä¹é‘« ESP32-S3-BOX3
M5Stack CoreS3
M5Stack AtomS3R + Echo Base
ç¥å¥‡æŒ‰é’® 2.4
å¾®é›ªç”µå­ ESP32-S3-Touch-AMOLED-1.8
LILYGO T-Circle-S3
è™¾å“¥ Mini C3
ç’€ç’¨Â·AI åŠå 
æ— åç§‘æŠ€ Nologo-æ˜Ÿæ™º-1.54TFT
SenseCAP Watcher
ESP-HI è¶…ä½æˆæœ¬æœºå™¨ç‹—
è½¯ä»¶
å›ºä»¶çƒ§å½•
æ–°æ‰‹ç¬¬ä¸€æ¬¡æ“ä½œå»ºè®®å…ˆä¸è¦æ­å»ºå¼€å‘ç¯å¢ƒï¼Œç›´æ¥ä½¿ç”¨å…å¼€å‘ç¯å¢ƒçƒ§å½•çš„å›ºä»¶ã€‚
å›ºä»¶é»˜è®¤æ¥å…¥
xiaozhi.me
å®˜æ–¹æœåŠ¡å™¨ï¼Œä¸ªäººç”¨æˆ·æ³¨å†Œè´¦å·å¯ä»¥å…è´¹ä½¿ç”¨ Qwen å®æ—¶æ¨¡å‹ã€‚
ğŸ‘‰
æ–°æ‰‹çƒ§å½•å›ºä»¶æ•™ç¨‹
å¼€å‘ç¯å¢ƒ
Cursor æˆ– VSCode
å®‰è£… ESP-IDF æ’ä»¶ï¼Œé€‰æ‹© SDK ç‰ˆæœ¬ 5.4 æˆ–ä»¥ä¸Š
Linux æ¯” Windows æ›´å¥½ï¼Œç¼–è¯‘é€Ÿåº¦å¿«ï¼Œä¹Ÿå…å»é©±åŠ¨é—®é¢˜çš„å›°æ‰°
æœ¬é¡¹ç›®ä½¿ç”¨ Google C++ ä»£ç é£æ ¼ï¼Œæäº¤ä»£ç æ—¶è¯·ç¡®ä¿ç¬¦åˆè§„èŒƒ
å¼€å‘è€…æ–‡æ¡£
è‡ªå®šä¹‰å¼€å‘æ¿æŒ‡å—
- å­¦ä¹ å¦‚ä½•ä¸ºå°æ™º AI åˆ›å»ºè‡ªå®šä¹‰å¼€å‘æ¿
MCP åè®®ç‰©è”ç½‘æ§åˆ¶ç”¨æ³•è¯´æ˜
- äº†è§£å¦‚ä½•é€šè¿‡ MCP åè®®æ§åˆ¶ç‰©è”ç½‘è®¾å¤‡
MCP åè®®äº¤äº’æµç¨‹
- è®¾å¤‡ç«¯ MCP åè®®çš„å®ç°æ–¹å¼
MQTT + UDP æ··åˆé€šä¿¡åè®®æ–‡æ¡£
ä¸€ä»½è¯¦ç»†çš„ WebSocket é€šä¿¡åè®®æ–‡æ¡£
å¤§æ¨¡å‹é…ç½®
å¦‚æœä½ å·²ç»æ‹¥æœ‰ä¸€ä¸ªå°æ™º AI èŠå¤©æœºå™¨äººè®¾å¤‡ï¼Œå¹¶ä¸”å·²æ¥å…¥å®˜æ–¹æœåŠ¡å™¨ï¼Œå¯ä»¥ç™»å½•
xiaozhi.me
æ§åˆ¶å°è¿›è¡Œé…ç½®ã€‚
ğŸ‘‰
åå°æ“ä½œè§†é¢‘æ•™ç¨‹ï¼ˆæ—§ç‰ˆç•Œé¢ï¼‰
ç›¸å…³å¼€æºé¡¹ç›®
åœ¨ä¸ªäººç”µè„‘ä¸Šéƒ¨ç½²æœåŠ¡å™¨ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹ç¬¬ä¸‰æ–¹å¼€æºçš„é¡¹ç›®ï¼š
xinnan-tech/xiaozhi-esp32-server
Python æœåŠ¡å™¨
joey-zhou/xiaozhi-esp32-server-java
Java æœåŠ¡å™¨
AnimeAIChat/xiaozhi-server-go
Golang æœåŠ¡å™¨
ä½¿ç”¨å°æ™ºé€šä¿¡åè®®çš„ç¬¬ä¸‰æ–¹å®¢æˆ·ç«¯é¡¹ç›®ï¼š
huangjunsen0406/py-xiaozhi
Python å®¢æˆ·ç«¯
TOM88812/xiaozhi-android-client
Android å®¢æˆ·ç«¯
100askTeam/xiaozhi-linux
ç™¾é—®ç§‘æŠ€æä¾›çš„ Linux å®¢æˆ·ç«¯
78/xiaozhi-sf32
æ€æ¾ˆç§‘æŠ€çš„è“ç‰™èŠ¯ç‰‡å›ºä»¶
QuecPython/solution-xiaozhiAI
ç§»è¿œæä¾›çš„ QuecPython å›ºä»¶
å…³äºé¡¹ç›®
è¿™æ˜¯ä¸€ä¸ªç”±è™¾å“¥å¼€æºçš„ ESP32 é¡¹ç›®ï¼Œä»¥ MIT è®¸å¯è¯å‘å¸ƒï¼Œå…è®¸ä»»ä½•äººå…è´¹ä½¿ç”¨ï¼Œä¿®æ”¹æˆ–ç”¨äºå•†ä¸šç”¨é€”ã€‚
æˆ‘ä»¬å¸Œæœ›é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œèƒ½å¤Ÿå¸®åŠ©å¤§å®¶äº†è§£ AI ç¡¬ä»¶å¼€å‘ï¼Œå°†å½“ä¸‹é£é€Ÿå‘å±•çš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨åˆ°å®é™…çš„ç¡¬ä»¶è®¾å¤‡ä¸­ã€‚
å¦‚æœä½ æœ‰ä»»ä½•æƒ³æ³•æˆ–å»ºè®®ï¼Œè¯·éšæ—¶æå‡º Issues æˆ–åŠ å…¥ QQ ç¾¤ï¼š1011329060
Star History
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 107
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 19,437

### References
- https://github.com/78/xiaozhi-esp32

## evershopcommerce/evershop

### Executive Summary
- ğŸ›ï¸ Typescript E-commerce Platform
- ---
- EverShop
Documentation
|
Demo
Introduction
EverShop is a modern, TypeScript-first eCommerce platform built with GraphQL and React. Designed for developers, it offers essential commerce features in a modular, fully customizable architectureâ€”perfect for building tailored shopping experiences with confidence and speed.
Installation Using Docker
You can get started with EverShop in minutes by using the Docker image. The Docker image is a great way to get started with EverShop without having to worry about installing dependencies or configuring your environment.
curl -sSL https://raw.githubusercontent.com/evershopcommerce/evershop/main/docker-compose.yml
>
docker-compose.yml
docker-compose up -d
For the full installation guide, please refer to our
Installation guide
.
Documentation
Installation guide
.
Extension development
.
Theme development
.
Demo
Explore our demo store.
Demo user:
Email:
demo@evershop.io
Password: 123456
Support
If you like my work, feel free to:
â­ this repository. It helps.
about EverShop. Thank you!
Contributing
EverShop is an open-source project. We are committed to a fully transparent development process and appreciate highly any contributions. Whether you are helping us fix bugs, proposing new features, improving our documentation or spreading the word - we would love to have you as part of the EverShop community.
Ask a question about EverShop
You can ask questions, and participate in discussions about EverShop-related topics in the EverShop Discord channel.
Create a bug report
If you see an error message or run into an issue, please
create bug report
. This effort is valued and it will help all EverShop users.
Submit a feature request
If you have an idea, or you're missing a capability that would make development easier and more robust, please
Submit feature request
.
If a similar feature request already exists, don't forget to leave a "+1".
If you add some more information such as your thoughts and vision about the feature, your comments will be embraced warmly :)
Please refer to our
Contribution Guidelines
and
Code of Conduct
.
License
GPL-3.0 License
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 99
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 6,383

### References
- https://github.com/evershopcommerce/evershop

## rizinorg/cutter

### Executive Summary
- Free and Open Source Reverse Engineering Platform powered by rizin
- ---
- Cutter
Cutter is a free and open-source reverse engineering platform powered by
rizin
. It aims at being an advanced and customizable reverse engineering platform while keeping the user experience in mind. Cutter is created by reverse engineers for reverse engineers.
Learn more at
cutter.re
.
Getting Cutter
Download
Cutter release binaries for all major platforms (Linux, macOS, Windows) can be downloaded from
GitHub Releases
.
Linux
: If your distribution provides it, check for
cutter
package in your package manager (or
cutter-re
/
rz-cutter
). If not available there, we have setup repositories in
OBS
for some common distributions. Look at
https://software.opensuse.org/package/cutter-re
and follow the instructions there. Otherwise download the
.AppImage
file from our release, make it executable and run as below or use
AppImageLauncher
.
chmod +x Cutter*.AppImage; ./Cutter*.AppImage
macOS
: Download the
.dmg
file or use
Homebrew Cask
:
brew install --cask cutter
Windows
: Download the
.zip
archive, or use either
Chocolatey
or
Scoop
:
choco install cutter
scoop bucket add extras
followed by
scoop install cutter
Build from sources
To build Cutter from sources, please check the
Building Docs
.
Docker image
To deploy
cutter
using a pre-built
Dockerfile
, it's possible to use the
provided configuration
. The corresponding
README.md
file also contains instructions on how to get started using the docker image with minimal effort.
Documentation
User Guide
Contribution Guidelines
Developers Docs
Plugins
Cutter supports both Python and Native C++ plugins.
Our community has built many plugins and useful scripts for Cutter such as the native integration of
Ghidra decompiler
or the plugin to visualize DynamoRIO code coverage. You can find a list of cutter plugins linked below. Feel free to extend it with your own plugins and scripts for Cutter.
Official & Community Plugins
Plugins Development Guide
Getting Help
Please use the following channels to ask for help from Cutter developers and community:
Telegram:
https://t.me/cutter_re
Mattermost:
https://im.rizin.re
IRC:
#cutter on
https://web.libera.chat/
Twitter:
@cutter_re
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 97
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 17,754

### References
- https://github.com/rizinorg/cutter

## xyflow/xyflow

### Executive Summary
- React Flow |Â Svelte Flow - Powerful open source libraries for building node-based UIs with React (https://reactflow.dev) or Svelte (https://svelteflow.dev). Ready out-of-the-box and infinitely customizable.
- ---
- Powerful open source libraries for building node-based UIs with React or Svelte. Ready out-of-the-box and infinitely customizable.
React Flow
Â·
Svelte Flow
Â·
React Flow Pro
Â·
Discord
The xyflow mono repo
The xyflow repository is the home of four packages:
React Flow 12
@xyflow/react
packages/react
React Flow 11
reactflow
v11 branch
Svelte Flow
@xyflow/svelte
packages/svelte
Shared helper library
@xyflow/system
packages/system
Commercial usage
Are you using React Flow or Svelte Flow for a personal project?
Great! No sponsorship needed, you can support us by reporting any bugs you find, sending us screenshots of your projects, and starring us on Github ğŸŒŸ
Are you using React Flow or Svelte Flow at your organization and making money from it?
Awesome! We rely on your support to keep our libraries developed and maintained under an MIT License, just how we like it. For React Flow you can do that on the
React Flow Pro website
and for both of our libraries you can do it through
Github Sponsors
.
Getting started
The best way to get started is to check out the
React Flow
or
Svelte Flow
learn section. However if you want to get a sneak peek of how to install and use the libraries you can see it here:
React Flow
basic usage
Installation
npm install @xyflow/react
Basic usage
import
{
useCallback
}
from
'react'
;
import
{
ReactFlow
,
MiniMap
,
Controls
,
Background
,
useNodesState
,
useEdgesState
,
addEdge
,
}
from
'@xyflow/react'
;
import
'@xyflow/react/dist/style.css'
;
const
initialNodes
=
[
{
id
:
'1'
,
position
:
{
x
:
0
,
y
:
0
}
,
data
:
{
label
:
'1'
}
}
,
{
id
:
'2'
,
position
:
{
x
:
0
,
y
:
100
}
,
data
:
{
label
:
'2'
}
}
,
]
;
const
initialEdges
=
[
{
id
:
'e1-2'
,
source
:
'1'
,
target
:
'2'
}
]
;
function
Flow
(
)
{
const
[
nodes
,
setNodes
,
onNodesChange
]
=
useNodesState
(
initialNodes
)
;
const
[
edges
,
setEdges
,
onEdgesChange
]
=
useEdgesState
(
initialEdges
)
;
const
onConnect
=
useCallback
(
(
params
)
=>
setEdges
(
(
eds
)
=>
addEdge
(
params
,
eds
)
)
,
[
setEdges
]
)
;
return
(
<
ReactFlow
nodes
=
{
nodes
}
edges
=
{
edges
}
onNodesChange
=
{
onNodesChange
}
onEdgesChange
=
{
onEdgesChange
}
onConnect
=
{
onConnect
}
>
<
MiniMap
/>
<
Controls
/>
<
Background
/>
</
ReactFlow
>
)
;
}
export
default
Flow
;
Svelte Flow
basic usage
Installation
npm install @xyflow/svelte
Basic usage
<
script
lang
=
"
ts
"
>
import
{
writable
}
from
'
svelte/store
'
;
import
{
SvelteFlow
,
Controls
,
Background
,
BackgroundVariant
,
MiniMap
,
}
from
'
@xyflow/svelte
'
;
import
'
@xyflow/svelte/dist/style.css
'
const
nodes
=
writable
([
{
id:
'
1
'
,
type:
'
input
'
,
data: { label:
'
Input Node
'
},
position: { x:
0
, y:
0
}
},
{
id:
'
2
'
,
type:
'
custom
'
,
data: { label:
'
Node
'
},
position: { x:
0
, y:
150
}
}
]);
const
edges
=
writable
([
{
id:
'
1-2
'
,
type:
'
default
'
,
source:
'
1
'
,
target:
'
2
'
,
label:
'
Edge Text
'
}
]);
</
script
>

<
SvelteFlow
{
nodes
}
{
edges
}
fitView
on:nodeclick
={(
event
)
=>
console
.
log
(
'
on node click
'
,
event
)}
>
<
Controls
/>
<
Background
variant
={
BackgroundVariant
.
Dots
} />
<
MiniMap
/>
</
SvelteFlow
>
Releases
For releasing packages we are using
changesets
in combination with the
changeset Github action
. The rough idea is:
create PRs for new features, updates and fixes (with a changeset if relevant for changelog)
merge into main
changset creates a PR that bumps all packages based on the changesets
merge changeset PR if you want to release to Github and npm
Built by
xyflow
React Flow and Svelte Flow are maintained by the
xyflow team
. If you need help or want to talk to us about a collaboration, reach out through our
contact form
or by joining our
Discord Server
.
License
React Flow and Svelte Flow are
MIT licensed
.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 92
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 32,452

### References
- https://github.com/xyflow/xyflow

## rust-lang/rustfmt

### Executive Summary
- Format Rust code
- ---
- rustfmt
A tool for formatting Rust code according to style guidelines.
If you'd like to help out (and you should, it's a fun project!), see
Contributing.md
and our
Code of
Conduct
.
You can use rustfmt in Travis CI builds. We provide a minimal Travis CI
configuration (see
here
).
Quick start
You can run
rustfmt
with Rust 1.24 and above.
On the Stable toolchain
To install:
rustup component add rustfmt
To run on a cargo project in the current working directory:
cargo fmt
On the Nightly toolchain
For the latest and greatest
rustfmt
, nightly is required.
To install:
rustup component add rustfmt --toolchain nightly
To run on a cargo project in the current working directory:
cargo +nightly fmt
Limitations
Rustfmt tries to work on as much Rust code as possible. Sometimes, the code
doesn't even need to compile! In general, we are looking to limit areas of
instability; in particular, post-1.0, the formatting of most code should not
change as Rustfmt improves. However, there are some things that Rustfmt can't
do or can't do well (and thus where formatting might change significantly,
even post-1.0). We would like to reduce the list of limitations over time.
The following list enumerates areas where Rustfmt does not work or where the
stability guarantees do not apply (we don't make a distinction between the two
because in the future Rustfmt might work on code where it currently does not):
a program where any part of the program does not parse (parsing is an early
stage of compilation and in Rust includes macro expansion).
Macro declarations and uses (current status: some macro declarations and uses
are formatted).
Comments, including any AST node with a comment 'inside' (Rustfmt does not
currently attempt to format comments, it does format code with comments inside, but that formatting may change in the future).
Rust code in code blocks in comments.
Any fragment of a program (i.e., stability guarantees only apply to whole
programs, even where fragments of a program can be formatted today).
Code containing non-ascii unicode characters (we believe Rustfmt mostly works
here, but do not have the test coverage or experience to be 100% sure).
Bugs in Rustfmt (like any software, Rustfmt has bugs, we do not consider bug
fixes to break our stability guarantees).
Running
You can run Rustfmt by just typing
rustfmt filename
if you used
cargo install
. This runs rustfmt on the given file, if the file includes out of line
modules, then we reformat those too. So to run on a whole module or crate, you
just need to run on the root file (usually mod.rs or lib.rs). Rustfmt can also
read data from stdin. Alternatively, you can use
cargo fmt
to format all
binary and library targets of your crate.
You can run
rustfmt --help
for information about available arguments.
The easiest way to run rustfmt against a project is with
cargo fmt
.
cargo fmt
works on both
single-crate projects and
cargo workspaces
.
Please see
cargo fmt --help
for usage information.
You can specify the path to your own
rustfmt
binary for cargo to use by setting the
RUSTFMT
environment variable. This was added in v1.4.22, so you must have this version or newer to leverage this feature (
cargo fmt --version
)
Running
rustfmt
directly
To format individual files or arbitrary codes from stdin, the
rustfmt
binary should be used. Some
examples follow:
rustfmt lib.rs main.rs
will format "lib.rs" and "main.rs" in place
rustfmt
will read a code from stdin and write formatting to stdout
echo "fn     main() {}" | rustfmt
would emit "fn main() {}".
For more information, including arguments and emit options, see
rustfmt --help
.
Verifying code is formatted
When running with
--check
, Rustfmt will exit with
0
if Rustfmt would not
make any formatting changes to the input, and
1
if Rustfmt would make changes.
In other modes, Rustfmt will exit with
1
if there was some error during
formatting (for example a parsing or internal error) and
0
if formatting
completed without error (whether or not changes were made).
Running Rustfmt from your editor
Vim
Emacs
Sublime Text 3
Atom
Visual Studio Code
IntelliJ or CLion
Checking style on a CI server
To keep your code base consistently formatted, it can be helpful to fail the CI build
when a pull request contains unformatted code. Using
--check
instructs
rustfmt to exit with an error code if the input is not formatted correctly.
It will also print any found differences. (Older versions of Rustfmt don't
support
--check
, use
--write-mode diff
).
A minimal Travis setup could look like this (requires Rust 1.31.0 or greater):
language
:
rust
before_script
:
-
rustup component add rustfmt
script
:
-
cargo build
-
cargo test
-
cargo fmt --all -- --check
See
this blog post
for more info.
How to build and test
cargo build
to build.
cargo test
to run all tests.
To run rustfmt after this, use
cargo run --bin rustfmt -- filename
. See the
notes above on running rustfmt.
Configuring Rustfmt
Rustfmt is designed to be very configurable. You can create a TOML file called
rustfmt.toml
or
.rustfmt.toml
, place it in the project or any other parent
directory and it will apply the options in that file. See
rustfmt --help=config
for the options which are available, or if you prefer to see
visual style previews,
GitHub page
.
By default, Rustfmt uses a style which conforms to the
Rust style guide
that has been formalized through the
style RFC
process
.
Configuration options are either stable or unstable. Stable options can always
be used, while unstable ones are only available on a nightly toolchain, and opt-in.
See
GitHub page
for details.
Rust's Editions
The
edition
option determines the Rust language edition used for parsing the code. This is important for syntax compatibility but does not directly control formatting behavior (see
Style Editions
).
When running
cargo fmt
, the
edition
is automatically read from the
Cargo.toml
file. However, when running
rustfmt
directly, the
edition
defaults to 2015. For consistent parsing between rustfmt and
cargo fmt
, you should configure the
edition
in your
rustfmt.toml
file:
edition
=
"
2018
"
Style Editions
This option is inferred from the
edition
if not specified.
See
Rust Style Editions
for details on formatting differences between style editions.
rustfmt has a default style edition of
2015
while
cargo fmt
infers the style edition from the
edition
set in
Cargo.toml
. This can lead to inconsistencies between
rustfmt
and
cargo fmt
if the style edition is not explicitly configured.
To ensure consistent formatting, it is recommended to specify the
style_edition
in a
rustfmt.toml
configuration file. For example:
style_edition
=
"
2024
"
Tips
To ensure consistent parsing between
cargo fmt
and
rustfmt
, you should configure the
edition
in your
rustfmt.toml
file.
To ensure consistent formatting between
cargo fmt
and
rustfmt
, you should configure the
style_edition
in your
rustfmt.toml
file.
For things you do not want rustfmt to mangle, use
#[rustfmt::skip]
To prevent rustfmt from formatting a macro or an attribute,
use
#[rustfmt::skip::macros(target_macro_name)]
or
#[rustfmt::skip::attributes(target_attribute_name)]
Example:
#!
[
rustfmt
::
skip
::
attributes
(
custom_attribute
)
]
#
[
custom_attribute
(
formatting
,
here
,
should
,
be
,
Skipped
)
]
#
[
rustfmt
::
skip
::
macros
(
html
)
]
fn
main
(
)
{
let
macro_result1 =
html
!
{
<div>
Hello
</div>
}
.
to_string
(
)
;
When you run rustfmt, place a file named
rustfmt.toml
or
.rustfmt.toml
in
target file directory or its parents to override the default settings of
rustfmt. You can generate a file containing the default configuration with
rustfmt --print-config default rustfmt.toml
and customize as needed.
After successful compilation, a
rustfmt
executable can be found in the
target directory.
If you're having issues compiling Rustfmt (or compile errors when trying to
install), make sure you have the most recent version of Rust installed.
You can change the way rustfmt emits the changes with the --emit flag:
Example:
cargo fmt -- --emit files
Options:
Flag
Description
Nightly Only
files
overwrites output to files
No
stdout
writes output to stdout
No
coverage
displays how much of the input file was processed
Yes
checkstyle
emits in a checkstyle format
Yes
json
emits diffs in a json format
Yes
License
Rustfmt is distributed under the terms of both the MIT license and the
Apache License (Version 2.0).
See
LICENSE-APACHE
and
LICENSE-MIT
for details.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 81
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 6,583

### References
- https://github.com/rust-lang/rustfmt

## tile-ai/tilelang

### Executive Summary
- Domain-specific language designed to streamline the development of high-performance GPU/CPU/Accelerators kernels
- ---
- Tile Language
Tile Language (
tile-lang
) is a concise domain-specific language designed to streamline the development of high-performance GPU/CPU kernels (e.g., GEMM, Dequant GEMM, FlashAttention, LinearAttention). By employing a Pythonic syntax with an underlying compiler infrastructure on top of
TVM
, tile-lang allows developers to focus on productivity without sacrificing the low-level optimizations necessary for state-of-the-art performance.
Latest News
10/07/2025 ğŸ: Added Apple Metal Device support, check out
Pull Request #799
for details.
09/29/2025  ğŸ‰: Thrilled to announce that â€‹â€‹AscendCâ€‹â€‹ and â€‹Ascendâ€‹NPU IRâ€‹â€‹ backends targeting Huawei Ascend chips are now supported!
Check out the preview here:
ğŸ”—
link
.
This includes implementations across two branches:
ascendc_pto
and
npuir
.
Feel free to explore and share your feedback!
07/04/2025 ğŸš€: Introduced
T.gemm_sp
for 2:4 sparse tensor core support, check out
Pull Request #526
for details.
06/05/2025 âœ¨: Added
NVRTC Backend
to significantly reduce compilation time for cute templates!
04/14/2025 ğŸš€: Added high-performance FlashMLA implementation for AMD MI300X, achieving performance parity with hand-optimized assembly kernels of Aiter! See
example_mla_amd
for details.
03/03/2025 ğŸš€: Added high-performance MLA Decoding support using only 80 lines of Python code, achieving performance on par with FlashMLA on H100 (see
example_mla_decode.py
)! We also provide
documentation
explaining how TileLang achieves this.
02/15/2025 âœ¨: Added WebGPU Codegen support, see
Pull Request #86
!
02/12/2025 âœ¨: Excited to announce the release of
v0.1.0
!
02/10/2025 ğŸš€: Added debug tools for TileLangâ€”
T.print
for printing variables/buffers (
docs
) and a memory layout plotter (
examples/plot_layout
).
01/20/2025 âœ¨: We are excited to announce that tile-lang, a dsl for high performance AI workloads, is now open source and available to the public!
Tested Devices
Although tile-lang aims to be portable across a range of Devices, it has been specifically tested and validated on the following devices: for NVIDIA GPUs, this includes the H100 (with Auto TMA/WGMMA support), A100, V100, RTX 4090, RTX 3090, and RTX A6000; for AMD GPUs, it includes the MI250 (with Auto MatrixCore support) and the MI300X (with Async Copy support).
OP Implementation Examples
tile-lang
provides the building blocks to implement a wide variety of operators. Some examples include:
Matrix Multiplication
Dequantization GEMM
Flash Attention
Flash Linear Attention
Flash MLA Decoding
Native Sparse Attention
Within the
examples
directory, you will also find additional complex kernelsâ€”such as convolutions, forward/backward passes for FlashAttention, more operators will continuously be added.
Benchmark Summary
TileLang achieves exceptional performance across a variety of computational patterns. Comprehensive benchmark scripts and settings are available at
tilelang-benchmark
. Below are selected results showcasing its capabilities:
MLA Decoding Performance on H100
Flash Attention Performance on H100
Matmul Performance on GPUs (RTX 4090, A100, H100, MI300X)
Dequantize Matmul Performance on A100
Installation
Method 1: Install with Pip
The quickest way to get started is to install the latest release from PyPI:
pip install tilelang
Alternatively, you can install directly from the GitHub repository:
pip install git+https://github.com/tile-ai/tilelang
Or install locally:
#
install required system dependencies
sudo apt-get update
sudo apt-get install -y python3-setuptools gcc libtinfo-dev zlib1g-dev build-essential cmake libedit-dev libxml2-dev

pip install -e
.
-v
#
remove -e option if you don't want to install in editable mode, -v for verbose output
Method 2: Build from Source
We currently provide three ways to install
tile-lang
from source:
Install from Source (using your own TVM installation)
Install from Source (using the bundled TVM submodule)
Install Using the Provided Script
Method 3: Install with Nightly Version
For users who want access to the latest features and improvements before official releases, we provide nightly builds of
tile-lang
.
pip install tilelang -f https://tile-ai.github.io/whl/nightly/cu121/
#
or pip install tilelang --find-links https://tile-ai.github.io/whl/nightly/cu121/
Note:
Nightly builds contain the most recent code changes but may be less stable than official releases. They're ideal for testing new features or if you need a specific bugfix that hasn't been released yet.
Quick Start
In this section, you'll learn how to write and execute a straightforward GEMM (matrix multiplication) kernel using tile-lang, followed by techniques for layout optimizations, pipelining, and L2-cacheâ€“friendly swizzling.
GEMM Example with Annotations (Layout, L2 Cache Swizzling, and Pipelining, etc.)
Below is an example that demonstrates more advanced features: layout annotation, parallelized copy, and swizzle for improved L2 cache locality. This snippet shows how to adapt your kernel to maximize performance on complex hardware.
import
tilelang
import
tilelang
.
language
as
T
# @tilelang.jit(target="cuda")
# target currently can be "cuda" or "hip" or "cpu".
# if not specified, it will be inferred from the input tensors during compile time
@
tilelang
.
jit
def
matmul
(
M
,
N
,
K
,
block_M
,
block_N
,
block_K
,
dtype
=
"float16"
,
accum_dtype
=
"float"
):
@
T
.
prim_func
def
matmul_relu_kernel
(
A
:
T
.
Tensor
((
M
,
K
),
dtype
),
B
:
T
.
Tensor
((
K
,
N
),
dtype
),
C
:
T
.
Tensor
((
M
,
N
),
dtype
),
    ):
# Initialize Kernel Context
with
T
.
Kernel
(
T
.
ceildiv
(
N
,
block_N
),
T
.
ceildiv
(
M
,
block_M
),
threads
=
128
)
as
(
bx
,
by
):
A_shared
=
T
.
alloc_shared
((
block_M
,
block_K
),
dtype
)
B_shared
=
T
.
alloc_shared
((
block_K
,
block_N
),
dtype
)
C_local
=
T
.
alloc_fragment
((
block_M
,
block_N
),
accum_dtype
)
# Enable rasterization for better L2 cache locality (Optional)
# T.use_swizzle(panel_size=10, enable=True)
# Clear local accumulation
T
.
clear
(
C_local
)
for
ko
in
T
.
Pipelined
(
T
.
ceildiv
(
K
,
block_K
),
num_stages
=
3
):
# Copy tile of A
# This is a sugar syntax for parallelized copy
T
.
copy
(
A
[
by
*
block_M
,
ko
*
block_K
],
A_shared
)
# Copy tile of B
T
.
copy
(
B
[
ko
*
block_K
,
bx
*
block_N
],
B_shared
)
# Perform a tile-level GEMM on the shared buffers
# Currently we dispatch to the cute/hip on Nvidia/AMD GPUs
T
.
gemm
(
A_shared
,
B_shared
,
C_local
)
# relu
for
i
,
j
in
T
.
Parallel
(
block_M
,
block_N
):
C_local
[
i
,
j
]
=
T
.
max
(
C_local
[
i
,
j
],
0
)
# Copy result back to global memory
T
.
copy
(
C_local
,
C
[
by
*
block_M
,
bx
*
block_N
])
return
matmul_relu_kernel
M
=
1024
# M = T.symbolic("m") if you want to use dynamic shape
N
=
1024
K
=
1024
block_M
=
128
block_N
=
128
block_K
=
32
# 1. Define the kernel (matmul) and compile/lower it into an executable module
matmul_relu_kernel
=
matmul
(
M
,
N
,
K
,
block_M
,
block_N
,
block_K
)
# 3. Test the kernel in Python with PyTorch data
import
torch
# Create random input tensors on the GPU
a
=
torch
.
randn
(
M
,
K
,
device
=
"cuda"
,
dtype
=
torch
.
float16
)
b
=
torch
.
randn
(
K
,
N
,
device
=
"cuda"
,
dtype
=
torch
.
float16
)
c
=
torch
.
empty
(
M
,
N
,
device
=
"cuda"
,
dtype
=
torch
.
float16
)
# Run the kernel through the Profiler
matmul_relu_kernel
(
a
,
b
,
c
)
print
(
c
)
# Reference multiplication using PyTorch
ref_c
=
torch
.
relu
(
a
@
b
)
# Validate correctness
torch
.
testing
.
assert_close
(
c
,
ref_c
,
rtol
=
1e-2
,
atol
=
1e-2
)
print
(
"Kernel output matches PyTorch reference."
)
# 4. Retrieve and inspect the generated CUDA source (optional)
# cuda_source = jit_kernel.get_kernel_source()
# print("Generated CUDA kernel:\n", cuda_source)
# 5.Profile latency with kernel
profiler
=
matmul_relu_kernel
.
get_profiler
(
tensor_supply_type
=
tilelang
.
TensorSupplyType
.
Normal
)
latency
=
profiler
.
do_bench
()
print
(
f"Latency:
{
latency
}
ms"
)
Dive Deep into TileLang Beyond GEMM
In addition to GEMM, we provide a variety of examples to showcase the versatility and power of TileLang, including:
Dequantize GEMM
: Achieve high-performance dequantization by
fine-grained control over per-thread operations
, with many features now adopted as default behaviors in
BitBLAS
, which utilizing magic layout transformation and intrins to accelerate dequantize gemm.
FlashAttention
: Enable cross-operator fusion with simple and intuitive syntax, and we also provide an example of auto tuning.
LinearAttention
: Examples include RetNet and Mamba implementations.
Convolution
: Implementations of Convolution with IM2Col.
Upcoming Features
Check our
tilelang v0.2.0 release plan
for upcoming features.
TileLang has now been used in project
BitBLAS
and
AttentionEngine
.
Join the Discussion
Welcome to join our Discord community for discussions, support, and collaboration!
Acknowledgements
We would like to express our gratitude to the
TVM
community for their invaluable contributions. The initial version of this project was mainly developed by
LeiWang1999
,
chengyupku
and
nox-410
with supervision from Prof.
Zhi Yang
at Peking University. Part of this work was carried out during an internship at Microsoft Research, where Dr. Lingxiao Ma, Dr. Yuqing Xia, Dr. Jilong Xue, and Dr. Fan Yang offered valuable advice and support. We deeply appreciate their mentorship and contributions.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 73
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 3,425

### References
- https://github.com/tile-ai/tilelang

## vllm-project/vllm

### Executive Summary
- A high-throughput and memory-efficient inference and serving engine for LLMs
- ---
- Easy, fast, and cheap LLM serving for everyone
|
Documentation
|
Blog
|
Paper
|
Twitter/X
|
User Forum
|
Developer Slack
|
Join us at the
PyTorch Conference, October 22-23
and
Ray Summit, November 3-5
in San Francisco for our latest updates on vLLM and to meet the vLLM team! Register now for the largest vLLM community events of the year!
Latest News
ğŸ”¥
[2025/09] We hosted
vLLM Toronto Meetup
focused on tackling inference at scale and speculative decoding with speakers from NVIDIA and Red Hat! Please find the meetup slides
here
.
[2025/08] We hosted
vLLM Shenzhen Meetup
focusing on the ecosystem around vLLM! Please find the meetup slides
here
.
[2025/08] We hosted
vLLM Singapore Meetup
. We shared V1 updates, disaggregated serving and MLLM speedups with speakers from Embedded LLM, AMD, WekaIO, and A*STAR. Please find the meetup slides
here
.
[2025/08] We hosted
vLLM Shanghai Meetup
focusing on building, developing, and integrating with vLLM! Please find the meetup slides
here
.
[2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement
here
.
[2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post
here
.
Previous News
[2025/08] We hosted
vLLM Korea Meetup
with Red Hat and Rebellions! We shared the latest advancements in vLLM along with project spotlights from the vLLM Korea community. Please find the meetup slides
here
.
[2025/08] We hosted
vLLM Beijing Meetup
focusing on large-scale LLM deployment! Please find the meetup slides
here
and the recording
here
.
[2025/05] We hosted
NYC vLLM Meetup
! Please find the meetup slides
here
.
[2025/04] We hosted
Asia Developer Day
! Please find the meetup slides from the vLLM team
here
.
[2025/03] We hosted
vLLM x Ollama Inference Night
! Please find the meetup slides from the vLLM team
here
.
[2025/03] We hosted
the first vLLM China Meetup
! Please find the meetup slides from vLLM team
here
.
[2025/03] We hosted
the East Coast vLLM Meetup
! Please find the meetup slides
here
.
[2025/02] We hosted
the ninth vLLM meetup
with Meta! Please find the meetup slides from vLLM team
here
and AMD
here
. The slides from Meta will not be posted.
[2025/01] We hosted
the eighth vLLM meetup
with Google Cloud! Please find the meetup slides from vLLM team
here
, and Google Cloud team
here
.
[2024/12] vLLM joins
pytorch ecosystem
! Easy, Fast, and Cheap LLM Serving for Everyone!
[2024/11] We hosted
the seventh vLLM meetup
with Snowflake! Please find the meetup slides from vLLM team
here
, and Snowflake team
here
.
[2024/10] We have just created a developer slack (
slack.vllm.ai
) focusing on coordinating contributions and discussing features. Please feel free to join us there!
[2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team
here
. Learn more from the
talks
from other vLLM contributors and users!
[2024/09] We hosted
the sixth vLLM meetup
with NVIDIA! Please find the meetup slides
here
.
[2024/07] We hosted
the fifth vLLM meetup
with AWS! Please find the meetup slides
here
.
[2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post
here
.
[2024/06] We hosted
the fourth vLLM meetup
with Cloudflare and BentoML! Please find the meetup slides
here
.
[2024/04] We hosted
the third vLLM meetup
with Roblox! Please find the meetup slides
here
.
[2024/01] We hosted
the second vLLM meetup
with IBM! Please find the meetup slides
here
.
[2023/10] We hosted
the first vLLM meetup
with a16z! Please find the meetup slides
here
.
[2023/08] We would like to express our sincere gratitude to
Andreessen Horowitz
(a16z) for providing a generous grant to support the open-source development and research of vLLM.
[2023/06] We officially released vLLM! FastChat-vLLM integration has powered
LMSYS Vicuna and Chatbot Arena
since mid-April. Check out our
blog post
.
About
vLLM is a fast and easy-to-use library for LLM inference and serving.
Originally developed in the
Sky Computing Lab
at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.
vLLM is fast with:
State-of-the-art serving throughput
Efficient management of attention key and value memory with
PagedAttention
Continuous batching of incoming requests
Fast model execution with CUDA/HIP graph
Quantizations:
GPTQ
,
AWQ
,
AutoRound
, INT4, INT8, and FP8
Optimized CUDA kernels, including integration with FlashAttention and FlashInfer
Speculative decoding
Chunked prefill
vLLM is flexible and easy to use with:
Seamless integration with popular Hugging Face models
High-throughput serving with various decoding algorithms, including
parallel sampling
,
beam search
, and more
Tensor, pipeline, data and expert parallelism support for distributed inference
Streaming outputs
OpenAI-compatible API server
Support for NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, and TPU. Additionally, support for diverse hardware plugins such as Intel Gaudi, IBM Spyre and Huawei Ascend.
Prefix caching support
Multi-LoRA support
vLLM seamlessly supports most popular open-source models on HuggingFace, including:
Transformer-like LLMs (e.g., Llama)
Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)
Embedding Models (e.g., E5-Mistral)
Multi-modal LLMs (e.g., LLaVA)
Find the full list of supported models
here
.
Getting Started
Install vLLM with
pip
or
from source
:
pip install vllm
Visit our
documentation
to learn more.
Installation
Quickstart
List of Supported Models
Contributing
We welcome and value any contributions and collaborations.
Please check out
Contributing to vLLM
for how to get involved.
Sponsors
vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!
Cash Donations:
a16z
Dropbox
Sequoia Capital
Skywork AI
ZhenFund
Compute Resources:
Alibaba Cloud
AMD
Anyscale
AWS
Crusoe Cloud
Databricks
DeepInfra
Google Cloud
Intel
Lambda Lab
Nebius
Novita AI
NVIDIA
Replicate
Roblox
RunPod
Trainy
UC Berkeley
UC San Diego
Volcengine
Slack Sponsor: Anyscale
We also have an official fundraising venue through
OpenCollective
. We plan to use the fund to support the development, maintenance, and adoption of vLLM.
Citation
If you use vLLM for your research, please cite our
paper
:
@inproceedings
{
kwon2023efficient
,
title
=
{
Efficient Memory Management for Large Language Model Serving with PagedAttention
}
,
author
=
{
Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica
}
,
booktitle
=
{
Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles
}
,
year
=
{
2023
}
}
Contact Us
For technical questions and feature requests, please use GitHub
Issues
For discussing with fellow users, please use the
vLLM Forum
For coordinating contributions and development, please use
Slack
For security disclosures, please use GitHub's
Security Advisories
feature
For collaborations and partnerships, please contact us at
vllm-questions@lists.berkeley.edu
Media Kit
If you wish to use vLLM's logo, please refer to
our media kit repo
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 70
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 59,787

### References
- https://github.com/vllm-project/vllm

## aaPanel/BillionMail

### Executive Summary
- BillionMail gives you open-source MailServer, NewsLetter, Email Marketing â€” fully self-hosted, dev-friendly, and free from monthly fees. Join the discord: https://discord.gg/asfXzBUhZr
- ---
- BillionMail ğŸ“§
An Open-Source MailServer, NewsLetter, Email Marketing Solution for Smarter Campaigns
English |
ç®€ä½“ä¸­æ–‡
|
æ—¥æœ¬èª
|
TÃ¼rkÃ§e
What is BillionMail?
BillionMail is a
future open-source Mail server, Email marketing platform
designed to help businesses and individuals manage their email campaigns with ease. Whether you're sending newsletters, promotional emails, or transactional messages, this tool will provide
full control
over your email marketing efforts. With features like
advanced analytics
, and
customer management
, you'll be able to create, send, and track emails like a pro.
Just 3 steps to send a billion emails!
Billion emails. Any business. Guaranteed.
Step 1ï¸âƒ£ Install BillionMail:
âœ… It takes
only 8ï¸âƒ£ minutes
from installation to
âœ… successful email sending
cd
/opt
&&
git clone https://github.com/aaPanel/BillionMail
&&
cd
BillionMail
&&
bash install.sh
Step 2ï¸âƒ£: Connect Your Domain
Add the sending domain
Verify DNS records
Auto-enable free SSL
Step 3ï¸âƒ£: Build Your Campaign
Write or paste your email
Choose list & tags
Set send time or send now
Watch on Youtube
Other installation methods
One-click installation on aaPanel
ğŸ‘‰
https://www.aapanel.com/new/download.html
(Log in to âœ…aaPanel --> ğŸ³Docker --> 1ï¸âƒ£OneClick install)
Docker
cd
/opt
&&
git clone https://github.com/aaPanel/BillionMail
&&
cd
BillionMail
&&
cp env_init .env
&&
docker compose up -d
||
docker-compose up -d
Management script
Management help
bm help
View Login default info
bm default
Show domain DNS record
bm show-record
Update BillionMail
bm update
Live Demo
BillionMail Demo:
https://demo.billionmail.com/billionmail
Username:
billionmail
Password:
billionmail
WebMail
BillionMail has integrated
RoundCube
, you can access WebMail via
/roundcube/
.
Why BillionMail?
Most email marketing platforms are either
expensive
,
closed-source
, or
lack essential features
. BillionMail aims to be different:
âœ…
Fully Open-Source
â€“ No hidden costs, no vendor lock-in.
ğŸ“Š
Advanced Analytics
â€“ Track email delivery, open rates, click-through rates, and more.
ğŸ“§
Unlimited Sending
â€“ No restrictions on the number of emails you can send.
ğŸ¨
Customizable Templates
â€“ Custom professional marketing templates for reuse.
ğŸ”’
Privacy-First
â€“ Your data stays with you, no third-party tracking.
ğŸš€
Self-Hosted
â€“ Run it on your own server for complete control.
How You Can Help ğŸŒŸ
BillionMail is a
community-driven project
, and we need your support to get started! Here's how you can help:
Star This Repository
: Show your interest by starring this repo.
Spread the Word
: Share BillionMail with your networkâ€”developers, marketers, and open-source enthusiasts.
Share Feedback
: Let us know what features you'd like to see in BillionMail by opening an issue or joining the discussion.
Contribute
: Once development begins, we'll welcome contributions from the community. Stay tuned for updates!
ğŸ“§
BillionMail â€“ The Future of Open-Source Email Marketing.
Issues
If you encounter any issues or have feature requests, please
open an issue
. Be sure to include:
A clear description of the problem or request.
Steps to reproduce the issue (if applicable).
Screenshots or error logs (if applicable).
Install Now:
âœ…It takes
only 8 minutes
from installation to
successful email sending
cd
/opt
&&
git clone https://github.com/aaPanel/BillionMail
&&
cd
BillionMail
&&
bash install.sh
Install with Docker:
(Please install Docker and docker-compose-plugin manually, and modify .env file)
cd
/opt
&&
git clone https://github.com/aaPanel/BillionMail
&&
cd
BillionMail
&&
cp env_init .env
&&
docker compose up -d
||
docker-compose up -d
Star History
License
BillionMail is licensed under the
AGPLv3 License
. This means you can:
âœ… Use the software for free.
âœ… Modify and distribute the code.
âœ… Use it privately without restrictions.
See the
LICENSE
file for more details.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 65
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 11,612

### References
- https://github.com/aaPanel/BillionMail

## supermemoryai/supermemory

### Executive Summary
- Memory engine and app that is extremely fast, scalable. The Memory API for the AI era.
- ---
- Features
Core Functionality
Add Memories from Any Content
: Easily add memories from URLs, PDFs, and plain textâ€”just paste, upload, or link.
Chat with Your Memories
: Converse with your stored content using natural language chat.
Supermemory MCP Integration
: Seamlessly connect with all major AI tools (Claude, Cursor, etc.) via Supermemory MCP.
How do i use this?
Go to
app.supermemory.ai
and sign into with your account
Start Adding Memory with your choice of format (Note, Link, File)
You can also Connect to your favourite services (Notion, Google Drive, OneDrive)
Once Memories are added, you can chat with Supermemory by clicking on "Open Chat" and retrieve info from your saved memories
Add MCP to your AI Tools (by clicking on "Connect to your AI" and select the AI tool you are trying to integrate)
Support
Have questions or feedback? We're here to help:
Email:
dhravya@supermemory.com
Documentation:
docs.supermemory.ai
Contributing
We welcome contributions from developers of all skill levels! Whether you're fixing bugs, adding features, or improving documentation, your help makes supermemory better for everyone.
Quick Start for Contributors
Fork and clone
the repository
Install dependencies
with
bun install
Set up your environment
by copying
.env.example
to
.env.local
Start developing
with
bun run dev
For detailed guidelines, development setup, coding standards, and the complete contribution workflow, please see our
Contributing Guide
.
Ways to Contribute
ğŸ›
Bug fixes
- Help us squash those pesky issues
âœ¨
New features
- Add functionality that users will love
ğŸ¨
UI/UX improvements
- Make the interface more intuitive
âš¡
Performance optimizations
- Help us make supermemory faster
Check out our
Issues
page for
good first issue
and
help wanted
labels to get started!
Updates & Roadmap
Stay up to date with the latest improvements:
Changelog
X
.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 62
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 11,473

### References
- https://github.com/supermemoryai/supermemory

## chen08209/FlClash

### Executive Summary
- A multi-platform proxy client based on ClashMeta,simple and easy to use, open-source and ad-free.
- ---
- ç®€ä½“ä¸­æ–‡
FlClash
A multi-platform proxy client based on ClashMeta, simple and easy to use, open-source and ad-free.
on Desktop:
on Mobile:
Features
âœˆï¸
Multi-platform: Android, Windows, macOS and Linux
ğŸ’» Adaptive multiple screen sizes, Multiple color themes available
ğŸ’¡ Based on Material You Design,
Surfboard
-like UI
â˜ï¸ Supports data sync via WebDAV
âœ¨ Support subscription link, Dark mode
Use
Linux
âš ï¸
Make sure to install the following dependencies before using them
sudo apt-get install libayatana-appindicator3-dev
 sudo apt-get install libkeybinder-3.0-dev
Android
Support the following actions
com.follow.clash.action.START
 
 com.follow.clash.action.STOP
 
 com.follow.clash.action.TOGGLE
Download
Build
Update submodules
git submodule update --init --recursive
Install
Flutter
and
Golang
environment
Build Application
android
Install
Android SDK
,
Android NDK
Set
ANDROID_NDK
environment variables
Run Build script
dart .
\s
etup.dart android
windows
You need a windows client
Install
Gcc
ï¼Œ
Inno Setup
Run build script
dart .
\s
etup.dart windows --arch
<
arm64
|
amd
64>
linux
You need a linux client
Run build script
dart .
\s
etup.dart linux --arch
<
arm64
|
amd
64>
macOS
You need a macOS client
Run build script
dart .
\s
etup.dart macos --arch
<
arm64
|
amd
64>
Star
The easiest way to support developers is to click on the star (â­) at the top of the page.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 61
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 23,044

### References
- https://github.com/chen08209/FlClash

## jiji262/douyin-downloader

### Executive Summary
- æŠ–éŸ³æ‰¹é‡ä¸‹è½½å·¥å…·ï¼Œå»æ°´å°ï¼Œæ”¯æŒè§†é¢‘ã€å›¾é›†ã€åˆé›†ã€éŸ³ä¹(åŸå£°)ã€‚å…è´¹ï¼å…è´¹ï¼å…è´¹ï¼
- ---
- æŠ–éŸ³ä¸‹è½½å™¨ - æ— æ°´å°æ‰¹é‡ä¸‹è½½å·¥å…·
ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„æŠ–éŸ³å†…å®¹æ‰¹é‡ä¸‹è½½å·¥å…·ï¼Œæ”¯æŒè§†é¢‘ã€å›¾é›†ã€éŸ³ä¹ã€ç›´æ’­ç­‰å¤šç§å†…å®¹ç±»å‹çš„ä¸‹è½½ã€‚æä¾›ä¸¤ä¸ªç‰ˆæœ¬ï¼šV1.0ï¼ˆç¨³å®šç‰ˆï¼‰å’Œ V2.0ï¼ˆå¢å¼ºç‰ˆï¼‰ã€‚
ğŸ“‹ ç›®å½•
å¿«é€Ÿå¼€å§‹
ç‰ˆæœ¬è¯´æ˜
V1.0 ä½¿ç”¨æŒ‡å—
V2.0 ä½¿ç”¨æŒ‡å—
Cookie é…ç½®å·¥å…·
æ”¯æŒçš„é“¾æ¥ç±»å‹
å¸¸è§é—®é¢˜
æ›´æ–°æ—¥å¿—
âš¡ å¿«é€Ÿå¼€å§‹
ç¯å¢ƒè¦æ±‚
Python 3.9+
æ“ä½œç³»ç»Ÿ
ï¼šWindowsã€macOSã€Linux
å®‰è£…æ­¥éª¤
å…‹éš†é¡¹ç›®
git clone https://github.com/jiji262/douyin-downloader.git
cd
douyin-downloader
å®‰è£…ä¾èµ–
pip install -r requirements.txt
é…ç½® Cookie
ï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦ï¼‰
#
æ–¹å¼1ï¼šè‡ªåŠ¨è·å–ï¼ˆæ¨èï¼‰
python cookie_extractor.py
#
æ–¹å¼2ï¼šæ‰‹åŠ¨è·å–
python get_cookies_manual.py
ğŸ“¦ ç‰ˆæœ¬è¯´æ˜
V1.0 (DouYinCommand.py) - ç¨³å®šç‰ˆ
âœ…
ç»è¿‡éªŒè¯
ï¼šç¨³å®šå¯é ï¼Œç»è¿‡å¤§é‡æµ‹è¯•
âœ…
ç®€å•æ˜“ç”¨
ï¼šé…ç½®æ–‡ä»¶é©±åŠ¨ï¼Œä½¿ç”¨ç®€å•
âœ…
åŠŸèƒ½å®Œæ•´
ï¼šæ”¯æŒæ‰€æœ‰å†…å®¹ç±»å‹ä¸‹è½½
âœ…
å•ä¸ªè§†é¢‘ä¸‹è½½
ï¼šå®Œå…¨æ­£å¸¸å·¥ä½œ
âš ï¸
éœ€è¦æ‰‹åŠ¨é…ç½®
ï¼šéœ€è¦æ‰‹åŠ¨è·å–å’Œé…ç½® Cookie
V2.0 (downloader.py) - å¢å¼ºç‰ˆ
ğŸš€
è‡ªåŠ¨ Cookie ç®¡ç†
ï¼šæ”¯æŒè‡ªåŠ¨è·å–å’Œåˆ·æ–° Cookie
ğŸš€
ç»Ÿä¸€å…¥å£
ï¼šæ•´åˆæ‰€æœ‰åŠŸèƒ½åˆ°å•ä¸€è„šæœ¬
ğŸš€
å¼‚æ­¥æ¶æ„
ï¼šæ€§èƒ½æ›´ä¼˜ï¼Œæ”¯æŒå¹¶å‘ä¸‹è½½
ğŸš€
æ™ºèƒ½é‡è¯•
ï¼šè‡ªåŠ¨é‡è¯•å’Œé”™è¯¯æ¢å¤
ğŸš€
å¢é‡ä¸‹è½½
ï¼šæ”¯æŒå¢é‡æ›´æ–°ï¼Œé¿å…é‡å¤ä¸‹è½½
âš ï¸
å•ä¸ªè§†é¢‘ä¸‹è½½
ï¼šç›®å‰ API è¿”å›ç©ºå“åº”ï¼ˆå·²çŸ¥é—®é¢˜ï¼‰
âœ…
ç”¨æˆ·ä¸»é¡µä¸‹è½½
ï¼šå®Œå…¨æ­£å¸¸å·¥ä½œ
ğŸ¯ V1.0 ä½¿ç”¨æŒ‡å—
é…ç½®æ–‡ä»¶è®¾ç½®
ç¼–è¾‘é…ç½®æ–‡ä»¶
cp config.example.yml config.yml
#
ç¼–è¾‘ config.yml æ–‡ä»¶
é…ç½®ç¤ºä¾‹
#
ä¸‹è½½é“¾æ¥
link
:
  -
https://v.douyin.com/xxxxx/
#
å•ä¸ªè§†é¢‘
-
https://www.douyin.com/user/xxxxx
#
ç”¨æˆ·ä¸»é¡µ
-
https://www.douyin.com/collection/xxxxx
#
åˆé›†
#
ä¿å­˜è·¯å¾„
path
:
./Downloaded/
#
Cookieé…ç½®ï¼ˆå¿…å¡«ï¼‰
cookies
:
msToken
:
YOUR_MS_TOKEN_HERE
ttwid
:
YOUR_TTWID_HERE
odin_tt
:
YOUR_ODIN_TT_HERE
passport_csrf_token
:
YOUR_PASSPORT_CSRF_TOKEN_HERE
sid_guard
:
YOUR_SID_GUARD_HERE
#
ä¸‹è½½é€‰é¡¹
music
:
True
#
ä¸‹è½½éŸ³ä¹
cover
:
True
#
ä¸‹è½½å°é¢
avatar
:
True
#
ä¸‹è½½å¤´åƒ
json
:
True
#
ä¿å­˜JSONæ•°æ®
#
ä¸‹è½½æ¨¡å¼
mode
:
  -
post
#
ä¸‹è½½å‘å¸ƒçš„ä½œå“
#
- like     # ä¸‹è½½å–œæ¬¢çš„ä½œå“
#
- mix      # ä¸‹è½½åˆé›†
#
ä¸‹è½½æ•°é‡ï¼ˆ0è¡¨ç¤ºå…¨éƒ¨ï¼‰
number
:
post
:
0
#
å‘å¸ƒä½œå“æ•°é‡
like
:
0
#
å–œæ¬¢ä½œå“æ•°é‡
allmix
:
0
#
åˆé›†æ•°é‡
mix
:
0
#
å•ä¸ªåˆé›†å†…ä½œå“æ•°é‡
#
å…¶ä»–è®¾ç½®
thread
:
5
#
ä¸‹è½½çº¿ç¨‹æ•°
database
:
True
#
ä½¿ç”¨æ•°æ®åº“è®°å½•
è¿è¡Œç¨‹åº
#
ä½¿ç”¨é…ç½®æ–‡ä»¶è¿è¡Œ
python DouYinCommand.py
#
æˆ–è€…ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
python DouYinCommand.py --cmd False
ä½¿ç”¨ç¤ºä¾‹
#
ä¸‹è½½å•ä¸ªè§†é¢‘
#
åœ¨ config.yml ä¸­è®¾ç½® link ä¸ºå•ä¸ªè§†é¢‘é“¾æ¥
python DouYinCommand.py
#
ä¸‹è½½ç”¨æˆ·ä¸»é¡µ
#
åœ¨ config.yml ä¸­è®¾ç½® link ä¸ºç”¨æˆ·ä¸»é¡µé“¾æ¥
python DouYinCommand.py
#
ä¸‹è½½åˆé›†
#
åœ¨ config.yml ä¸­è®¾ç½® link ä¸ºåˆé›†é“¾æ¥
python DouYinCommand.py
ğŸš€ V2.0 ä½¿ç”¨æŒ‡å—
å‘½ä»¤è¡Œä½¿ç”¨
#
ä¸‹è½½å•ä¸ªè§†é¢‘ï¼ˆéœ€è¦å…ˆé…ç½® Cookieï¼‰
python downloader.py -u
"
https://v.douyin.com/xxxxx/
"
#
ä¸‹è½½ç”¨æˆ·ä¸»é¡µï¼ˆæ¨èï¼‰
python downloader.py -u
"
https://www.douyin.com/user/xxxxx
"
#
è‡ªåŠ¨è·å– Cookie å¹¶ä¸‹è½½
python downloader.py --auto-cookie -u
"
https://www.douyin.com/user/xxxxx
"
#
æŒ‡å®šä¿å­˜è·¯å¾„
python downloader.py -u
"
é“¾æ¥
"
--path
"
./my_videos/
"
#
ä½¿ç”¨é…ç½®æ–‡ä»¶
python downloader.py --config
é…ç½®æ–‡ä»¶ä½¿ç”¨
åˆ›å»ºé…ç½®æ–‡ä»¶
cp config.example.yml config_simple.yml
é…ç½®ç¤ºä¾‹
#
ä¸‹è½½é“¾æ¥
link
:
  -
https://www.douyin.com/user/xxxxx
#
ä¿å­˜è·¯å¾„
path
:
./Downloaded/
#
è‡ªåŠ¨ Cookie ç®¡ç†
auto_cookie
:
true
#
ä¸‹è½½é€‰é¡¹
music
:
true
cover
:
true
avatar
:
true
json
:
true
#
ä¸‹è½½æ¨¡å¼
mode
:
  -
post
#
ä¸‹è½½æ•°é‡
number
:
post
:
10
#
å¢é‡ä¸‹è½½
increase
:
post
:
false
#
æ•°æ®åº“
database
:
true
è¿è¡Œç¨‹åº
python downloader.py --config
å‘½ä»¤è¡Œå‚æ•°
python downloader.py [é€‰é¡¹] [é“¾æ¥...]

é€‰é¡¹ï¼š
  -u, --url URL          ä¸‹è½½é“¾æ¥
  -p, --path PATH        ä¿å­˜è·¯å¾„
  -c, --config           ä½¿ç”¨é…ç½®æ–‡ä»¶
  --auto-cookie          è‡ªåŠ¨è·å– Cookie
  --cookies COOKIES      æ‰‹åŠ¨æŒ‡å®š Cookie
  -h, --help            æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
ğŸª Cookie é…ç½®å·¥å…·
1. cookie_extractor.py - è‡ªåŠ¨è·å–å·¥å…·
åŠŸèƒ½
ï¼šä½¿ç”¨ Playwright è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼Œè‡ªåŠ¨è·å– Cookie
ä½¿ç”¨æ–¹å¼
ï¼š
#
å®‰è£… Playwright
pip install playwright
playwright install chromium
#
è¿è¡Œè‡ªåŠ¨è·å–
python cookie_extractor.py
ç‰¹ç‚¹
ï¼š
âœ… è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
âœ… æ”¯æŒæ‰«ç ç™»å½•
âœ… è‡ªåŠ¨æ£€æµ‹ç™»å½•çŠ¶æ€
âœ… è‡ªåŠ¨ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
âœ… æ”¯æŒå¤šç§ç™»å½•æ–¹å¼
ä½¿ç”¨æ­¥éª¤
ï¼š
è¿è¡Œ
python cookie_extractor.py
é€‰æ‹©æå–æ–¹å¼ï¼ˆæ¨èé€‰æ‹©1ï¼‰
åœ¨æ‰“å¼€çš„æµè§ˆå™¨ä¸­å®Œæˆç™»å½•
ç¨‹åºè‡ªåŠ¨æå–å¹¶ä¿å­˜ Cookie
2. get_cookies_manual.py - æ‰‹åŠ¨è·å–å·¥å…·
åŠŸèƒ½
ï¼šé€šè¿‡æµè§ˆå™¨å¼€å‘è€…å·¥å…·æ‰‹åŠ¨è·å– Cookie
ä½¿ç”¨æ–¹å¼
ï¼š
python get_cookies_manual.py
ç‰¹ç‚¹
ï¼š
âœ… æ— éœ€å®‰è£… Playwright
âœ… è¯¦ç»†çš„æ“ä½œæ•™ç¨‹
âœ… æ”¯æŒ Cookie éªŒè¯
âœ… è‡ªåŠ¨ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
âœ… æ”¯æŒå¤‡ä»½å’Œæ¢å¤
ä½¿ç”¨æ­¥éª¤
ï¼š
è¿è¡Œ
python get_cookies_manual.py
é€‰æ‹©"è·å–æ–°çš„Cookie"
æŒ‰ç…§æ•™ç¨‹åœ¨æµè§ˆå™¨ä¸­è·å– Cookie
ç²˜è´´ Cookie å†…å®¹
ç¨‹åºè‡ªåŠ¨è§£æå¹¶ä¿å­˜
Cookie è·å–æ•™ç¨‹
æ–¹æ³•ä¸€ï¼šæµè§ˆå™¨å¼€å‘è€…å·¥å…·
æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—®
æŠ–éŸ³ç½‘é¡µç‰ˆ
ç™»å½•ä½ çš„æŠ–éŸ³è´¦å·
æŒ‰
F12
æ‰“å¼€å¼€å‘è€…å·¥å…·
åˆ‡æ¢åˆ°
Network
æ ‡ç­¾é¡µ
åˆ·æ–°é¡µé¢ï¼Œæ‰¾åˆ°ä»»æ„è¯·æ±‚
åœ¨è¯·æ±‚å¤´ä¸­æ‰¾åˆ°
Cookie
å­—æ®µ
å¤åˆ¶ä»¥ä¸‹å…³é”® cookie å€¼ï¼š
msToken
ttwid
odin_tt
passport_csrf_token
sid_guard
æ–¹æ³•äºŒï¼šä½¿ç”¨è‡ªåŠ¨å·¥å…·
#
æ¨èä½¿ç”¨è‡ªåŠ¨å·¥å…·
python cookie_extractor.py
ğŸ“‹ æ”¯æŒçš„é“¾æ¥ç±»å‹
ğŸ¬ è§†é¢‘å†…å®¹
å•ä¸ªè§†é¢‘åˆ†äº«é“¾æ¥
ï¼š
https://v.douyin.com/xxxxx/
å•ä¸ªè§†é¢‘ç›´é“¾
ï¼š
https://www.douyin.com/video/xxxxx
å›¾é›†ä½œå“
ï¼š
https://www.douyin.com/note/xxxxx
ğŸ‘¤ ç”¨æˆ·å†…å®¹
ç”¨æˆ·ä¸»é¡µ
ï¼š
https://www.douyin.com/user/xxxxx
æ”¯æŒä¸‹è½½ç”¨æˆ·å‘å¸ƒçš„æ‰€æœ‰ä½œå“
æ”¯æŒä¸‹è½½ç”¨æˆ·å–œæ¬¢çš„ä½œå“ï¼ˆéœ€è¦æƒé™ï¼‰
ğŸ“š åˆé›†å†…å®¹
ç”¨æˆ·åˆé›†
ï¼š
https://www.douyin.com/collection/xxxxx
éŸ³ä¹åˆé›†
ï¼š
https://www.douyin.com/music/xxxxx
ğŸ”´ ç›´æ’­å†…å®¹
ç›´æ’­é—´
ï¼š
https://live.douyin.com/xxxxx
ğŸ”§ å¸¸è§é—®é¢˜
Q: ä¸ºä»€ä¹ˆå•ä¸ªè§†é¢‘ä¸‹è½½å¤±è´¥ï¼Ÿ
A
:
V1.0ï¼šè¯·æ£€æŸ¥ Cookie æ˜¯å¦æœ‰æ•ˆï¼Œç¡®ä¿åŒ…å«å¿…è¦çš„å­—æ®µ
V2.0ï¼šç›®å‰å·²çŸ¥é—®é¢˜ï¼ŒAPI è¿”å›ç©ºå“åº”ï¼Œå»ºè®®ä½¿ç”¨ç”¨æˆ·ä¸»é¡µä¸‹è½½
Q: Cookie è¿‡æœŸæ€ä¹ˆåŠï¼Ÿ
A
:
ä½¿ç”¨
python cookie_extractor.py
é‡æ–°è·å–
æˆ–ä½¿ç”¨
python get_cookies_manual.py
æ‰‹åŠ¨è·å–
Q: ä¸‹è½½é€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ
A
:
è°ƒæ•´
thread
å‚æ•°å¢åŠ å¹¶å‘æ•°
æ£€æŸ¥ç½‘ç»œè¿æ¥
é¿å…åŒæ—¶ä¸‹è½½è¿‡å¤šå†…å®¹
Q: å¦‚ä½•æ‰¹é‡ä¸‹è½½ï¼Ÿ
A
:
V1.0ï¼šåœ¨
config.yml
ä¸­æ·»åŠ å¤šä¸ªé“¾æ¥
V2.0ï¼šä½¿ç”¨å‘½ä»¤è¡Œä¼ å…¥å¤šä¸ªé“¾æ¥æˆ–ä½¿ç”¨é…ç½®æ–‡ä»¶
Q: æ”¯æŒå“ªäº›æ ¼å¼ï¼Ÿ
A
:
è§†é¢‘ï¼šMP4 æ ¼å¼ï¼ˆæ— æ°´å°ï¼‰
å›¾ç‰‡ï¼šJPG æ ¼å¼
éŸ³é¢‘ï¼šMP3 æ ¼å¼
æ•°æ®ï¼šJSON æ ¼å¼
ğŸ“ æ›´æ–°æ—¥å¿—
V2.0 (2025-08)
âœ…
ç»Ÿä¸€å…¥å£
ï¼šæ•´åˆæ‰€æœ‰åŠŸèƒ½åˆ°
downloader.py
âœ…
è‡ªåŠ¨ Cookie ç®¡ç†
ï¼šæ”¯æŒè‡ªåŠ¨è·å–å’Œåˆ·æ–°
âœ…
å¼‚æ­¥æ¶æ„
ï¼šæ€§èƒ½ä¼˜åŒ–ï¼Œæ”¯æŒå¹¶å‘ä¸‹è½½
âœ…
æ™ºèƒ½é‡è¯•
ï¼šè‡ªåŠ¨é‡è¯•å’Œé”™è¯¯æ¢å¤
âœ…
å¢é‡ä¸‹è½½
ï¼šæ”¯æŒå¢é‡æ›´æ–°
âœ…
ç”¨æˆ·ä¸»é¡µä¸‹è½½
ï¼šå®Œå…¨æ­£å¸¸å·¥ä½œ
âš ï¸
å•ä¸ªè§†é¢‘ä¸‹è½½
ï¼šAPI è¿”å›ç©ºå“åº”ï¼ˆå·²çŸ¥é—®é¢˜ï¼‰
V1.0 (2024-12)
âœ…
ç¨³å®šå¯é 
ï¼šç»è¿‡å¤§é‡æµ‹è¯•éªŒè¯
âœ…
åŠŸèƒ½å®Œæ•´
ï¼šæ”¯æŒæ‰€æœ‰å†…å®¹ç±»å‹
âœ…
å•ä¸ªè§†é¢‘ä¸‹è½½
ï¼šå®Œå…¨æ­£å¸¸å·¥ä½œ
âœ…
é…ç½®æ–‡ä»¶é©±åŠ¨
ï¼šç®€å•æ˜“ç”¨
âœ…
æ•°æ®åº“æ”¯æŒ
ï¼šè®°å½•ä¸‹è½½å†å²
âš–ï¸ æ³•å¾‹å£°æ˜
æœ¬é¡¹ç›®ä»…ä¾›
å­¦ä¹ äº¤æµ
ä½¿ç”¨
è¯·éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„å’Œå¹³å°æœåŠ¡æ¡æ¬¾
ä¸å¾—ç”¨äºå•†ä¸šç”¨é€”æˆ–ä¾µçŠ¯ä»–äººæƒç›Š
ä¸‹è½½å†…å®¹è¯·å°Šé‡åŸä½œè€…ç‰ˆæƒ
ğŸ¤ è´¡çŒ®æŒ‡å—
æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼
æŠ¥å‘Šé—®é¢˜
ä½¿ç”¨
Issues
æŠ¥å‘Š bug
è¯·æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œå¤ç°æ­¥éª¤
åŠŸèƒ½å»ºè®®
åœ¨ Issues ä¸­æå‡ºæ–°åŠŸèƒ½å»ºè®®
è¯¦ç»†æè¿°åŠŸèƒ½éœ€æ±‚å’Œä½¿ç”¨åœºæ™¯
ğŸ“„ è®¸å¯è¯
æœ¬é¡¹ç›®é‡‡ç”¨
MIT License
å¼€æºè®¸å¯è¯ã€‚
å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª â­ Star æ”¯æŒä¸€ä¸‹ï¼
ğŸ› æŠ¥å‘Šé—®é¢˜
â€¢
ğŸ’¡ åŠŸèƒ½å»ºè®®
â€¢
ğŸ“– æŸ¥çœ‹æ–‡æ¡£
Made with â¤ï¸ by
jiji262
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 60
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 5,265

### References
- https://github.com/jiji262/douyin-downloader

## wmjordan/PDFPatcher

### Executive Summary
- PDFè¡¥ä¸ä¸â€”â€”PDFå·¥å…·ç®±ï¼Œå¯ä»¥ç¼–è¾‘ä¹¦ç­¾ã€å‰ªè£æ—‹è½¬é¡µé¢ã€è§£é™¤é™åˆ¶ã€æå–æˆ–åˆå¹¶æ–‡æ¡£ï¼Œæ¢æŸ¥æ–‡æ¡£ç»“æ„ï¼Œæå–å›¾ç‰‡ã€è½¬æˆå›¾ç‰‡ç­‰ç­‰
- ---
- PDF è¡¥ä¸ä¸ï¼ˆPDFPatcherï¼‰
æ„Ÿè°¢æ‚¨å…³æ³¨ PDF è¡¥ä¸ä¸ï¼Œè¯·åœ¨ä½¿ç”¨è½¯ä»¶æˆ–æºä»£ç å‰é˜…è¯»æœ¬è¯´æ˜å’Œæˆæƒåè®®ã€‚æœ¬è½¯ä»¶åŠæºä»£ç é‡‡ç”¨ AGPLï¼‹â€œ
è‰¯å¿ƒæˆæƒ
â€åè®®â€”â€”
ç”¨æˆ·æ¯æ¬¡ä½¿ç”¨æœ¬è½¯ä»¶åå¦‚æœ‰æ‰€è·ç›Šï¼Œåº”è¡Œä¸€å–„äº‹ï¼›å¦‚ä½¿ç”¨æºä»£ç å¼€å‘äº†æ–°çš„è½¯ä»¶å¹¶è·å¾—æ”¶ç›Šï¼Œåº”å°†æ”¶ç›Šä¸­ä¸ä½äºåƒåˆ†ä¹‹ä¸€çš„é‡‘é¢æèµ ç»™ç¤¾ä¼šçš„å¼±åŠ¿ç¾¤ä½“
ã€‚
åŠŸèƒ½ç®€ä»‹
PDF è¡¥ä¸ä¸æ˜¯ä¸€ä¸ª PDF å¤„ç†å·¥å…·ã€‚å®ƒå…·æœ‰ä»¥ä¸‹åŠŸèƒ½ï¼š
ä¿®æ”¹ PDF æ–‡æ¡£ï¼šä¿®æ”¹æ–‡æ¡£å±æ€§ã€é¡µç ç¼–å·ã€é¡µé¢é“¾æ¥ï¼›ç»Ÿä¸€é¡µé¢å°ºå¯¸ï¼›åˆ é™¤è‡ªåŠ¨æ‰“å¼€ç½‘é¡µç­‰åŠ¨ä½œï¼›å»é™¤å¤åˆ¶åŠæ‰“å°é™åˆ¶ï¼›è®¾ç½®é˜…è¯»å™¨åˆå§‹æ¨¡å¼ï¼›æ¸…ç†æ–‡æ¡£éšè—åƒåœ¾æ•°æ®ï¼›é‡æ–°å‹ç¼©é»‘ç™½å›¾ç‰‡ï¼›æ—‹è½¬é¡µé¢ã€‚
è´´å¿ƒ PDF ä¹¦ç­¾ç¼–è¾‘å™¨ï¼šå¸¦æœ‰é˜…è¯»ç•Œé¢ï¼ˆå…·æœ‰ä¾¿äºé˜…è¯»ç«–æ’æ–‡æ¡£çš„ä»å³åˆ°å·¦é˜…è¯»æ–¹å¼ï¼‰ï¼Œå¯æ‰¹é‡ä¿®æ”¹ PDF ä¹¦ç­¾å±æ€§ï¼ˆé¢œè‰²ã€æ ·å¼ã€ç›®æ ‡é¡µç ã€ç¼©æ”¾æ¯”ä¾‹ç­‰ï¼‰ï¼Œä¹¦ç­¾å¯ç²¾ç¡®å®šä½åˆ°é¡µé¢ä¸­é—´ï¼›åœ¨ä¹¦ç­¾ä¸­æ‰§è¡ŒæŸ¥æ‰¾æ›¿æ¢ï¼ˆæ”¯æŒæ­£åˆ™è¡¨è¾¾å¼åŠ XPath åŒ¹é…ã€å¯å¿«é€Ÿé€‰æ‹©ç¯‡ã€ç« ã€èŠ‚ä¹¦ç­¾ï¼‰ï¼Œ
è‡ªåŠ¨å¿«é€Ÿç”Ÿæˆæ–‡æ¡£ä¹¦ç­¾
ã€‚
åˆ¶ä½œ PDF æ–‡ä»¶ï¼šåˆå¹¶å·²æœ‰ PDF æ–‡ä»¶æˆ–å›¾ç‰‡ï¼Œç”Ÿæˆæ–°çš„ PDF æ–‡ä»¶ï¼›åˆå¹¶åçš„ PDF æ–‡æ¡£å¸¦æœ‰åŸæ–‡æ¡£çš„ä¹¦ç­¾ï¼Œè¿˜å¯æŒ‚ä¸Šæ–°ä¹¦ç­¾ï¼ˆæˆ–æ ¹æ®æ–‡ä»¶åç”Ÿæˆï¼‰ï¼Œæ–°ä¹¦ç­¾æ–‡æœ¬å’Œæ ·å¼å¯è‡ªå®šä¹‰ï¼›åˆå¹¶çš„ PDF æ–‡æ¡£å¯æŒ‡å®šç»Ÿä¸€çš„é¡µé¢å°ºå¯¸ï¼Œä»¥ä¾¿æ‰“å°å’Œé˜…è¯»ã€‚
æ‹†åˆ†æˆ–åˆå¹¶ PDF æ–‡ä»¶ï¼Œå¹¶ä¿ç•™åŸæ–‡ä»¶çš„ä¹¦ç­¾æˆ–æŒ‚ä¸Šæ–°çš„ä¹¦ç­¾ã€‚
é«˜é€Ÿæ— æŸå¯¼å‡º PDF æ–‡æ¡£çš„å›¾ç‰‡ã€‚
å°† PDF é¡µé¢è½¬æ¢ä¸ºå›¾ç‰‡ã€‚
æå–æˆ–åˆ é™¤ PDF æ–‡æ¡£ä¸­æŒ‡å®šçš„é¡µé¢ï¼Œè°ƒæ•´ PDF æ–‡æ¡£çš„é¡µé¢é¡ºåºã€‚
æ ¹æ® PDF æ–‡æ¡£å…ƒæ•°æ®é‡å‘½å PDF æ–‡ä»¶åã€‚
è°ƒç”¨å¾®è½¯ Office çš„å›¾åƒè¯†åˆ«å¼•æ“åˆ†æ PDF æ–‡æ¡£å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼›å°†å›¾ç‰‡ PDF çš„ç›®å½•é¡µè½¬æ¢ä¸º PDF ä¹¦ç­¾ã€‚è¯†åˆ«ç»“æœå¯å†™å…¥ PDF æ–‡ä»¶ã€‚
æ›¿æ¢å­—ä½“ï¼šæ›¿æ¢æ–‡æ¡£ä¸­ä½¿ç”¨çš„å­—ä½“ï¼›åµŒå…¥å­—åº“åˆ° PDF æ–‡æ¡£ï¼Œæ¶ˆé™¤å¤åˆ¶æ–‡æœ¬æ—¶çš„ä¹±ç ï¼Œä½¿ä¹‹å¯åœ¨æ²¡æœ‰å­—åº“çš„è®¾å¤‡ï¼ˆå¦‚ Kindle ç­‰ç”µå­ä¹¦é˜…è¯»å™¨ï¼‰ä¸Šé˜…è¯»ã€‚
åˆ†ææ–‡æ¡£ç»“æ„ï¼šä»¥æ ‘è§†å›¾æ˜¾ç¤º PDF æ–‡æ¡£ç»“æ„ï¼Œå¯ç¼–è¾‘ä¿®æ”¹ PDF æ–‡æ¡£èŠ‚ç‚¹ï¼Œæˆ–å°† PDF æ–‡æ¡£å¯¼å‡ºæˆ XML æ–‡ä»¶ï¼Œä¾› PDF çˆ±å¥½è€…åˆ†æã€è°ƒè¯•ä¹‹ç”¨ã€‚
æ°¸ä¹…å…è´¹ï¼Œç»ä¸è¿‡æœŸï¼Œæ— å¹¿å‘Šï¼Œæ— å¼¹å‡ºåºŸè¯å¯¹è¯æ¡†ï¼Œä¸çª¥æ¢éšç§ã€‚
æˆæƒåè®®
ã€ŠPDF è¡¥ä¸ä¸ã€‹è½¯ä»¶ï¼ˆä»¥ä¸‹ç®€ç§°æœ¬è½¯ä»¶ï¼‰å—è‘—ä½œæƒæ³•åŠå›½é™…æ¡çº¦æ¡æ¬¾å’Œå…¶å®ƒçŸ¥è¯†äº§æƒæ³•åŠæ¡çº¦çš„ä¿æŠ¤ã€‚
æœ¬è½¯ä»¶å¯¹äºæœ€ç»ˆç”¨æˆ·å…è´¹ã€‚ç”±äºæœ¬è½¯ä»¶ä½¿ç”¨äº†å¸¦æœ‰ AGPL æ¡æ¬¾çš„ç¬¬ä¸‰æ–¹å¼€æºç»„ä»¶ï¼Œå› æ­¤ï¼Œæœ¬è½¯ä»¶åŠå…¶æºä»£ç çš„ä½¿ç”¨åè®®ä¹ŸåŸºäº AGPLã€‚å¦å¤–è¿˜å¸¦æœ‰å¦‚ä¸‹é™„åŠ æ¡ä»¶ã€‚åœ¨éµå®ˆæœ¬è½¯ä»¶çš„å‰ææ¡ä»¶ä¸‹ï¼Œä½ å¯ä»¥åœ¨éµå¾ªæœ¬åè®®çš„åŸºç¡€ä¸Šè‡ªç”±çš„ä½¿ç”¨å’Œä¼ æ’­å®ƒï¼Œä½ ä¸€æ—¦å®‰è£…ã€å¤åˆ¶æˆ–ä½¿ç”¨æœ¬è½¯ä»¶ï¼Œåˆ™è¡¨ç¤ºæ‚¨å·²ç»åŒæ„æœ¬åè®®æ¡æ¬¾ã€‚å¦‚æœä½ ä¸åŒæ„æœ¬åè®®ï¼Œè¯·ä¸è¦å®‰è£…ä½¿ç”¨æœ¬è½¯ä»¶ï¼Œä¹Ÿä¸åº”åˆ©ç”¨å…¶æºä»£ç ã€‚
é™„åŠ æ¡ä»¶ï¼š
æ¯ä¸€ä¸ªä½¿ç”¨æœ¬è½¯ä»¶çš„ç”¨æˆ·ï¼Œå¦‚æœæœ¬è½¯ä»¶å¸®åŠ©äº†æ‚¨ï¼Œæ¯ä½¿ç”¨æœ¬è½¯ä»¶åï¼Œæ‚¨åº”å½“åš 1 ä»¶å–„äº‹ã€‚å–„äº‹æ— åˆ†å¤§å°ï¼Œæœ‰å¿ƒåˆ™è¡Œã€‚ä¾‹å¦‚ï¼š
å¦‚æœæ‚¨çš„çˆ¶æ¯åœ¨èº«è¾¹ï¼Œä½ å¯ä»¥ä¸ºæ‚¨çš„çˆ¶æ¯åšä¸€é¡¿ç¾å‘³çš„é¥­èœï¼Œæˆ–è€…ä¸ºä»–ä»¬æŒ‰æ‘©ã€æ´—è„šï¼›å¦‚æœä»–ä»¬èº«å¤„è¿œæ–¹ï¼Œä½ å¯ä»¥å‘ä»–ä»¬å‘èµ·é€šè¯ï¼Œé—®å€™ä»–ä»¬çš„å¥åº·å’Œç”Ÿæ´»ã€‚
åœ¨å¤§é›¨æ»‚æ²±çš„æ—¶å€™ï¼Œå¦‚æœæ‚¨æœ‰é›¨ä¼ï¼Œå¯ä¸åŒè·¯çš„äººå…±äº«ï¼›åœ¨çƒˆæ—¥å½“ç©ºçš„æ—¶èŠ‚ï¼Œå¦‚æœæ‚¨çœ‹åˆ°ç¯å«å·¥äººå¤ªé˜³ä¸‹å·¥ä½œï¼Œæ‚¨å¯ä»¥ä¸ºä»–ä»¬ä¹°ä¸€ç“¶æ°´é€ç»™ä»–ä»¬ï¼›åœ¨æ‹¥æŒ¤çš„å…¬å…±äº¤é€šå·¥å…·ä¸Šï¼Œæˆ–åœ¨å…¬å…±åœºåˆæ’é˜Ÿç­‰å€™ä¹‹é™…ï¼Œå¦‚æœæ‚¨æœ‰åº§ä½ï¼Œå¯ä»¥è®©ç»™è€äººã€å­•å¦‡æˆ–æç€é‡ç‰©çš„äººå°±åã€‚
æ‚¨å¯ä»¥ç”¨æ‚¨æ“…é•¿çš„æŠ€èƒ½ï¼Œä¸ºèº«è¾¹çš„äººæ’éš¾è§£å›°ï¼›æ‚¨å¯ä»¥å°†æ‚¨çš„çŸ¥è¯†ï¼Œåˆ†äº«ç»™å…¶ä»–äººï¼Œè®©ä»–ä»¬æœ‰æ‰€è·ç›Šï¼›æ‚¨å¯ä»¥å‘æ¯”æ‚¨å›°éš¾çš„äººæèµ„èµ ç‰©ã€‚
å¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªè½¯ä»¶çœŸçš„å¥½ç”¨ï¼Œè¯·å°†å®ƒçš„ä½¿ç”¨æ–¹æ³•ä»‹ç»ç»™åˆ«äººï¼Œè®©åˆ«äººä¹Ÿé€šè¿‡ä½¿ç”¨æœ¬è½¯ä»¶è€Œå¾—åˆ°å¥½å¤„ï¼›æˆ–è€…å°†å…¶å®ƒæ‚¨è§‰å¾—å¥½ç”¨çš„è½¯ä»¶ä»‹ç»ç»™åˆ«äººã€‚
å¦‚æœæ‚¨æ— æ³•åšåˆ°ä½¿ç”¨æœ¬è½¯ä»¶ååš 1 ä»¶å–„äº‹ï¼Œè¯·è®°åœ¨å¿ƒä¸­ã€‚åœ¨æœ‰æœºä¼šçš„æ—¶å€™ï¼Œå¤šè¡Œå–„ç§¯å¾·ã€‚æœ¬ç”¨æˆ·åè®®ä¹‹éµå¾ªä¸å¦ï¼Œå…¨åœ¨äºæ‚¨çš„è‰¯å¿ƒã€‚æ˜¯ä¸ºâ€œ
è‰¯å¿ƒæˆæƒ
â€ã€‚
ç›¸å…³å®šä¹‰ï¼š
è½¯ä»¶ï¼šè½¯ä»¶æ˜¯æŒ‡ã€ŠPDF è¡¥ä¸ä¸ã€‹è½¯ä»¶ä»¥åŠå®ƒçš„æ›´æ–°ã€äº§å“æ‰‹å†Œï¼Œä»¥åŠåœ¨çº¿æ–‡æ¡£ç­‰ç›¸å…³è½½ä½“ã€‚
é™åˆ¶ï¼šä½ å¯ä»¥ä½¿ç”¨æœ¬è½¯ä»¶çš„æºä»£ç å¼€å‘åº”ç”¨ç¨‹åºï¼ˆè‡ªç”±ã€å…±äº«æˆ–å•†ç”¨ï¼‰ï¼Œä¹Ÿå¯ä»¥ä»»æ„æ–¹å¼åˆ†å‘æ•°é‡ä¸é™çš„æœ¬è½¯ä»¶çš„å®Œæ•´æ‹·è´ï¼Œä½†å‰ææ˜¯ï¼š
â‘  ä½ åˆ†å‘è½¯ä»¶æ—¶å¿…é¡»æä¾›æœ¬è½¯ä»¶çš„å®Œæ•´ç‰ˆæœ¬ï¼Œæœªç»è®¸å¯ä¸å¾—å¯¹è½¯ä»¶ä¹ƒè‡³å®ƒçš„å®‰è£…ç¨‹åºåšä»»ä½•ä¿®æ”¹ï¼›
â‘¡ ä½ åˆ†å‘è½¯ä»¶æ—¶ä¸èƒ½æ›´æ”¹æœ¬æˆæƒåè®®ï¼›
â‘¢ ä½ å¦‚æœåœ¨å•†ä¸šæ€§å®£ä¼ æ´»åŠ¨ã€äº§å“ä¸­é™„åŠ æœ¬è½¯ä»¶ï¼Œåº”å½“è·å¾—è‘—ä½œæƒäººçš„ä¹¦é¢è®¸å¯ï¼›
â‘£ ä½ å¦‚æœåˆ©ç”¨æœ¬è½¯ä»¶çš„æºä»£ç ç¼–å†™äº†å…¶å®ƒè½¯ä»¶ï¼Œå¹¶ä¸”äº§ç”Ÿäº†é”€å”®æ”¶å…¥ï¼Œåº”å½“å°†è¯¥è½¯ä»¶é”€å”®æ”¶å…¥ä¸ä½äºåƒåˆ†ä¹‹ä¸€çš„é‡‘é¢æçŒ®ç»™ç¤¾ä¼šä¸Šçš„å¼±åŠ¿ç¾¤ä½“ã€‚
æ”¯æŒï¼šè½¯ä»¶ä¼šç”±äºç”¨æˆ·çš„éœ€æ±‚è€Œä¸æ–­æ›´æ–°ï¼Œè‘—ä½œæƒäººå°†æä¾›åŒ…æ‹¬ç”¨æˆ·æ‰‹å†Œã€ç”µå­é‚®ä»¶ç­‰å„ç§ç›¸å…³ä¿¡æ¯æ”¯æŒï¼Œä½†è½¯ä»¶ä¸ç¡®ä¿æ”¯æŒå†…å®¹å’ŒåŠŸèƒ½ä¸å‘ç”Ÿå˜æ›´ã€‚
ç»ˆæ­¢ï¼šå½“ä½ ä¸åŒæ„æˆ–è€…è¿èƒŒæœ¬åè®®çš„æ—¶å€™ï¼Œåè®®å°†è‡ªåŠ¨ç»ˆæ­¢ï¼Œä½ å¿…é¡»ç«‹å³åˆ é™¤æœ¬è½¯ä»¶äº§å“ã€‚
ç‰ˆæƒï¼šæœ¬è½¯ä»¶åŠæºä»£ç å—è‘—ä½œæƒæ³•åŠå›½é™…æ¡çº¦æ¡æ¬¾å’Œå…¶å®ƒçŸ¥è¯†äº§æƒæ³•åŠæ¡çº¦çš„ä¿æŠ¤ã€‚
å…è´£ï¼šå¯¹äºæœ¬è½¯ä»¶å®‰è£…ã€å¤åˆ¶ã€ä½¿ç”¨ä¸­å¯¼è‡´çš„ä»»ä½•æŸå¤±ï¼Œæœ¬è½¯ä»¶åŠè‘—ä½œæƒäººä¸è´Ÿè´£ä»»ã€‚
å¸¸ç”¨çš„ PDF å¼€æºç»„ä»¶ç®€ä»‹
PDF æ–‡æ¡£çš„è§„èŒƒï¼ˆISO 32000-1:2008 ã€ŠDocument management â€” Portable document format â€” Part 1:PDF 1.7ã€‹ï¼‰å¯ä»ç½‘ä¸Šæ‰¾åˆ°ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œå®ƒæ˜¯ PDF å¤„ç†ç¨‹åºå¼€å‘è€…çš„å¿…è¯»æ–‡çŒ®ã€‚
PDF æ–‡æ¡£æ ¼å¼ä¸­æ¶‰åŠå°åˆ·é¢†åŸŸçš„å¤šé¡¹æŠ€æœ¯ï¼Œå¹¶æœ‰å…¶ç‹¬ç‰¹çš„æ–‡æ¡£ç»“æ„ï¼Œè¿˜ä½¿ç”¨äº†å¤šç§æ•°æ®å‹ç¼©ç®—æ³•ã€‚è¦ä»é›¶å¼€å§‹ç¼–å†™ PDF æ–‡æ¡£çš„å¤„ç†ç¨‹åºï¼Œå¯¹äºä¸€èˆ¬äººè€Œè¨€ï¼Œé€šå¸¸æ˜¯å›°éš¾è€Œä¸å¤ªç°å®çš„ã€‚PDF è¡¥ä¸ä¸ä½¿ç”¨ .NET Framework å¼€å‘ï¼Œä¸»è¦é‡‡ç”¨ iText å’Œ MuPDF è¿™ä¸¤ä¸ªå¼€æ”¾æºä»£ç çš„ç»„ä»¶åº“æ¥å¤„ç† PDF æ–‡æ¡£ã€‚
å‰è€…æ˜¯ .NET ç»„ä»¶ï¼Œä¸ PDF ä¸»ç¨‹åºå…·æœ‰è¾ƒå¥½çš„äº’æ“ä½œæ€§ï¼Œå¹¶ä¸”åœ¨è§£æã€ç”Ÿæˆå’Œä¿®æ”¹ PDF æ–‡æ¡£ï¼Œä»¥åŠåµŒå…¥ TTF å­—ä½“å­é›†è¿™äº›åŠŸèƒ½ä¸Šï¼Œä¼˜èƒœäºåè€…ã€‚
åè€…é‡‡ç”¨ C è¯­è¨€å¼€å‘å¹¶ç¼–è¯‘ï¼Œä¸å‰è€…ç›¸æ¯”ï¼Œå…¶æœ€å¤§çš„ä¼˜ç‚¹æ˜¯å…·æœ‰æ¸²æŸ“ PDF æ–‡æ¡£ä¸ºä½å›¾çš„åŠŸèƒ½ã€‚MuPDF ç¼–è¯‘å‡ºæ¥çš„åŠ¨æ€ç»„ä»¶åº“å¯åœ¨ä½œè€…å¦ä¸€ä¸ªå¼€æ”¾æºä»£ç åº“
SharpMuPDF
ä¸‹è½½ã€‚PDF è¡¥ä¸ä¸é€šè¿‡ P/Invoke æŠ€æœ¯è°ƒç”¨è¯¥ç»„ä»¶åº“çš„åŠŸèƒ½ã€‚
é™¤äº† PDF å¼€æºç»„ä»¶ä¹‹å¤–ï¼Œç¨‹åºè¿˜ä½¿ç”¨äº†å…¶å®ƒä¼˜ç§€å¼€æºç»„ä»¶ã€‚ä¾‹å¦‚ ObjectListView è¿™ä¸ªå¼ºå¤§çš„åˆ—è¡¨æ§ä»¶ã€FreeImage æ¥è¯»å–å’Œè§£ç å„ç§ç±»å‹çš„ç‚¹é˜µå›¾åƒæ–‡ä»¶ã€Cyotek çš„ ImageBox ç”¨äºæ˜¾ç¤ºæ¸²æŸ“å¥½çš„ PDF æ–‡æ¡£é¡µé¢ã€TabControlExtra ç”¨äºæ„å»ºé€‰é¡¹å¡å¼æ–‡æ¡£ç•Œé¢ã€HTMLRenderer ç”¨äºæ˜¾ç¤º HTML ç½‘é¡µç•Œé¢ç­‰ç­‰ã€‚
æºä»£ç çš„ç»“æ„
App ç›®å½•ï¼šPDF è¡¥ä¸ä¸ä¸»ç¨‹åº
Commonï¼šä¸€äº›å¸¸ç”¨çš„å·¥å…·ç±»
Functionsï¼šç”¨äºå‘ˆç°è½¯ä»¶å„ç±»åŠŸèƒ½çš„çª—ä½“å’Œæ§ä»¶
Libï¼šç¨‹åºä½¿ç”¨çš„ç¬¬ä¸‰æ–¹ç»„ä»¶
Modelï¼šç¼–è¾‘æ–‡æ¡£æ—¶æ‰€ç”¨çš„é«˜çº§æ¨¡å‹ï¼ˆåŸºç¡€æ•°æ®æ¨¡å‹ç”± iText å’Œ MuPDF çš„ç±»å®ç°ï¼‰
Optionsï¼šç¨‹åºçš„é€‰é¡¹
Processorï¼šå¤„ç† PDF æ–‡æ¡£çš„ç®—æ³•ï¼ˆå…¶ä¸­ Mupdf ç›®å½•é‡Œæ”¾ç½®äº† P/Invoke è°ƒç”¨ MuPDF çš„ç±»ï¼‰
doc ç›®å½•ï¼šæ”¾ç½®ç¨‹åºçš„ä½¿ç”¨æ–‡æ¡£
JBig2 ç›®å½•ï¼šæ”¾ç½® JBIG2 å›¾åƒçš„ç¼–ç å’Œè§£ç åº“ä»£ç 
è¿è¡Œç¯å¢ƒ
Windows 7 ä»¥ä¸Šç‰ˆæœ¬çš„æ“ä½œç³»ç»Ÿã€‚
.NET Framework 4.0 åˆ° 4.8 ç‰ˆæœ¬ã€‚
ä½¿ç”¨æ–‡å­—è¯†åˆ«åŠŸèƒ½éœ€è¦å®‰è£… Microsoft Office 2003ï¼ˆæˆ– 2007ï¼‰çš„ Document Imaging ç»„ä»¶ï¼ˆMODIï¼‰ã€‚
ç¼–è¯‘ç¨‹åºæºä»£ç ï¼Œå»ºè®®ä½¿ç”¨ Visual Studio 2022 æˆ–æ›´æ–°ç‰ˆæœ¬ï¼Œå¹¶å®‰è£…â€œ.NET æ¡Œé¢å¼€å‘â€ï¼ˆç”¨äºç¼–è¯‘ PDF è¡¥ä¸ä¸æºä»£ç ï¼‰å’Œâ€œC++ æ¡Œé¢å¼€å‘â€ï¼ˆç”¨äºç¼–è¯‘ JBIG2 ç¼–ç ç»„ä»¶ï¼‰ä¸¤ä¸ªå·¥ä½œè´Ÿè½½ã€‚å¯èƒ½ä¼šé‡åˆ°é¡¹ç›®â€œé¢å‘ä¸å†å—æ”¯æŒçš„ .NET Frameworkâ€ã€éœ€è¦â€œå°†ç›®æ ‡æ›´æ–°ä¸º .NET Framework 4.8â€çš„é—®é¢˜ã€‚ç®€å•æ–¹æ³•æ˜¯å°†ç›®æ ‡æ›´æ–°ä¸º .NET Framework 4.8ï¼Œå¦‚ä¸æ›´æ–°ç›®æ ‡ï¼Œè¯·å‚è€ƒ
è¿™ç¯‡æ–‡ç« ä»‹ç»çš„æ–¹æ³•
ã€‚
è”ç³»ä½œè€…
é™¤ç¬¬ä¸‰æ–¹ç»„ä»¶å¤–ï¼Œæœ¬è½¯ä»¶çš„æºä»£ç å®Œå…¨å¼€æ”¾ï¼š
https://github.com/wmjordan/PDFPatcher
https://gitee.com/wmjordan/pdfpatcher
å»ºè®®é€šè¿‡å¼€æ”¾æºä»£ç ç½‘ç«™é€šè¿‡æäº¤ issue çš„æ–¹å¼æäº¤æ‚¨çš„å»ºè®®æˆ–éœ€æ±‚ã€‚å› æ—¥å¸¸å·¥ä½œç¹å¿™ï¼Œæš‚ä¸æä¾›åŠ  QQ æˆ–å¾®ä¿¡å’¨è¯¢çš„æœåŠ¡ï¼Œæ•¬è¯·è°…è§£ã€‚
åœ¨é‚®ä»¶æˆ–æ¶ˆæ¯ä¸­ï¼Œè¯·æ³¨æ˜ä½ çš„ç‰ˆæœ¬å·ï¼Œé™„ä¸Šæˆªå›¾å’Œé™„ä»¶ï¼Œè¯¦ç»†è¯´æ˜ä½ é‡åˆ°çš„é—®é¢˜ã€‚
å¦‚é‡åˆ°éœ€è¦æä¾›é™„ä»¶çš„æƒ…å†µï¼Œè¯·æŠŠå®ƒæå°ä¸€ç‚¹ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæœ€å¥½ä¸è¦å‘é€è¶…è¿‡ 10M çš„é™„ä»¶ã€‚
å¯¹äº PDF æ–‡ä»¶ï¼Œå¯ç”¨â€œæå–é¡µé¢â€åŠŸèƒ½æå–æœ‰ä»£è¡¨æ€§çš„é¡µé¢ã€‚
å¯¹äºå›¾ç‰‡æ–‡ä»¶ï¼Œè¯·å‹ç¼©æºæ–‡ä»¶ï¼Œæˆ–æä¾›æœ‰ä»£è¡¨æ€§çš„ä¸€ä¸¤é¡µå›¾ç‰‡ã€‚
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 56
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 10,561

### References
- https://github.com/wmjordan/PDFPatcher

## Mentra-Community/MentraOS

### Executive Summary
- The open-source OS for smart glasses with dozens of apps. Get captions, AI assistant, notifications, translation, and more. Devs now write 1 app that runs on any pair of smart glases.
- ---
- MentraOS
The open source operating system for smart glasses
Website
â€¢
Documentation
â€¢
Developer Console
â€¢
Mentra Store
Supported Smart Glasses
Works with Even Realities G1, Mentra Mach 1, Mentra Live. See
smart glasses compatibility list here
.
Apps on Mentra Store
The Mentra Store already has a ton of useful apps that real users are running everyday. Here are some apps already published by developers on the Mentra Store:
Live Captions
Link
Merge
Notes
Calendar
Dash
Translation
â†’ Browse All Apps
Write Once, Run on Any Smart Glasses
MentraOS is how developers build smart glasses apps.
We handle the pairing, connection, data streaming, and cross-compatibility, so you can focus on creating amazing apps. Every component is 100% open source (MIT license).
Why Build with MentraOS?
Cross Compatibility
: Your app runs on any pair of smart glasses
Speed
: TypeScript SDK means you're making apps in minutes, not months
Control
: Access smart glasses I/O - displays, microphones, cameras, speakers
Distribution
: Get your app in front of everyone using smart glasses
MentraOS Community
The MentraOS Community is a group of developers, companies, and users dedicated to ensuring the next personal computer is open, cross-compatible, and user-controlled. That's why we're building MentraOS.
To get involved, join the
MentraOS Community Discord server
.
Contact
Have questions or ideas? We'd love to hear from you!
Email
:
team@mentra.glass
Discord
:
Join our community
Twitter
:
Follow @mentralabs
Contributing
MentraOS is made by a community and we welcome PRs. Here's the Contributors Guide:
docs.mentra.glass/contributing
License
MIT License Copyright 2025 MentraOS Community
Â© 2025 Mentra Labs
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 55
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 1,316

### References
- https://github.com/Mentra-Community/MentraOS

## rustfs/rustfs

### Executive Summary
- ğŸš€ High-performance distributed object storage for MinIO alternative.
- ---
- RustFS is a high-performance distributed object storage software built using Rust
Getting Started
Â·
Docs
Â·
Bug reports
Â·
Discussions
English |
ç®€ä½“ä¸­æ–‡
|
Deutsch
|
EspaÃ±ol
|
franÃ§ais
|
æ—¥æœ¬èª
|
í•œêµ­ì–´
|
PortuguÃªs
|
Ğ ÑƒÑÑĞºĞ¸Ğ¹
RustFS is a high-performance distributed object storage software built using Rust, one of the most popular languages worldwide. Along with MinIO, it shares a range of advantages such as simplicity, S3 compatibility, open-source nature, support for data lakes, AI, and big data. Furthermore, it has a better and more user-friendly open-source license in comparison to other storage systems, being constructed under the Apache license. As Rust serves as its foundation, RustFS provides faster speed and safer distributed features for high-performance object storage.
âš ï¸
RustFS is under rapid development. Do NOT use in production environments!
Features
High Performance
: Built with Rust, ensuring speed and efficiency.
Distributed Architecture
: Scalable and fault-tolerant design for large-scale deployments.
S3 Compatibility
: Seamless integration with existing S3-compatible applications.
Data Lake Support
: Optimized for big data and AI workloads.
Open Source
: Licensed under Apache 2.0, encouraging community contributions and transparency.
User-Friendly
: Designed with simplicity in mind, making it easy to deploy and manage.
RustFS vs MinIO
Stress test server parameters
Type
parameter
Remark
CPU
2 Core
Intel Xeon(Sapphire Rapids) Platinum 8475B , 2.7/3.2 GHz
Memory
4GB
Network
15Gbp
Driver
40GB x 4
IOPS 3800 / Driver
rustfs.mp4
RustFS vs Other object storage
RustFS
Other object storage
Powerful Console
Simple and useless Console
Developed based on Rust language, memory is safer
Developed in Go or C, with potential issues like memory GC/leaks
Does not report logs to third-party countries
Reporting logs to other third countries may violate national security laws
Licensed under Apache, more business-friendly
AGPL V3 License and other License, polluted open source and License traps, infringement of intellectual property rights
Comprehensive S3 support, works with domestic and international cloud providers
Full support for S3, but no local cloud vendor support
Rust-based development, strong support for secure and innovative devices
Poor support for edge gateways and secure innovative devices
Stable commercial prices, free community support
High pricing, with costs up to $250,000 for 1PiB
No risk
Intellectual property risks and risks of prohibited uses
Quickstart
To get started with RustFS, follow these steps:
One-click installation script (Option 1)â€‹â€‹
curl -O  https://rustfs.com/install_rustfs.sh
&&
bash install_rustfs.sh
Docker Quick Start (Option 2)â€‹â€‹
#
create data and logs directories
mkdir -p data logs
#
using latest alpha version
docker run -d -p 9000:9000 -v
$(
pwd
)
/data:/data -v
$(
pwd
)
/logs:/logs rustfs/rustfs:alpha
#
Specific version
docker run -d -p 9000:9000 -v
$(
pwd
)
/data:/data -v
$(
pwd
)
/logs:/logs rustfs/rustfs:1.0.0.alpha.45
For docker installation, you can also run the container with docker compose. With the
docker-compose.yml
file under root directory, running the command:
docker compose --profile observability up -d
NOTE
: You should be better to have a look for
docker-compose.yaml
file. Because, several services contains in the file. Grafan,prometheus,jaeger containers will be launched using docker compose file, which is helpful for rustfs observability. If you want to start redis as well as nginx container, you can specify the corresponding profiles.
Build from Source (Option 3) - Advanced Users
For developers who want to build RustFS Docker images from source with multi-architecture support:
#
Build multi-architecture images locally
./docker-buildx.sh --build-arg RELEASE=latest
#
Build and push to registry
./docker-buildx.sh --push
#
Build specific version
./docker-buildx.sh --release v1.0.0 --push
#
Build for custom registry
./docker-buildx.sh --registry your-registry.com --namespace yourname --push
The
docker-buildx.sh
script supports:
Multi-architecture builds
:
linux/amd64
,
linux/arm64
Automatic version detection
: Uses git tags or commit hashes
Registry flexibility
: Supports Docker Hub, GitHub Container Registry, etc.
Build optimization
: Includes caching and parallel builds
You can also use Make targets for convenience:
make docker-buildx
#
Build locally
make docker-buildx-push
#
Build and push
make docker-buildx-version VERSION=v1.0.0
#
Build specific version
make help-docker
#
Show all Docker-related commands
Access the Console
: Open your web browser and navigate to
http://localhost:9000
to access the RustFS console, default username and password is
rustfsadmin
.
Create a Bucket
: Use the console to create a new bucket for your objects.
Upload Objects
: You can upload files directly through the console or use S3-compatible APIs to interact with your RustFS instance.
NOTE
: If you want to access RustFS instance with
https
, you can refer to
TLS configuration docs
.
Documentation
For detailed documentation, including configuration options, API references, and advanced usage, please visit our
Documentation
.
Getting Help
If you have any questions or need assistance, you can:
Check the
FAQ
for common issues and solutions.
Join our
GitHub Discussions
to ask questions and share your experiences.
Open an issue on our
GitHub Issues
page for bug reports or feature requests.
Links
Documentation
- The manual you should read
Changelog
- What we broke and fixed
GitHub Discussions
- Where the community lives
Contact
Bugs
:
GitHub Issues
Business
:
hello@rustfs.com
Jobs
:
jobs@rustfs.com
General Discussion
:
GitHub Discussions
Contributing
:
CONTRIBUTING.md
Contributors
RustFS is a community-driven project, and we appreciate all contributions. Check out the
Contributors
page to see the amazing people who have helped make RustFS better.
License
Apache 2.0
RustFS
is a trademark of RustFS, Inc. All other trademarks are the property of their respective owners.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 52
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 8,877

### References
- https://github.com/rustfs/rustfs

## krahets/hello-algo

### Executive Summary
- ã€ŠHello ç®—æ³•ã€‹ï¼šåŠ¨ç”»å›¾è§£ã€ä¸€é”®è¿è¡Œçš„æ•°æ®ç»“æ„ä¸ç®—æ³•æ•™ç¨‹ã€‚æ”¯æŒ Python, Java, C++, C, C#, JS, Go, Swift, Rust, Ruby, Kotlin, TS, Dart ä»£ç ã€‚ç®€ä½“ç‰ˆå’Œç¹ä½“ç‰ˆåŒæ­¥æ›´æ–°ï¼ŒEnglish version in translation
- ---
- åŠ¨ç”»å›¾è§£ã€ä¸€é”®è¿è¡Œçš„æ•°æ®ç»“æ„ä¸ç®—æ³•æ•™ç¨‹
ç®€ä½“ä¸­æ–‡
  ï½œ
ç¹é«”ä¸­æ–‡
ï½œ
English
å…³äºæœ¬ä¹¦
æœ¬é¡¹ç›®æ—¨åœ¨æ‰“é€ ä¸€æœ¬å¼€æºå…è´¹ã€æ–°æ‰‹å‹å¥½çš„æ•°æ®ç»“æ„ä¸ç®—æ³•å…¥é—¨æ•™ç¨‹ã€‚
å…¨ä¹¦é‡‡ç”¨åŠ¨ç”»å›¾è§£ï¼Œå†…å®¹æ¸…æ™°æ˜“æ‡‚ã€å­¦ä¹ æ›²çº¿å¹³æ»‘ï¼Œå¼•å¯¼åˆå­¦è€…æ¢ç´¢æ•°æ®ç»“æ„ä¸ç®—æ³•çš„çŸ¥è¯†åœ°å›¾ã€‚
æºä»£ç å¯ä¸€é”®è¿è¡Œï¼Œå¸®åŠ©è¯»è€…åœ¨ç»ƒä¹ ä¸­æå‡ç¼–ç¨‹æŠ€èƒ½ï¼Œäº†è§£ç®—æ³•å·¥ä½œåŸç†å’Œæ•°æ®ç»“æ„åº•å±‚å®ç°ã€‚
æå€¡è¯»è€…äº’åŠ©å­¦ä¹ ï¼Œæ¬¢è¿å¤§å®¶åœ¨è¯„è®ºåŒºæå‡ºé—®é¢˜ä¸åˆ†äº«è§è§£ï¼Œåœ¨äº¤æµè®¨è®ºä¸­å…±åŒè¿›æ­¥ã€‚
è‹¥æœ¬ä¹¦å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œè¯·åœ¨é¡µé¢å³ä¸Šè§’ç‚¹ä¸ª Star â­ æ”¯æŒä¸€ä¸‹ï¼Œè°¢è°¢ï¼
æ¨èè¯­
â€œä¸€æœ¬é€šä¿—æ˜“æ‡‚çš„æ•°æ®ç»“æ„ä¸ç®—æ³•å…¥é—¨ä¹¦ï¼Œå¼•å¯¼è¯»è€…æ‰‹è„‘å¹¶ç”¨åœ°å­¦ä¹ ï¼Œå¼ºçƒˆæ¨èç®—æ³•åˆå­¦è€…é˜…è¯»ã€‚â€
â€”â€” é‚“ä¿Šè¾‰ï¼Œæ¸…åå¤§å­¦è®¡ç®—æœºç³»æ•™æˆ
â€œå¦‚æœæˆ‘å½“å¹´å­¦æ•°æ®ç»“æ„ä¸ç®—æ³•çš„æ—¶å€™æœ‰ã€ŠHello ç®—æ³•ã€‹ï¼Œå­¦èµ·æ¥åº”è¯¥ä¼šç®€å• 10 å€ï¼â€
â€”â€” ææ²ï¼Œäºšé©¬é€Šèµ„æ·±é¦–å¸­ç§‘å­¦å®¶
é¸£è°¢
Warp is built for coding with multiple AI agents.
å¼ºçƒˆæ¨è Warp ç»ˆç«¯ï¼Œé«˜é¢œå€¼ + å¥½ç”¨çš„ AIï¼Œä½“éªŒéå¸¸æ£’ï¼
è´¡çŒ®
æœ¬å¼€æºä¹¦ä»åœ¨æŒç»­æ›´æ–°ä¹‹ä¸­ï¼Œæ¬¢è¿æ‚¨å‚ä¸æœ¬é¡¹ç›®ï¼Œä¸€åŒä¸ºè¯»è€…æä¾›æ›´ä¼˜è´¨çš„å­¦ä¹ å†…å®¹ã€‚
å†…å®¹ä¿®æ­£
ï¼šè¯·æ‚¨ååŠ©ä¿®æ­£æˆ–åœ¨è¯„è®ºåŒºæŒ‡å‡ºè¯­æ³•é”™è¯¯ã€å†…å®¹ç¼ºå¤±ã€æ–‡å­—æ­§ä¹‰ã€æ— æ•ˆé“¾æ¥æˆ–ä»£ç  bug ç­‰é—®é¢˜ã€‚
ä»£ç è½¬è¯‘
ï¼šæœŸå¾…æ‚¨è´¡çŒ®å„ç§è¯­è¨€ä»£ç ï¼Œå·²æ”¯æŒ Pythonã€Javaã€C++ã€Goã€JavaScript ç­‰ 12 é—¨ç¼–ç¨‹è¯­è¨€ã€‚
ä¸­è¯‘è‹±
ï¼šè¯šé‚€æ‚¨åŠ å…¥æˆ‘ä»¬çš„ç¿»è¯‘å°ç»„ï¼Œæˆå‘˜ä¸»è¦æ¥è‡ªè®¡ç®—æœºç›¸å…³ä¸“ä¸šã€è‹±è¯­ä¸“ä¸šå’Œè‹±æ–‡æ¯è¯­è€…ã€‚
æ¬¢è¿æ‚¨æå‡ºå®è´µæ„è§å’Œå»ºè®®ï¼Œå¦‚æœ‰ä»»ä½•é—®é¢˜è¯·æäº¤ Issues æˆ–å¾®ä¿¡è”ç³»
krahets-jyd
ã€‚
æ„Ÿè°¢æœ¬å¼€æºä¹¦çš„æ¯ä¸€ä½æ’°ç¨¿äººï¼Œæ˜¯ä»–ä»¬çš„æ— ç§å¥‰çŒ®è®©è¿™æœ¬ä¹¦å˜å¾—æ›´å¥½ï¼Œä»–ä»¬æ˜¯ï¼š
License
The texts, code, images, photos, and videos in this repository are licensed under
CC BY-NC-SA 4.0
.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 51
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 117,629

### References
- https://github.com/krahets/hello-algo

## coollabsio/coolify

### Executive Summary
- An open-source & self-hostable Heroku / Netlify / Vercel alternative.
- ---
- About the Project
Coolify is an open-source & self-hostable alternative to Heroku / Netlify / Vercel / etc.
It helps you manage your servers, applications, and databases on your own hardware; you only need an SSH connection. You can manage VPS, Bare Metal, Raspberry PIs, and anything else.
Imagine having the ease of a cloud but with your own servers. That is
Coolify
.
No vendor lock-in, which means that all the configurations for your applications/databases/etc are saved to your server. So, if you decide to stop using Coolify (oh nooo), you could still manage your running resources. You lose the automations and all the magic. ğŸª„ï¸
For more information, take a look at our landing page at
coolify.io
.
Installation
curl -fsSL https://cdn.coollabs.io/coolify/install.sh
|
bash
You can find the installation script source
here
.
Note
Please refer to the
docs
for more information about the installation.
Support
Contact us at
coolify.io/docs/contact
.
Cloud
If you do not want to self-host Coolify, there is a paid cloud version available:
app.coolify.io
For more information & pricing, take a look at our landing page
coolify.io
.
Why should I use the Cloud version?
The recommended way to use Coolify is to have one server for Coolify and one (or more) for the resources you are deploying. A server is around 4-5$/month.
By subscribing to the cloud version, you get the Coolify server for the same price, but with:
High-availability
Free email notifications
Better support
Less maintenance for you
Donations
To stay completely free and open-source, with no feature behind the paywall and evolve the project, we need your help. If you like Coolify, please consider donating to help us fund the project's future development.
coolify.io/sponsorships
Thank you so much!
Big Sponsors
CubePath
- Dedicated Servers & Instant Deploy
GlueOps
- DevOps automation and infrastructure management
Algora
- Open source contribution platform
Ubicloud
- Open source cloud infrastructure platform
LiquidWeb
- Premium managed hosting solutions
Convex
- Open-source reactive database for web app developers
Arcjet
- Advanced web security and performance solutions
SaasyKit
- Complete SaaS starter kit for developers
SupaGuide
- Your comprehensive guide to Supabase
Logto
- The better identity infrastructure for developers
Trieve
- AI-powered search and analytics
Supadata AI
- Scrape YouTube, web, and files. Get AI-ready, clean data
Darweb
- Design. Develop. Deliver. Specialized in 3D CPQ Solutions
Hetzner
- Server, cloud, hosting, and data center solutions
COMIT
- New York Times awardâ€“winning contractor
Blacksmith
- Infrastructure automation platform
WZ-IT
- German agency for customised cloud solutions
BC Direct
- Your trusted technology consulting partner
Tigris
- Modern developer data platform
Hostinger
- Web hosting and VPS solutions
QuantCDN
- Enterprise-grade content delivery network
PFGLabs
- Build Real Projects with Golang
JobsCollider
- 30,000+ remote jobs for developers
Juxtdigital
- Digital PR & AI Authority Building Agency
Cloudify.ro
- Cloud hosting solutions
CodeRabbit
- Cut Code Review Time & Bugs in Half
American Cloud
- US-based cloud infrastructure services
MassiveGrid
- Enterprise cloud hosting solutions
Syntax.fm
- Podcast for web developers
Tolgee
- The open source localization platform
CompAI
- Open source compliance automation platform
GoldenVM
- Premium virtual machine hosting solutions
Gozunga
- Seriously Simple Cloud Infrastructure
Macarne
- Best IP Transit & Carrier Ethernet Solutions for Simplified Network Connectivity
Small Sponsors
...and many more at
GitHub Sponsors
Recognitions
Core Maintainers
Andras Bacsai
ğŸ”ï¸ Peak
Repo Activity
Star History
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 49
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 46,176

### References
- https://github.com/coollabsio/coolify

## hyprwm/Hyprland

### Executive Summary
- Hyprland is an independent, highly customizable, dynamic tiling Wayland compositor that doesn't sacrifice on its looks.
- ---
- Hyprland is a 100% independent, dynamic tiling Wayland compositor that doesn't sacrifice on its looks.
It provides the latest Wayland features, is highly customizable, has all the eyecandy, the most powerful plugins,
easy IPC, much more QoL stuff than other compositors and more...
Install
Quick Start
Configure
Contribute
Features
All of the eyecandy: gradient borders, blur, animations, shadows and much more
A lot of customization
100% independent, no wlroots, no libweston, no kwin, no mutter.
Custom bezier curves for the best animations
Powerful plugin support
Built-in plugin manager
Tearing support for better gaming performance
Easily expandable and readable codebase
Fast and active development
Not afraid to provide bleeding-edge features
Config reloaded instantly upon saving
Fully dynamic workspaces
Two built-in layouts and more available as plugins
Global keybinds passed to your apps of choice
Tiling/pseudotiling/floating/fullscreen windows
Special workspaces (scratchpads)
Window groups (tabbed mode)
Powerful window/monitor/layer rules
Socket-based IPC
Native IME and Input Panels Support
and much more...
Gallery
Special Thanks
wlroots
-
For powering Hyprland in the past
tinywl
-
For showing how 2 do stuff
Sway
-
For showing how 2 do stuff the overkill way
Vivarium
-
For showing how 2 do stuff the simple way
dwl
-
For showing how 2 do stuff the hacky way
Wayfire
-
For showing how 2 do some graphics stuff
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 49
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 31,051

### References
- https://github.com/hyprwm/Hyprland

## tangyoha/telegram_media_downloader

### Executive Summary
- åŸºäºDineshkarthikçš„é¡¹ç›®ï¼Œ ç”µæŠ¥è§†é¢‘ä¸‹è½½ï¼Œç”µæŠ¥èµ„æºä¸‹è½½ï¼Œè·¨å¹³å°ï¼Œæ”¯æŒwebæŸ¥çœ‹ä¸‹è½½è¿›åº¦ ï¼Œæ”¯æŒbotä¸‹å‘æŒ‡ä»¤ä¸‹è½½ï¼Œæ”¯æŒä¸‹è½½å·²ç»åŠ å…¥çš„ç§æœ‰ç¾¤ä½†æ˜¯é™åˆ¶ä¸‹è½½çš„èµ„æºï¼Œ telegram media download,Download media files from a telegram conversation/chat/channel up to 2GiB per file
- ---
- Telegram Media Downloader
ä¸­æ–‡
Â·
Feature request
Â·
Report a bug
Â·
Support:
Discussions
&
Telegram Community
Overview
Support two default running
The robot is running, and the command
download
or
forward
is issued from the robot
Download as a one-time download tool
UI
Web page
After running, open a browser and visit
localhost:5000
If it is a remote machine, you need to configure web_host: 0.0.0.0
Robot
Need to configure bot_token, please refer to
Documentation
Support
Category
Support
Language
Python 3.7
and above
Download media types
audio, document, photo, video, video_note, voice
Version release plan
v2.2.0
Installation
For *nix os distributions with
make
availability
git clone https://github.com/tangyoha/telegram_media_downloader.git
cd
telegram_media_downloader
make install
For Windows which doesn't have
make
inbuilt
git clone https://github.com/tangyoha/telegram_media_downloader.git
cd
telegram_media_downloader
pip3 install -r requirements.txt
Docker
For more detailed installation tutorial, please check the wiki
Make sure you have
docker
and
docker-compose
installed
docker pull tangyoha/telegram_media_downloader:latest
mkdir -p
~
/app
&&
mkdir -p
~
/app/log/
&&
cd
~
/app
wget https://raw.githubusercontent.com/tangyoha/telegram_media_downloader/master/docker-compose.yaml -O docker-compose.yaml
wget https://raw.githubusercontent.com/tangyoha/telegram_media_downloader/master/config.yaml -O config.yaml
wget https://raw.githubusercontent.com/tangyoha/telegram_media_downloader/master/data.yaml -O data.yaml
#
vi config.yaml and docker-compose.yaml
vi config.yaml
#
The first time you need to start the foreground
#
enter your phone number and code, then exit(ctrl + c)
docker-compose run --rm telegram_media_downloader
#
After performing the above operations, all subsequent startups will start in the background
docker-compose up -d
#
Upgrade
docker pull tangyoha/telegram_media_downloader:latest
cd
~
/app
docker-compose down
docker-compose up -d
Upgrade installation
cd
telegram_media_downloader
pip3 install -r requirements.txt
Configuration
All the configurations are  passed to the Telegram Media Downloader via
config.yaml
file.
Getting your API Keys:
The very first step requires you to obtain a valid Telegram API key (API id/hash pair):
Visit
https://my.telegram.org/apps
and log in with your Telegram Account.
Fill out the form to register a new Telegram application.
Done! The API key consists of two parts:
api_id
and
api_hash
.
Getting chat id:
1. Using web telegram:
Open
https://web.telegram.org/?legacy=1#/im
Now go to the chat/channel and you will see the URL as something like
https://web.telegram.org/?legacy=1#/im?p=u853521067_2449618633394
here
853521067
is the chat id.
https://web.telegram.org/?legacy=1#/im?p=@somename
here
somename
is the chat id.
https://web.telegram.org/?legacy=1#/im?p=s1301254321_6925449697188775560
here take
1301254321
and add
-100
to the start of the id =>
-1001301254321
.
https://web.telegram.org/?legacy=1#/im?p=c1301254321_6925449697188775560
here take
1301254321
and add
-100
to the start of the id =>
-1001301254321
.
2. Using bot:
Use
@username_to_id_bot
to get the chat_id of
almost any telegram user: send username to the bot or just forward their message to the bot
any chat: send chat username or copy and send its joinchat link to the bot
public or private channel: same as chats, just copy and send to the bot
id of any telegram bot
config.yaml
api_hash
:
your_api_hash
api_id
:
your_api_id
chat
:
-
chat_id
:
telegram_chat_id
last_read_message_id
:
0
download_filter
:
message_date >= 2022-12-01 00:00:00 and message_date <= 2023-01-17 00:00:00
-
chat_id
:
telegram_chat_id_2
last_read_message_id
:
0
#
note we remove ids_to_retry to data.yaml
ids_to_retry
:
[]
media_types
:
-
audio
-
document
-
photo
-
video
-
voice
-
animation
#
gif
file_formats
:
audio
:
  -
all
document
:
  -
pdf
-
epub
video
:
  -
mp4
save_path
:
D:\telegram_media_downloader
file_path_prefix
:
-
chat_title
-
media_datetime
upload_drive
:
#
required
enable_upload_file
:
true
#
required
remote_dir
:
drive:/telegram
#
required
upload_adapter
:
rclone
#
option,when config upload_adapter rclone then this config are required
rclone_path
:
D:\rclone\rclone.exe
#
option
before_upload_file_zip
:
True
#
option
after_upload_file_delete
:
True
hide_file_name
:
true
file_name_prefix
:
-
message_id
-
file_name
file_name_prefix_split
:
'
-
'
max_download_task
:
5
web_host
:
127.0.0.1
web_port
:
5000
language
:
EN
web_login_secret
:
123
allowed_user_ids
:
-
'
me
'
date_format
:
'
%Y_%m
'
enable_download_txt
:
false
api_hash
- The api_hash you got from telegram apps
api_id
- The api_id you got from telegram apps
bot_token
- Your bot token
chat
- Chat list
chat_id
-  The id of the chat/channel you want to download media. Which you get from the above-mentioned steps.
download_filter
- Download filter, see
How to use Filter
last_read_message_id
- If it is the first time you are going to read the channel let it be
0
or if you have already used this script to download media it will have some numbers which are auto-updated after the scripts successful execution. Don't change it.
ids_to_retry
-
Leave it as it is.
This is used by the downloader script to keep track of all skipped downloads so that it can be downloaded during the next execution of the script.
media_types
- Type of media to download, you can update which type of media you want to download it can be one or any of the available types.
file_formats
- File types to download for supported media types which are
audio
,
document
and
video
. Default format is
all
, downloads all files.
save_path
- The root directory where you want to store downloaded files.
file_path_prefix
- Store file subfolders, the order of the list is not fixed, can be randomly combined.
chat_title
- Channel or group title, it will be chat id if not exist title.
media_datetime
- Media date.
media_type
- Media type, also see
media_types
.
upload_drive
- You can upload file to cloud drive.
enable_upload_file
- Enable upload file, default
false
.
remote_dir
- Where you upload, like
drive_id/drive_name
.
upload_adapter
- Upload file adapter, which can be
rclone
,
aligo
. If it is
rclone
, it supports all
rclone
servers that support uploading. If it is
aligo
, it supports uploading
Ali cloud disk
.
rclone_path
- RClone exe path, see
How to use rclone
before_upload_file_zip
- Zip file before upload, default
false
.
after_upload_file_delete
- Delete file after upload success, default
false
.
file_name_prefix
- Custom file name, use the same as
file_path_prefix
message_id
- Message id
file_name
- File name (may be empty)
caption
- The title of the message (may be empty)
file_name_prefix_split
- Custom file name prefix symbol, the default is
-
max_download_task
- The maximum number of task download tasks, the default is 5.
hide_file_name
- Whether to hide the web interface file name, default
false
web_host
- Web host
web_port
- Web port
language
- Application language, the default is English (
EN
), optional
ZH
(Chinese),
RU
,
UA
web_login_secret
- Web page login password, if not configured, no login is required to access the web page
log_level
- see
logging._nameToLevel
.
forward_limit
- Limit the number of forwards per minute, the default is 33, please do not modify this parameter by default.
allowed_user_ids
- Who is allowed to use the robot? The default login account can be used. Please add single quotes to the name with @.
date_format
Support custom configuration of media_datetime format in file_path_prefix.see
python-datetime
enable_download_txt
Enable download txt file, default
false
Execution
python3 media_downloader.py
All downloaded media will be stored at the root of
save_path
.
The specific location reference is as follows:
The complete directory of video download is:
save_path
/
chat_title
/
media_datetime
/
media_type
.
The order of the list is not fixed and can be randomly combined.
If the configuration is empty, all files are saved under
save_path
.
Proxy
socks4, socks5, http
proxies are supported in this project currently. To use it, add the following to the bottom of your
config.yaml
file
proxy
:
scheme
:
socks5
hostname
:
127.0.0.1
port
:
1234
username
:
your_username(delete the line if none)
password
:
your_password(delete the line if none)
If your proxy doesnâ€™t require authorization you can omit username and password. Then the proxy will automatically be enabled.
Contributing
Contributing Guidelines
Read through our
contributing guidelines
to learn about our submission process, coding rules and more.
Want to Help?
Want to file a bug, contribute some code, or improve documentation? Excellent! Read up on our guidelines for
contributing
.
Code of Conduct
Help us keep Telegram Media Downloader open and inclusive. Please read and follow our
Code of Conduct
.
Sponsor
PayPal
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 48
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 4,112

### References
- https://github.com/tangyoha/telegram_media_downloader

## DioxusLabs/dioxus

### Executive Summary
- Fullstack app framework for web, desktop, and mobile.
- ---
- Website
|
Examples
|
Guide
|
ä¸­æ–‡
|
PT-BR
|
æ—¥æœ¬èª
|
TÃ¼rkÃ§e
|
í•œêµ­ì–´
âœ¨ Dioxus 0.7 is in alpha - test it out! âœ¨
Build for web, desktop, and mobile, and more with a single codebase. Zero-config setup, integrated hot-reloading, and signals-based state management. Add backend functionality with Server Functions and bundle with our CLI.
fn
app
(
)
->
Element
{
let
mut
count =
use_signal
(
||
0
)
;
rsx
!
{
h1
{
"High-Five counter: {count}"
}
button
{
onclick
:
move |_| count +=
1
,
"Up high!"
}
button
{
onclick
:
move |_| count -=
1
,
"Down low!"
}
}
}
â­ï¸ Unique features:
Cross-platform apps in three lines of code (web, desktop, mobile, server, and more)
Ergonomic state management
combines the best of React, Solid, and Svelte
Built-in featureful, type-safe, fullstack web framework
Integrated bundler for deploying to the web, macOS, Linux, and Windows
Subsecond Rust hot-patching and asset hot-reloading
And more!
Take a tour of Dioxus
.
Instant hot-reloading
With one command,
dx serve
and your app is running. Edit your markup, styles, and see changes in milliseconds. Use our experimental
dx serve --hotpatch
to update Rust code in real time.
Build Beautiful Apps
Dioxus apps are styled with HTML and CSS. Use the built-in TailwindCSS support or load your favorite CSS library. Easily call into native code (objective-c, JNI, Web-Sys) for a perfect native touch.
Truly fullstack applications
Dioxus deeply integrates with
axum
to provide powerful fullstack capabilities for both clients and servers. Pick from a wide array of built-in batteries like WebSockets, SSE, Streaming, File Upload/Download, Server-Side-Rendering, Forms, Middleware, and Hot-Reload, or go fully custom and integrate your existing axum backend.
Experimental Native Renderer
Render using web-sys, webview, server-side-rendering, liveview, or even with our experimental WGPU-based renderer. Embed Dioxus in Bevy, WGPU, or even run on embedded Linux!
First-party primitive components
Get started quickly with a complete set of primitives modeled after shadcn/ui and Radix-Primitives.
First-class Android and iOS support
Dioxus is the fastest way to build native mobile apps with Rust. Simply run
dx serve --platform android
and your app is running in an emulator or on device in seconds. Call directly into JNI and Native APIs.
Bundle for web, desktop, and mobile
Simply run
dx bundle
and your app will be built and bundled with maximization optimizations. On the web, take advantage of
.avif
generation,
.wasm
compression, minification
, and more. Build WebApps weighing
less than 50kb
and desktop/mobile apps less than 5mb.
Fantastic documentation
We've put a ton of effort into building clean, readable, and comprehensive documentation. All html elements and listeners are documented with MDN docs, and our Docs runs continuous integration with Dioxus itself to ensure that the docs are always up to date. Check out the
Dioxus website
for guides, references, recipes, and more. Fun fact: we use the Dioxus website as a testbed for new Dioxus features -
check it out!
Modular and Customizable
Build your own renderer, or use a community renderer like
Freya
. Use our modular components like RSX, VirtualDom, Blitz, Taffy, and Subsecond.
Community
Dioxus is a community-driven project, with a very active
Discord
and
GitHub
community. We're always looking for help, and we're happy to answer questions and help you get started.
Our SDK
is community-run and we even have a
GitHub organization
for the best Dioxus crates that receive free upgrades and support.
Full-time core team
Dioxus has grown from a side project to a small team of fulltime engineers. Thanks to the generous support of FutureWei, Satellite.im, the GitHub Accelerator program, we're able to work on Dioxus full-time. Our long term goal is for Dioxus to become self-sustaining by providing paid high-quality enterprise tools. If your company is interested in adopting Dioxus and would like to work with us, please reach out!
Supported Platforms
Web
Render directly to the DOM using WebAssembly
Pre-render with SSR and rehydrate on the client
Simple "hello world" at about 50kb, comparable to React
Built-in dev server and hot reloading for quick iteration
Desktop
Render using Webview or - experimentally - with WGPU or
Freya
(Skia)
Zero-config setup. Simply `cargo run` or `dx serve` to build your app
Full support for native system access without IPC
Supports macOS, Linux, and Windows. Portable <3mb binaries
Mobile
Render using Webview or - experimentally - with WGPU or Skia
Build .ipa and .apk files for iOS and Android
Call directly into Java and Objective-C with minimal overhead
From "hello world" to running on device in seconds
Server-side Rendering
Suspense, hydration, and server-side rendering
Quickly drop in backend functionality with server functions
Extractors, middleware, and routing integrations
Static-site generation and incremental regeneration
Running the examples
The examples in the main branch of this repository target the git version of dioxus and the CLI. If you are looking for examples that work with the latest stable release of dioxus, check out the
0.6 branch
.
The examples in the top level of this repository can be run with:
cargo run --example
<
example
>
However, we encourage you to download the dioxus-cli to test out features like hot-reloading. To install the most recent binary CLI, you can use cargo binstall.
cargo binstall dioxus-cli@0.7.0-rc.1 --force
If this CLI is out-of-date, you can install it directly from git
cargo install --git https://github.com/DioxusLabs/dioxus dioxus-cli --locked
With the CLI, you can also run examples with the web platform. You will need to disable the default desktop feature and enable the web feature with this command:
dx serve --example
<
example
>
--platform web -- --no-default-features
Contributing
Check out the website
section on contributing
.
Report issues on our
issue tracker
.
Join
the discord and ask questions!
License
This project is licensed under either the
MIT license
or the
Apache-2 License
.
Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in Dioxus by you, shall be licensed as MIT or Apache-2, without any additional
terms or conditions.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 47
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 31,009

### References
- https://github.com/DioxusLabs/dioxus

## WECENG/ticket-purchase

### Executive Summary
- å¤§éº¦è‡ªåŠ¨æŠ¢ç¥¨ï¼Œæ”¯æŒäººå‘˜ã€åŸå¸‚ã€æ—¥æœŸåœºæ¬¡ã€ä»·æ ¼é€‰æ‹©
- ---
- å¤§éº¦æŠ¢ç¥¨è„šæœ¬ V1.0
ç‰¹å¾
è‡ªåŠ¨æ— å»¶æ—¶æŠ¢ç¥¨
æ”¯æŒäººå‘˜ã€åŸå¸‚ã€æ—¥æœŸåœºæ¬¡ã€ä»·æ ¼é€‰æ‹©
åŠŸèƒ½ä»‹ç»
é€šè¿‡seleniumæ‰“å¼€é¡µé¢è¿›è¡Œç™»å½•ï¼Œæ¨¡æ‹Ÿç”¨æˆ·è´­ç¥¨æµç¨‹è‡ªåŠ¨è´­ç¥¨
å…¶æµç¨‹å›¾å¦‚ä¸‹:
å‡†å¤‡å·¥ä½œ
1. é…ç½®ç¯å¢ƒ
1.1å®‰è£…python3ç¯å¢ƒ
Windows
è®¿é—®Pythonå®˜æ–¹ç½‘ç«™ï¼š
https://www.python.org/downloads/windows/
ä¸‹è½½æœ€æ–°çš„Python 3.9+ç‰ˆæœ¬çš„å®‰è£…ç¨‹åºã€‚
è¿è¡Œå®‰è£…ç¨‹åºã€‚
åœ¨å®‰è£…ç¨‹åºä¸­ï¼Œç¡®ä¿å‹¾é€‰ "Add Python X.X to PATH" é€‰é¡¹ï¼Œè¿™å°†è‡ªåŠ¨å°†Pythonæ·»åŠ åˆ°ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­ï¼Œæ–¹ä¾¿åœ¨å‘½ä»¤è¡Œä¸­ä½¿ç”¨Pythonã€‚
å®Œæˆå®‰è£…åï¼Œä½ å¯ä»¥åœ¨å‘½ä»¤æç¤ºç¬¦æˆ–PowerShellä¸­è¾“å…¥
python3
æ¥å¯åŠ¨Pythonè§£é‡Šå™¨ã€‚
macOS
ä½ å¯ä»¥ä½¿ç”¨Homebrewæ¥å®‰è£…Python 3ã€‚
å®‰è£…Homebrewï¼ˆå¦‚æœæœªå®‰è£…ï¼‰ï¼šæ‰“å¼€ç»ˆç«¯å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
/bin/bash -c
"
$(
curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh
)
"
å®‰è£…Python 3ï¼šè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…Python 3ï¼š
brew install python@3
1.2 å®‰è£…æ‰€éœ€è¦çš„ç¯å¢ƒ
åœ¨å‘½ä»¤çª—å£è¾“å…¥å¦‚ä¸‹æŒ‡ä»¤
pip3 install selenium
1.3 ä¸‹è½½google chromeæµè§ˆå™¨
ä¸‹è½½åœ°å€:
https://www.google.cn/intl/zh-CN/chrome/?brand=YTUH&gclid=Cj0KCQjwj5mpBhDJARIsAOVjBdoV_1sBwdqKGHV3rUU1vJmNKZdy5QNzbRT8F5O0-_jq1WHXurE8a7MaAkWrEALw_wcB&gclsrc=aw.ds
2. ä¿®æ”¹é…ç½®æ–‡ä»¶
åœ¨è¿è¡Œç¨‹åºä¹‹å‰ï¼Œéœ€è¦å…ˆä¿®æ”¹
config.json
æ–‡ä»¶ã€‚è¯¥æ–‡ä»¶ç”¨äºæŒ‡å®šç”¨æˆ·éœ€è¦æŠ¢ç¥¨çš„ç›¸å…³ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¼”å”±ä¼šçš„åœºæ¬¡ã€è§‚æ¼”çš„äººå‘˜ã€åŸå¸‚ã€æ—¥æœŸã€ä»·æ ¼ç­‰ã€‚æ–‡ä»¶ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
2.1 æ–‡ä»¶å†…å®¹è¯´æ˜
index_url
ä¸ºå¤§éº¦ç½‘çš„åœ°å€ï¼Œ
æ— éœ€ä¿®æ”¹
login_url
ä¸ºå¤§éº¦ç½‘çš„ç™»å½•åœ°å€ï¼Œ
æ— éœ€ä¿®æ”¹
target_url
ä¸ºç”¨æˆ·éœ€è¦æŠ¢çš„æ¼”å”±ä¼šç¥¨çš„ç›®æ ‡åœ°å€ï¼Œ
å¾…ä¿®æ”¹
users
ä¸ºè§‚æ¼”äººçš„å§“åï¼Œ
è§‚æ¼”äººéœ€è¦ç”¨æˆ·åœ¨æ‰‹æœºå¤§éº¦APPä¸­å…ˆå¡«å†™å¥½ï¼Œç„¶åå†å¡«å…¥è¯¥é…ç½®æ–‡ä»¶ä¸­
ï¼Œ
å¾…ä¿®æ”¹
city
ä¸ºåŸå¸‚ï¼Œ
å¦‚æœç”¨æˆ·éœ€è¦æŠ¢çš„æ¼”å”±ä¼šç¥¨éœ€è¦é€‰æ‹©åŸå¸‚ï¼Œè¯·æŠŠåŸå¸‚å¡«å…¥æ­¤å¤„ã€‚å¦‚æ— éœ€é€‰æ‹©ï¼Œåˆ™ä¸å¡«
date
ä¸ºåœºæ¬¡æ—¥æœŸï¼Œ
å¾…ä¿®æ”¹ï¼Œå¯å¤šé€‰
price
ä¸ºç¥¨æ¡£çš„ä»·æ ¼ï¼Œ
å¾…ä¿®æ”¹ï¼Œå¯å¤šé€‰
if_commit_order
ä¸ºæ˜¯å¦è¦è‡ªåŠ¨æäº¤è®¢å•ï¼Œ
æ”¹æˆ true
if_listenä¸ºæ˜¯å¦å›æµç›‘å¬ï¼Œ
æ”¹æˆtrue
2.2 ç¤ºä¾‹è¯´æ˜
è¿›å…¥å¤§éº¦ç½‘
https://www.damai.cn/ï¼Œé€‰æ‹©ä½ éœ€è¦æŠ¢ç¥¨çš„æ¼”å”±ä¼šã€‚å‡è®¾å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
æ¥ä¸‹æ¥æŒ‰ç…§ä¸‹å›¾çš„æ ‡æ³¨å¯¹é…ç½®æ–‡ä»¶è¿›è¡Œä¿®æ”¹ï¼š
æœ€ç»ˆ
config.json
çš„æ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š
{
"index_url"
:
"
https://www.damai.cn/
"
,
"login_url"
:
"
https://passport.damai.cn/login?ru=https%3A%2F%2Fwww.damai.cn%2F
"
,
"target_url"
:
"
https://detail.damai.cn/item.htm?spm=a2oeg.home.card_0.ditem_1.591b23e1JQGWHg&id=740680932762
"
,
"users"
: [
"
åå­—1
"
,
"
åå­—2
"
],
"city"
:
"
å¹¿å·
"
,
"date"
:
"
2023-10-28
"
,
"price"
:
"
1039
"
,
"if_listen"
:
true
,
"if_commit_order"
:
true
}
3.è¿è¡Œç¨‹åº
è¿è¡Œç¨‹åºå¼€å§‹æŠ¢ç¥¨ï¼Œè¿›å…¥å‘½ä»¤çª—å£ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š
cd
damai
python3 damai.py
å¤§éº¦appæŠ¢ç¥¨
å¤§éº¦appæŠ¢ç¥¨è„šæœ¬éœ€è¦ä¾èµ–appiumï¼Œå› æ­¤éœ€è¦ç°åœ¨å®‰è£…appium server&clientç¯å¢ƒï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š
appium server
ä¸‹è½½
å…ˆå®‰è£…å¥½nodeç¯å¢ƒï¼ˆå…·å¤‡npmï¼‰nodeç‰ˆæœ¬å·18.0.0
å…ˆä¸‹è½½å¹¶å®‰è£…å¥½android sdkï¼Œå¹¶é…ç½®ç¯å¢ƒå˜é‡ï¼ˆappium serverè¿è¡Œéœ€ä¾èµ–android sdk)
ä¸‹è½½appium
npm install -g appium
æŸ¥çœ‹appiumæ˜¯å¦å®‰è£…æˆåŠŸ
appium -v
ä¸‹è½½UiAutomator2é©±åŠ¨
npm install appium-uiautomator2-driver
â€‹		å¯èƒ½ä¼šé‡åˆ°å¦‚ä¸‹é”™è¯¯ï¼š
âœ  xcode git:(master) âœ— npm install appium-uiautomator2-driver

npm ERR! code 1
npm ERR! path /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/appium-chromedriver
npm ERR! command failed
npm ERR! command sh -c node install-npm.js
npm ERR! [11:57:54] Error installing Chromedriver: Request failed with status code 404
npm ERR! [11:57:54] AxiosError: Request failed with status code 404
npm ERR!     at settle (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/core/settle.js:19:12)
npm ERR!     at IncomingMessage.handleStreamEnd (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/adapters/http.js:572:11)
npm ERR!     at IncomingMessage.emit (node:events:539:35)
npm ERR!     at endReadableNT (node:internal/streams/readable:1344:12)
npm ERR!     at processTicksAndRejections (node:internal/process/task_queues:82:21)
npm ERR! [11:57:54] Downloading Chromedriver can be skipped by setting the'APPIUM_SKIP_CHROMEDRIVER_INSTALL' environment variable.

npm ERR! A complete log of this run can be found in:
npm ERR!     /Users/chenweicheng/.npm/_logs/2023-10-26T03_57_35_950Z-debug-0.log
â€‹		è§£å†³åŠæ³•ï¼ˆæ·»åŠ ç¯å¢ƒå˜é‡ï¼Œé”™è¯¯åŸå› æ˜¯æ²¡æœ‰æ‰¾åˆ°chromeæµè§ˆå™¨é©±åŠ¨ï¼Œå¿½ç•¥å³å¯ï¼‰
export
APPIUM_SKIP_CHROMEDRIVER_INSTALL=true
å¯åŠ¨
å¯åŠ¨appium serverå¹¶ä½¿ç”¨uiautomator2é©±åŠ¨
appium --use-plugins uiautomator2
å¯åŠ¨æˆåŠŸå°†å‡ºç°å¦‚ä¸‹ä¿¡æ¯ï¼š
[Appium] Welcome to Appium v2.2.1 (REV 2176894a5be5da17a362bf3f20678641a78f4b69)
[Appium] Non-default server args:
[Appium] {
[Appium]   usePlugins: [
[Appium]     'uiautomator2'
[Appium]   ]
[Appium] }
[Appium] Attempting to load driver uiautomator2...
[Appium] Requiring driver at /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver
[Appium] Appium REST http interface listener started on http://0.0.0.0:4723
[Appium] You can provide the following URLs in your client code to connect to this server:
[Appium] 	http://127.0.0.1:4723/ (only accessible from the same host)
[Appium] 	http://172.31.102.45:4723/
[Appium] 	http://198.18.0.1:4723/
[Appium] Available drivers:
[Appium]   - uiautomator2@2.32.3 (automationName 'UiAutomator2')
[Appium] No plugins have been installed. Use the "appium plugin" command to install the one(s) you want to use.
å…¶ä¸­
[Appium] 	http://127.0.0.1:4723/ (only accessible from the same host) [Appium] 	http://172.31.102.45:4723/ [Appium] 	http://198.18.0.1:4723/
ä¸ºappium serverè¿æ¥åœ°å€
appium client
å…ˆä¸‹è½½å¹¶å®‰è£…å¥½python3å’Œpip3
å®‰è£…
pip3 install appium-python-client
åœ¨ä»£ç ä¸­å¼•å…¥å¹¶ä½¿ç”¨appium
from
appium
import
webdriver
from
appium
.
options
.
common
.
base
import
AppiumOptions
device_app_info
=
AppiumOptions
()
device_app_info
.
set_capability
(
'platformName'
,
'Android'
)
device_app_info
.
set_capability
(
'platformVersion'
,
'10'
)
device_app_info
.
set_capability
(
'deviceName'
,
'YourDeviceName'
)
device_app_info
.
set_capability
(
'appPackage'
,
'cn.damai'
)
device_app_info
.
set_capability
(
'appActivity'
,
'.launcher.splash.SplashMainActivity'
)
device_app_info
.
set_capability
(
'unicodeKeyboard'
,
True
)
device_app_info
.
set_capability
(
'resetKeyboard'
,
True
)
device_app_info
.
set_capability
(
'noReset'
,
True
)
device_app_info
.
set_capability
(
'newCommandTimeout'
,
6000
)
device_app_info
.
set_capability
(
'automationName'
,
'UiAutomator2'
)
# è¿æ¥appium serverï¼Œserveråœ°å€æŸ¥çœ‹appiumå¯åŠ¨ä¿¡æ¯
driver
=
webdriver
.
Remote
(
'http://127.0.0.1:4723'
,
options
=
device_app_info
)
å¯åŠ¨è„šæœ¬ç¨‹åº
cd
damai_appium
python3 damai_appium.py
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 46
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 4,482

### References
- https://github.com/WECENG/ticket-purchase

## coze-dev/coze-studio

### Executive Summary
- An AI agent development platform with all-in-one visual tools, simplifying agent creation, debugging, and deployment like never before. Coze your way to AI Agent creation.
- ---
- Coze Studio
â€¢
Feature list
â€¢
Quickstart
â€¢
Developer Guide
English |
ä¸­æ–‡
What is Coze Studio?
Coze Studio
is an all-in-one AI agent development tool. Providing the latest large models and tools, various development modes and frameworks, Coze Studio offers the most convenient AI agent development environment, from development to deployment.
Provides all core technologies needed for AI agent development
: prompt, RAG, plugin, workflow, enabling developers to focus on creating the core value of AI.
Ready to use for professional AI agent development at the lowest cost
: Coze Studio provides developers with complete app templates and build frameworks, allowing you to quickly construct various AI agents and turn creative ideas into reality.
Coze Studio, derived from the "Coze Development Platform" which has served tens of thousands of enterprises and millions of developers, we have made its core engine completely open. It is a one-stop visual development tool for AI Agents that makes creating, debugging, and deploying AI Agents unprecedentedly simple. Through Coze Studio's visual design and build tools, developers can quickly create and debug agents, apps, and workflows using no-code or low-code approaches, enabling powerful AI app development and more customized business logic. It's an ideal choice for building low-code AI products tailored . Coze Studio aims to lower the threshold for AI agent development and application, encouraging community co-construction and sharing for deeper exploration and practice in the AI field.
The backend of Coze Studio is developed using Golang, the frontend uses React + TypeScript, and the overall architecture is based on microservices and built following domain-driven design (DDD) principles. Provide developers with a high-performance, highly scalable, and easy-to-customize underlying framework to help them address complex business needs.
Feature list
Module
Feature
Model service
Manage the model list, integrate services such as OpenAI and Volcengine
Build agent
* Build, publish, and manage agent
* Support configuring workflows, knowledge bases, and other resources
Build apps
* Create and publish apps
* Build business logic through workflows
Build a workflow
Create, modify, publish, and delete workflows
Develop resources
Support creating and managing the following resources:
* Plugins
* Knowledge bases
* Databases
* Prompts
API and SDK
* Create conversations, initiate chats, and other OpenAPI
* Integrate agents or apps into your own app through Chat SDK
Quickstart
Learn how to obtain and deploy the open-source version of Coze Studio, quickly build projects, and experience Coze Studio's open-source version.
Environment requirements:
Before installing Coze Studio, please ensure that your machine meets the following minimum system requirements: 2 Coreã€4 GB
Pre-install Docker and Docker Compose, and start the Docker service.
Deployment steps:
Retrieve the source code.
#
Clone code
git clone https://github.com/coze-dev/coze-studio.git
Configure the model.
Copy the template files of the doubao-seed-1.6 model from the template directory and paste them into the configuration file directory.
cd
coze-studio
#
Copy model configuration template
cp backend/conf/model/template/model_template_ark_doubao-seed-1.6.yaml backend/conf/model/ark_doubao-seed-1.6.yaml
Modify the template file in the configuration file directory.
Enter the directory
backend/conf/model
. Open the file
ark_doubao-seed-1.6.yaml
.
Set the fields
id
,
meta.conn_config.api_key
,
meta.conn_config.model
, and save the file.
id
: The model ID in Coze Studio, defined by the developer, must be a non-zero integer and globally unique. Agents or workflows call models based on model IDs. For models that have already been launched, do not modify their IDs; otherwise, it may result in model call failures.
meta.conn_config.api_key
: The API Key for the model service. In this example, it is the API Key for Ark API Key. For more information, see
Get Volcengine Ark API Key
or
Get BytePlus ModelArk API Key
.
meta.conn_config.model
: The Model name for the model service. In this example, it refers to the Model ID or Endpoint ID of Ark. For more information, see
Get Volcengine Ark Model ID
/
Get Volcengine Ark Endpoint ID
or
Get BytePlus ModelArk Model ID
/
Get BytePlus ModelArk Endpoint ID
.
For users in China, you may use Volcengine Ark; for users outside China, you may use BytePlus ModelArk instead.
Deploy and start the service.
When deploying and starting Coze Studio for the first time, it may take a while to retrieve images and build local images. Please be patient. During deployment, you will see the following log information. If you see the message "Container coze-server Started," it means the Coze Studio service has started successfully.
#
Start the service
cd
docker
cp .env.example .env
docker compose up -d
For common startup failure issues,
please refer to the
FAQ
.
After starting the service, you can open Coze Studio by accessing
http://localhost:8888/
through your browser.
Warning
If you want to deploy Coze Studio in a public network environment, it is recommended to assess security risks before you begin, and take corresponding protection measures. Possible security risks include account registration functions, Python execution environments in workflow code nodes, Coze Server listening address configurations, SSRF (Server - Side Request Forgery), and some horizontal privilege escalations in APIs.  For more details, refer to
Quickstart
.
Developer Guide
Project Configuration
:
Model Configuration
: Before deploying the open-source version of Coze Studio, you must configure the model service. Otherwise, you cannot select models when building agents, workflows, and apps.
Plugin Configuration
: To use official plugins from the plugin store, you must first configure the plugins and add the authentication keys for third-party services.
Basic Component Configuration
: Learn how to configure components such as image uploaders to use functions like image uploading in Coze Studio .
API Reference
: The Coze Studio Community Edition API and Chat SDK are authenticated using Personal Access Token, providing APIs for conversations and workflows.
Development Guidelines
:
Project Architecture
: Learn about the technical architecture and core components of the open-source version of Coze Studio.
Code Development and Testing
: Learn how to perform secondary development and testing based on the open-source version of Coze Studio.
Troubleshooting
: Learn how to view container states and system logs.
Using the open-source version of Coze Studio
Regarding how to use Coze Studio, refer to the
Coze Development Platform Official Documentation Center
for more information. Please note that certain features, such as tone customization, are limited to the commercial version. Differences between the open-source and commercial versions can be found in the
Feature List
.
Quick Start
: Quickly build an AI assistant agent with Coze Studio.
Developing Agents
: Learn how to create, build, publish, and manage agents. You can use functions such as knowledge, plugins, etc., to resolve model hallucination and lack of expertise in professional fields. In addition, Coze Studio provides rich memory features that enable agents to generate more accurate responses based on a personal user's historical conversations during interactions.
Develop workflows
: A workflow is a set of executable instructions used to implement business logic or complete specific tasks. It structures data flow and task processing for apps or agents. Coze Studio provides a visual canvas where you can quickly build workflows by dragging and dropping nodes.
Resources such as plugins
: In Coze Studio, workflows, plugins, databases, knowledge bases, and variables are collectively referred to as resources.
API & SDK
: Coze Studio supports
API related to chat and workflows
, and you can also integrate agents or apps with local business systems through
Chat SDK
.
Tutorials for practice
: Learn how to use Coze Studio to implement various AI scenarios, such as building web-based online customer service using Chat SDK.
License
This project uses the Apache 2.0 license. For details, please refer to the
LICENSE
file.
Community contributions
We welcome community contributions. For contribution guidelines, please refer to
CONTRIBUTING
and
Code of conduct
. We look forward to your contributions!
Security and privacy
If you discover potential security issues in the project, or believe you may have found a security issue, please notify the ByteDance security team through our
security center
or
vulnerability reporting email
.
Please
do not
create public GitHub Issues.
Join Community
We are committed to building an open and friendly developer community. All developers interested in AI Agent development are welcome to join us!
ğŸ› Issue Reports & Feature Requests
To efficiently track and resolve issues while ensuring transparency and collaboration, we recommend participating through:
GitHub Issues
:
Submit bug reports or feature requests
Pull Requests
:
Contribute code or documentation improvements
ğŸ’¬ Technical Discussion & Communication
Join our technical discussion groups to share experiences with other developers and stay updated with the latest project developments:
Feishu Group Chat
Scan the QR code below with Feishu mobile app to join:
Discord Server
Click to join:
Coze Community
Telegram Group
Click to join: Telegram Group
Coze
Acknowledgments
Thank you to all the developers and community members who have contributed to the Coze Studio project. Special thanks:
The
Eino
framework team - providing powerful support for Coze Studio's agent and workflow runtime engines, model abstractions and implementations, and knowledge base indexing and retrieval
The
FlowGram
team - providing a high-quality workflow building engine for Coze Studio's frontend workflow canvas editor
The
Hertz
team - Go HTTP framework with high-performance and strong-extensibility for building micro-services
All users who participated in testing and feedback
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 44
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 17,538

### References
- https://github.com/coze-dev/coze-studio

## jgraph/drawio-desktop

### Executive Summary
- Official electron build of draw.io
- ---
- About
drawio-desktop
is a diagramming desktop app based on
Electron
that wraps the
core draw.io editor
.
Download built binaries from the
releases section
.
Can I use this app for free?
Yes, under the apache 2.0 license. If you don't change the code and accept it is provided "as-is", you can use it for any purpose.
Security
draw.io Desktop is designed to be completely isolated from the Internet, apart from the update process. This checks github.com at startup for a newer version and downloads it from an AWS S3 bucket owned by Github. All JavaScript files are self-contained, the Content Security Policy forbids running remotely loaded JavaScript.
No diagram data is ever sent externally, nor do we send any analytics about app usage externally. There is a Content Security Policy in place on the web part of the interface to ensure external transmission cannot happen, even by accident.
Security and isolating the app are the primarily objectives of draw.io desktop. If you ask for anything that involves external connections enabled in the app by default, the answer will be no.
Support
Support is provided on a reasonable business constraints basis, but without anything contractually binding. All support is provided via this repo. There is no private ticketing support for non-paying users.
Purchasing draw.io for Confluence or Jira does not entitle you to commercial support for draw.io desktop.
Developing
draw.io
is a git submodule of
drawio-desktop
. To get both you need to clone recursively:
git clone --recursive https://github.com/jgraph/drawio-desktop.git
To run this:
npm install
(in the root directory of this repo)
[internal use only] export DRAWIO_ENV=dev if you want to develop/debug in dev mode.
npm start
in the root directory of this repo
runs the app. For debugging, use
npm start --enable-logging
.
Note: If a symlink is used to refer to drawio repo (instead of the submodule), then symlink the
node_modules
directory inside
drawio/src/main/webapp
also.
To release:
Update the draw.io sub-module and push the change. Add version tag before pushing to origin.
Wait for the builds to complete (
https://travis-ci.org/jgraph/drawio-desktop
and
https://ci.appveyor.com/project/davidjgraph/drawio-desktop
)
Go to
https://github.com/jgraph/drawio-desktop/releases
, edit the preview release.
Download the windows exe and windows portable, sign them using
signtool sign /a /tr http://rfc3161timestamp.globalsign.com/advanced /td SHA256 c:/path/to/your/file.exe
Re-upload signed file as
draw.io-windows-installer-x.y.z.exe
and
draw.io-windows-no-installer-x.y.z.exe
Add release notes
Publish release
Note
: In Windows release, when using both x64 and is32 as arch, the result is one big file with both archs. This is why we split them.
Local Storage and Session Storage is stored in the AppData folder:
macOS:
~/Library/Application Support/draw.io
Windows:
C:\Users\<USER-NAME>\AppData\Roaming\draw.io\
Not open-contribution
draw.io is closed to contributions (unless a maintainer permits it, which is extremely rare).
The level of complexity of this project means that even simple changes
can break a
lot
of other moving parts. The amount of testing required
is far more than it first seems. If we were to receive a PR, we'd have
to basically throw it away and write it how we want it to be implemented.
We are grateful for community involvement, bug reports, & feature requests. We do
not wish to come off as anything but welcoming, however, we've
made the decision to keep this project closed to contributions for
the long term viability of the project.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 42
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 57,217

### References
- https://github.com/jgraph/drawio-desktop

## microsoft/RD-Agent

### Executive Summary
- Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through R&D-Agent, which lets AI drive data-driven AI. ğŸ”—https://aka.ms/RD-Agent-Tech-Report
- ---
- ğŸ–¥ï¸ Live Demo
|
ğŸ¥ Demo Video
â–¶ï¸
YouTube
|
ğŸ“– Documentation
|
ğŸ“„ Tech Report
|
ğŸ“ƒ Papers
ğŸ“° News
ğŸ—ï¸ News
ğŸ“ Description
NeurIPS 2025 Acceptance
We are thrilled to announce that our paper
R&D-Agent-Quant
has been accepted to NeurIPS 2025
Technical Report Release
Overall framework description and results on MLE-bench
R&D-Agent-Quant Release
Apply R&D-Agent to quant trading
MLE-Bench Results Released
R&D-Agent currently leads as the
top-performing machine learning engineering agent
on MLE-bench
Support LiteLLM Backend
We now fully support
LiteLLM
as our default backend for integration with multiple LLM providers.
General Data Science Agent
Data Science Agent
Kaggle Scenario release
We release
Kaggle Agent
, try the new features!
Official WeChat group release
We created a WeChat group, welcome to join! (ğŸ—ª
QR Code
)
Official Discord release
We launch our first chatting channel in Discord (ğŸ—ª
)
First release
R&D-Agent
is released on GitHub
ğŸ† The Best Machine Learning Engineering Agent!
MLE-bench
is a comprehensive benchmark evaluating the performance of AI agents on machine learning engineering tasks. Utilizing datasets from 75 Kaggle competitions, MLE-bench provides robust assessments of AI systems' capabilities in real-world ML engineering scenarios.
R&D-Agent currently leads as the top-performing machine learning engineering agent on MLE-bench:
Agent
Low == Lite (%)
Medium (%)
High (%)
All (%)
R&D-Agent o3(R)+GPT-4.1(D)
51.52 Â± 6.9
19.3 Â± 5.5
26.67 Â± 0
30.22 Â± 1.5
R&D-Agent o1-preview
48.18 Â± 2.49
8.95 Â± 2.36
18.67 Â± 2.98
22.4 Â± 1.1
AIDE o1-preview
34.3 Â± 2.4
8.8 Â± 1.1
10.0 Â± 1.9
16.9 Â± 1.1
Notes:
O3(R)+GPT-4.1(D)
: This version is designed to both reduce average time per loop and leverage a cost-effective combination of backend LLMs by seamlessly integrating Research Agent (o3) with Development Agent (GPT-4.1).
AIDE o1-preview
: Represents the previously best public result on MLE-bench as reported in the original MLE-bench paper.
Average and standard deviation results for R&D-Agent o1-preview is based on a independent of 5 seeds and for R&D-Agent o3(R)+GPT-4.1(D) is based on 6 seeds.
According to MLE-Bench, the 75 competitions are categorized into three levels of complexity:
Low==Lite
if we estimate that an experienced ML engineer can produce a sensible solution in under 2 hours, excluding the time taken to train any models;
Medium
if it takes between 2 and 10 hours; and
High
if it takes more than 10 hours.
You can inspect the detailed runs of the above results online.
R&D-Agent o1-preview detailed runs
R&D-Agent o3(R)+GPT-4.1(D) detailed runs
For running R&D-Agent on MLE-bench, refer to
MLE-bench Guide: Running ML Engineering via MLE-bench
ğŸ¥‡ The First Data-Centric Quant Multi-Agent Framework!
R&D-Agent for Quantitative Finance, in short
RD-Agent(Q)
, is the first data-centric, multi-agent framework designed to automate the full-stack research and development of quantitative strategies via coordinated factor-model co-optimization.
Extensive experiments in real stock markets show that, at a cost under $10, RD-Agent(Q) achieves approximately 2Ã— higher ARR than benchmark factor libraries while using over 70% fewer factors. It also surpasses state-of-the-art deep time-series models under smaller resource budgets. Its alternating factorâ€“model optimization further delivers excellent trade-off between predictive accuracy and strategy robustness.
You can learn more details about
RD-Agent(Q)
through the
paper
and reproduce it through the
documentation
.
Data Science Agent Preview
Check out our demo video showcasing the current progress of our Data Science Agent under development:
DS.Agent.Preview.mp4
ğŸŒŸ Introduction
R&D-Agent aims to automate the most critical and valuable aspects of the industrial R&D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data.
Methodologically, we have identified a framework with two key components: 'R' for proposing new ideas and 'D' for implementing them.
We believe that the automatic evolution of R&D will lead to solutions of significant industrial value.
R&D is a very general scenario. The advent of R&D-Agent can be your
ğŸ’°
Automatic Quant Factory
(
ğŸ¥Demo Video
|
â–¶ï¸
YouTube
)
ğŸ¤–
Data Mining Agent:
Iteratively proposing data & models (
ğŸ¥Demo Video 1
|
â–¶ï¸
YouTube
) (
ğŸ¥Demo Video 2
|
â–¶ï¸
YouTube
)  and implementing them by gaining knowledge from data.
ğŸ¦¾
Research Copilot:
Auto read research papers (
ğŸ¥Demo Video
|
â–¶ï¸
YouTube
) / financial reports (
ğŸ¥Demo Video
|
â–¶ï¸
YouTube
) and implement model structures or building datasets.
ğŸ¤–
Kaggle Agent:
Auto Model Tuning and Feature Engineering(
ğŸ¥Demo Video Coming Soon...
) and implementing them to achieve more in competitions.
...
You can click the links above to view the demo. We're continuously adding more methods and scenarios to the project to enhance your R&D processes and boost productivity.
Additionally, you can take a closer look at the examples in our
ğŸ–¥ï¸ Live Demo
.
âš¡ Quick start
RD-Agent currently only supports Linux.
You can try above demos by running the following command:
ğŸ³ Docker installation.
Users must ensure Docker is installed before attempting most scenarios. Please refer to the
official ğŸ³Docker page
for installation instructions.
Ensure the current user can run Docker commands
without using sudo
. You can verify this by executing
docker run hello-world
.
ğŸ Create a Conda Environment
Create a new conda environment with Python (3.10 and 3.11 are well-tested in our CI):
conda create -n rdagent python=3.10
Activate the environment:
conda activate rdagent
ğŸ› ï¸ Install the R&D-Agent
For Users
You can directly install the R&D-Agent package from PyPI:
pip install rdagent
For Developers
If you want to try the latest version or contribute to RD-Agent, you can install it from the source and follow the development setup:
git clone https://github.com/microsoft/RD-Agent
cd
RD-Agent
make dev
More details can be found in the
development setup
.
ğŸ’Š Health check
rdagent provides a health check that currently checks two things.
whether the docker installation was successful.
whether the default port used by the
rdagent ui
is occupied.
rdagent health_check --no-check-env
âš™ï¸ Configuration
The demos requires following ability:
ChatCompletion
json_mode
embedding query
You can set your Chat Model and Embedding Model in the following ways:
ğŸ”¥ Attention
: We now provide experimental support for
DeepSeek
models! You can use DeepSeek's official API for cost-effective and high-performance inference. See the configuration example below for DeepSeek setup.
Using LiteLLM (Default)
: We now support LiteLLM as a backend for integration with multiple LLM providers. You can configure in multiple ways:
Option 1: Unified API base for both models
Configuration Example:
OpenAI
Setup :
cat
<<
EOF
> .env
# Set to any model supported by LiteLLM.
CHAT_MODEL=gpt-4o
EMBEDDING_MODEL=text-embedding-3-small
# Configure unified API base
OPENAI_API_BASE=<your_unified_api_base>
OPENAI_API_KEY=<replace_with_your_openai_api_key>
Configuration Example:
Azure OpenAI
Setup :
Before using this configuration, please confirm in advance that your
Azure OpenAI API key
supports
embedded models
.
cat
<<
EOF
> .env
EMBEDDING_MODEL=azure/<Model deployment supporting embedding>
CHAT_MODEL=azure/<your deployment name>
AZURE_API_KEY=<replace_with_your_openai_api_key>
AZURE_API_BASE=<your_unified_api_base>
AZURE_API_VERSION=<azure api version>
Option 2: Separate API bases for Chat and Embedding models
cat
<<
EOF
> .env
# Set to any model supported by LiteLLM.
# Configure separate API bases for chat and embedding
# CHAT MODEL:
CHAT_MODEL=gpt-4o
OPENAI_API_BASE=<your_chat_api_base>
OPENAI_API_KEY=<replace_with_your_openai_api_key>
# EMBEDDING MODEL:
# TAKE siliconflow as an example, you can use other providers.
# Note: embedding requires litellm_proxy prefix
EMBEDDING_MODEL=litellm_proxy/BAAI/bge-large-en-v1.5
LITELLM_PROXY_API_KEY=<replace_with_your_siliconflow_api_key>
LITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1
Configuration Example:
DeepSeek
Setup :
Since many users encounter configuration errors when setting up DeepSeek. Here's a complete working example for DeepSeek Setup:
cat
<<
EOF
> .env
# CHAT MODEL: Using DeepSeek Official API
CHAT_MODEL=deepseek/deepseek-chat
DEEPSEEK_API_KEY=<replace_with_your_deepseek_api_key>
# EMBEDDING MODEL: Using SiliconFlow for embedding since deepseek has no embedding model.
# Note: embedding requires litellm_proxy prefix
EMBEDDING_MODEL=litellm_proxy/BAAI/bge-m3
LITELLM_PROXY_API_KEY=<replace_with_your_siliconflow_api_key>
LITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1
Notice: If you are using reasoning models that include thought processes in their responses (such as <think> tags), you need to set the following environment variable:
REASONING_THINK_RM=True
You can also use a deprecated backend if you only use
OpenAI API
or
Azure OpenAI
directly. For this deprecated setting and more configuration information, please refer to the
documentation
.
If your environment configuration is complete, please execute the following commands to check if your configuration is valid. This step is necessary.
rdagent health_check
ğŸš€ Run the Application
The
ğŸ–¥ï¸ Live Demo
is implemented by the following commands(each item represents one demo, you can select the one you prefer):
Run the
Automated Quantitative Trading & Iterative Factors Model Joint Evolution
:
Qlib
self-loop factor & model proposal and implementation application
rdagent fin_quant
Run the
Automated Quantitative Trading & Iterative Factors Evolution
:
Qlib
self-loop factor proposal and implementation application
rdagent fin_factor
Run the
Automated Quantitative Trading & Iterative Model Evolution
:
Qlib
self-loop model proposal and implementation application
rdagent fin_model
Run the
Automated Quantitative Trading & Factors Extraction from Financial Reports
:  Run the
Qlib
factor extraction and implementation application based on financial reports
#
1. Generally, you can run this scenario using the following command:
rdagent fin_factor_report --report-folder=
<
Your financial reports folder path
>
#
2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:
wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip
unzip all_reports.zip -d git_ignore_folder/reports
rdagent fin_factor_report --report-folder=git_ignore_folder/reports
Run the
Automated Model Research & Development Copilot
: model extraction and implementation application
#
1. Generally, you can run your own papers/reports with the following command:
rdagent general_model
<
Your paper URL
>
#
2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:
rdagent general_model
"
https://arxiv.org/pdf/2210.09789
"
Run the
Automated Medical Prediction Model Evolution
: Medical self-loop model proposal and implementation application
#
Generally, you can run the data science program with the following command:
rdagent data_science --competition
<
your competition name
>
#
Specifically, you need to create a folder for storing competition files (e.g., competition description file, competition datasets, etc.), and configure the path to the folder in your environment. In addition, you need to use chromedriver when you download the competition descriptors, which you can follow for this specific example:
#
1. Download the dataset, extract it to the target folder.
wget https://github.com/SunsetWolf/rdagent_resource/releases/download/ds_data/arf-12-hours-prediction-task.zip
unzip arf-12-hours-prediction-task.zip -d ./git_ignore_folder/ds_data/
#
2. Configure environment variables in the `.env` file
dotenv
set
DS_LOCAL_DATA_PATH
"
$(
pwd
)
/git_ignore_folder/ds_data
"
dotenv
set
DS_CODER_ON_WHOLE_PIPELINE True
dotenv
set
DS_IF_USING_MLE_DATA False
dotenv
set
DS_SAMPLE_DATA_BY_LLM False
dotenv
set
DS_SCEN rdagent.scenarios.data_science.scen.DataScienceScen
#
3. run the application
rdagent data_science --competition arf-12-hours-prediction-task
NOTE:
For more information about the dataset, please refer to the
documentation
.
Run the
Automated Kaggle Model Tuning & Feature Engineering
:  self-loop model proposal and feature engineering implementation application
Using
tabular-playground-series-dec-2021
as an example.
Register and login on the
Kaggle
website.
Configuring the Kaggle API.
(1) Click on the avatar (usually in the top right corner of the page) ->
Settings
->
Create New Token
, A file called
kaggle.json
will be downloaded.
(2) Move
kaggle.json
to
~/.config/kaggle/
(3) Modify the permissions of the kaggle.json file. Reference command:
chmod 600 ~/.config/kaggle/kaggle.json
Join the competition: Click
Join the competition
->
I Understand and Accept
at the bottom of the
competition details page
.
#
Generally, you can run the Kaggle competition program with the following command:
rdagent data_science --competition
<
your competition name
>
#
1. Configure environment variables in the `.env` file
mkdir -p ./git_ignore_folder/ds_data
dotenv
set
DS_LOCAL_DATA_PATH
"
$(
pwd
)
/git_ignore_folder/ds_data
"
dotenv
set
DS_CODER_ON_WHOLE_PIPELINE True
dotenv
set
DS_IF_USING_MLE_DATA True
dotenv
set
DS_SAMPLE_DATA_BY_LLM True
dotenv
set
DS_SCEN rdagent.scenarios.data_science.scen.KaggleScen
#
2. run the application
rdagent data_science --competition tabular-playground-series-dec-2021
ğŸ–¥ï¸ Monitor the Application Results
You can run the following command for our demo program to see the run logs.
rdagent ui --port 19899 --log-dir
<
your log folder like
"
log/
"
>
--data-science
About the
data_science
parameter: If you want to see the logs of the data science scenario, set the
data_science
parameter to
True
; otherwise set it to
False
.
Although port 19899 is not commonly used, but before you run this demo, you need to check if port 19899 is occupied. If it is, please change it to another port that is not occupied.
You can check if a port is occupied by running the following command.
rdagent health_check --no-check-env --no-check-docker
ğŸ­ Scenarios
We have applied R&D-Agent to multiple valuable data-driven industrial scenarios.
ğŸ¯ Goal: Agent for Data-driven R&D
In this project, we are aiming to build an Agent to automate Data-Driven R&D that can
ğŸ“„ Read real-world material (reports, papers, etc.) and
extract
key formulas, descriptions of interested
features
and
models
, which are the key components of data-driven R&D .
ğŸ› ï¸
Implement
the extracted formulas (e.g., features, factors, and models) in runnable codes.
Due to the limited ability of LLM in implementing at once, build an evolving process for the agent to improve performance by learning from feedback and knowledge.
ğŸ’¡ Propose
new ideas
based on current knowledge and observations.
ğŸ“ˆ Scenarios/Demos
In the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: ğŸ¦¾Copilot and ğŸ¤–Agent.
The ğŸ¦¾Copilot follows human instructions to automate repetitive tasks.
The ğŸ¤–Agent, being more autonomous, actively proposes ideas for better results in the future.
The supported scenarios are listed below:
Scenario/Target
Model Implementation
Data Building
ğŸ’¹ Finance
ğŸ¤–
Iteratively Proposing Ideas & Evolving
â–¶ï¸
YouTube
ğŸ¤–
Iteratively Proposing Ideas & Evolving
â–¶ï¸
YouTube
ğŸ¦¾
Auto reports reading & implementation
â–¶ï¸
YouTube
ğŸ©º Medical
ğŸ¤–
Iteratively Proposing Ideas & Evolving
â–¶ï¸
YouTube
-
ğŸ­ General
ğŸ¦¾
Auto paper reading & implementation
â–¶ï¸
YouTube
ğŸ¤– Auto Kaggle Model Tuning
ğŸ¤–Auto Kaggle feature Engineering
RoadMap
: Currently, we are working hard to add new features to the Kaggle scenario.
Different scenarios vary in entrance and configuration. Please check the detailed setup tutorial in the scenarios documents.
Here is a gallery of
successful explorations
(5 traces showed in
ğŸ–¥ï¸ Live Demo
). You can download and view the execution trace using
this command
from the documentation.
Please refer to
ğŸ“–readthedocs_scen
for more details of the scenarios.
âš™ï¸ Framework
Automating the R&D process in data science is a highly valuable yet underexplored area in industry. We propose a framework to push the boundaries of this important research field.
The research questions within this framework can be divided into three main categories:
Research Area
Paper/Work List
Benchmark the R&D abilities
Benchmark
Idea proposal:
Explore new ideas or refine existing ones
Research
Ability to realize ideas:
Implement and execute ideas
Development
We believe that the key to delivering high-quality solutions lies in the ability to evolve R&D capabilities. Agents should learn like human experts, continuously improving their R&D skills.
More documents can be found in the
ğŸ“– readthedocs
.
ğŸ“ƒ Paper/Work list
Overall Technical Report
R&D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution
@misc
{
yang2024rdagent
,
title
=
{
R\&D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution
}
,
author
=
{
Xu Yang and Xiao Yang and Shikai Fang and Bowen Xian and Yuante Li and Jian Wang and Minrui Xu and Haoran Pan and Xinpeng Hong and Weiqing Liu and Yelong Shen and Weizhu Chen and Jiang Bian
}
,
year
=
{
2025
}
,
eprint
=
{
2505.14738
}
,
archivePrefix
=
{
arXiv
}
,
primaryClass
=
{
cs.AI
}
,
url
=
{
https://arxiv.org/abs/2505.14738
}
}
ğŸ“Š Benchmark
Towards Data-Centric Automatic R&D
@misc
{
chen2024datacentric
,
title
=
{
Towards Data-Centric Automatic R&D
}
,
author
=
{
Haotian Chen and Xinjie Shen and Zeqi Ye and Wenjun Feng and Haoxue Wang and Xiao Yang and Xu Yang and Weiqing Liu and Jiang Bian
}
,
year
=
{
2024
}
,
eprint
=
{
2404.11276
}
,
archivePrefix
=
{
arXiv
}
,
primaryClass
=
{
cs.AI
}
}
ğŸ” Research
In a data mining expert's daily research and development process, they propose a hypothesis (e.g., a model structure like RNN can capture patterns in time-series data), design experiments (e.g., finance data contains time-series and we can verify the hypothesis in this scenario), implement the experiment as code (e.g., Pytorch model structure), and then execute the code to get feedback (e.g., metrics, loss curve, etc.). The experts learn from the feedback and improve in the next iteration.
Based on the principles above, we have established a basic method framework that continuously proposes hypotheses, verifies them, and gets feedback from the real-world practice. This is the first scientific research automation framework that supports linking with real-world verification.
For more detail, please refer to our
ğŸ–¥ï¸ Live Demo page
.
ğŸ› ï¸ Development
Collaborative Evolving Strategy for Automatic Data-Centric Development
@misc
{
yang2024collaborative
,
title
=
{
Collaborative Evolving Strategy for Automatic Data-Centric Development
}
,
author
=
{
Xu Yang and Haotian Chen and Wenjun Feng and Haoxue Wang and Zeqi Ye and Xinjie Shen and Xiao Yang and Shizhao Sun and Weiqing Liu and Jiang Bian
}
,
year
=
{
2024
}
,
eprint
=
{
2407.18690
}
,
archivePrefix
=
{
arXiv
}
,
primaryClass
=
{
cs.AI
}
}
Deep Application in Diverse Scenarios
R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization
@misc
{
li2025rdagentquant
,
title
=
{
R\&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization
}
,
author
=
{
Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian
}
,
year
=
{
2025
}
,
eprint
=
{
2505.15155
}
,
archivePrefix
=
{
arXiv
}
,
primaryClass
=
{
cs.AI
}
}
ğŸ¤ Contributing
We welcome contributions and suggestions to improve R&D-Agent. Please refer to the
Contributing Guide
for more details on how to contribute.
Before submitting a pull request, ensure that your code passes the automatic CI checks.
ğŸ“ Guidelines
This project welcomes contributions and suggestions.
Contributing to this project is straightforward and rewarding. Whether it's solving an issue, addressing a bug, enhancing documentation, or even correcting a typo, every contribution is valuable and helps improve R&D-Agent.
To get started, you can explore the issues list, or search for
TODO:
comments in the codebase by running the command
grep -r "TODO:"
.
Before we released R&D-Agent as an open-source project on GitHub, it was an internal project within our group. Unfortunately, the internal commit history was not preserved when we removed some confidential code. As a result, some contributions from our group members, including Haotian Chen, Wenjun Feng, Haoxue Wang, Zeqi Ye, Xinjie Shen, and Jinhui Li, were not included in the public commits.
âš–ï¸ Legal disclaimer
The RD-agent is provided â€œas isâ€, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. The RD-agent is aimed to facilitate research and development process in the financial industry and not ready-to-use for any financial investment or advice. Users shall independently assess and test the risks of the RD-agent in a specific use scenario, ensure the responsible use of AI technology, including but not limited to developing and integrating risk mitigation measures, and comply with all applicable laws and regulations in all applicable jurisdictions. The RD-agent does not provide financial opinions or reflect the opinions of Microsoft, nor is it designed to replace the role of qualified financial professionals in formulating, assessing, and approving finance products. The inputs and outputs of the RD-agent belong to the users and users shall assume all liability under any theory of liability, whether in contract, torts, regulatory, negligence, products liability, or otherwise, associated with use of the RD-agent and any inputs and outputs thereof.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 42
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 8,357

### References
- https://github.com/microsoft/RD-Agent

## JannisX11/blockbench

### Executive Summary
- Blockbench - A low poly 3D model editor
- ---
- Blockbench
Blockbench is a free and open source model editor for low-poly models with pixel art textures.
Models can be exported into standardized formats, to be shared, rendered, 3D-printed, or used in game engines. There are also multiple dedicated formats for Minecraft Java and Bedrock Edition with format-specific features.
Blockbench features a modern and beginner friendly interface, but also offers lots of customization and advanced features for experienced 3D artists. Plugins can extend the functionality of the program even further.
Website and download:
blockbench.net
Contribution
Check out the
Contribution Guidelines
.
Launching Blockbench
To launch Blockbench from source, you can clone the repository, navigate to the correct branch and launch the program in development mode using the instructions below. If you just want to use the latest version, please download the app from the website.
Install
NodeJS
.
Then install all dependencies via
npm install
Bundle the code via
npm run bundle
Finally, launch Blockbench using
npm run dev
Plugins
Blockbench supports Javascript-based plugins. Learn more about creating plugins on
https://www.blockbench.net/wiki/docs/plugin
.
License
The Blockbench source-code is licensed under the GPL license version 3. See
LICENSE.MD
.
Modifications to the source code can be made under the terms of that license.
Blockbench plugins (external scripts) and themes (theme files to customize the design) that interact with the Blockbench API are an exception. Plugins and themes can be created and/or published as open source, proprietary or paid software.
All assets created with Blockbench (models, textures, animations, screenshots etc.) are your own!
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 39
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 4,437

### References
- https://github.com/JannisX11/blockbench

## fatedier/frp

### Executive Summary
- A fast reverse proxy to help you expose a local server behind a NAT or firewall to the internet.
- ---
- frp
README
|
ä¸­æ–‡æ–‡æ¡£
Sponsors
frp is an open source project with its ongoing development made possible entirely by the support of our awesome sponsors. If you'd like to join them, please consider
sponsoring frp's development
.
Gold Sponsors
Recall.ai - API for meeting recordings
If you're looking for a meeting recording API, consider checking out
Recall.ai
,
an API that records Zoom, Google Meet, Microsoft Teams, in-person meetings, and more.
Warp, built for collaborating with AI Agents
Available for macOS, Linux and Windows
The complete IDE crafted for professional Go developers
Secure and Elastic Infrastructure for Running Your AI-Generated Code
The sovereign cloud that puts you in control
An open source, self-hosted alternative to public clouds, built for data ownership and privacy
What is frp?
frp is a fast reverse proxy that allows you to expose a local server located behind a NAT or firewall to the Internet. It currently supports
TCP
and
UDP
, as well as
HTTP
and
HTTPS
protocols, enabling requests to be forwarded to internal services via domain name.
frp also offers a P2P connect mode.
Table of Contents
Development Status
About V2
Architecture
Example Usage
Access your computer in a LAN network via SSH
Multiple SSH services sharing the same port
Accessing Internal Web Services with Custom Domains in LAN
Forward DNS query requests
Forward Unix Domain Socket
Expose a simple HTTP file server
Enable HTTPS for a local HTTP(S) service
Expose your service privately
P2P Mode
Features
Configuration Files
Using Environment Variables
Split Configures Into Different Files
Server Dashboard
Client Admin UI
Monitor
Prometheus
Authenticating the Client
Token Authentication
OIDC Authentication
Encryption and Compression
TLS
Hot-Reloading frpc configuration
Get proxy status from client
Only allowing certain ports on the server
Port Reuse
Bandwidth Limit
For Each Proxy
TCP Stream Multiplexing
Support KCP Protocol
Support QUIC Protocol
Connection Pooling
Load balancing
Service Health Check
Rewriting the HTTP Host Header
Setting other HTTP Headers
Get Real IP
HTTP X-Forwarded-For
Proxy Protocol
Require HTTP Basic Auth (Password) for Web Services
Custom Subdomain Names
URL Routing
TCP Port Multiplexing
Connecting to frps via PROXY
Port range mapping
Client Plugins
Server Manage Plugins
SSH Tunnel Gateway
Virtual Network (VirtualNet)
Feature Gates
Available Feature Gates
Enabling Feature Gates
Feature Lifecycle
Related Projects
Contributing
Donation
GitHub Sponsors
PayPal
Development Status
frp is currently under development. You can try the latest release version in the
master
branch, or use the
dev
branch to access the version currently in development.
We are currently working on version 2 and attempting to perform some code refactoring and improvements. However, please note that it will not be compatible with version 1.
We will transition from version 0 to version 1 at the appropriate time and will only accept bug fixes and improvements, rather than big feature requests.
About V2
The complexity and difficulty of the v2 version are much higher than anticipated. I can only work on its development during fragmented time periods, and the constant interruptions disrupt productivity significantly. Given this situation, we will continue to optimize and iterate on the current version until we have more free time to proceed with the major version overhaul.
The concept behind v2 is based on my years of experience and reflection in the cloud-native domain, particularly in K8s and ServiceMesh. Its core is a modernized four-layer and seven-layer proxy, similar to envoy. This proxy itself is highly scalable, not only capable of implementing the functionality of intranet penetration but also applicable to various other domains. Building upon this highly scalable core, we aim to implement all the capabilities of frp v1 while also addressing the functionalities that were previously unachievable or difficult to implement in an elegant manner. Furthermore, we will maintain efficient development and iteration capabilities.
In addition, I envision frp itself becoming a highly extensible system and platform, similar to how we can provide a range of extension capabilities based on K8s. In K8s, we can customize development according to enterprise needs, utilizing features such as CRD, controller mode, webhook, CSI, and CNI. In frp v1, we introduced the concept of server plugins, which implemented some basic extensibility. However, it relies on a simple HTTP protocol and requires users to start independent processes and manage them on their own. This approach is far from flexible and convenient, and real-world demands vary greatly. It is unrealistic to expect a non-profit open-source project maintained by a few individuals to meet everyone's needs.
Finally, we acknowledge that the current design of modules such as configuration management, permission verification, certificate management, and API management is not modern enough. While we may carry out some optimizations in the v1 version, ensuring compatibility remains a challenging issue that requires a considerable amount of effort to address.
We sincerely appreciate your support for frp.
Architecture
Example Usage
To begin, download the latest program for your operating system and architecture from the
Release
page.
Next, place the
frps
binary and server configuration file on Server A, which has a public IP address.
Finally, place the
frpc
binary and client configuration file on Server B, which is located on a LAN that cannot be directly accessed from the public internet.
Some antiviruses improperly mark frpc as malware and delete it. This is due to frp being a networking tool capable of creating reverse proxies. Antiviruses sometimes flag reverse proxies due to their ability to bypass firewall port restrictions. If you are using antivirus, then you may need to whitelist/exclude frpc in your antivirus settings to avoid accidental quarantine/deletion. See
issue 3637
for more details.
Access your computer in a LAN network via SSH
Modify
frps.toml
on server A by setting the
bindPort
for frp clients to connect to:
#
frps.toml
bindPort
=
7000
Start
frps
on server A:
./frps -c ./frps.toml
Modify
frpc.toml
on server B and set the
serverAddr
field to the public IP address of your frps server:
#
frpc.toml
serverAddr
=
"
x.x.x.x
"
serverPort
=
7000
[[
proxies
]]
name
=
"
ssh
"
type
=
"
tcp
"
localIP
=
"
127.0.0.1
"
localPort
=
22
remotePort
=
6000
Note that the
localPort
(listened on the client) and
remotePort
(exposed on the server) are used for traffic going in and out of the frp system, while the
serverPort
is used for communication between frps and frpc.
Start
frpc
on server B:
./frpc -c ./frpc.toml
To access server B from another machine through server A via SSH (assuming the username is
test
), use the following command:
ssh -oPort=6000 test@x.x.x.x
Multiple SSH services sharing the same port
This example implements multiple SSH services exposed through the same port using a proxy of type tcpmux. Similarly, as long as the client supports the HTTP Connect proxy connection method, port reuse can be achieved in this way.
Deploy frps on a machine with a public IP and modify the frps.toml file. Here is a simplified configuration:
bindPort
=
7000
tcpmuxHTTPConnectPort
=
5002
Deploy frpc on the internal machine A with the following configuration:
serverAddr
=
"
x.x.x.x
"
serverPort
=
7000
[[
proxies
]]
name
=
"
ssh1
"
type
=
"
tcpmux
"
multiplexer
=
"
httpconnect
"
customDomains
= [
"
machine-a.example.com
"
]
localIP
=
"
127.0.0.1
"
localPort
=
22
Deploy another frpc on the internal machine B with the following configuration:
serverAddr
=
"
x.x.x.x
"
serverPort
=
7000
[[
proxies
]]
name
=
"
ssh2
"
type
=
"
tcpmux
"
multiplexer
=
"
httpconnect
"
customDomains
= [
"
machine-b.example.com
"
]
localIP
=
"
127.0.0.1
"
localPort
=
22
To access internal machine A using SSH ProxyCommand, assuming the username is "test":
ssh -o 'proxycommand socat - PROXY:x.x.x.x:%h:%p,proxyport=5002' test@machine-a.example.com
To access internal machine B, the only difference is the domain name, assuming the username is "test":
ssh -o 'proxycommand socat - PROXY:x.x.x.x:%h:%p,proxyport=5002' test@machine-b.example.com
Accessing Internal Web Services with Custom Domains in LAN
Sometimes we need to expose a local web service behind a NAT network to others for testing purposes with our own domain name.
Unfortunately, we cannot resolve a domain name to a local IP. However, we can use frp to expose an HTTP(S) service.
Modify
frps.toml
and set the HTTP port for vhost to 8080:
#
frps.toml
bindPort
=
7000
vhostHTTPPort
=
8080
If you want to configure an https proxy, you need to set up the
vhostHTTPSPort
.
Start
frps
:
./frps -c ./frps.toml
Modify
frpc.toml
and set
serverAddr
to the IP address of the remote frps server. Specify the
localPort
of your web service:
#
frpc.toml
serverAddr
=
"
x.x.x.x
"
serverPort
=
7000
[[
proxies
]]
name
=
"
web
"
type
=
"
http
"
localPort
=
80
customDomains
= [
"
www.example.com
"
]
Start
frpc
:
./frpc -c ./frpc.toml
Map the A record of
www.example.com
to either the public IP of the remote frps server or a CNAME record pointing to your original domain.
Visit your local web service using url
http://www.example.com:8080
.
Forward DNS query requests
Modify
frps.toml
:
#
frps.toml
bindPort
=
7000
Start
frps
:
./frps -c ./frps.toml
Modify
frpc.toml
and set
serverAddr
to the IP address of the remote frps server. Forward DNS query requests to the Google Public DNS server
8.8.8.8:53
:
#
frpc.toml
serverAddr
=
"
x.x.x.x
"
serverPort
=
7000
[[
proxies
]]
name
=
"
dns
"
type
=
"
udp
"
localIP
=
"
8.8.8.8
"
localPort
=
53
remotePort
=
6000
Start frpc:
./frpc -c ./frpc.toml
Test DNS resolution using the
dig
command:
dig @x.x.x.x -p 6000 www.google.com
Forward Unix Domain Socket
Expose a Unix domain socket (e.g. the Docker daemon socket) as TCP.
Configure
frps
as above.
Start
frpc
with the following configuration:
#
frpc.toml
serverAddr
=
"
x.x.x.x
"
serverPort
=
7000
[[
proxies
]]
name
=
"
unix_domain_socket
"
type
=
"
tcp
"
remotePort
=
6000
[
proxies
.
plugin
]
type
=
"
unix_domain_socket
"
unixPath
=
"
/var/run/docker.sock
"
Test the configuration by getting the docker version using
curl
:
curl http://x.x.x.x:6000/version
Expose a simple HTTP file server
Expose a simple HTTP file server to access files stored in the LAN from the public Internet.
Configure
frps
as described above, then:
Start
frpc
with the following configuration:
#
frpc.toml
serverAddr
=
"
x.x.x.x
"
serverPort
=
7000
[[
proxies
]]
name
=
"
test_static_file
"
type
=
"
tcp
"
remotePort
=
6000
[
proxies
.
plugin
]
type
=
"
static_file
"
localPath
=
"
/tmp/files
"
stripPrefix
=
"
static
"
httpUser
=
"
abc
"
httpPassword
=
"
abc
"
Visit
http://x.x.x.x:6000/static/
from your browser and specify correct username and password to view files in
/tmp/files
on the
frpc
machine.
Enable HTTPS for a local HTTP(S) service
You may substitute
https2https
for the plugin, and point the
localAddr
to a HTTPS endpoint.
Start
frpc
with the following configuration:
#
frpc.toml
serverAddr
=
"
x.x.x.x
"
serverPort
=
7000
[[
proxies
]]
name
=
"
test_https2http
"
type
=
"
https
"
customDomains
= [
"
test.example.com
"
]

[
proxies
.
plugin
]
type
=
"
https2http
"
localAddr
=
"
127.0.0.1:80
"
crtPath
=
"
./server.crt
"
keyPath
=
"
./server.key
"
hostHeaderRewrite
=
"
127.0.0.1
"
requestHeaders.set.x-from-where
=
"
frp
"
Visit
https://test.example.com
.
Expose your service privately
To mitigate risks associated with exposing certain services directly to the public network, STCP (Secret TCP) mode requires a preshared key to be used for access to the service from other clients.
Configure
frps
same as above.
Start
frpc
on machine B with the following config. This example is for exposing the SSH service (port 22), and note the
secretKey
field for the preshared key, and that the
remotePort
field is removed here:
#
frpc.toml
serverAddr
=
"
x.x.x.x
"
serverPort
=
7000
[[
proxies
]]
name
=
"
secret_ssh
"
type
=
"
stcp
"
secretKey
=
"
abcdefg
"
localIP
=
"
127.0.0.1
"
localPort
=
22
Start another
frpc
(typically on another machine C) with the following config to access the SSH service with a security key (
secretKey
field):
#
frpc.toml
serverAddr
=
"
x.x.x.x
"
serverPort
=
7000
[[
visitors
]]
name
=
"
secret_ssh_visitor
"
type
=
"
stcp
"
serverName
=
"
secret_ssh
"
secretKey
=
"
abcdefg
"
bindAddr
=
"
127.0.0.1
"
bindPort
=
6000
On machine C, connect to SSH on machine B, using this command:
ssh -oPort=6000 127.0.0.1
P2P Mode
xtcp
is designed to transmit large amounts of data directly between clients. A frps server is still needed, as P2P here only refers to the actual data transmission.
Note that it may not work with all types of NAT devices. You might want to fallback to stcp if xtcp doesn't work.
Start
frpc
on machine B, and expose the SSH port. Note that the
remotePort
field is removed:
#
frpc.toml
serverAddr
=
"
x.x.x.x
"
serverPort
=
7000
#
set up a new stun server if the default one is not available.
#
natHoleStunServer = "xxx"
[[
proxies
]]
name
=
"
p2p_ssh
"
type
=
"
xtcp
"
secretKey
=
"
abcdefg
"
localIP
=
"
127.0.0.1
"
localPort
=
22
Start another
frpc
(typically on another machine C) with the configuration to connect to SSH using P2P mode:
#
frpc.toml
serverAddr
=
"
x.x.x.x
"
serverPort
=
7000
#
set up a new stun server if the default one is not available.
#
natHoleStunServer = "xxx"
[[
visitors
]]
name
=
"
p2p_ssh_visitor
"
type
=
"
xtcp
"
serverName
=
"
p2p_ssh
"
secretKey
=
"
abcdefg
"
bindAddr
=
"
127.0.0.1
"
bindPort
=
6000
#
when automatic tunnel persistence is required, set it to true
keepTunnelOpen
=
false
On machine C, connect to SSH on machine B, using this command:
ssh -oPort=6000 127.0.0.1
Features
Configuration Files
Since v0.52.0, we support TOML, YAML, and JSON for configuration. Please note that INI is deprecated and will be removed in future releases. New features will only be available in TOML, YAML, or JSON. Users wanting these new features should switch their configuration format accordingly.
Read the full example configuration files to find out even more features not described here.
Examples use TOML format, but you can still use YAML or JSON.
These configuration files is for reference only. Please do not use this configuration directly to run the program as it may have various issues.
Full configuration file for frps (Server)
Full configuration file for frpc (Client)
Using Environment Variables
Environment variables can be referenced in the configuration file, using Go's standard format:
#
frpc.toml
serverAddr
=
"
{{ .Envs.FRP_SERVER_ADDR }}
"
serverPort
=
7000
[[
proxies
]]
name
=
"
ssh
"
type
=
"
tcp
"
localIP
=
"
127.0.0.1
"
localPort
=
22
remotePort
= {{ .Envs.FRP_SSH_REMOTE_PORT }
}
With the config above, variables can be passed into
frpc
program like this:
export FRP_SERVER_ADDR=x.x.x.x
export FRP_SSH_REMOTE_PORT=6000
./frpc -c ./frpc.toml
frpc
will render configuration file template using OS environment variables. Remember to prefix your reference with
.Envs
.
Split Configures Into Different Files
You can split multiple proxy configs into different files and include them in the main file.
#
frpc.toml
serverAddr
=
"
x.x.x.x
"
serverPort
=
7000
includes
= [
"
./confd/*.toml
"
]
#
./confd/test.toml
[[
proxies
]]
name
=
"
ssh
"
type
=
"
tcp
"
localIP
=
"
127.0.0.1
"
localPort
=
22
remotePort
=
6000
Server Dashboard
Check frp's status and proxies' statistics information by Dashboard.
Configure a port for dashboard to enable this feature:
#
The default value is 127.0.0.1. Change it to 0.0.0.0 when you want to access it from a public network.
webServer.addr
=
"
0.0.0.0
"
webServer.port
=
7500
#
dashboard's username and password are both optional
webServer.user
=
"
admin
"
webServer.password
=
"
admin
"
Then visit
http://[serverAddr]:7500
to see the dashboard, with username and password both being
admin
.
Additionally, you can use HTTPS port by using your domains wildcard or normal SSL certificate:
webServer.port
=
7500
#
dashboard's username and password are both optional
webServer.user
=
"
admin
"
webServer.password
=
"
admin
"
webServer.tls.certFile
=
"
server.crt
"
webServer.tls.keyFile
=
"
server.key
"
Then visit
https://[serverAddr]:7500
to see the dashboard in secure HTTPS connection, with username and password both being
admin
.
Client Admin UI
The Client Admin UI helps you check and manage frpc's configuration.
Configure an address for admin UI to enable this feature:
webServer.addr
=
"
127.0.0.1
"
webServer.port
=
7400
webServer.user
=
"
admin
"
webServer.password
=
"
admin
"
Then visit
http://127.0.0.1:7400
to see admin UI, with username and password both being
admin
.
Monitor
When web server is enabled, frps will save monitor data in cache for 7 days. It will be cleared after process restart.
Prometheus is also supported.
Prometheus
Enable dashboard first, then configure
enablePrometheus = true
in
frps.toml
.
http://{dashboard_addr}/metrics
will provide prometheus monitor data.
Authenticating the Client
There are 2 authentication methods to authenticate frpc with frps.
You can decide which one to use by configuring
auth.method
in
frpc.toml
and
frps.toml
, the default one is token.
Configuring
auth.additionalScopes = ["HeartBeats"]
will use the configured authentication method to add and validate authentication on every heartbeat between frpc and frps.
Configuring
auth.additionalScopes = ["NewWorkConns"]
will do the same for every new work connection between frpc and frps.
Token Authentication
When specifying
auth.method = "token"
in
frpc.toml
and
frps.toml
- token based authentication will be used.
Make sure to specify the same
auth.token
in
frps.toml
and
frpc.toml
for frpc to pass frps validation
Token Source
frp supports reading authentication tokens from external sources using the
tokenSource
configuration. Currently, file-based token source is supported.
File-based token source:
#
frpc.toml
auth.method
=
"
token
"
auth.tokenSource.type
=
"
file
"
auth.tokenSource.file.path
=
"
/path/to/token/file
"
The token will be read from the specified file at startup. This is useful for scenarios where tokens are managed by external systems or need to be kept separate from configuration files for security reasons.
OIDC Authentication
When specifying
auth.method = "oidc"
in
frpc.toml
and
frps.toml
- OIDC based authentication will be used.
OIDC stands for OpenID Connect, and the flow used is called
Client Credentials Grant
.
To use this authentication type - configure
frpc.toml
and
frps.toml
as follows:
#
frps.toml
auth.method
=
"
oidc
"
auth.oidc.issuer
=
"
https://example-oidc-issuer.com/
"
auth.oidc.audience
=
"
https://oidc-audience.com/.default
"
#
frpc.toml
auth.method
=
"
oidc
"
auth.oidc.clientID
=
"
98692467-37de-409a-9fac-bb2585826f18
"
#
Replace with OIDC client ID
auth.oidc.clientSecret
=
"
oidc_secret
"
auth.oidc.audience
=
"
https://oidc-audience.com/.default
"
auth.oidc.tokenEndpointURL
=
"
https://example-oidc-endpoint.com/oauth2/v2.0/token
"
Encryption and Compression
The features are off by default. You can turn on encryption and/or compression:
#
frpc.toml
[[
proxies
]]
name
=
"
ssh
"
type
=
"
tcp
"
localPort
=
22
remotePort
=
6000
transport.useEncryption
=
true
transport.useCompression
=
true
TLS
Since v0.50.0, the default value of
transport.tls.enable
and
transport.tls.disableCustomTLSFirstByte
has been changed to true, and tls is enabled by default.
For port multiplexing, frp sends a first byte
0x17
to dial a TLS connection. This only takes effect when you set
transport.tls.disableCustomTLSFirstByte
to false.
To
enforce
frps
to only accept TLS connections - configure
transport.tls.force = true
in
frps.toml
.
This is optional.
frpc
TLS settings:
transport.tls.enable
=
true
transport.tls.certFile
=
"
certificate.crt
"
transport.tls.keyFile
=
"
certificate.key
"
transport.tls.trustedCaFile
=
"
ca.crt
"
frps
TLS settings:
transport.tls.force
=
true
transport.tls.certFile
=
"
certificate.crt
"
transport.tls.keyFile
=
"
certificate.key
"
transport.tls.trustedCaFile
=
"
ca.crt
"
You will need
a root CA cert
and
at least one SSL/TLS certificate
. It
can
be self-signed or regular (such as Let's Encrypt or another SSL/TLS certificate provider).
If you using
frp
via IP address and not hostname, make sure to set the appropriate IP address in the Subject Alternative Name (SAN) area when generating SSL/TLS Certificates.
Given an example:
Prepare openssl config file. It exists at
/etc/pki/tls/openssl.cnf
in Linux System and
/System/Library/OpenSSL/openssl.cnf
in MacOS, and you can copy it to current path, like
cp /etc/pki/tls/openssl.cnf ./my-openssl.cnf
. If not, you can build it by yourself, like:
cat > my-openssl.cnf << EOF
[ ca ]
default_ca = CA_default
[ CA_default ]
x509_extensions = usr_cert
[ req ]
default_bits        = 2048
default_md          = sha256
default_keyfile     = privkey.pem
distinguished_name  = req_distinguished_name
attributes          = req_attributes
x509_extensions     = v3_ca
string_mask         = utf8only
[ req_distinguished_name ]
[ req_attributes ]
[ usr_cert ]
basicConstraints       = CA:FALSE
nsComment              = "OpenSSL Generated Certificate"
subjectKeyIdentifier   = hash
authorityKeyIdentifier = keyid,issuer
[ v3_ca ]
subjectKeyIdentifier   = hash
authorityKeyIdentifier = keyid:always,issuer
basicConstraints       = CA:true
EOF
build ca certificates:
openssl genrsa -out ca.key 2048
openssl req -x509 -new -nodes -key ca.key -subj "/CN=example.ca.com" -days 5000 -out ca.crt
build frps certificates:
openssl genrsa -out server.key 2048

openssl req -new -sha256 -key server.key \
    -subj "/C=XX/ST=DEFAULT/L=DEFAULT/O=DEFAULT/CN=server.com" \
    -reqexts SAN \
    -config <(cat my-openssl.cnf <(printf "\n[SAN]\nsubjectAltName=DNS:localhost,IP:127.0.0.1,DNS:example.server.com")) \
    -out server.csr

openssl x509 -req -days 365 -sha256 \
	-in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial \
	-extfile <(printf "subjectAltName=DNS:localhost,IP:127.0.0.1,DNS:example.server.com") \
	-out server.crt
build frpc certificatesï¼š
openssl genrsa -out client.key 2048
openssl req -new -sha256 -key client.key \
    -subj "/C=XX/ST=DEFAULT/L=DEFAULT/O=DEFAULT/CN=client.com" \
    -reqexts SAN \
    -config <(cat my-openssl.cnf <(printf "\n[SAN]\nsubjectAltName=DNS:client.com,DNS:example.client.com")) \
    -out client.csr

openssl x509 -req -days 365 -sha256 \
    -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial \
	-extfile <(printf "subjectAltName=DNS:client.com,DNS:example.client.com") \
	-out client.crt
Hot-Reloading frpc configuration
The
webServer
fields are required for enabling HTTP API:
#
frpc.toml
webServer.addr
=
"
127.0.0.1
"
webServer.port
=
7400
Then run command
frpc reload -c ./frpc.toml
and wait for about 10 seconds to let
frpc
create or update or remove proxies.
Note that global client parameters won't be modified except 'start'.
You can run command
frpc verify -c ./frpc.toml
before reloading to check if there are config errors.
Get proxy status from client
Use
frpc status -c ./frpc.toml
to get status of all proxies. The
webServer
fields are required for enabling HTTP API.
Only allowing certain ports on the server
allowPorts
in
frps.toml
is used to avoid abuse of ports:
#
frps.toml
allowPorts
= [
  {
start
=
2000
,
end
=
3000
},
  {
single
=
3001
},
  {
single
=
3003
},
  {
start
=
4000
,
end
=
50000
}
]
Port Reuse
vhostHTTPPort
and
vhostHTTPSPort
in frps can use same port with
bindPort
. frps will detect the connection's protocol and handle it correspondingly.
What you need to pay attention to is that if you want to configure
vhostHTTPSPort
and
bindPort
to the same port, you need to first set
transport.tls.disableCustomTLSFirstByte
to false.
We would like to try to allow multiple proxies bind a same remote port with different protocols in the future.
Bandwidth Limit
For Each Proxy
#
frpc.toml
[[
proxies
]]
name
=
"
ssh
"
type
=
"
tcp
"
localPort
=
22
remotePort
=
6000
transport.bandwidthLimit
=
"
1MB
"
Set
transport.bandwidthLimit
in each proxy's configure to enable this feature. Supported units are
MB
and
KB
.
Set
transport.bandwidthLimitMode
to
client
or
server
to limit bandwidth on the client or server side. Default is
client
.
TCP Stream Multiplexing
frp supports tcp stream multiplexing since v0.10.0 like HTTP2 Multiplexing, in which case all logic connections to the same frpc are multiplexed into the same TCP connection.
You can disable this feature by modify
frps.toml
and
frpc.toml
:
#
frps.toml and frpc.toml, must be same
transport.tcpMux
=
false
Support KCP Protocol
KCP is a fast and reliable protocol that can achieve the transmission effect of a reduction of the average latency by 30% to 40% and reduction of the maximum delay by a factor of three, at the cost of 10% to 20% more bandwidth wasted than TCP.
KCP mode uses UDP as the underlying transport. Using KCP in frp:
Enable KCP in frps:
#
frps.toml
bindPort
=
7000
#
Specify a UDP port for KCP.
kcpBindPort
=
7000
The
kcpBindPort
number can be the same number as
bindPort
, since
bindPort
field specifies a TCP port.
Configure
frpc.toml
to use KCP to connect to frps:
#
frpc.toml
serverAddr
=
"
x.x.x.x
"
#
Same as the 'kcpBindPort' in frps.toml
serverPort
=
7000
transport.protocol
=
"
kcp
"
Support QUIC Protocol
QUIC is a new multiplexed transport built on top of UDP.
Using QUIC in frp:
Enable QUIC in frps:
#
frps.toml
bindPort
=
7000
#
Specify a UDP port for QUIC.
quicBindPort
=
7000
The
quicBindPort
number can be the same number as
bindPort
, since
bindPort
field specifies a TCP port.
Configure
frpc.toml
to use QUIC to connect to frps:
#
frpc.toml
serverAddr
=
"
x.x.x.x
"
#
Same as the 'quicBindPort' in frps.toml
serverPort
=
7000
transport.protocol
=
"
quic
"
Connection Pooling
By default, frps creates a new frpc connection to the backend service upon a user request. With connection pooling, frps keeps a certain number of pre-established connections, reducing the time needed to establish a connection.
This feature is suitable for a large number of short connections.
Configure the limit of pool count each proxy can use in
frps.toml
:
#
frps.toml
transport.maxPoolCount
=
5
Enable and specify the number of connection pool:
#
frpc.toml
transport.poolCount
=
1
Load balancing
Load balancing is supported by
group
.
This feature is only available for types
tcp
,
http
,
tcpmux
now.
#
frpc.toml
[[
proxies
]]
name
=
"
test1
"
type
=
"
tcp
"
localPort
=
8080
remotePort
=
80
loadBalancer.group
=
"
web
"
loadBalancer.groupKey
=
"
123
"
[[
proxies
]]
name
=
"
test2
"
type
=
"
tcp
"
localPort
=
8081
remotePort
=
80
loadBalancer.group
=
"
web
"
loadBalancer.groupKey
=
"
123
"
loadBalancer.groupKey
is used for authentication.
Connections to port 80 will be dispatched to proxies in the same group randomly.
For type
tcp
,
remotePort
in the same group should be the same.
For type
http
,
customDomains
,
subdomain
,
locations
should be the same.
Service Health Check
Health check feature can help you achieve high availability with load balancing.
Add
healthCheck.type = "tcp"
or
healthCheck.type = "http"
to enable health check.
With health check type
tcp
, the service port will be pinged (TCPing):
#
frpc.toml
[[
proxies
]]
name
=
"
test1
"
type
=
"
tcp
"
localPort
=
22
remotePort
=
6000
#
Enable TCP health check
healthCheck.type
=
"
tcp
"
#
TCPing timeout seconds
healthCheck.timeoutSeconds
=
3
#
If health check failed 3 times in a row, the proxy will be removed from frps
healthCheck.maxFailed
=
3
#
A health check every 10 seconds
healthCheck.intervalSeconds
=
10
With health check type
http
, an HTTP request will be sent to the service and an HTTP 2xx OK response is expected:
#
frpc.toml
[[
proxies
]]
name
=
"
web
"
type
=
"
http
"
localIP
=
"
127.0.0.1
"
localPort
=
80
customDomains
= [
"
test.example.com
"
]
#
Enable HTTP health check
healthCheck.type
=
"
http
"
#
frpc will send a GET request to '/status'
#
and expect an HTTP 2xx OK response
healthCheck.path
=
"
/status
"
healthCheck.timeoutSeconds
=
3
healthCheck.maxFailed
=
3
healthCheck.intervalSeconds
=
10
Rewriting the HTTP Host Header
By default frp does not modify the tunneled HTTP requests at all as it's a byte-for-byte copy.
However, speaking of web servers and HTTP requests, your web server might rely on the
Host
HTTP header to determine the website to be accessed. frp can rewrite the
Host
header when forwarding the HTTP requests, with the
hostHeaderRewrite
field:
#
frpc.toml
[[
proxies
]]
name
=
"
web
"
type
=
"
http
"
localPort
=
80
customDomains
= [
"
test.example.com
"
]
hostHeaderRewrite
=
"
dev.example.com
"
The HTTP request will have the
Host
header rewritten to
Host: dev.example.com
when it reaches the actual web server, although the request from the browser probably has
Host: test.example.com
.
Setting other HTTP Headers
Similar to
Host
, You can override other HTTP request and response headers with proxy type
http
.
#
frpc.toml
[[
proxies
]]
name
=
"
web
"
type
=
"
http
"
localPort
=
80
customDomains
= [
"
test.example.com
"
]
hostHeaderRewrite
=
"
dev.example.com
"
requestHeaders.set.x-from-where
=
"
frp
"
responseHeaders.set.foo
=
"
bar
"
In this example, it will set header
x-from-where: frp
in the HTTP request and
foo: bar
in the HTTP response.
Get Real IP
HTTP X-Forwarded-For
This feature is for
http
proxies or proxies with the
https2http
and
https2https
plugins enabled.
You can get user's real IP from HTTP request headers
X-Forwarded-For
.
Proxy Protocol
frp supports Proxy Protocol to send user's real IP to local services.
Here is an example for https service:
#
frpc.toml
[[
proxies
]]
name
=
"
web
"
type
=
"
https
"
localPort
=
443
customDomains
= [
"
test.example.com
"
]
#
now v1 and v2 are supported
transport.proxyProtocolVersion
=
"
v2
"
You can enable Proxy Protocol support in nginx to expose user's real IP in HTTP header
X-Real-IP
, and then read
X-Real-IP
header in your web service for the real IP.
Require HTTP Basic Auth (Password) for Web Services
Anyone who can guess your tunnel URL can access your local web server unless you protect it with a password.
This enforces HTTP Basic Auth on all requests with the username and password specified in frpc's configure file.
It can only be enabled when proxy type is http.
#
frpc.toml
[[
proxies
]]
name
=
"
web
"
type
=
"
http
"
localPort
=
80
customDomains
= [
"
test.example.com
"
]
httpUser
=
"
abc
"
httpPassword
=
"
abc
"
Visit
http://test.example.com
in the browser and now you are prompted to enter the username and password.
Custom Subdomain Names
It is convenient to use
subdomain
configure for http and https types when many people share one frps server.
#
frps.toml
subDomainHost
=
"
frps.com
"
Resolve
*.frps.com
to the frps server's IP. This is usually called a Wildcard DNS record.
#
frpc.toml
[[
proxies
]]
name
=
"
web
"
type
=
"
http
"
localPort
=
80
subdomain
=
"
test
"
Now you can visit your web service on
test.frps.com
.
Note that if
subdomainHost
is not empty,
customDomains
should not be the subdomain of
subdomainHost
.
URL Routing
frp supports forwarding HTTP requests to different backend web services by url routing.
locations
specifies the prefix of URL used for routing. frps first searches for the most specific prefix location given by literal strings regardless of the listed order.
#
frpc.toml
[[
proxies
]]
name
=
"
web01
"
type
=
"
http
"
localPort
=
80
customDomains
= [
"
web.example.com
"
]
locations
= [
"
/
"
]

[[
proxies
]]
name
=
"
web02
"
type
=
"
http
"
localPort
=
81
customDomains
= [
"
web.example.com
"
]
locations
= [
"
/news
"
,
"
/about
"
]
HTTP requests with URL prefix
/news
or
/about
will be forwarded to
web02
and other requests to
web01
.
TCP Port Multiplexing
frp supports receiving TCP sockets directed to different proxies on a single port on frps, similar to
vhostHTTPPort
and
vhostHTTPSPort
.
The only supported TCP port multiplexing method available at the moment is
httpconnect
- HTTP CONNECT tunnel.
When setting
tcpmuxHTTPConnectPort
to anything other than 0 in frps, frps will listen on this port for HTTP CONNECT requests.
The host of the HTTP CONNECT request will be used to match the proxy in frps. Proxy hosts can be configured in frpc by configuring
customDomains
and / or
subdomain
under
tcpmux
proxies, when
multiplexer = "httpconnect"
.
For example:
#
frps.toml
bindPort
=
7000
tcpmuxHTTPConnectPort
=
1337
#
frpc.toml
serverAddr
=
"
x.x.x.x
"
serverPort
=
7000
[[
proxies
]]
name
=
"
proxy1
"
type
=
"
tcpmux
"
multiplexer
=
"
httpconnect
"
customDomains
= [
"
test1
"
]
localPort
=
80
[[
proxies
]]
name
=
"
proxy2
"
type
=
"
tcpmux
"
multiplexer
=
"
httpconnect
"
customDomains
= [
"
test2
"
]
localPort
=
8080
In the above configuration - frps can be contacted on port 1337 with a HTTP CONNECT header such as:
CONNECT test1 HTTP/1.1\r\n\r\n
and the connection will be routed to
proxy1
.
Connecting to frps via PROXY
frpc can connect to frps through proxy if you set OS environment variable
HTTP_PROXY
, or if
transport.proxyURL
is set in frpc.toml file.
It only works when protocol is tcp.
#
frpc.toml
serverAddr
=
"
x.x.x.x
"
serverPort
=
7000
transport.proxyURL
=
"
http://user:pwd@192.168.1.128:8080
"
Port range mapping
Added in v0.56.0
We can use the range syntax of Go template combined with the built-in
parseNumberRangePair
function to achieve port range mapping.
The following example, when run, will create 8 proxies named
test-6000, test-6001 ... test-6007
, each mapping the remote port to the local port.
{{- range $_, $v := parseNumberRangePair "6000-6006,6007" "6000-6006,6007" }}
[[proxies]]
name = "tcp-{{ $v.First }}"
type = "tcp"
localPort = {{ $v.First }}
remotePort = {{ $v.Second }}
{{- end }}
Client Plugins
frpc only forwards requests to local TCP or UDP ports by default.
Plugins are used for providing rich features. There are built-in plugins such as
unix_domain_socket
,
http_proxy
,
socks5
,
static_file
,
http2https
,
https2http
,
https2https
and you can see
example usage
.
Using plugin
http_proxy
:
#
frpc.toml
[[
proxies
]]
name
=
"
http_proxy
"
type
=
"
tcp
"
remotePort
=
6000
[
proxies
.
plugin
]
type
=
"
http_proxy
"
httpUser
=
"
abc
"
httpPassword
=
"
abc
"
httpUser
and
httpPassword
are configuration parameters used in
http_proxy
plugin.
Server Manage Plugins
Read the
document
.
Find more plugins in
gofrp/plugin
.
SSH Tunnel Gateway
added in v0.53.0
frp supports listening to an SSH port on the frps side and achieves TCP protocol proxying through the SSH -R protocol, without relying on frpc.
#
frps.toml
sshTunnelGateway.bindPort
=
2200
When running
./frps -c frps.toml
, a private key file named
.autogen_ssh_key
will be automatically created in the current working directory. This generated private key file will be used by the SSH server in frps.
Executing the command
ssh -R :80:127.0.0.1:8080 v0@{frp address} -p 2200 tcp --proxy_name
"
test-tcp
"
--remote_port 9090
sets up a proxy on frps that forwards the local 8080 service to the port 9090.
frp (via SSH) (Ctrl+C to quit)

User:
ProxyName: test-tcp
Type: tcp
RemoteAddress: :9090
This is equivalent to:
frpc tcp --proxy_name
"
test-tcp
"
--local_ip 127.0.0.1 --local_port 8080 --remote_port 9090
Please refer to this
document
for more information.
Virtual Network (VirtualNet)
Alpha feature added in v0.62.0
The VirtualNet feature enables frp to create and manage virtual network connections between clients and visitors through a TUN interface. This allows for IP-level routing between machines, extending frp beyond simple port forwarding to support full network connectivity.
For detailed information about configuration and usage, please refer to the
VirtualNet documentation
.
Feature Gates
frp supports feature gates to enable or disable experimental features. This allows users to try out new features before they're considered stable.
Available Feature Gates
Name
Stage
Default
Description
VirtualNet
ALPHA
false
Virtual network capabilities for frp
Enabling Feature Gates
To enable an experimental feature, add the feature gate to your configuration:
featureGates
= {
VirtualNet
=
true
}
Feature Lifecycle
Features typically go through three stages:
ALPHA
: Disabled by default, may be unstable
BETA
: May be enabled by default, more stable but still evolving
GA (Generally Available)
: Enabled by default, ready for production use
Related Projects
gofrp/plugin
- A repository for frp plugins that contains a variety of plugins implemented based on the frp extension mechanism, meeting the customization needs of different scenarios.
gofrp/tiny-frpc
- A lightweight version of the frp client (around 3.5MB at minimum) implemented using the ssh protocol, supporting some of the most commonly used features, suitable for devices with limited resources.
Contributing
Interested in getting involved? We would like to help you!
Take a look at our
issues list
and consider sending a Pull Request to
dev branch
.
If you want to add a new feature, please create an issue first to describe the new feature, as well as the implementation approach. Once a proposal is accepted, create an implementation of the new features and submit it as a pull request.
Sorry for my poor English. Improvements for this document are welcome, even some typo fixes.
If you have great ideas, send an email to
fatedier@gmail.com
.
Note: We prefer you to give your advise in
issues
, so others with a same question can search it quickly and we don't need to answer them repeatedly.
Donation
If frp helps you a lot, you can support us by:
GitHub Sponsors
Support us by
Github Sponsors
.
You can have your company's logo placed on README file of this project.
PayPal
Donate money by
PayPal
to my account
fatedier@gmail.com
.
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 36
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 99,464

### References
- https://github.com/fatedier/frp

## NaiboWang/EasySpider

### Executive Summary
- A visual no-code/code-free web crawler/spideræ˜“é‡‡é›†ï¼šä¸€ä¸ªå¯è§†åŒ–æµè§ˆå™¨è‡ªåŠ¨åŒ–æµ‹è¯•/æ•°æ®é‡‡é›†/çˆ¬è™«è½¯ä»¶ï¼Œå¯ä»¥æ— ä»£ç å›¾å½¢åŒ–çš„è®¾è®¡å’Œæ‰§è¡Œçˆ¬è™«ä»»åŠ¡ã€‚åˆ«åï¼šServiceWrapperé¢å‘Webåº”ç”¨çš„æ™ºèƒ½åŒ–æœåŠ¡å°è£…ç³»ç»Ÿã€‚
- ---
- æ˜“é‡‡é›†/EasySpider: Visual Code-Free Web Crawler
ä¸€ä¸ª
å®Œå…¨å…è´¹
ï¼ˆ
åŒ…æ‹¬å•†ä¸šä½¿ç”¨å’ŒäºŒæ¬¡å¼€å‘
ï¼‰çš„å¯è§†åŒ–æµè§ˆå™¨è‡ªåŠ¨åŒ–æµ‹è¯•/æ•°æ®é‡‡é›†/çˆ¬è™«è½¯ä»¶ï¼Œå¯ä»¥ä½¿ç”¨å›¾å½¢åŒ–ç•Œé¢ï¼Œæ— ä»£ç å¯è§†åŒ–çš„è®¾è®¡å’Œæ‰§è¡Œä»»åŠ¡ã€‚åªéœ€è¦åœ¨ç½‘é¡µä¸Šé€‰æ‹©è‡ªå·±æƒ³è¦æ“ä½œçš„å†…å®¹å¹¶æ ¹æ®æç¤ºæ¡†æ“ä½œå³å¯å®Œæˆä»»åŠ¡çš„è®¾è®¡å’Œæ‰§è¡Œã€‚åŒæ—¶è½¯ä»¶è¿˜å¯ä»¥å•ç‹¬ä»¥å‘½ä»¤è¡Œçš„æ–¹å¼è¿›è¡Œæ‰§è¡Œï¼Œä»è€Œå¯ä»¥å¾ˆæ–¹ä¾¿çš„åµŒå…¥åˆ°å…¶ä»–ç³»ç»Ÿä¸­ã€‚
A
completely free (including for commercial use and secondary development)
visual browser automation test/data collection/crawler software, which can be used to design and execute tasks in a code-free visual way. You only need to select the content you want to operate on the web page and follow the prompts to complete the design and execution of the task. At the same time, the software can also be executed separately in the command line, so that it can be easily embedded into other systems.
ä¸‹è½½æ˜“é‡‡é›†/Download EasySpider
è¿›å…¥
Releases Page
ä¸‹è½½æœ€æ–°ç‰ˆæœ¬ã€‚å¦‚æœä¸‹è½½é€Ÿåº¦æ…¢ï¼Œå¯ä»¥è€ƒè™‘ä¸­å›½å¢ƒå†…ä¸‹è½½åœ°å€ï¼š
ä¸­å›½å¢ƒå†…ä¸‹è½½åœ°å€
ã€‚
Refer to the
Releases Page
to download the latest version of EasySpider.
èµåŠ©è€…/Sponsors
äº®æ•°æ®BrightData
æ˜¯ä»£ç†å¸‚åœºé¢†å¯¼è€…ï¼Œè¦†ç›–å…¨çƒçš„7200ä¸‡IPï¼Œæä¾›çœŸäººä½å®…IPï¼Œå³æ—¶æ‰¹é‡é‡‡é›†ç½‘ç»œå…¬å¼€æ•°æ®ï¼ŒæˆåŠŸç‡äº²æµ‹æœ‰ä¿è¯ã€‚éœ€è¦æ€§ä»·æ¯”é«˜ä»£ç†IPçš„å¯
ç‚¹å‡»ä¸Šæ–¹å›¾ç‰‡æ³¨å†Œ
åè”ç³»ä¸­æ–‡å®¢æœï¼Œå¼€é€šåå…è´¹è¯•ç”¨ï¼Œ
ç°åœ¨æœ‰é¦–å……å¤šå°‘å°±é€å¤šå°‘çš„æ´»åŠ¨
ã€‚BrightDataå¯é…åˆEasySpiderè¿›è¡Œæ•°æ®é‡‡é›†ã€‚
BestProxy
å…¨çƒç‹¬äº«ä¸“å±èµ„æºæ± ï¼Œä¼˜é€‰æµ·å¤–195+å›½å®¶/åœ°åŒºé«˜è´¨é‡ä½å®…IPï¼Œæœ¬åœ°ISPåŸç”ŸIPï¼Œä¸é™é‡ä½å®…ä»£ç†ã€é•¿æ•ˆISPä»£ç†ã€é™æ€æ•°æ®ä¸­å¿ƒä»£ç†ã€ç½‘é¡µçˆ¬è™«APIï¼ŒåŸå¸‚çº§ç²¾å‡†å®šä½ï¼Œæ”¯æŒHTTP(S)å’ŒSOCKS5åè®®ï¼Œä½æ£€æµ‹é£é™©ï¼Œå…¨æ–¹ä½ä»£ç†æœåŠ¡è§£å†³æ–¹æ¡ˆï¼ŒåŠ©åŠ›å„ç§åœºæ™¯ä¸šåŠ¡IPä»£ç†éœ€æ±‚ã€‚$0.66/Gèµ·æŒ‰éœ€ä»˜è´¹å’Œé•¿æœŸå¥—é¤ï¼Œé€‚åˆä¸åŒé¢„ç®—éœ€æ±‚ï¼Œ24/7å¤šè¯­è¨€æ”¯æŒï¼Œè”ç³»å®¢æœå…è´¹è¯•ç”¨500Mã€‚å¯ä¸EasySpiderå·¥å…·é…åˆä½¿ç”¨ï¼Œé«˜æ•ˆé‡‡é›†ç½‘ç»œæ•°æ®ã€‚
IPdodo
ä¸“æ³¨ä¸ºè·¨å¢ƒç”¨æˆ·ï¼Œæä¾›ç‹¬äº«/çº¯å‡€/å®¶å®½/åŸç”Ÿ/åŒISPçš„å…¨çƒä»£ç†IPï¼Œä¸é™æµé‡ã€‚å…¨çƒ8000ä¸‡çœŸå®ä½å®…IPï¼Œè¦†ç›–200+å›½å®¶/åœ°åŒºï¼Œ99.9%åŒ¿åä¿æŠ¤ï¼Œä¸”æ”¯æŒHttp/Https/Socks5åè®®ï¼Œæ»¡è¶³çˆ¬è™«ã€æ•°æ®é‡‡é›†ã€è·¨å¢ƒç”µå•†ã€tk/fbæµåª’ä½“ç­‰ä¸šåŠ¡åœºæ™¯ã€‚ç°åœ¨å‰å¾€IPdodoæ³¨å†Œï¼Œæ”¯æŒå…è´¹è¯•ç”¨ã€‚
å®˜æ–¹ç½‘ç«™/Official Website
è®¿é—®æ˜“é‡‡é›†å®˜ç½‘ï¼š
www.easyspider.cn
Visit the official website of EasySpider:
www.easyspider.net
è½¯ä»¶ä½¿ç”¨ç¤ºä¾‹/Software Usage Example
ç¤ºä¾‹1/Example 1
ï¼ˆå³é”®ï¼‰é€‰ä¸­ä¸€ä¸ªå¤§å•†å“å— -> è½¯ä»¶è‡ªåŠ¨æ£€æµ‹åˆ°åŒç±»å‹å•†å“å— -> ç‚¹å‡»â€œé€‰ä¸­å…¨éƒ¨â€é€‰é¡¹ -> ç‚¹å‡»â€œé€‰ä¸­å­å…ƒç´ â€é€‰é¡¹ -> ç‚¹å‡»â€œé‡‡é›†æ•°æ®â€é€‰é¡¹ï¼Œå³å¯é‡‡é›†åˆ°æ‰€æœ‰å•†å“çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå¹¶åˆ†æˆä¸åŒå­—æ®µä¿å­˜ã€‚
(Right click) Select a large product block -> The software will automatically detect similar blocks -> Click the 'Select All' option -> Click the 'Select Child Elements' option -> Click the 'Collect Data' option, you can collect the information of all products, and will be saved by sub-field.
ç¤ºä¾‹2/Example 2
ï¼ˆå³é”®ï¼‰é€‰ä¸­ä¸€ä¸ªå•†å“æ ‡é¢˜ï¼ŒåŒç±»å‹æ ‡é¢˜ä¼šè¢«è‡ªåŠ¨åŒ¹é…ï¼Œç‚¹å‡»â€œé€‰ä¸­å…¨éƒ¨â€é€‰é¡¹ -> ç‚¹å‡»â€œé‡‡é›†æ•°æ®â€é€‰é¡¹ï¼Œå³å¯é‡‡é›†åˆ°æ‰€æœ‰å•†å“çš„æ ‡é¢˜ä¿¡æ¯ã€‚
åŒæ—¶ï¼Œé€‰ä¸­å…¨éƒ¨åå¦‚æœé€‰æ‹©â€œå¾ªç¯ç‚¹å‡»æ¯ä¸ªå…ƒç´ â€é€‰é¡¹ï¼Œå³å¯è‡ªåŠ¨æ‰“å¼€æ¯ä¸ªå•†å“çš„è¯¦æƒ…é¡µï¼Œç„¶åå¯ä»¥å†ç»§ç»­è®¾ç½®é‡‡é›†è¯¦æƒ…é¡µçš„ä¿¡æ¯ã€‚
(Right Click) Select a product title, the same type of title will be automatically matched, click the 'Select All' option -> Click the 'Collect Data' option, you can collect the title information of all products.
At the same time, if you select the 'Loop-click every element' option after selecting all, you can automatically open the details page of each product, and then can set to collect the information of the details page.
æ›´å¤šç‰¹æ€§/More Features
æ›´å¤šç‰¹æ€§è¯·ç¿»åˆ°é¡µé¢åº•éƒ¨æŸ¥çœ‹ã€‚
More features please scroll to the bottom of this page to view.
æ”¯æŒä½œè€…/Support Author
æ˜“é‡‡é›†EasySpideræ˜¯ä¸€æ¬¾å®Œå…¨å…è´¹ä¸”ä½¿ç”¨ä¸­æ— å¹¿å‘Šçš„å¼€æºè½¯ä»¶ï¼Œè½¯ä»¶å¼€å‘å’Œç»´æŠ¤å…¨é ä½œè€…ç”¨çˆ±å‘ç”µï¼Œå› æ­¤æ‚¨å¯ä»¥é€‰æ‹©æ”¯æŒä½œè€…è®©ä½œè€…æœ‰æ›´å¤šçš„çƒ­æƒ…å’Œç²¾åŠ›ç»´æŠ¤æ­¤è½¯ä»¶ï¼Œæˆ–è€…æ‚¨ä½¿ç”¨äº†æ­¤è½¯ä»¶è¿›è¡Œäº†ç›ˆåˆ©ï¼Œæ¬¢è¿æ‚¨é€šè¿‡ä¸‹é¢çš„æ–¹å¼æ”¯æŒä½œè€…ï¼š
Github Sponsorï¼šç›´æ¥ç‚¹å‡»å³ä¾§
Sponsor
æŒ‰é’®èµåŠ©ã€‚
æ”¯ä»˜å®è´¦å·ï¼š
naibowang@foxmail.com
ï¼Œä¹Ÿå¯ä»¥æ‰«æä¸‹æ–¹äºŒç»´ç ã€‚
å¾®ä¿¡æ”¶æ¬¾ï¼šæ‰«æä¸‹æ–¹äºŒç»´ç ã€‚
PayPalè´¦å·ï¼šnaibowangï¼Œä¹Ÿå¯ä»¥æ‰«æä¸‹æ–¹äºŒç»´ç ã€‚
You can support the author by clicking the
Sponsor
button at right side or pay via paypal: naibowang.
æ–‡æ¡£/Documentation
è¯·ç‚¹æ­¤è¿›å…¥
æ•™ç¨‹æ–‡æ¡£
ï¼Œå¦‚æœ‰è‹±æ–‡å¯æš‚æ—¶ç¿»è¯‘ä¸€ä¸‹ï¼Œæˆ–çœ‹ä½œè€…çš„
ç¡•å£«æ¯•ä¸šè®ºæ–‡
ï¼ˆä¸»è¦çœ‹ç¬¬ä¸‰ç« å’Œç¬¬äº”ç« ï¼‰ã€‚
Ebayæ ·ä¾‹åšå®¢ï¼š
https://blog.csdn.net/ihero/article/details/130805504
ã€‚
Documentation can be found from
GitHub Wiki
.
è§†é¢‘æ•™ç¨‹/Video Tutorials
Bilibili/Bç«™è§†é¢‘æ•™ç¨‹:
EasySpiderä»‹ç» - ä¸­å›½åœ°éœ‡å°ç½‘é‡‡é›†æ¡ˆä¾‹
è®¾ç½®é¡µé¢å‘ä¸‹æ»šåŠ¨
å¦‚ä½•æ— ä»£ç å¯è§†åŒ–çš„çˆ¬å–éœ€è¦ç™»å½•æ‰èƒ½çˆ¬çš„ç½‘ç«™ - çŸ¥ä¹ç½‘ç«™æ¡ˆä¾‹
å¾ªç¯ç‚¹å‡»åˆ—è¡¨ä¸­æ¯ä¸ªé“¾æ¥è¿›å…¥è¯¦æƒ…é¡µé‡‡é›†è¯¦æƒ…é¡µå†…å®¹+è®¾è®¡æ—¶åŠ¨æ€è°ƒè¯•+åŠ¨æ€JS
å®æˆ˜é‡‡é›†æ±½è½¦ç½‘æ–‡ç« å†…å®¹å¹¶ä¸‹è½½æ–‡ç« å†…å›¾ç‰‡
å®šæ—¶æ‰§è¡Œä»»åŠ¡+é€‰ä¸­å­å…ƒç´ å¤šç§æ¨¡å¼+å°†æå–å€¼ä½œä¸ºå˜é‡è¾“å…¥
ã€é‡è¦ã€‘è‡ªå®šä¹‰æ¡ä»¶åˆ¤æ–­ä¹‹ä½¿ç”¨å¾ªç¯é¡¹å†…çš„JSå‘½ä»¤è¿”å›å€¼ - ç¬¬äºŒå¼¹
æµç¨‹å›¾æ‰§è¡Œé€»è¾‘è§£æ - 58åŒåŸæˆ¿æºæè¿°é‡‡é›†æ¡ˆä¾‹
MacOSç³»ç»Ÿè®¾è®¡å’Œæ‰§è¡ŒeBayç½‘ç«™çˆ¬è™«ä»»åŠ¡æ•™ç¨‹
å¦‚ä½•æ‰§è¡Œè‡ªå·±å†™çš„JSä»£ç å’Œç³»ç»Ÿä»£ç  ï¼ˆè‡ªå®šä¹‰æ“ä½œï¼‰
å¦‚ä½•è‡ªå®šä¹‰å¾ªç¯å’Œåˆ¤æ–­æ¡ä»¶ - ç¬¬ä¸€å¼¹
å¦‚ä½•å¯¹å…ƒç´ å’Œç½‘é¡µæˆªå›¾åŠå‘½ä»¤è¡Œæ‰§è¡ŒæŒ‡å—
OCRè¯†åˆ«å…ƒç´ å†…å®¹åŠŸèƒ½ï¼ˆå¸¸ç”¨äºæ–‡å­—éªŒè¯ç ï¼‰
å¦‚ä½•çˆ¬éœ€è¦è¾“å…¥éªŒè¯ç çš„ç½‘ç«™
å¦‚ä½•åˆ‡æ¢IPæ± å’Œä½¿ç”¨éš§é“IP - æ‰“å¼€è¯¦æƒ…é¡µé‡‡é›†æ¡ˆä¾‹
å¦‚ä½•åŒæ—¶æ‰§è¡Œå¤šä¸ªä»»åŠ¡ï¼ˆå¹¶è¡Œå¤šå¼€ï¼‰
Pythonä»£ç è¿ç®—åçš„ç»“æœä½œä¸ºæ–‡æœ¬æ¡†çš„è¾“å…¥
å®ä¾‹ - åäººç±»ç½‘ç«™æ–‡ç« é‡‡é›†å’Œä»£ç è°ƒè¯•
å†™å…¥MySQLæ•°æ®åº“æ•™ç¨‹
ä»æºä»£ç ç¼–è¯‘ç¨‹åºå¹¶è®¾è®¡è¿è¡Œå’Œè°ƒè¯•ä»»åŠ¡æŒ‡å—ï¼ˆåŸºäºUbuntu24.04ï¼‰
Refer to
Youtube Playlist
to see the video tutorials of EasySpider.
æ ·ä¾‹ä»»åŠ¡/Sample Tasks
ä»æœ¬é¡¹ç›®çš„
Examples
æ–‡ä»¶å¤¹ä¸­ä¸‹è½½æ ·ä¾‹ä»»åŠ¡ï¼Œæ›´åä¸ºå¤§äº0çš„æ•°å­—ï¼Œå¯¼å…¥åˆ°EasySpiderä¸­çš„
tasks
æ–‡ä»¶å¤¹ä¸­ï¼Œç„¶ååœ¨EasySpiderä¸­æ‰“å¼€å³å¯ã€‚
Download sample tasks from the
Examples
folder of this project, rename them to numbers greater than 0, import them into the
tasks
folder in EasySpider, and then open them in EasySpider.
å£°æ˜/Declaration
æœ¬è½¯ä»¶ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œ
ä¸¥ç¦ä½¿ç”¨è½¯ä»¶è¿›è¡Œä»»ä½•è¿æ³•è¿è§„çš„æ“ä½œï¼Œå¦‚çˆ¬å–ä¸å…è®¸çˆ¬å–çš„æ”¿åºœ/å†›äº‹æœºå…³ç½‘ç«™ç­‰
ã€‚ä½¿ç”¨æœ¬è½¯ä»¶æ‰€é€ æˆçš„
ä¸€åˆ‡åæœç”±ä½¿ç”¨è€…è‡ªè´Ÿ
ï¼Œä¸ä½œè€…æœ¬äººæ— å…³ï¼Œ
ä½œè€…ä¸ä¼šæ‰¿æ‹…ä»»ä½•è´£ä»»
ã€‚
This software is for learning and communication only.
It is strictly forbidden to use the software for any illegal operations, such as crawling government/military websites that are not allowed to be crawled.
All consequences caused by the use of this software are
at the user's own risk, and the author is not responsible for any consequences
.
å¯¹äºæ”¿åºœå’Œå†›äº‹æœºå…³ç­‰ç½‘ç«™çš„çˆ¬è™«æ“ä½œï¼Œ
ä½œè€…å°†ä¸ä¼šè¿›è¡Œä»»ä½•ç­”ç–‘
ï¼Œä»¥å…è¿åå›½å®¶ç›¸å…³æ³•å¾‹æ³•è§„å’Œæ”¿ç­–ã€‚
For the crawler operations of government and military websites,
the author will not answer any questions
in order to avoid violating relevant national laws, regulations and policies.
EasySpideréµå¾ªAGPL-3.0åè®®ï¼Œ
ä»»ä½•ä¸ªäººå’Œä¼ä¸šéƒ½å¯ä»¥å…è´¹ä½¿ç”¨è½¯ä»¶æœ¬èº«æˆ–ä½¿ç”¨æºä»£ç è¿›è¡ŒäºŒæ¬¡å¼€å‘ï¼Œæ— éœ€è”ç³»ä½œè€…è¿›è¡Œå•†ä¸šï¼ˆä¸“åˆ©ï¼‰æˆæƒ
ï¼Œä½†éœ€è¦æ³¨æ„AGPL-3.0åè®®çš„ç›¸å…³è§„åˆ™ï¼š
EasySpider complies with the AGPL-3.0 agreement.
Any individual or enterprise can use the software for free and use the software source code for secondary development without contacting the author for commercial (patent) authorization.
However, it is necessary to pay attention to the related rules of the AGPL-3.0 agreement:
1. Copyleftï¼ˆä¼ æŸ“æ€§ï¼‰ / Copyleft (Viral Clause)
è¡ç”Ÿä½œå“ / Derivative Works
ä»»ä½•åŸºäº AGPL ä»£ç çš„ä¿®æ”¹æˆ–è¡ç”Ÿä½œå“ï¼Œå¿…é¡»
ä»¥ç›¸åŒè®¸å¯è¯ï¼ˆAGPL-3.0ï¼‰å‘å¸ƒ
ã€‚
Any modifications or derivative works based on AGPL code must be
licensed under AGPL-3.0
.
è”åŠ¨èŒƒå›´ / Scope of Copyleft
è‹¥ AGPL ä»£ç ä¸å…¶ä»–ä»£ç ç»“åˆï¼ˆå¦‚é™æ€é“¾æ¥ã€ç´§å¯†é›†æˆï¼‰ï¼Œæ•´ä¸ªä½œå“éœ€éµå®ˆ AGPLã€‚
If AGPL code is combined with other code (e.g., static linking), the entire work must comply with AGPL.
2. ç½‘ç»œä½¿ç”¨æ¡æ¬¾ / Network Use Clause
SaaS è§¦å‘å¼€æºä¹‰åŠ¡ / SaaS Trigger
è‹¥è½¯ä»¶ä»¥æœåŠ¡å½¢å¼æä¾›ï¼ˆå¦‚ç½‘ç«™ã€APIï¼‰ï¼Œå¿…é¡»å‘æ‰€æœ‰ç”¨æˆ·å…¬å¼€
å®Œæ•´å¯¹åº”æºä»£ç 
ï¼ˆåŒ…æ‹¬ä¿®æ”¹åçš„ä»£ç ï¼‰ã€‚
If the software is provided as a service (e.g., website, API), the
full corresponding source code
(including modifications) must be made available to all users.
ç”¨æˆ·æƒåˆ© / User Rights
æœåŠ¡çš„æ¥æ”¶è€…å¯é€šè¿‡ä¸‹è½½æˆ–ä¹¦é¢è¯·æ±‚è·å–æºç ã€‚
Service recipients may obtain the source code via download or written request.
3. æºç æä¾›è¦æ±‚ / Source Code Provision
äºŒè¿›åˆ¶åˆ†å‘ / Binary Distribution
å¿…é¡»é™„å¸¦æºç æˆ–æä¾›è·å–æ¸ é“ï¼ˆå¦‚ä¸‹è½½é“¾æ¥ï¼‰ã€‚
Source code must be included or a download link provided.
ç½‘ç»œæœåŠ¡åœºæ™¯ / Network Service Scenario
éœ€é€šè¿‡æœåŠ¡ç•Œé¢
æ˜¾å¼æä¾›æºç é“¾æ¥
ï¼Œæˆ–å‘ç”¨æˆ·ä¹¦é¢æ‰¿è¯ºæä¾›æºç ã€‚
The service interface must
explicitly provide a source code link
or offer a written offer for source code.
4. ä¸“åˆ©æˆæƒ / Patent Grant
è´¡çŒ®è€…è‡ªåŠ¨æˆäºˆç”¨æˆ·ä¸è½¯ä»¶ç›¸å…³çš„ä¸“åˆ©è®¸å¯ï¼Œç¦æ­¢ä¸“åˆ©è¯‰è®¼ã€‚
Contributors automatically grant users patent rights related to the software, and prohibit patent litigation.
5. å…è´£å£°æ˜ / Disclaimer
è½¯ä»¶æŒ‰â€œåŸæ ·â€æä¾›ï¼Œä½œè€…
ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»
ï¼ˆæ— æ‹…ä¿æ¡æ¬¾ï¼‰ã€‚
The software is provided "as is" with
no warranties or liabilities
.
ç­”ç–‘QQç¾¤
ç¾¤å·ï¼š
682921940
ï¼Œå»ºè®®é€šè¿‡GithubæIssueçš„æ–¹å¼ç­”ç–‘ï¼Œå¦‚æœå®åœ¨æœ‰éœ€è¦æ‰è¯·åŠ QQç¾¤ï¼Œå› ä¸ºç¾¤äººæ•°æœ‰ä¸Šé™ï¼Œ
QQç¾¤ä¸æä¾›è½¯ä»¶ä¸‹è½½åŠŸèƒ½
ã€‚
å‡ºç‰ˆç‰©/Publications
This software has been accepted by The Web Conference (WWW) 2023 (ä¸­å›½è®¡ç®—æœºå­¦ä¼šé¡¶çº§ä¼šè®®ï¼ŒCCF A):
EasySpider: A No-Code Visual System for Crawling the Web
, April 2023.
ä¸­å›½å›½å®¶çŸ¥è¯†äº§æƒå±€å‘æ˜ä¸“åˆ©ï¼Œ
ä¸€ç§è‡ªå®šä¹‰æå–æµç¨‹çš„æœåŠ¡å°è£…ç³»ç»Ÿ
ï¼Œ 2022å¹´5æœˆã€‚
æµ™æ±Ÿå¤§å­¦ç¡•å£«è®ºæ–‡
ï¼Œ
é¢å‘WEBåº”ç”¨çš„æ™ºèƒ½åŒ–æœåŠ¡å°è£…ç³»ç»Ÿè®¾è®¡ä¸å®ç°
ï¼Œ2020å¹´6æœˆã€‚
ç¼–è¯‘è¯´æ˜/Compilation Instructions
æŸ¥çœ‹
ç¼–è¯‘è¯´æ˜
ã€‚
Refer to
Compilation Instructions
.
æ”¯æŒç‰¹æ€§/Supported Features
ä¸­æ–‡ç•Œé¢æˆªå›¾
è½¯ä»¶ç•Œé¢ç¤ºä¾‹
å—å’Œå­å—åŠè¡¨å•å®šä¹‰
å·²é€‰ä¸­å’Œå¾…é€‰æ‹©ç¤ºä¾‹
äº¬ä¸œå•†å“å—é€‰æ‹©ç¤ºä¾‹ï¼š
äº¬ä¸œå•†å“æ ‡é¢˜è‡ªåŠ¨åŒ¹é…é€‰æ‹©ç¤ºä¾‹
åˆ†å—é€‰æ‹©æ‰€æœ‰å­å…ƒç´ ç¤ºä¾‹
åŒç±»å‹å…ƒç´ è‡ªåŠ¨å’Œæ‰‹åŠ¨åŒ¹é…ç¤ºä¾‹
å››ç§é€‰æ‹©æ–¹å¼ç¤ºä¾‹
è¾“å…¥æ–‡å­—ç¤ºä¾‹
å¾ªç¯ç‚¹å‡»58åŒåŸæˆ¿å±‹æ ‡é¢˜ä»¥è¿›å…¥è¯¦æƒ…é¡µé‡‡é›†ç¤ºä¾‹
é‡‡é›†å…ƒç´ æ–‡æœ¬ç¤ºä¾‹
æµç¨‹å›¾ç•Œé¢ä»‹ç»
å¾ªç¯é€‰é¡¹ç¤ºä¾‹
å¾ªç¯ç‚¹å‡»ä¸‹ä¸€é¡µç¤ºä¾‹
æ¡ä»¶åˆ†æ”¯ç¤ºä¾‹
å®Œæ•´é‡‡é›†æµç¨‹å›¾ç¤ºä¾‹
å®Œæ•´é‡‡é›†æµç¨‹å›¾è½¬æ¢ä¸ºå¸¸è§„æµç¨‹å›¾ç¤ºä¾‹
æœåŠ¡ä¿¡æ¯ç¤ºä¾‹
æœåŠ¡è°ƒç”¨ç¤ºä¾‹
58 åŒåŸæˆ¿æºä¿¡æ¯é‡‡é›†æœåŠ¡éƒ¨åˆ†é‡‡é›†ç»“æœå±•ç¤º
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 36
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 42,881

### References
- https://github.com/NaiboWang/EasySpider

## katanemo/archgw

### Executive Summary
- The smart edge and AI gateway for agents. Arch is a high-performance proxy server that handles the low-level work in building agents: like applying guardrails, routing prompts to the right agent, and unifying access to LLMs, etc. Natively designed to handle and process prompts, Arch helps you build agents faster.
- ---
- Arch is a modular ai-native edge and AI gateway for agents.
Arch handles the
pesky low-level work
in building agentic apps â€” like applying guardrails, clarifying vague user input, routing prompts to the right agent, and unifying access to any LLM. Itâ€™s a language and framework friendly infrastructure layer designed to help you build and ship agentic apps faster.
Quickstart
â€¢
Demos
â€¢
Route LLMs
â€¢
Build agentic apps with Arch
â€¢
Documentation
â€¢
Contact
About The Latest Release:
[0.3.15]
Preference-aware multi LLM routing for Claude Code 2.0
Overview
AI demos are easy to hack. But once you move past a prototype, youâ€™re stuck building and maintaining low-level plumbing code that slows down real innovation. For example:
Routing & orchestration.
Put routing in code and youâ€™ve got two choices: maintain it yourself or live with a frameworkâ€™s baked-in logic. Either way, keeping routing consistent means pushing code changes across all your agents, slowing iteration and turning every policy tweak into a refactor instead of a config flip.
Model integration churn.
Frameworks wire LLM integrations directly into code abstractions, making it hard to add or swap models without touching application code â€” meaning youâ€™ll have to do codewide search/replace every time you want to experiment with a new model or version.
Observability & governance.
Logging, tracing, and guardrails are baked in as tightly coupled features, so bringing in best-of-breed solutions is painful and often requires digging through the guts of a framework.
Prompt engineering overhead
. Input validation, clarifying vague user input, and coercing outputs into the right schema all pile up, turning what should be design work into low-level plumbing work.
Brittle upgrades
. Every change (new model, new guardrail, new trace format) means patching and redeploying application servers. Contrast that with bouncing a central proxyâ€”one upgrade, instantly consistent everywhere.
With Arch, you can move faster by focusing on higher-level objectives in a language and framework agnostic way.
Arch
was built by the contributors of
Envoy Proxy
with the belief that:
Prompts are nuanced and opaque user requests, which require the same capabilities as traditional HTTP requests including secure handling, intelligent routing, robust observability, and integration with backend (API) systems to improve speed and accuracy for common agentic scenarios  â€“ all outside core application logic.*
Core Features
:
ğŸš¦ Route to Agents
: Engineered with purpose-built
LLMs
for fast (<100ms) agent routing and hand-off
ğŸ”— Route to LLMs
: Unify access to LLMs with support for
three routing strategies
.
â›¨ Guardrails
: Centrally configure and prevent harmful outcomes and ensure safe user interactions
âš¡ Tools Use
: For common agentic scenarios let Arch instantly clarify and convert prompts to tools/API calls
ğŸ•µ Observability
: W3C compatible request tracing and LLM metrics that instantly plugin with popular tools
ğŸ§± Built on Envoy
: Arch runs alongside app servers as a containerized process, and builds on top of
Envoy's
proven HTTP management and scalability features to handle ingress and egress traffic related to prompts and LLMs.
High-Level Sequence Diagram
:
Jump to our
docs
to learn how you can use Arch to improve the speed, security and personalization of your GenAI apps.
Important
Today, the function calling LLM (Arch-Function) designed for the agentic and RAG scenarios is hosted free of charge in the US-central region. To offer consistent latencies and throughput, and to manage our expenses, we will enable access to the hosted version via developers keys soon, and give you the option to run that LLM locally. For more details see this issue
#258
Contact
To get in touch with us, please join our
discord server
. We will be monitoring that actively and offering support there.
Demos
Sample App: Weather Forecast Agent
- A sample agentic weather forecasting app that highlights core function calling capabilities of Arch.
Sample App: Network Operator Agent
- A simple network device switch operator agent that can retrive device statistics and reboot them.
User Case: Connecting to SaaS APIs
- Connect 3rd party SaaS APIs to your agentic chat experience.
Quickstart
Follow this quickstart guide to use Arch as a router for local or hosted LLMs, including dynamic routing. Later in the section we will see how you can Arch to build highly capable agentic applications, and to provide e2e observability.
Prerequisites
Before you begin, ensure you have the following:
Docker System
(v24)
Docker compose
(v2.29)
Python
(v3.13)
Arch's CLI allows you to manage and interact with the Arch gateway efficiently. To install the CLI, simply run the following command:
Tip
We recommend that developers create a new Python virtual environment to isolate dependencies before installing Arch. This ensures that archgw and its dependencies do not interfere with other packages on your system.
$
python3.12 -m venv venv
$
source
venv/bin/activate
#
On Windows, use: venv\Scripts\activate
$
pip install archgw==0.3.15
Use Arch as a LLM Router
Arch supports three powerful routing strategies for LLMs: model-based routing, alias-based routing, and preference-based routing. Each strategy offers different levels of abstraction and control for managing your LLM infrastructure.
Model-based Routing
Model-based routing allows you to configure specific models with static routing. This is ideal when you need direct control over which models handle specific requests. Arch supports 11+ LLM providers including OpenAI, Anthropic, DeepSeek, Mistral, Groq, and more.
version
:
v0.1.0
listeners
:
egress_traffic
:
address
:
0.0.0.0
port
:
12000
message_format
:
openai
timeout
:
30s
llm_providers
:
  -
model
:
openai/gpt-4o
access_key
:
$OPENAI_API_KEY
default
:
true
-
model
:
anthropic/claude-3-5-sonnet-20241022
access_key
:
$ANTHROPIC_API_KEY
You can then route to specific models using any OpenAI-compatible client:
from
openai
import
OpenAI
client
=
OpenAI
(
base_url
=
"http://127.0.0.1:12000/v1"
,
api_key
=
"test"
)
# Route to specific model
response
=
client
.
chat
.
completions
.
create
(
model
=
"anthropic/claude-3-5-sonnet-20241022"
,
messages
=
[{
"role"
:
"user"
,
"content"
:
"Explain quantum computing"
}]
)
Alias-based Routing
Alias-based routing lets you create semantic model names that map to underlying providers. This approach decouples your application code from specific model names, making it easy to experiment with different models or handle provider changes.
version
:
v0.1.0
listeners
:
egress_traffic
:
address
:
0.0.0.0
port
:
12000
message_format
:
openai
timeout
:
30s
llm_providers
:
  -
model
:
openai/gpt-4o
access_key
:
$OPENAI_API_KEY
-
model
:
anthropic/claude-3-5-sonnet-20241022
access_key
:
$ANTHROPIC_API_KEY
model_aliases
:
#
Model aliases - friendly names that map to actual model names
fast-model
:
target
:
gpt-4o-mini
reasoning-model
:
target
:
gpt-4o
creative-model
:
target
:
claude-3-5-sonnet-20241022
Use semantic aliases in your application code:
# Your code uses semantic names instead of provider-specific ones
response
=
client
.
chat
.
completions
.
create
(
model
=
"reasoning-model"
,
# Routes to best available reasoning model
messages
=
[{
"role"
:
"user"
,
"content"
:
"Solve this complex problem..."
}]
)
Preference-aligned Routing
Preference-aligned routing provides intelligent, dynamic model selection based on natural language descriptions of tasks and preferences. Instead of hardcoded routing logic, you describe what each model is good at using plain English.
version
:
v0.1.0
listeners
:
egress_traffic
:
address
:
0.0.0.0
port
:
12000
message_format
:
openai
timeout
:
30s
llm_providers
:
  -
model
:
openai/gpt-4o
access_key
:
$OPENAI_API_KEY
routing_preferences
:
      -
name
:
complex_reasoning
description
:
deep analysis, mathematical problem solving, and logical reasoning
-
name
:
creative_writing
description
:
storytelling, creative content, and artistic writing
-
model
:
deepseek/deepseek-coder
access_key
:
$DEEPSEEK_API_KEY
routing_preferences
:
      -
name
:
code_generation
description
:
generating new code, writing functions, and creating scripts
-
name
:
code_review
description
:
analyzing existing code for bugs, improvements, and optimization
Arch uses a lightweight 1.5B autoregressive model to intelligently map user prompts to these preferences, automatically selecting the best model for each request. This approach adapts to intent drift, supports multi-turn conversations, and avoids brittle embedding-based classifiers or manual if/else chains. No retraining required when adding models or updating policies â€” routing is governed entirely by human-readable rules.
Learn More
: Check our
documentation
for comprehensive provider setup guides and routing strategies. You can learn more about the design, benchmarks, and methodology behind preference-based routing in our paper:
Build Agentic Apps with Arch
In following quickstart we will show you how easy it is to build AI agent with Arch gateway. We will build a currency exchange agent using following simple steps. For this demo we will use
https://api.frankfurter.dev/
to fetch latest price for currencies and assume USD as base currency.
Step 1. Create arch config file
Create
arch_config.yaml
file with following content,
version
:
v0.1.0
listeners
:
ingress_traffic
:
address
:
0.0.0.0
port
:
10000
message_format
:
openai
timeout
:
30s
llm_providers
:
  -
access_key
:
$OPENAI_API_KEY
model
:
openai/gpt-4o
system_prompt
:
|
You are a helpful assistant.
prompt_guards
:
input_guards
:
jailbreak
:
on_exception
:
message
:
Looks like you're curious about my abilities, but I can only provide assistance for currency exchange.
prompt_targets
:
  -
name
:
currency_exchange
description
:
Get currency exchange rate from USD to other currencies
parameters
:
      -
name
:
currency_symbol
description
:
the currency that needs conversion
required
:
true
type
:
str
in_path
:
true
endpoint
:
name
:
frankfurther_api
path
:
/v1/latest?base=USD&symbols={currency_symbol}
system_prompt
:
|
You are a helpful assistant. Show me the currency symbol you want to convert from USD.
-
name
:
get_supported_currencies
description
:
Get list of supported currencies for conversion
endpoint
:
name
:
frankfurther_api
path
:
/v1/currencies
endpoints
:
frankfurther_api
:
endpoint
:
api.frankfurter.dev:443
protocol
:
https
Step 2. Start arch gateway with currency conversion config
$ archgw up arch_config.yaml
2024-12-05 16:56:27,979 - cli.main - INFO - Starting archgw cli version: 0.3.15
2024-12-05 16:56:28,485 - cli.utils - INFO - Schema validation successful
!
2024-12-05 16:56:28,485 - cli.main - INFO - Starting arch model server and arch gateway
2024-12-05 16:56:51,647 - cli.core - INFO - Container is healthy
!
Once the gateway is up you can start interacting with at port 10000 using openai chat completion API.
Some of the sample queries you can ask could be
what is currency rate for gbp?
or
show me list of currencies for conversion
.
Step 3. Interacting with gateway using curl command
Here is a sample curl command you can use to interact,
$ curl --header
'
Content-Type: application/json
'
\
  --data
'
{"messages": [{"role": "user","content": "what is exchange rate for gbp"}], "model": "none"}
'
\
  http://localhost:10000/v1/chat/completions
|
jq
"
.choices[0].message.content
"
"
As of the date provided in your context, December 5, 2024, the exchange rate for GBP (British Pound) from USD (United States Dollar) is 0.78558. This means that 1 USD is equivalent to 0.78558 GBP.
"
And to get list of supported currencies,
$ curl --header
'
Content-Type: application/json
'
\
  --data
'
{"messages": [{"role": "user","content": "show me list of currencies that are supported for conversion"}], "model": "none"}
'
\
  http://localhost:10000/v1/chat/completions
|
jq
"
.choices[0].message.content
"
"
Here is a list of the currencies that are supported for conversion from USD, along with their symbols:\n\n1. AUD - Australian Dollar\n2. BGN - Bulgarian Lev\n3. BRL - Brazilian Real\n4. CAD - Canadian Dollar\n5. CHF - Swiss Franc\n6. CNY - Chinese Renminbi Yuan\n7. CZK - Czech Koruna\n8. DKK - Danish Krone\n9. EUR - Euro\n10. GBP - British Pound\n11. HKD - Hong Kong Dollar\n12. HUF - Hungarian Forint\n13. IDR - Indonesian Rupiah\n14. ILS - Israeli New Sheqel\n15. INR - Indian Rupee\n16. ISK - Icelandic KrÃ³na\n17. JPY - Japanese Yen\n18. KRW - South Korean Won\n19. MXN - Mexican Peso\n20. MYR - Malaysian Ringgit\n21. NOK - Norwegian Krone\n22. NZD - New Zealand Dollar\n23. PHP - Philippine Peso\n24. PLN - Polish ZÅ‚oty\n25. RON - Romanian Leu\n26. SEK - Swedish Krona\n27. SGD - Singapore Dollar\n28. THB - Thai Baht\n29. TRY - Turkish Lira\n30. USD - United States Dollar\n31. ZAR - South African Rand\n\nIf you want to convert USD to any of these currencies, you can select the one you are interested in.
"
Observability
Arch is designed to support best-in class observability by supporting open standards. Please read our
docs
on observability for more details on tracing, metrics, and logs. The screenshot below is from our integration with Signoz (among others)
Debugging
When debugging issues / errors application logs and access logs provide key information to give you more context on whats going on with the system. Arch gateway runs in info log level and following is a typical output you could see in a typical interaction between developer and arch gateway,
$ archgw up --service archgw --foreground
...
[2025-03-26 18:32:01.350][26][info] prompt_gateway: on_http_request_body: sending request to model server
[2025-03-26 18:32:01.851][26][info] prompt_gateway: on_http_call_response: model server response received
[2025-03-26 18:32:01.852][26][info] prompt_gateway: on_http_call_response: dispatching api call to developer endpoint: weather_forecast_service, path: /weather, method: POST
[2025-03-26 18:32:01.882][26][info] prompt_gateway: on_http_call_response: developer api call response received: status code: 200
[2025-03-26 18:32:01.882][26][info] prompt_gateway: on_http_call_response: sending request to upstream llm
[2025-03-26 18:32:01.883][26][info] llm_gateway: on_http_request_body: provider: gpt-4o-mini, model requested: None, model selected: gpt-4o-mini
[2025-03-26 18:32:02.818][26][info] llm_gateway: on_http_response_body: time to first token: 1468ms
[2025-03-26 18:32:04.532][26][info] llm_gateway: on_http_response_body: request latency: 3183ms
...
Log level can be changed to debug to get more details. To enable debug logs edit (supervisord.conf)[arch/supervisord.conf], change the log level
--component-log-level wasm:info
to
--component-log-level wasm:debug
. And after that you need to rebuild docker image and restart the arch gateway using following set of commands,
# make sure you are at the root of the repo
$ archgw build
# go to your service that has arch_config.yaml file and issue following command,
$ archgw up --service archgw --foreground
Contribution
We would love feedback on our
Roadmap
and we welcome contributions to
Arch
!
Whether you're fixing bugs, adding new features, improving documentation, or creating tutorials, your help is much appreciated.
Please visit our
Contribution Guide
for more details
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 36
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 3,928

### References
- https://github.com/katanemo/archgw

## gkd-kit/gkd

### Executive Summary
- åŸºäºæ— éšœç¢ï¼Œé«˜çº§é€‰æ‹©å™¨ï¼Œè®¢é˜…è§„åˆ™çš„è‡ªå®šä¹‰å±å¹•ç‚¹å‡» Android åº”ç”¨ | An Android APP with custom screen tapping based on Accessibility, Advanced Selectors, and Subscription Rules
- ---
- gkd
åŸºäº
é«˜çº§é€‰æ‹©å™¨
+
è®¢é˜…è§„åˆ™
+
å¿«ç…§å®¡æŸ¥
çš„è‡ªå®šä¹‰å±å¹•ç‚¹å‡» Android åº”ç”¨
é€šè¿‡è‡ªå®šä¹‰è§„åˆ™ï¼Œåœ¨æŒ‡å®šç•Œé¢ï¼Œæ»¡è¶³æŒ‡å®šæ¡ä»¶(å¦‚å±å¹•ä¸Šå­˜åœ¨ç‰¹å®šæ–‡å­—)æ—¶ï¼Œç‚¹å‡»ç‰¹å®šçš„èŠ‚ç‚¹æˆ–ä½ç½®æˆ–æ‰§è¡Œå…¶ä»–æ“ä½œ
å¿«æ·æ“ä½œ
å¸®åŠ©ä½ ç®€åŒ–ä¸€äº›é‡å¤çš„æµç¨‹, å¦‚æŸäº›è½¯ä»¶è‡ªåŠ¨ç¡®è®¤ç”µè„‘ç™»å½•
è·³è¿‡æµç¨‹
æŸäº›è½¯ä»¶å¯èƒ½åœ¨å¯åŠ¨æ—¶å­˜åœ¨ä¸€äº›çƒ¦äººçš„æµç¨‹, è¿™ä¸ªè½¯ä»¶å¯ä»¥å¸®åŠ©ä½ ç‚¹å‡»è·³è¿‡è¿™ä¸ªæµç¨‹
å…è´£å£°æ˜
æœ¬é¡¹ç›®éµå¾ª
GPL-3.0
å¼€æºï¼Œé¡¹ç›®ä»…ä¾›å­¦ä¹ äº¤æµï¼Œç¦æ­¢ç”¨äºå•†ä¸šæˆ–éæ³•ç”¨é€”
å®‰è£…
å¦‚é‡é—®é¢˜è¯·å…ˆæŸ¥çœ‹
ç–‘éš¾è§£ç­”
æˆªå›¾
è®¢é˜…
GKD
é»˜è®¤ä¸æä¾›è§„åˆ™
ï¼Œéœ€è‡ªè¡Œæ·»åŠ æœ¬åœ°è§„åˆ™ï¼Œæˆ–è€…é€šè¿‡è®¢é˜…é“¾æ¥çš„æ–¹å¼è·å–è¿œç¨‹è§„åˆ™
ä¹Ÿå¯é€šè¿‡
subscription-template
å¿«é€Ÿæ„å»ºè‡ªå·±çš„è¿œç¨‹è®¢é˜…
ç¬¬ä¸‰æ–¹è®¢é˜…åˆ—è¡¨å¯åœ¨
https://github.com/topics/gkd-subscription
æŸ¥çœ‹
è¦åŠ å…¥æ­¤åˆ—è¡¨, éœ€ç‚¹å‡»ä»“åº“ä¸»é¡µå³ä¸Šè§’è®¾ç½®å›¾æ ‡ååœ¨ Topics ä¸­æ·»åŠ 
gkd-subscription
ç¤ºä¾‹å›¾ç‰‡ - æ·»åŠ è‡³ Topics (ç‚¹å‡»å±•å¼€)
é€‰æ‹©å™¨
ä¸€ä¸ªç±»ä¼¼ CSS é€‰æ‹©å™¨çš„é€‰æ‹©å™¨, èƒ½è”ç³»èŠ‚ç‚¹ä¸Šä¸‹æ–‡ä¿¡æ¯, æ›´å®¹æ˜“ä¹Ÿæ›´ç²¾ç¡®æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹
https://gkd.li/guide/selector
@[vid="menu"] < [vid="menu_container"] - [vid="dot_text_layout"] > [text^="å¹¿å‘Š"]
ç¤ºä¾‹å›¾ç‰‡ - é€‰æ‹©å™¨è·¯å¾„è§†å›¾ (ç‚¹å‡»å±•å¼€)
æèµ 
å¦‚æœ GKD å¯¹ä½ æœ‰ç”¨, å¯ä»¥é€šè¿‡ä»¥ä¸‹é“¾æ¥æ”¯æŒè¯¥é¡¹ç›®
https://github.com/lisonge/sponsor
æˆ–å‰å¾€
Google Play
ç»™ä¸ªå¥½è¯„
Star History
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 35
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 31,388

### References
- https://github.com/gkd-kit/gkd

## keycloak/keycloak

### Executive Summary
- Open Source Identity and Access Management For Modern Applications and Services
- ---
- Open Source Identity and Access Management
Add authentication to applications and secure services with minimum effort. No need to deal with storing users or authenticating users.
Keycloak provides user federation, strong authentication, user management, fine-grained authorization, and more.
Help and Documentation
Documentation
User Mailing List
- Mailing list for help and general questions about Keycloak
Join
#keycloak
for general questions, or
#keycloak-dev
on Slack for design and development discussions, by creating an account at
https://slack.cncf.io/
.
Reporting Security Vulnerabilities
If you have found a security vulnerability, please look at the
instructions on how to properly report it
.
Reporting an issue
If you believe you have discovered a defect in Keycloak, please open
an issue
.
Please remember to provide a good summary, description as well as steps to reproduce the issue.
Getting started
To run Keycloak, download the distribution from our
website
. Unzip and run:
bin/kc.[sh|bat] start-dev
Alternatively, you can use the Docker image by running:
docker run quay.io/keycloak/keycloak start-dev
For more details refer to the
Keycloak Documentation
.
Building from Source
To build from source, refer to the
building and working with the code base
guide.
Testing
To run tests, refer to the
running tests
guide.
Writing Tests
To write tests, refer to the
writing tests
guide.
Contributing
Before contributing to Keycloak, please read our
contributing guidelines
. Participation in the Keycloak project is governed by the
CNCF Code of Conduct
.
Joining a
community meeting
is a great way to get involved and help shape the future of Keycloak.
Other Keycloak Projects
Keycloak
- Keycloak Server and Java adapters
Keycloak QuickStarts
- QuickStarts for getting started with Keycloak
Keycloak Node.js Connect
- Node.js adapter for Keycloak
License
Apache License, Version 2.0
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 35
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 30,074

### References
- https://github.com/keycloak/keycloak

## dataease/SQLBot

### Executive Summary
- ğŸ”¥ åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿã€‚Text-to-SQL Generation via LLMs using RAG.
- ---
- åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿ
SQLBot æ˜¯ä¸€æ¬¾åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿã€‚SQLBot çš„ä¼˜åŠ¿åŒ…æ‹¬ï¼š
å¼€ç®±å³ç”¨
: åªéœ€é…ç½®å¤§æ¨¡å‹å’Œæ•°æ®æºå³å¯å¼€å¯é—®æ•°ä¹‹æ—…ï¼Œé€šè¿‡å¤§æ¨¡å‹å’Œ RAG çš„ç»“åˆæ¥å®ç°é«˜è´¨é‡çš„ text2sqlï¼›
æ˜“äºé›†æˆ
: æ”¯æŒå¿«é€ŸåµŒå…¥åˆ°ç¬¬ä¸‰æ–¹ä¸šåŠ¡ç³»ç»Ÿï¼Œä¹Ÿæ”¯æŒè¢« n8nã€MaxKBã€Difyã€Coze ç­‰ AI åº”ç”¨å¼€å‘å¹³å°é›†æˆè°ƒç”¨ï¼Œè®©å„ç±»åº”ç”¨å¿«é€Ÿæ‹¥æœ‰æ™ºèƒ½é—®æ•°èƒ½åŠ›ï¼›
å®‰å…¨å¯æ§
: æä¾›åŸºäºå·¥ä½œç©ºé—´çš„èµ„æºéš”ç¦»æœºåˆ¶ï¼Œèƒ½å¤Ÿå®ç°ç»†ç²’åº¦çš„æ•°æ®æƒé™æ§åˆ¶ã€‚
å·¥ä½œåŸç†
å¿«é€Ÿå¼€å§‹
å®‰è£…éƒ¨ç½²
å‡†å¤‡ä¸€å° Linux æœåŠ¡å™¨ï¼Œå®‰è£…å¥½
Docker
ï¼Œæ‰§è¡Œä»¥ä¸‹ä¸€é”®å®‰è£…è„šæœ¬ï¼š
docker run -d \
  --name sqlbot \
  --restart unless-stopped \
  -p 8000:8000 \
  -p 8001:8001 \
  -v ./data/sqlbot/excel:/opt/sqlbot/data/excel \
  -v ./data/sqlbot/file:/opt/sqlbot/data/file \
  -v ./data/sqlbot/images:/opt/sqlbot/images \
  -v ./data/sqlbot/logs:/opt/sqlbot/app/logs \
  -v ./data/postgresql:/var/lib/postgresql/data \
  --privileged=true \
  dataease/sqlbot
ä½ ä¹Ÿå¯ä»¥é€šè¿‡
1Panel åº”ç”¨å•†åº—
å¿«é€Ÿéƒ¨ç½² SQLBotã€‚
å¦‚æœæ˜¯å†…ç½‘ç¯å¢ƒï¼Œä½ å¯ä»¥é€šè¿‡
ç¦»çº¿å®‰è£…åŒ…æ–¹å¼
éƒ¨ç½² SQLBotã€‚
è®¿é—®æ–¹å¼
åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€: http://<ä½ çš„æœåŠ¡å™¨IP>:8000/
ç”¨æˆ·å: admin
å¯†ç : SQLBot@123456
è”ç³»æˆ‘ä»¬
å¦‚ä½ æœ‰æ›´å¤šé—®é¢˜ï¼Œå¯ä»¥åŠ å…¥æˆ‘ä»¬çš„æŠ€æœ¯äº¤æµç¾¤ä¸æˆ‘ä»¬äº¤æµã€‚
UI å±•ç¤º
Star History
é£è‡´äº‘æ——ä¸‹çš„å…¶ä»–æ˜æ˜Ÿé¡¹ç›®
DataEase
- äººäººå¯ç”¨çš„å¼€æº BI å·¥å…·
1Panel
- ç°ä»£åŒ–ã€å¼€æºçš„ Linux æœåŠ¡å™¨è¿ç»´ç®¡ç†é¢æ¿
MaxKB
- å¼ºå¤§æ˜“ç”¨çš„ä¼ä¸šçº§æ™ºèƒ½ä½“å¹³å°
JumpServer
- å¹¿å—æ¬¢è¿çš„å¼€æºå ¡å’æœº
Cordys CRM
- æ–°ä¸€ä»£çš„å¼€æº AI CRM ç³»ç»Ÿ
Halo
- å¼ºå¤§æ˜“ç”¨çš„å¼€æºå»ºç«™å·¥å…·
MeterSphere
- æ–°ä¸€ä»£çš„å¼€æºæŒç»­æµ‹è¯•å·¥å…·
License
æœ¬ä»“åº“éµå¾ª
FIT2CLOUD Open Source License
å¼€æºåè®®ï¼Œè¯¥è®¸å¯è¯æœ¬è´¨ä¸Šæ˜¯ GPLv3ï¼Œä½†æœ‰ä¸€äº›é¢å¤–çš„é™åˆ¶ã€‚
ä½ å¯ä»¥åŸºäº SQLBot çš„æºä»£ç è¿›è¡ŒäºŒæ¬¡å¼€å‘ï¼Œä½†æ˜¯éœ€è¦éµå®ˆä»¥ä¸‹è§„å®šï¼š
ä¸èƒ½æ›¿æ¢å’Œä¿®æ”¹ SQLBot çš„ Logo å’Œç‰ˆæƒä¿¡æ¯ï¼›
äºŒæ¬¡å¼€å‘åçš„è¡ç”Ÿä½œå“å¿…é¡»éµå®ˆ GPL V3 çš„å¼€æºä¹‰åŠ¡ã€‚
å¦‚éœ€å•†ä¸šæˆæƒï¼Œè¯·è”ç³»
support@fit2cloud.com
ã€‚
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 33
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 3,711

### References
- https://github.com/dataease/SQLBot

## intruder-io/autoswagger

### Executive Summary
- Autoswagger by Intruder - detect API auth weaknesses
- ---
- Autoswagger
by
Intruder
Autoswagger
is a command-line tool designed to discover, parse, and test for unauthenticated endpoints using
Swagger/OpenAPI
documentation. It helps identify potential security issues in unprotected endpoints of APIs, such as PII leaks and common secret exposures.
Please note that this initial release of Autoswagger is by no means complete, and there are some types of specification which the tool does not currently handle. Please feel free to use it as you wish, and extend its detection capabilities or add detection regexes to cover your specific use-case!
Table of Contents
Introduction
Key Features
Installation & Usage
Discovery Phases
Endpoint Testing
PII Detection
Output Examples
Stats & Reporting
Acknowledgments
Introduction
Autoswagger automates the process of finding
OpenAPI/Swagger
specifications, extracting API endpoints, and systematically testing them for
PII
exposure,
secrets
, and large or interesting responses. It leverages
Presidio
for PII recognition and
regex
for sensitive key/token detection.
Key Features
Multiple Discovery Phases
Discovers OpenAPI specs in three ways:
Direct Spec
: If a full URL with a path ending in
.json
,
.yaml
, or
.yml
is provided, parse that file directly.
Swagger UI
: Parse known paths of Swagger UI (e.g.
/swagger-ui.html
), and extract spec from HTML or JavaScript.
Direct Spec by Bruteforce
: Attempt discovery using common OpenAPI schema locations (
/swagger.json
,
/openapi.json
, etc.). Only attempt this if 1. and 2. did not yield a result.
Parallel Endpoint Testing
Multi-threaded concurrent testing of many endpoints, respecting a configurable rate limit (
-rate
).
Brute-Force of Parameter Values
If
-b
or
--brute
is used, try using various data types with a few example values in an attempt to bypass parameter-specific validations.
Presidio PII Detection
Check output for phone numbers, emails, addresses, and names (with context validation to reduce false positives). Also parse CSV rows and naive â€œkey: valueâ€ lines.
Secrets Detection
Leverages a set of regex patterns to detect tokens, keys, and debugging artifacts (like environment variables).
Command Line or JSON Output
In default mode, displays results in a table. With
-json
, output a JSON structure.
-product
mode filters output to only show those that contain PII, secrets, or large responses.
Installation & Usage
Clone
or
download
the repository containing Autoswagger.
git clone git@github.com:intruder-io/autoswagger.git
Install dependencies
(e.g., using Python 3.7+):
pip install -r requirements.txt
(It's recommended to use a virtual environment for this:
python3 -m venv venv;source venv/bin/activate
)
Check installation, show help:
python3 autoswagger.py -h
Flags
Flag
Description
urls
List of base URLs or direct spec URLs.
-v, --verbose
Enables verbose logging. Creates a log file under
~/.autoswagger/logs
.
-risk
Includes non-GET methods (POST, PUT, PATCH, DELETE) in testing.
-all
Includes 200 and 404 endpoints in output (excludes 401/403).
-product
Outputs only endpoints with PII or large responses, in JSON format.
-stats
Displays scan statistics (e.g. requests, RPS, hosts with PII).
-rate <N>
Throttles requests to N requests per second. Default is 30. Use 0 to disable rate limiting.
-b, --brute
Enables brute-forcing of parameter values (multiple test combos).
-json
Outputs results in JSON format instead of a Rich table in default mode.
Help
/   | __  __/ /_____  ______      ______ _____ _____ ____  _____
     / /| |/ / / / __/ __ \/ ___/ | /| / / __ `/ __ `/ __ `/ _ \/ ___/
    / ___ / /_/ / /_/ /_/ (__  )| |/ |/ / /_/ / /_/ / /_/ /  __/ /
    /_/  |_\__,_/\__/\____/____/ |__/|__/_\__,_/\__, /\__, /\___/_/
                                              /____//____/
                              https://intruder.io
                          Find unauthenticated endpoints

usage: autoswagger.py [-h] [-v] [-risk] [-all] [-product] [-stats] [-rate RATE] [-b] [-json] [urls ...]

Autoswagger: Detect unauthenticated access control issues via Swagger/OpenAPI documentation.

positional arguments:
  urls           Base URL(s) or spec URL(s) of the target API(s)

options:
  -h, --help     show this help message and exit
  -v, --verbose  Enable verbose output
  -risk          Include non-GET requests in testing
  -all           Include all HTTP status codes in the results, excluding 401 and 403
  -product       Output all endpoints in JSON, flagging those that contain PII or have large responses.
  -stats         Display scan statistics. Included in JSON if -product or -json is used.
  -rate RATE     Set the rate limit in requests per second (default: 30). Use 0 to disable rate limiting.
  -b, --brute    Enable exhaustive testing of parameter values.
  -json          Output results in JSON format in default mode.

Example usage:
  python autoswagger.py https://api.example.com -v
Discovery Phases
Direct Spec
If a provided URL ends with
.json/.yaml/.yml
, Autoswagger
directly
attempts to parse the OpenAPI schema.
Swagger-UI Detection
Tries known UI paths (e.g.,
/swagger-ui.html
).
If found, parses the HTML or local JavaScript files for a
swagger.json
or
openapi.json
.
Can detect embedded configs like
window.swashbuckleConfig
.
Direct Spec by Bruteforce
If no spec is found so far, Autoswagger attempts a list of default endpoints like
/swagger.json
,
/openapi.json
, etc.
Stops when a valid spec is discovered or none are found.
Endpoint Testing
Collect Endpoints
After loading a spec, Autoswagger extracts each path and method under the
paths
key.
HTTP Methods
By default, tests
GET
only.
Use
-risk
to include other methods (
POST
,
PUT
,
PATCH
,
DELETE
).
Parameter Values
Fill path/query parameters with defaults or values to enumerate.
Optionally builds request bodies from the specâ€™s
requestBody
(OpenAPI 3) or body parameters (Swagger 2).
Rate Limiting & Concurrency
Supports threading with a cap on requests per second (
-rate
).
Each endpoint is tested in a dedicated job.
Response Analysis
Decodes responses, checks for PII, secrets, and large content.
Logs relevant findings.
PII Detection
Presidio-Based Analysis
Searches for phone numbers, emails, addresses, names.
Context-based scanning (e.g., CSV headers, key-value lines).
Secrets & Debug Info
TruffleHog-like regex checks for API keys, tokens, environment variables.
Merges any matches into the PII data structure for final reporting.
Large Response Check
Flags responses with 100+ JSON elements or large XML structures as â€œinteresting.â€
Also checks raw size threshold (e.g., >100k bytes).
Output
By default, output is shown in a table.
-json
produces JSON objects, grouping results by endpoint.
-product
filters down to only â€œinterestingâ€ endpoints (PII, large responses and responses with secrets).
Interpreting Results
For most use cases, interpreting results involves looking at the output (endpoints resulting in Status Code 200s), and paying particular attention to endpoints which are marked as 'PII or Secret Detected'. These endpoints are the ones that contain impactful exposures, but they should be manually checked to confirm. You may also wish to look at other 200s that do not contain PII, and determine whether it's intended for these endpoints to be public or not.
Simple GET endpoints can be triaged using command line tools like curl, but we would recommend using your usual API testing suite (tools such as Postman or Burp Suite) to replay requests and read responses to confirm whether an exposure is present.
Stats & Reporting
-stats
appends or prints overall statistics, such as:
Hosts with valid specs
Hosts with PII
Total requests sent, average RPS
Percentage of endpoints responding with 2xx or 4xx
Shown in either a Rich table in default mode or embedded in JSON if
-json
or
-product
is used.
Acknowledgments
Autoswagger is maintained and owned by
Intruder
. It was primarily developed by Cale Anderson
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 32
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 1,473

### References
- https://github.com/intruder-io/autoswagger

## microsoft/edit

### Executive Summary
- We all edit.
- ---
- Edit
A simple editor for simple needs.
This editor pays homage to the classic
MS-DOS Editor
, but with a modern interface and input controls similar to VS Code. The goal is to provide an accessible editor that even users largely unfamiliar with terminals can easily use.
Installation
You can also download binaries from
our Releases page
.
Windows
You can install the latest version with WinGet:
winget install Microsoft.Edit
Build Instructions
Install Rust
Install the nightly toolchain:
rustup install nightly
Alternatively, set the environment variable
RUSTC_BOOTSTRAP=1
Clone the repository
For a release build, run:
cargo build --config .cargo/release.toml --release
Build Configuration
During compilation you can set various environment variables to configure the build. The following table lists the available configuration options:
Environment variable
Description
EDIT_CFG_ICU*
See
ICU library name (SONAME)
for details.
EDIT_CFG_LANGUAGES
A comma-separated list of languages to include in the build. See
i18n/edit.toml
for available languages.
Notes to Package Maintainers
Package Naming
The canonical executable name is "edit" and the alternative name is "msedit".
We're aware of the potential conflict of "edit" with existing commands and recommend alternatively naming packages and executables "msedit".
Names such as "ms-edit" should be avoided.
Assigning an "edit" alias is recommended, if possible.
ICU library name (SONAME)
This project
optionally
depends on the ICU library for its Search and Replace functionality.
By default, the project will look for a SONAME without version suffix:
Windows:
icuuc.dll
macOS:
libicuuc.dylib
UNIX, and other OS:
libicuuc.so
If your installation uses a different SONAME, please set the following environment variable at build time:
EDIT_CFG_ICUUC_SONAME
:
For instance,
libicuuc.so.76
.
EDIT_CFG_ICUI18N_SONAME
:
For instance,
libicui18n.so.76
.
Additionally, this project assumes that the ICU exports are exported without
_
prefix and without version suffix, such as
u_errorName
.
If your installation uses versioned exports, please set:
EDIT_CFG_ICU_CPP_EXPORTS
:
If set to
true
, it'll look for C++ symbols such as
_u_errorName
.
Enabled by default on macOS.
EDIT_CFG_ICU_RENAMING_VERSION
:
If set to a version number, such as
76
, it'll look for symbols such as
u_errorName_76
.
Finally, you can set the following environment variables:
EDIT_CFG_ICU_RENAMING_AUTO_DETECT
:
If set to
true
, the executable will try to detect the
EDIT_CFG_ICU_RENAMING_VERSION
value at runtime.
The way it does this is not officially supported by ICU and as such is not recommended to be relied upon.
Enabled by default on UNIX (excluding macOS) if no other options are set.
To test your settings, run
cargo test
again but with the
--ignored
flag. For instance:
cargo
test
-- --ignored
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 31
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 12,384

### References
- https://github.com/microsoft/edit

## YunaiV/ruoyi-vue-pro

### Executive Summary
- ğŸ”¥ å®˜æ–¹æ¨è ğŸ”¥ RuoYi-Vue å…¨æ–° Pro ç‰ˆæœ¬ï¼Œä¼˜åŒ–é‡æ„æ‰€æœ‰åŠŸèƒ½ã€‚åŸºäº Spring Boot + MyBatis Plus + Vue & Element å®ç°çš„åå°ç®¡ç†ç³»ç»Ÿ + å¾®ä¿¡å°ç¨‹åºï¼Œæ”¯æŒ RBAC åŠ¨æ€æƒé™ã€æ•°æ®æƒé™ã€SaaS å¤šç§Ÿæˆ·ã€Flowable å·¥ä½œæµã€ä¸‰æ–¹ç™»å½•ã€æ”¯ä»˜ã€çŸ­ä¿¡ã€å•†åŸã€CRMã€ERPã€AI å¤§æ¨¡å‹ç­‰åŠŸèƒ½ã€‚ä½ çš„ â­ï¸ Star â­ï¸ï¼Œæ˜¯ä½œè€…ç”Ÿå‘çš„åŠ¨åŠ›ï¼
- ---
- ä¸¥è‚ƒå£°æ˜ï¼šç°åœ¨ã€æœªæ¥éƒ½ä¸ä¼šæœ‰å•†ä¸šç‰ˆæœ¬ï¼Œæ‰€æœ‰ä»£ç å…¨éƒ¨å¼€æº!ï¼
ã€Œæˆ‘å–œæ¬¢å†™ä»£ç ï¼Œä¹æ­¤ä¸ç–²ã€
ã€Œæˆ‘å–œæ¬¢åšå¼€æºï¼Œä»¥æ­¤ä¸ºä¹ã€
æˆ‘ ğŸ¶ åœ¨ä¸Šæµ·è‰°è‹¦å¥‹æ–—ï¼Œæ—©ä¸­æ™šåœ¨ top3 å¤§å‚è®¤çœŸæ¬ç –ï¼Œå¤œé‡Œä¸ºå¼€æºåšè´¡çŒ®ã€‚
å¦‚æœè¿™ä¸ªé¡¹ç›®è®©ä½ æœ‰æ‰€æ”¶è·ï¼Œè®°å¾— Star å…³æ³¨å“¦ï¼Œè¿™å¯¹æˆ‘æ˜¯éå¸¸ä¸é”™çš„é¼“åŠ±ä¸æ”¯æŒã€‚
ğŸ¶ æ–°æ‰‹å¿…è¯»
æ¼”ç¤ºåœ°å€ã€Vue3 + element-plusã€‘ï¼š
http://dashboard-vue3.yudao.iocoder.cn
æ¼”ç¤ºåœ°å€ã€Vue3 + vben(ant-design-vue)ã€‘ï¼š
http://dashboard-vben.yudao.iocoder.cn
æ¼”ç¤ºåœ°å€ã€Vue2 + element-uiã€‘ï¼š
http://dashboard.yudao.iocoder.cn
å¯åŠ¨æ–‡æ¡£ï¼š
https://doc.iocoder.cn/quick-start/
è§†é¢‘æ•™ç¨‹ï¼š
https://doc.iocoder.cn/video/
ğŸ° ç‰ˆæœ¬è¯´æ˜
ç‰ˆæœ¬
JDK 8 + Spring Boot 2.7
JDK 17/21 + Spring Boot 3.2
ã€å®Œæ•´ç‰ˆã€‘
ruoyi-vue-pro
master
åˆ†æ”¯
master-jdk17
åˆ†æ”¯
ã€ç²¾ç®€ç‰ˆã€‘
yudao-boot-mini
master
åˆ†æ”¯
master-jdk17
åˆ†æ”¯
ã€å®Œæ•´ç‰ˆã€‘ï¼šåŒ…æ‹¬ç³»ç»ŸåŠŸèƒ½ã€åŸºç¡€è®¾æ–½ã€ä¼šå‘˜ä¸­å¿ƒã€æ•°æ®æŠ¥è¡¨ã€å·¥ä½œæµç¨‹ã€å•†åŸç³»ç»Ÿã€å¾®ä¿¡å…¬ä¼—å·ã€CRMã€ERP ç­‰åŠŸèƒ½
ã€ç²¾ç®€ç‰ˆã€‘ï¼šåªåŒ…æ‹¬ç³»ç»ŸåŠŸèƒ½ã€åŸºç¡€è®¾æ–½åŠŸèƒ½ï¼Œä¸åŒ…æ‹¬ä¼šå‘˜ä¸­å¿ƒã€æ•°æ®æŠ¥è¡¨ã€å·¥ä½œæµç¨‹ã€å•†åŸç³»ç»Ÿã€å¾®ä¿¡å…¬ä¼—å·ã€CRMã€ERP ç­‰åŠŸèƒ½
å¯å‚è€ƒ
ã€Šè¿ç§»æ–‡æ¡£ã€‹
ï¼Œåªéœ€è¦ 5-10 åˆ†é’Ÿï¼Œå³å¯å°†ã€å®Œæ•´ç‰ˆã€‘æŒ‰éœ€è¿ç§»åˆ°ã€ç²¾ç®€ç‰ˆã€‘
ğŸ¯ å¹³å°ç®€ä»‹
èŠ‹é“
ï¼Œä»¥å¼€å‘è€…ä¸ºä¸­å¿ƒï¼Œæ‰“é€ ä¸­å›½ç¬¬ä¸€æµçš„å¿«é€Ÿå¼€å‘å¹³å°ï¼Œå…¨éƒ¨å¼€æºï¼Œä¸ªäººä¸ä¼ä¸šå¯ 100% å…è´¹ä½¿ç”¨ã€‚
æœ‰ä»»ä½•é—®é¢˜ï¼Œæˆ–è€…æƒ³è¦çš„åŠŸèƒ½ï¼Œå¯ä»¥åœ¨
Issues
ä¸­æç»™è‰¿è‰¿ã€‚
ğŸ˜œ ç»™é¡¹ç›®ç‚¹ç‚¹ Star å§ï¼Œè¿™å¯¹æˆ‘ä»¬çœŸçš„å¾ˆé‡è¦ï¼
Java åç«¯ï¼š
master
åˆ†æ”¯ä¸º JDK 8 + Spring Boot 2.7ï¼Œ
master-jdk17
åˆ†æ”¯ä¸º JDK 17/21 + Spring Boot 3.2
ç®¡ç†åå°çš„ç”µè„‘ç«¯ï¼šVue3 æä¾›
element-plus
ã€
vben(ant-design-vue)
ä¸¤ä¸ªç‰ˆæœ¬ï¼ŒVue2 æä¾›
element-ui
ç‰ˆæœ¬
ç®¡ç†åå°çš„ç§»åŠ¨ç«¯ï¼šé‡‡ç”¨
uni-app
æ–¹æ¡ˆï¼Œä¸€ä»½ä»£ç å¤šç»ˆç«¯é€‚é…ï¼ŒåŒæ—¶æ”¯æŒ APPã€å°ç¨‹åºã€H5ï¼
åç«¯é‡‡ç”¨ Spring Boot å¤šæ¨¡å—æ¶æ„ã€MySQL + MyBatis Plusã€Redis + Redisson
æ•°æ®åº“å¯ä½¿ç”¨ MySQLã€Oracleã€PostgreSQLã€SQL Serverã€MariaDBã€å›½äº§è¾¾æ¢¦ DMã€TiDB ç­‰
æ¶ˆæ¯é˜Ÿåˆ—å¯ä½¿ç”¨ Eventã€Redisã€RabbitMQã€Kafkaã€RocketMQ ç­‰
æƒé™è®¤è¯ä½¿ç”¨ Spring Security & Token & Redisï¼Œæ”¯æŒå¤šç»ˆç«¯ã€å¤šç§ç”¨æˆ·çš„è®¤è¯ç³»ç»Ÿï¼Œæ”¯æŒ SSO å•ç‚¹ç™»å½•
æ”¯æŒåŠ è½½åŠ¨æ€æƒé™èœå•ï¼ŒæŒ‰é’®çº§åˆ«æƒé™æ§åˆ¶ï¼ŒRedis ç¼“å­˜æå‡æ€§èƒ½
æ”¯æŒ SaaS å¤šç§Ÿæˆ·ï¼Œå¯è‡ªå®šä¹‰æ¯ä¸ªç§Ÿæˆ·çš„æƒé™ï¼Œæä¾›é€æ˜åŒ–çš„å¤šç§Ÿæˆ·åº•å±‚å°è£…
å·¥ä½œæµä½¿ç”¨ Flowableï¼Œæ”¯æŒåŠ¨æ€è¡¨å•ã€åœ¨çº¿è®¾è®¡æµç¨‹ã€ä¼šç­¾ / æˆ–ç­¾ã€å¤šç§ä»»åŠ¡åˆ†é…æ–¹å¼
é«˜æ•ˆç‡å¼€å‘ï¼Œä½¿ç”¨ä»£ç ç”Ÿæˆå™¨å¯ä»¥ä¸€é”®ç”Ÿæˆ Javaã€Vue å‰åç«¯ä»£ç ã€SQL è„šæœ¬ã€æ¥å£æ–‡æ¡£ï¼Œæ”¯æŒå•è¡¨ã€æ ‘è¡¨ã€ä¸»å­è¡¨
å®æ—¶é€šä¿¡ï¼Œé‡‡ç”¨ Spring WebSocket å®ç°ï¼Œå†…ç½® Token èº«ä»½æ ¡éªŒï¼Œæ”¯æŒ WebSocket é›†ç¾¤
é›†æˆå¾®ä¿¡å°ç¨‹åºã€å¾®ä¿¡å…¬ä¼—å·ã€ä¼ä¸šå¾®ä¿¡ã€é’‰é’‰ç­‰ä¸‰æ–¹ç™»é™†ï¼Œé›†æˆæ”¯ä»˜å®ã€å¾®ä¿¡ç­‰æ”¯ä»˜ä¸é€€æ¬¾
é›†æˆé˜¿é‡Œäº‘ã€è…¾è®¯äº‘ç­‰çŸ­ä¿¡æ¸ é“ï¼Œé›†æˆ MinIOã€é˜¿é‡Œäº‘ã€è…¾è®¯äº‘ã€ä¸ƒç‰›äº‘ç­‰äº‘å­˜å‚¨æœåŠ¡
é›†æˆæŠ¥è¡¨è®¾è®¡å™¨ã€å¤§å±è®¾è®¡å™¨ï¼Œé€šè¿‡æ‹–æ‹½å³å¯ç”Ÿæˆé…·ç‚«çš„æŠ¥è¡¨ä¸å¤§å±
ğŸ³ é¡¹ç›®å…³ç³»
ä¸‰ä¸ªé¡¹ç›®çš„åŠŸèƒ½å¯¹æ¯”ï¼Œå¯è§ç¤¾åŒºå…±åŒæ•´ç†çš„
å›½äº§å¼€æºé¡¹ç›®å¯¹æ¯”
è¡¨æ ¼ã€‚
åç«¯é¡¹ç›®
é¡¹ç›®
Star
ç®€ä»‹
ruoyi-vue-pro
åŸºäº Spring Boot å¤šæ¨¡å—æ¶æ„
yudao-cloud
åŸºäº Spring Cloud å¾®æœåŠ¡æ¶æ„
Spring-Boot-Labs
ç³»ç»Ÿå­¦ä¹  Spring Boot & Cloud ä¸“æ 
å‰ç«¯é¡¹ç›®
é¡¹ç›®
Star
ç®€ä»‹
yudao-ui-admin-vue3
åŸºäº Vue3 + element-plus å®ç°çš„ç®¡ç†åå°
yudao-ui-admin-vben
åŸºäº Vue3 + vben(ant-design-vue) å®ç°çš„ç®¡ç†åå°
yudao-mall-uniapp
åŸºäº uni-app å®ç°çš„å•†åŸå°ç¨‹åº
yudao-ui-admin-vue2
åŸºäº Vue2 + element-ui å®ç°çš„ç®¡ç†åå°
yudao-ui-admin-uniapp
åŸºäº Vue2 + element-ui å®ç°çš„ç®¡ç†åå°
yudao-ui-go-view
åŸºäº Vue3 + naive-ui å®ç°çš„å¤§å±æŠ¥è¡¨
ğŸ˜ å¼€æºåè®®
ä¸ºä»€ä¹ˆæ¨èä½¿ç”¨æœ¬é¡¹ç›®ï¼Ÿ
â‘  æœ¬é¡¹ç›®é‡‡ç”¨æ¯” Apache 2.0 æ›´å®½æ¾çš„
MIT License
å¼€æºåè®®ï¼Œä¸ªäººä¸ä¼ä¸šå¯ 100% å…è´¹ä½¿ç”¨ï¼Œä¸ç”¨ä¿ç•™ç±»ä½œè€…ã€Copyright ä¿¡æ¯ã€‚
â‘¡ ä»£ç å…¨éƒ¨å¼€æºï¼Œä¸ä¼šåƒå…¶ä»–é¡¹ç›®ä¸€æ ·ï¼Œåªå¼€æºéƒ¨åˆ†ä»£ç ï¼Œè®©ä½ æ— æ³•äº†è§£æ•´ä¸ªé¡¹ç›®çš„æ¶æ„è®¾è®¡ã€‚
å›½äº§å¼€æºé¡¹ç›®å¯¹æ¯”
â‘¢ ä»£ç æ•´æ´ã€æ¶æ„æ•´æ´ï¼Œéµå¾ªã€Šé˜¿é‡Œå·´å·´ Java å¼€å‘æ‰‹å†Œã€‹è§„èŒƒï¼Œä»£ç æ³¨é‡Šè¯¦ç»†ï¼Œ113770 è¡Œ Java ä»£ç ï¼Œ42462 è¡Œä»£ç æ³¨é‡Šã€‚
ğŸ¤ é¡¹ç›®å¤–åŒ…
æˆ‘ä»¬ä¹Ÿæ˜¯æ¥å¤–åŒ…æ»´ï¼Œå¦‚æœä½ æœ‰é¡¹ç›®æƒ³è¦å¤–åŒ…ï¼Œå¯ä»¥å¾®ä¿¡è”ç³»ã€
Aix9975
ã€‘ã€‚
å›¢é˜ŸåŒ…å«ä¸“ä¸šçš„é¡¹ç›®ç»ç†ã€æ¶æ„å¸ˆã€å‰ç«¯å·¥ç¨‹å¸ˆã€åç«¯å·¥ç¨‹å¸ˆã€æµ‹è¯•å·¥ç¨‹å¸ˆã€è¿ç»´å·¥ç¨‹å¸ˆï¼Œå¯ä»¥æä¾›å…¨æµç¨‹çš„å¤–åŒ…æœåŠ¡ã€‚
é¡¹ç›®å¯ä»¥æ˜¯å•†åŸã€SCRM ç³»ç»Ÿã€OA ç³»ç»Ÿã€ç‰©æµç³»ç»Ÿã€ERP ç³»ç»Ÿã€CMS ç³»ç»Ÿã€HIS ç³»ç»Ÿã€æ”¯ä»˜ç³»ç»Ÿã€IM èŠå¤©ã€å¾®ä¿¡å…¬ä¼—å·ã€å¾®ä¿¡å°ç¨‹åºç­‰ç­‰ã€‚
ğŸ¼ å†…ç½®åŠŸèƒ½
ç³»ç»Ÿå†…ç½®å¤šç§å¤šç§ä¸šåŠ¡åŠŸèƒ½ï¼Œå¯ä»¥ç”¨äºå¿«é€Ÿä½ çš„ä¸šåŠ¡ç³»ç»Ÿï¼š
é€šç”¨æ¨¡å—ï¼ˆå¿…é€‰ï¼‰ï¼šç³»ç»ŸåŠŸèƒ½ã€åŸºç¡€è®¾æ–½
é€šç”¨æ¨¡å—ï¼ˆå¯é€‰ï¼‰ï¼šå·¥ä½œæµç¨‹ã€æ”¯ä»˜ç³»ç»Ÿã€æ•°æ®æŠ¥è¡¨ã€ä¼šå‘˜ä¸­å¿ƒ
ä¸šåŠ¡ç³»ç»Ÿï¼ˆæŒ‰éœ€ï¼‰ï¼šERP ç³»ç»Ÿã€CRM ç³»ç»Ÿã€å•†åŸç³»ç»Ÿã€å¾®ä¿¡å…¬ä¼—å·ã€AI å¤§æ¨¡å‹
å‹æƒ…æç¤ºï¼šæœ¬é¡¹ç›®åŸºäº RuoYi-Vue ä¿®æ”¹ï¼Œ
é‡æ„ä¼˜åŒ–
åç«¯çš„ä»£ç ï¼Œ
ç¾åŒ–
å‰ç«¯çš„ç•Œé¢ã€‚
é¢å¤–æ–°å¢çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬ä½¿ç”¨ ğŸš€ æ ‡è®°ã€‚
é‡æ–°å®ç°çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬ä½¿ç”¨ â­ï¸ æ ‡è®°ã€‚
ğŸ™‚ æ‰€æœ‰åŠŸèƒ½ï¼Œéƒ½é€šè¿‡
å•å…ƒæµ‹è¯•
ä¿è¯é«˜è´¨é‡ã€‚
ç³»ç»ŸåŠŸèƒ½
åŠŸèƒ½
æè¿°
ç”¨æˆ·ç®¡ç†
ç”¨æˆ·æ˜¯ç³»ç»Ÿæ“ä½œè€…ï¼Œè¯¥åŠŸèƒ½ä¸»è¦å®Œæˆç³»ç»Ÿç”¨æˆ·é…ç½®
â­ï¸
åœ¨çº¿ç”¨æˆ·
å½“å‰ç³»ç»Ÿä¸­æ´»è·ƒç”¨æˆ·çŠ¶æ€ç›‘æ§ï¼Œæ”¯æŒæ‰‹åŠ¨è¸¢ä¸‹çº¿
è§’è‰²ç®¡ç†
è§’è‰²èœå•æƒé™åˆ†é…ã€è®¾ç½®è§’è‰²æŒ‰æœºæ„è¿›è¡Œæ•°æ®èŒƒå›´æƒé™åˆ’åˆ†
èœå•ç®¡ç†
é…ç½®ç³»ç»Ÿèœå•ã€æ“ä½œæƒé™ã€æŒ‰é’®æƒé™æ ‡è¯†ç­‰ï¼Œæœ¬åœ°ç¼“å­˜æä¾›æ€§èƒ½
éƒ¨é—¨ç®¡ç†
é…ç½®ç³»ç»Ÿç»„ç»‡æœºæ„ï¼ˆå…¬å¸ã€éƒ¨é—¨ã€å°ç»„ï¼‰ï¼Œæ ‘ç»“æ„å±•ç°æ”¯æŒæ•°æ®æƒé™
å²—ä½ç®¡ç†
é…ç½®ç³»ç»Ÿç”¨æˆ·æ‰€å±æ‹…ä»»èŒåŠ¡
ğŸš€
ç§Ÿæˆ·ç®¡ç†
é…ç½®ç³»ç»Ÿç§Ÿæˆ·ï¼Œæ”¯æŒ SaaS åœºæ™¯ä¸‹çš„å¤šç§Ÿæˆ·åŠŸèƒ½
ğŸš€
ç§Ÿæˆ·å¥—é¤
é…ç½®ç§Ÿæˆ·å¥—é¤ï¼Œè‡ªå®šæ¯ä¸ªç§Ÿæˆ·çš„èœå•ã€æ“ä½œã€æŒ‰é’®çš„æƒé™
å­—å…¸ç®¡ç†
å¯¹ç³»ç»Ÿä¸­ç»å¸¸ä½¿ç”¨çš„ä¸€äº›è¾ƒä¸ºå›ºå®šçš„æ•°æ®è¿›è¡Œç»´æŠ¤
ğŸš€
çŸ­ä¿¡ç®¡ç†
çŸ­ä¿¡æ¸ é“ã€çŸ­æ¯æ¨¡æ¿ã€çŸ­ä¿¡æ—¥å¿—ï¼Œå¯¹æ¥é˜¿é‡Œäº‘ã€è…¾è®¯äº‘ç­‰ä¸»æµçŸ­ä¿¡å¹³å°
ğŸš€
é‚®ä»¶ç®¡ç†
é‚®ç®±è´¦å·ã€é‚®ä»¶æ¨¡ç‰ˆã€é‚®ä»¶å‘é€æ—¥å¿—ï¼Œæ”¯æŒæ‰€æœ‰é‚®ä»¶å¹³å°
ğŸš€
ç«™å†…ä¿¡
ç³»ç»Ÿå†…çš„æ¶ˆæ¯é€šçŸ¥ï¼Œæä¾›ç«™å†…ä¿¡æ¨¡ç‰ˆã€ç«™å†…ä¿¡æ¶ˆæ¯
ğŸš€
æ“ä½œæ—¥å¿—
ç³»ç»Ÿæ­£å¸¸æ“ä½œæ—¥å¿—è®°å½•å’ŒæŸ¥è¯¢ï¼Œé›†æˆ Swagger ç”Ÿæˆæ—¥å¿—å†…å®¹
â­ï¸
ç™»å½•æ—¥å¿—
ç³»ç»Ÿç™»å½•æ—¥å¿—è®°å½•æŸ¥è¯¢ï¼ŒåŒ…å«ç™»å½•å¼‚å¸¸
ğŸš€
é”™è¯¯ç ç®¡ç†
ç³»ç»Ÿæ‰€æœ‰é”™è¯¯ç çš„ç®¡ç†ï¼Œå¯åœ¨çº¿ä¿®æ”¹é”™è¯¯æç¤ºï¼Œæ— éœ€é‡å¯æœåŠ¡
é€šçŸ¥å…¬å‘Š
ç³»ç»Ÿé€šçŸ¥å…¬å‘Šä¿¡æ¯å‘å¸ƒç»´æŠ¤
ğŸš€
æ•æ„Ÿè¯
é…ç½®ç³»ç»Ÿæ•æ„Ÿè¯ï¼Œæ”¯æŒæ ‡ç­¾åˆ†ç»„
ğŸš€
åº”ç”¨ç®¡ç†
ç®¡ç† SSO å•ç‚¹ç™»å½•çš„åº”ç”¨ï¼Œæ”¯æŒå¤šç§ OAuth2 æˆæƒæ–¹å¼
ğŸš€
åœ°åŒºç®¡ç†
å±•ç¤ºçœä»½ã€åŸå¸‚ã€åŒºé•‡ç­‰åŸå¸‚ä¿¡æ¯ï¼Œæ”¯æŒ IP å¯¹åº”åŸå¸‚
å·¥ä½œæµç¨‹
åŸºäº Flowable æ„å»ºï¼Œå¯æ”¯æŒä¿¡åˆ›ï¼ˆå›½äº§ï¼‰æ•°æ®åº“ï¼Œæ»¡è¶³ä¸­å›½ç‰¹è‰²æµç¨‹æ“ä½œï¼š
BPMN è®¾è®¡å™¨
é’‰é’‰/é£ä¹¦è®¾è®¡å™¨
å†ç»å¤´éƒ¨ä¼ä¸šç”Ÿäº§éªŒè¯ï¼Œå·¥ä½œæµå¼•æ“é¡»æ ‡é…ä»¿é’‰é’‰/é£ä¹¦ + BPMN åŒè®¾è®¡å™¨ï¼ï¼ï¼
å‰è€…æ”¯æŒè½»é‡é…ç½®ç®€å•æµç¨‹ï¼Œåè€…å®ç°å¤æ‚åœºæ™¯æ·±åº¦ç¼–æ’
åŠŸèƒ½åˆ—è¡¨
åŠŸèƒ½æè¿°
æ˜¯å¦å®Œæˆ
SIMPLE è®¾è®¡å™¨
ä»¿é’‰é’‰/é£ä¹¦è®¾è®¡å™¨ï¼Œæ”¯æŒæ‹–æ‹½æ­å»ºè¡¨å•æµç¨‹ï¼Œ10 åˆ†é’Ÿå¿«é€Ÿå®Œæˆå®¡æ‰¹æµç¨‹é…ç½®
âœ…
BPMN è®¾è®¡å™¨
åŸºäº BPMN æ ‡å‡†å¼€å‘ï¼Œé€‚é…å¤æ‚ä¸šåŠ¡åœºæ™¯ï¼Œæ»¡è¶³å¤šå±‚çº§å®¡æ‰¹åŠæµç¨‹è‡ªåŠ¨åŒ–éœ€æ±‚
âœ…
ä¼šç­¾
åŒä¸€ä¸ªå®¡æ‰¹èŠ‚ç‚¹è®¾ç½®å¤šä¸ªäººï¼ˆå¦‚ Aã€Bã€C ä¸‰äººï¼Œä¸‰äººä¼šåŒæ—¶æ”¶åˆ°å¾…åŠä»»åŠ¡ï¼‰ï¼Œéœ€å…¨éƒ¨åŒæ„ä¹‹åï¼Œå®¡æ‰¹æ‰å¯åˆ°ä¸‹ä¸€å®¡æ‰¹èŠ‚ç‚¹
âœ…
æˆ–ç­¾
åŒä¸€ä¸ªå®¡æ‰¹èŠ‚ç‚¹è®¾ç½®å¤šä¸ªäººï¼Œä»»æ„ä¸€ä¸ªäººå¤„ç†åï¼Œå°±èƒ½è¿›å…¥ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
âœ…
ä¾æ¬¡å®¡æ‰¹
ï¼ˆé¡ºåºä¼šç­¾ï¼‰åŒä¸€ä¸ªå®¡æ‰¹èŠ‚ç‚¹è®¾ç½®å¤šä¸ªäººï¼ˆå¦‚ Aã€Bã€C ä¸‰äººï¼‰ï¼Œä¸‰äººæŒ‰é¡ºåºä¾æ¬¡æ”¶åˆ°å¾…åŠï¼Œå³ A å…ˆå®¡æ‰¹ï¼ŒA æäº¤å B æ‰èƒ½å®¡æ‰¹ï¼Œéœ€å…¨éƒ¨åŒæ„ä¹‹åï¼Œå®¡æ‰¹æ‰å¯åˆ°ä¸‹ä¸€å®¡æ‰¹èŠ‚ç‚¹
âœ…
æŠ„é€
å°†å®¡æ‰¹ç»“æœé€šçŸ¥ç»™æŠ„é€äººï¼ŒåŒä¸€ä¸ªå®¡æ‰¹é»˜è®¤æ’é‡ï¼Œä¸é‡å¤æŠ„é€ç»™åŒä¸€äºº
âœ…
é©³å›
ï¼ˆé€€å›ï¼‰å°†å®¡æ‰¹é‡ç½®å‘é€ç»™æŸèŠ‚ç‚¹ï¼Œé‡æ–°å®¡æ‰¹ã€‚å¯é©³å›è‡³å‘èµ·äººã€ä¸Šä¸€èŠ‚ç‚¹ã€ä»»æ„èŠ‚ç‚¹
âœ…
è½¬åŠ
A è½¬ç»™å…¶ B å®¡æ‰¹ï¼ŒB å®¡æ‰¹åï¼Œè¿›å…¥ä¸‹ä¸€èŠ‚ç‚¹
âœ…
å§”æ´¾
A è½¬ç»™å…¶ B å®¡æ‰¹ï¼ŒB å®¡æ‰¹åï¼Œè½¬ç»™ Aï¼ŒA ç»§ç»­å®¡æ‰¹åè¿›å…¥ä¸‹ä¸€èŠ‚ç‚¹
âœ…
åŠ ç­¾
å…è®¸å½“å‰å®¡æ‰¹äººæ ¹æ®éœ€è¦ï¼Œè‡ªè¡Œå¢åŠ å½“å‰èŠ‚ç‚¹çš„å®¡æ‰¹äººï¼Œæ”¯æŒå‘å‰ã€å‘ååŠ ç­¾
âœ…
å‡ç­¾
ï¼ˆå–æ¶ˆåŠ ç­¾ï¼‰åœ¨å½“å‰å®¡æ‰¹äººæ“ä½œä¹‹å‰ï¼Œå‡å°‘å®¡æ‰¹äºº
âœ…
æ’¤é”€
ï¼ˆå–æ¶ˆæµç¨‹ï¼‰æµç¨‹å‘èµ·äººï¼Œå¯ä»¥å¯¹æµç¨‹è¿›è¡Œæ’¤é”€å¤„ç†
âœ…
ç»ˆæ­¢
ç³»ç»Ÿç®¡ç†å‘˜ï¼Œåœ¨ä»»æ„èŠ‚ç‚¹ç»ˆæ­¢æµç¨‹å®ä¾‹
âœ…
è¡¨å•æƒé™
æ”¯æŒæ‹–æ‹‰æ‹½é…ç½®è¡¨å•ï¼Œæ¯ä¸ªå®¡æ‰¹èŠ‚ç‚¹å¯é…ç½®åªè¯»ã€ç¼–è¾‘ã€éšè—æƒé™
âœ…
è¶…æ—¶å®¡æ‰¹
é…ç½®è¶…æ—¶å®¡æ‰¹æ—¶é—´ï¼Œè¶…æ—¶åè‡ªåŠ¨è§¦å‘å®¡æ‰¹é€šè¿‡ã€ä¸é€šè¿‡ã€é©³å›ç­‰æ“ä½œ
âœ…
è‡ªåŠ¨æé†’
é…ç½®æé†’æ—¶é—´ï¼Œåˆ°è¾¾æ—¶é—´åè‡ªåŠ¨è§¦å‘çŸ­ä¿¡ã€é‚®ç®±ã€ç«™å†…ä¿¡ç­‰é€šçŸ¥æé†’ï¼Œæ”¯æŒè‡ªå®šä¹‰é‡å¤æé†’é¢‘æ¬¡
âœ…
çˆ¶å­æµç¨‹
ä¸»æµç¨‹è®¾ç½®å­æµç¨‹èŠ‚ç‚¹ï¼Œå­æµç¨‹èŠ‚ç‚¹ä¼šè‡ªåŠ¨è§¦å‘å­æµç¨‹ã€‚å­æµç¨‹ç»“æŸåï¼Œä¸»æµç¨‹æ‰ä¼šæ‰§è¡Œï¼ˆç»§ç»­å¾€ä¸‹ä¸‹æ‰§è¡Œï¼‰ï¼Œæ”¯æŒåŒæ­¥å­æµç¨‹ã€å¼‚æ­¥å­æµç¨‹
âœ…
æ¡ä»¶åˆ†æ”¯
ï¼ˆæ’å®ƒåˆ†æ”¯ï¼‰ç”¨äºåœ¨æµç¨‹ä¸­å®ç°å†³ç­–ï¼Œå³æ ¹æ®æ¡ä»¶é€‰æ‹©ä¸€ä¸ªåˆ†æ”¯æ‰§è¡Œ
âœ…
å¹¶è¡Œåˆ†æ”¯
å…è®¸å°†æµç¨‹åˆ†æˆå¤šæ¡åˆ†æ”¯ï¼Œä¸è¿›è¡Œæ¡ä»¶åˆ¤æ–­ï¼Œæ‰€æœ‰åˆ†æ”¯éƒ½ä¼šæ‰§è¡Œ
âœ…
åŒ…å®¹åˆ†æ”¯
ï¼ˆæ¡ä»¶åˆ†æ”¯ + å¹¶è¡Œåˆ†æ”¯çš„ç»“åˆä½“ï¼‰å…è®¸åŸºäºæ¡ä»¶é€‰æ‹©å¤šæ¡åˆ†æ”¯æ‰§è¡Œï¼Œä½†å¦‚æœæ²¡æœ‰ä»»ä½•ä¸€ä¸ªåˆ†æ”¯æ»¡è¶³æ¡ä»¶ï¼Œåˆ™å¯ä»¥é€‰æ‹©é»˜è®¤åˆ†æ”¯
âœ…
è·¯ç”±åˆ†æ”¯
æ ¹æ®æ¡ä»¶é€‰æ‹©ä¸€ä¸ªåˆ†æ”¯æ‰§è¡Œï¼ˆé‡å®šå‘åˆ°æŒ‡å®šé…ç½®èŠ‚ç‚¹ï¼‰ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©é»˜è®¤åˆ†æ”¯æ‰§è¡Œï¼ˆç»§ç»­å¾€ä¸‹æ‰§è¡Œï¼‰
âœ…
è§¦å‘èŠ‚ç‚¹
æ‰§è¡Œåˆ°è¯¥èŠ‚ç‚¹ï¼Œè§¦å‘ HTTP è¯·æ±‚ã€HTTP å›è°ƒã€æ›´æ–°æ•°æ®ã€åˆ é™¤æ•°æ®ç­‰
âœ…
å»¶è¿ŸèŠ‚ç‚¹
æ‰§è¡Œåˆ°è¯¥èŠ‚ç‚¹ï¼Œå®¡æ‰¹ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ‰§è¡Œï¼Œæ”¯æŒå›ºå®šæ—¶é•¿ã€å›ºå®šæ—¥æœŸç­‰
âœ…
æ‹“å±•è®¾ç½®
æµç¨‹å‰ç½®/åç½®é€šçŸ¥ï¼ŒèŠ‚ç‚¹ï¼ˆä»»åŠ¡ï¼‰å‰ç½®ã€åç½®é€šçŸ¥ï¼Œæµç¨‹æŠ¥è¡¨ï¼Œè‡ªåŠ¨å®¡æ‰¹å»é‡ï¼Œè‡ªå®šæµç¨‹ç¼–å·ã€æ ‡é¢˜ã€æ‘˜è¦ï¼Œæµç¨‹æŠ¥è¡¨ç­‰
âœ…
æ”¯ä»˜ç³»ç»Ÿ
åŠŸèƒ½
æè¿°
ğŸš€
åº”ç”¨ä¿¡æ¯
é…ç½®å•†æˆ·çš„åº”ç”¨ä¿¡æ¯ï¼Œå¯¹æ¥æ”¯ä»˜å®ã€å¾®ä¿¡ç­‰å¤šä¸ªæ”¯ä»˜æ¸ é“
ğŸš€
æ”¯ä»˜è®¢å•
æŸ¥çœ‹ç”¨æˆ·å‘èµ·çš„æ”¯ä»˜å®ã€å¾®ä¿¡ç­‰çš„ã€æ”¯ä»˜ã€‘è®¢å•
ğŸš€
é€€æ¬¾è®¢å•
æŸ¥çœ‹ç”¨æˆ·å‘èµ·çš„æ”¯ä»˜å®ã€å¾®ä¿¡ç­‰çš„ã€é€€æ¬¾ã€‘è®¢å•
ğŸš€
å›è°ƒé€šçŸ¥
æŸ¥çœ‹æ”¯ä»˜å›è°ƒä¸šåŠ¡çš„ã€æ”¯ä»˜ã€‘ã€é€€æ¬¾ã€‘çš„é€šçŸ¥ç»“æœ
ğŸš€
æ¥å…¥ç¤ºä¾‹
æä¾›æ¥å…¥æ”¯ä»˜ç³»ç»Ÿçš„ã€æ”¯ä»˜ã€‘ã€é€€æ¬¾ã€‘çš„åŠŸèƒ½å®æˆ˜
åŸºç¡€è®¾æ–½
åŠŸèƒ½
æè¿°
ğŸš€
ä»£ç ç”Ÿæˆ
å‰åç«¯ä»£ç çš„ç”Ÿæˆï¼ˆJavaã€Vueã€SQLã€å•å…ƒæµ‹è¯•ï¼‰ï¼Œæ”¯æŒ CRUD ä¸‹è½½
ğŸš€
ç³»ç»Ÿæ¥å£
åŸºäº Swagger è‡ªåŠ¨ç”Ÿæˆç›¸å…³çš„ RESTful API æ¥å£æ–‡æ¡£
ğŸš€
æ•°æ®åº“æ–‡æ¡£
åŸºäº Screw è‡ªåŠ¨ç”Ÿæˆæ•°æ®åº“æ–‡æ¡£ï¼Œæ”¯æŒå¯¼å‡º Wordã€HTMLã€MD æ ¼å¼
è¡¨å•æ„å»º
æ‹–åŠ¨è¡¨å•å…ƒç´ ç”Ÿæˆç›¸åº”çš„ HTML ä»£ç ï¼Œæ”¯æŒå¯¼å‡º JSONã€Vue æ–‡ä»¶
ğŸš€
é…ç½®ç®¡ç†
å¯¹ç³»ç»ŸåŠ¨æ€é…ç½®å¸¸ç”¨å‚æ•°ï¼Œæ”¯æŒ SpringBoot åŠ è½½
â­ï¸
å®šæ—¶ä»»åŠ¡
åœ¨çº¿ï¼ˆæ·»åŠ ã€ä¿®æ”¹ã€åˆ é™¤)ä»»åŠ¡è°ƒåº¦åŒ…å«æ‰§è¡Œç»“æœæ—¥å¿—
ğŸš€
æ–‡ä»¶æœåŠ¡
æ”¯æŒå°†æ–‡ä»¶å­˜å‚¨åˆ° S3ï¼ˆMinIOã€é˜¿é‡Œäº‘ã€è…¾è®¯äº‘ã€ä¸ƒç‰›äº‘ï¼‰ã€æœ¬åœ°ã€FTPã€æ•°æ®åº“ç­‰
ğŸš€
WebSocket
æä¾› WebSocket æ¥å…¥ç¤ºä¾‹ï¼Œæ”¯æŒä¸€å¯¹ä¸€ã€ä¸€å¯¹å¤šå‘é€æ–¹å¼
ğŸš€
API æ—¥å¿—
åŒ…æ‹¬ RESTful API è®¿é—®æ—¥å¿—ã€å¼‚å¸¸æ—¥å¿—ä¸¤éƒ¨åˆ†ï¼Œæ–¹ä¾¿æ’æŸ¥ API ç›¸å…³çš„é—®é¢˜
MySQL ç›‘æ§
ç›‘è§†å½“å‰ç³»ç»Ÿæ•°æ®åº“è¿æ¥æ± çŠ¶æ€ï¼Œå¯è¿›è¡Œåˆ†æSQLæ‰¾å‡ºç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆ
Redis ç›‘æ§
ç›‘æ§ Redis æ•°æ®åº“çš„ä½¿ç”¨æƒ…å†µï¼Œä½¿ç”¨çš„ Redis Key ç®¡ç†
ğŸš€
æ¶ˆæ¯é˜Ÿåˆ—
åŸºäº Redis å®ç°æ¶ˆæ¯é˜Ÿåˆ—ï¼ŒStream æä¾›é›†ç¾¤æ¶ˆè´¹ï¼ŒPub/Sub æä¾›å¹¿æ’­æ¶ˆè´¹
ğŸš€
Java ç›‘æ§
åŸºäº Spring Boot Admin å®ç° Java åº”ç”¨çš„ç›‘æ§
ğŸš€
é“¾è·¯è¿½è¸ª
æ¥å…¥ SkyWalking ç»„ä»¶ï¼Œå®ç°é“¾è·¯è¿½è¸ª
ğŸš€
æ—¥å¿—ä¸­å¿ƒ
æ¥å…¥ SkyWalking ç»„ä»¶ï¼Œå®ç°æ—¥å¿—ä¸­å¿ƒ
ğŸš€
æœåŠ¡ä¿éšœ
åŸºäº Redis å®ç°åˆ†å¸ƒå¼é”ã€å¹‚ç­‰ã€é™æµåŠŸèƒ½ï¼Œæ»¡è¶³é«˜å¹¶å‘åœºæ™¯
ğŸš€
æ—¥å¿—æœåŠ¡
è½»é‡çº§æ—¥å¿—ä¸­å¿ƒï¼ŒæŸ¥çœ‹è¿œç¨‹æœåŠ¡å™¨çš„æ—¥å¿—
ğŸš€
å•å…ƒæµ‹è¯•
åŸºäº JUnit + Mockito å®ç°å•å…ƒæµ‹è¯•ï¼Œä¿è¯åŠŸèƒ½çš„æ­£ç¡®æ€§ã€ä»£ç çš„è´¨é‡ç­‰
æ•°æ®æŠ¥è¡¨
åŠŸèƒ½
æè¿°
ğŸš€
æŠ¥è¡¨è®¾è®¡å™¨
æ”¯æŒæ•°æ®æŠ¥è¡¨ã€å›¾å½¢æŠ¥è¡¨ã€æ‰“å°è®¾è®¡ç­‰
ğŸš€
å¤§å±è®¾è®¡å™¨
æ‹–æ‹½ç”Ÿæˆæ•°æ®å¤§å±ï¼Œå†…ç½®å‡ åç§å›¾è¡¨ç»„ä»¶
å¾®ä¿¡å…¬ä¼—å·
åŠŸèƒ½
æè¿°
ğŸš€
è´¦å·ç®¡ç†
é…ç½®æ¥å…¥çš„å¾®ä¿¡å…¬ä¼—å·ï¼Œå¯æ”¯æŒå¤šä¸ªå…¬ä¼—å·
ğŸš€
æ•°æ®ç»Ÿè®¡
ç»Ÿè®¡å…¬ä¼—å·çš„ç”¨æˆ·å¢å‡ã€ç´¯è®¡ç”¨æˆ·ã€æ¶ˆæ¯æ¦‚å†µã€æ¥å£åˆ†æç­‰æ•°æ®
ğŸš€
ç²‰ä¸ç®¡ç†
æŸ¥çœ‹å·²å…³æ³¨ã€å–å…³çš„ç²‰ä¸åˆ—è¡¨ï¼Œå¯å¯¹ç²‰ä¸è¿›è¡ŒåŒæ­¥ã€æ‰“æ ‡ç­¾ç­‰æ“ä½œ
ğŸš€
æ¶ˆæ¯ç®¡ç†
æŸ¥çœ‹ç²‰ä¸å‘é€çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œå¯ä¸»åŠ¨å›å¤ç²‰ä¸æ¶ˆæ¯
ğŸš€
è‡ªåŠ¨å›å¤
è‡ªåŠ¨å›å¤ç²‰ä¸å‘é€çš„æ¶ˆæ¯ï¼Œæ”¯æŒå…³æ³¨å›å¤ã€æ¶ˆæ¯å›å¤ã€å…³é”®å­—å›å¤
ğŸš€
æ ‡ç­¾ç®¡ç†
å¯¹å…¬ä¼—å·çš„æ ‡ç­¾è¿›è¡Œåˆ›å»ºã€æŸ¥è¯¢ã€ä¿®æ”¹ã€åˆ é™¤ç­‰æ“ä½œ
ğŸš€
èœå•ç®¡ç†
è‡ªå®šä¹‰å…¬ä¼—å·çš„èœå•ï¼Œä¹Ÿå¯ä»¥ä»å…¬ä¼—å·åŒæ­¥èœå•
ğŸš€
ç´ æç®¡ç†
ç®¡ç†å…¬ä¼—å·çš„å›¾ç‰‡ã€è¯­éŸ³ã€è§†é¢‘ç­‰ç´ æï¼Œæ”¯æŒåœ¨çº¿æ’­æ”¾è¯­éŸ³ã€è§†é¢‘
ğŸš€
å›¾æ–‡è‰ç¨¿ç®±
æ–°å¢å¸¸ç”¨çš„å›¾æ–‡ç´ æåˆ°è‰ç¨¿ç®±ï¼Œå¯å‘å¸ƒåˆ°å…¬ä¼—å·
ğŸš€
å›¾æ–‡å‘è¡¨è®°å½•
æŸ¥çœ‹å·²å‘å¸ƒæˆåŠŸçš„å›¾æ–‡ç´ æï¼Œæ”¯æŒåˆ é™¤æ“ä½œ
å•†åŸç³»ç»Ÿ
æ¼”ç¤ºåœ°å€ï¼š
https://doc.iocoder.cn/mall-preview/
ä¼šå‘˜ä¸­å¿ƒ
åŠŸèƒ½
æè¿°
ğŸš€
ä¼šå‘˜ç®¡ç†
ä¼šå‘˜æ˜¯ C ç«¯çš„æ¶ˆè´¹è€…ï¼Œè¯¥åŠŸèƒ½ç”¨äºä¼šå‘˜çš„æœç´¢ä¸ç®¡ç†
ğŸš€
ä¼šå‘˜æ ‡ç­¾
å¯¹ä¼šå‘˜çš„æ ‡ç­¾è¿›è¡Œåˆ›å»ºã€æŸ¥è¯¢ã€ä¿®æ”¹ã€åˆ é™¤ç­‰æ“ä½œ
ğŸš€
ä¼šå‘˜ç­‰çº§
å¯¹ä¼šå‘˜çš„ç­‰çº§ã€æˆé•¿å€¼è¿›è¡Œç®¡ç†ï¼Œå¯ç”¨äºè®¢å•æŠ˜æ‰£ç­‰ä¼šå‘˜æƒç›Š
ğŸš€
ä¼šå‘˜åˆ†ç»„
å¯¹ä¼šå‘˜è¿›è¡Œåˆ†ç»„ï¼Œç”¨äºç”¨æˆ·ç”»åƒã€å†…å®¹æ¨é€ç­‰è¿è¥æ‰‹æ®µ
ğŸš€
ç§¯åˆ†ç­¾åˆ°
å›é¦ˆç»™ç­¾åˆ°ã€æ¶ˆè´¹ç­‰è¡Œä¸ºçš„ç§¯åˆ†ï¼Œä¼šå‘˜å¯è®¢å•æŠµç°ã€ç§¯åˆ†å…‘æ¢ç­‰é€”å¾„æ¶ˆè€—
ERP ç³»ç»Ÿ
æ¼”ç¤ºåœ°å€ï¼š
https://doc.iocoder.cn/erp-preview/
CRM ç³»ç»Ÿ
æ¼”ç¤ºåœ°å€ï¼š
https://doc.iocoder.cn/crm-preview/
AI å¤§æ¨¡å‹
æ¼”ç¤ºåœ°å€ï¼š
https://doc.iocoder.cn/ai-preview/
ğŸ¨ æŠ€æœ¯æ ˆ
æ¨¡å—
é¡¹ç›®
è¯´æ˜
yudao-dependencies
Maven ä¾èµ–ç‰ˆæœ¬ç®¡ç†
yudao-framework
Java æ¡†æ¶æ‹“å±•
yudao-server
ç®¡ç†åå° + ç”¨æˆ· APP çš„æœåŠ¡ç«¯
yudao-module-system
ç³»ç»ŸåŠŸèƒ½çš„ Module æ¨¡å—
yudao-module-member
ä¼šå‘˜ä¸­å¿ƒçš„ Module æ¨¡å—
yudao-module-infra
åŸºç¡€è®¾æ–½çš„ Module æ¨¡å—
yudao-module-bpm
å·¥ä½œæµç¨‹çš„ Module æ¨¡å—
yudao-module-pay
æ”¯ä»˜ç³»ç»Ÿçš„ Module æ¨¡å—
yudao-module-mall
å•†åŸç³»ç»Ÿçš„ Module æ¨¡å—
yudao-module-erp
ERP ç³»ç»Ÿçš„ Module æ¨¡å—
yudao-module-crm
CRM ç³»ç»Ÿçš„ Module æ¨¡å—
yudao-module-ai
AI å¤§æ¨¡å‹çš„ Module æ¨¡å—
yudao-module-mp
å¾®ä¿¡å…¬ä¼—å·çš„ Module æ¨¡å—
yudao-module-report
å¤§å±æŠ¥è¡¨ Module æ¨¡å—
æ¡†æ¶
æ¡†æ¶
è¯´æ˜
ç‰ˆæœ¬
å­¦ä¹ æŒ‡å—
Spring Boot
åº”ç”¨å¼€å‘æ¡†æ¶
2.7.18
æ–‡æ¡£
MySQL
æ•°æ®åº“æœåŠ¡å™¨
5.7 / 8.0+
Druid
JDBC è¿æ¥æ± ã€ç›‘æ§ç»„ä»¶
1.2.23
æ–‡æ¡£
MyBatis Plus
MyBatis å¢å¼ºå·¥å…·åŒ…
3.5.7
æ–‡æ¡£
Dynamic Datasource
åŠ¨æ€æ•°æ®æº
3.6.1
æ–‡æ¡£
Redis
key-value æ•°æ®åº“
5.0 / 6.0 /7.0
Redisson
Redis å®¢æˆ·ç«¯
3.32.0
æ–‡æ¡£
Spring MVC
MVC æ¡†æ¶
5.3.24
æ–‡æ¡£
Spring Security
Spring å®‰å…¨æ¡†æ¶
5.7.11
æ–‡æ¡£
Hibernate Validator
å‚æ•°æ ¡éªŒç»„ä»¶
6.2.5
æ–‡æ¡£
Flowable
å·¥ä½œæµå¼•æ“
6.8.0
æ–‡æ¡£
Quartz
ä»»åŠ¡è°ƒåº¦ç»„ä»¶
2.3.2
æ–‡æ¡£
Springdoc
Swagger æ–‡æ¡£
1.7.0
æ–‡æ¡£
SkyWalking
åˆ†å¸ƒå¼åº”ç”¨è¿½è¸ªç³»ç»Ÿ
8.12.0
æ–‡æ¡£
Spring Boot Admin
Spring Boot ç›‘æ§å¹³å°
2.7.10
æ–‡æ¡£
Jackson
JSON å·¥å…·åº“
2.13.5
MapStruct
Java Bean è½¬æ¢
1.6.3
æ–‡æ¡£
Lombok
æ¶ˆé™¤å†—é•¿çš„ Java ä»£ç 
1.18.34
æ–‡æ¡£
JUnit
Java å•å…ƒæµ‹è¯•æ¡†æ¶
5.8.2
-
Mockito
Java Mock æ¡†æ¶
4.8.0
-
ğŸ· æ¼”ç¤ºå›¾
ç³»ç»ŸåŠŸèƒ½
æ¨¡å—
biu
biu
biu
ç™»å½• & é¦–é¡µ
ç”¨æˆ· & åº”ç”¨
ç§Ÿæˆ· & å¥—é¤
-
éƒ¨é—¨ & å²—ä½
-
èœå• & è§’è‰²
-
å®¡è®¡æ—¥å¿—
-
çŸ­ä¿¡
å­—å…¸ & æ•æ„Ÿè¯
é”™è¯¯ç  & é€šçŸ¥
-
å·¥ä½œæµç¨‹
æ¨¡å—
biu
biu
biu
æµç¨‹æ¨¡å‹
è¡¨å• & åˆ†ç»„
-
æˆ‘çš„æµç¨‹
å¾…åŠ & å·²åŠ
OA è¯·å‡
åŸºç¡€è®¾æ–½
æ¨¡å—
biu
biu
biu
ä»£ç ç”Ÿæˆ
-
æ–‡æ¡£
-
æ–‡ä»¶ & é…ç½®
å®šæ—¶ä»»åŠ¡
-
API æ—¥å¿—
-
MySQL & Redis
-
ç›‘æ§å¹³å°
æ”¯ä»˜ç³»ç»Ÿ
æ¨¡å—
biu
biu
biu
å•†å®¶ & åº”ç”¨
æ”¯ä»˜ & é€€æ¬¾
---
æ•°æ®æŠ¥è¡¨
æ¨¡å—
biu
biu
biu
æŠ¥è¡¨è®¾è®¡å™¨
å¤§å±è®¾è®¡å™¨
ç§»åŠ¨ç«¯ï¼ˆç®¡ç†åå°ï¼‰
biu
biu
biu
ç›®å‰å·²ç»å®ç°ç™»å½•ã€æˆ‘çš„ã€å·¥ä½œå°ã€ç¼–è¾‘èµ„æ–™ã€å¤´åƒä¿®æ”¹ã€å¯†ç ä¿®æ”¹ã€å¸¸è§é—®é¢˜ã€å…³äºæˆ‘ä»¬ç­‰åŸºç¡€åŠŸèƒ½ã€‚
- ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 30
- ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 33,522

### References
- https://github.com/YunaiV/ruoyi-vue-pro
