{
  "generated_at": "2025-10-11T02:07:31Z",
  "site": "github-trending",
  "num_articles": 47,
  "articles": [
    {
      "url": "https://github.com/TibixDev/winboat",
      "title": "TibixDev/winboat",
      "date": null,
      "executive_summary": [
        "Run Windows apps on ğŸ§ Linux with âœ¨ seamless integration",
        "---",
        "WinBoat\nWindows for Penguins.\nRun Windows apps on ğŸ§ Linux with âœ¨ seamless integration\nScreenshots\nâš ï¸\nWork in Progress\nâš ï¸\nWinBoat is currently in beta, so expect to occasionally run into hiccups and bugs. You should be comfortable with some level of troubleshooting if you decide to try it, however we encourage you to give it a shot anyway.\nFeatures\nğŸ¨ Elegant Interface\n: Sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience\nğŸ“¦ Automated Installs\n: Simple installation process through our interface - pick your preferences & specs and let us handle the rest\nğŸš€ Run Any App\n: If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications as native OS-level windows in your Linux environment\nğŸ–¥ï¸ Full Windows Desktop\n: Access the complete Windows desktop experience when you need it, or run individual apps seamlessly integrated into your Linux workflow\nğŸ“ Filesystem Integration\n: Your home directory is mounted in Windows, allowing easy file sharing between the two systems without any hassle\nâœ¨ And many more\n: Smartcard passthrough, resource monitoring, and more features being added regularly\nHow Does It Work?\nWinBoat is an Electron app which allows you to run Windows apps on Linux using a containerized approach. Windows runs as a VM inside a Docker container, we communicate with it using the\nWinBoat Guest Server\nto retrieve data we need from Windows. For compositing applications as native OS-level windows, we use FreeRDP together with Windows's RemoteApp protocol.\nPrerequisites\nBefore running WinBoat, ensure your system meets the following requirements:\nRAM\n: At least 4 GB of RAM\nCPU\n: At least 2 CPU threads\nStorage\n: At least 32 GB free space in\n/var\nVirtualization\n: KVM enabled in BIOS/UEFI\nHow to enable virtualization\nDocker\n: Required for containerization\nInstallation Guide\nâš ï¸\nNOTE:\nDocker Desktop is\nnot\nsupported, you will run into issues if you use it\nDocker Compose v2\n: Required for compatibility with docker-compose.yml files\nInstallation Guide\nDocker User Group\n: Add your user to the\ndocker\ngroup\nSetup Instructions\nFreeRDP\n: Required for remote desktop connection (Please make sure you have\nVersion 3.x.x\nwith sound support included)\nInstallation Guide\n[OPTIONAL]\nKernel Modules\n: The\niptables\n/\nnftables\nand\niptable_nat\nkernel modules can be loaded for network autodiscovery and better shared filesystem performance, but this is not obligatory in newer versions of WinBoat\nModule loading instructions\nDownloading\nYou can download the latest Linux builds under the\nReleases\ntab. We currently offer four variants:\nAppImage:\nA popular & portable app format which should run fine on most distributions\nUnpacked:\nThe raw unpacked files, simply run the executable (\nlinux-unpacked/winboat\n)\n.deb:\nThe intended format for Debian based distributions\n.rpm:\nThe intended format for Fedora based distributions\nKnown Issues About Container Runtimes\nPodman is\nunsupported\nfor now\nDocker Desktop is\nunsupported\nfor now\nDistros that emulate Docker through a Podman socket are\nunsupported\nAny rootless containerization solution is currently\nunsupported\nBuilding WinBoat\nFor building you need to have NodeJS and Go installed on your system\nClone the repo (\ngit clone https://github.com/TibixDev/WinBoat\n)\nInstall the dependencies (\nnpm i\n)\nBuild the app and the guest server using\nnpm run build:linux-gs\nYou can now find the built app under\ndist\nwith an AppImage and an Unpacked variant\nRunning WinBoat in development mode\nMake sure you meet the\nprerequisites\nAdditionally, for development you need to have NodeJS and Go installed on your system\nClone the repo (\ngit clone https://github.com/TibixDev/WinBoat\n)\nInstall the dependencies (\nnpm i\n)\nBuild the guest server (\nnpm run build-guest-server\n)\nRun the app (\nnpm run dev\n)\nContributing\nContributions are welcome! Whether it's bug fixes, feature improvements, or documentation updates, we appreciate your help making WinBoat better.\nPlease note\n: We maintain a focus on technical contributions only. Pull requests containing political/sexual content, or other sensitive/controversial topics will not be accepted. Let's keep things focused on making great software! ğŸš€\nFeel free to:\nReport bugs and issues\nSubmit feature requests\nContribute code improvements\nHelp with documentation\nShare feedback and suggestions\nCheck out our issues page to get started, or feel free to open a new issue if you've found something that needs attention.\nLicense\nWinBoat is licensed under the\nMIT\nlicense\nInspiration / Alternatives\nThese past few years some cool projects have surfaced with similar concepts, some of which we've also taken inspirations from.\nThey're awesome and you should check them out:\nWinApps\nCassowary\ndockur/windows\n(ğŸŒŸ Also used in WinBoat)\nSocials & Contact\nStar History",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 1,263",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 9,280"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/TibixDev/winboat"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/Stremio/stremio-web",
      "title": "Stremio/stremio-web",
      "date": null,
      "executive_summary": [
        "Stremio - Freedom to Stream",
        "---",
        "Stremio - Freedom to Stream\nStremio is a modern media center that's a one-stop solution for your video entertainment. You discover, watch and organize video content from easy to install addons.\nBuild\nPrerequisites\nNode.js 12 or higher\npnpm\n10 or higher\nInstall dependencies\npnpm install\nStart development server\npnpm start\nProduction build\npnpm run build\nRun with Docker\ndocker build -t stremio-web\n.\ndocker run -p 8080:8080 stremio-web\nScreenshots\nBoard\nDiscover\nMeta Details\nLicense\nStremio is copyright 2017-2023 Smart code and available under GPLv2 license. See the\nLICENSE\nfile in the project for more information.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 956",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 7,250"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/Stremio/stremio-web"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/TapXWorld/ChinaTextbook",
      "title": "TapXWorld/ChinaTextbook",
      "date": null,
      "executive_summary": [
        "æ‰€æœ‰å°åˆé«˜ã€å¤§å­¦PDFæ•™æã€‚",
        "---",
        "é¡¹ç›®çš„ç”±æ¥\nè™½ç„¶å›½å†…æ•™è‚²ç½‘ç«™å·²æä¾›å…è´¹èµ„æºï¼Œä½†å¤§å¤šæ•°æ™®é€šäººè·å–ä¿¡æ¯çš„é€”å¾„ä¾ç„¶å—é™ã€‚æœ‰äº›äººåˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œåœ¨æŸç«™ä¸Šé”€å”®è¿™äº›å¸¦æœ‰ç§äººæ°´å°çš„èµ„æºã€‚ä¸ºäº†åº”å¯¹è¿™ç§æƒ…å†µï¼Œæˆ‘è®¡åˆ’å°†è¿™äº›èµ„æºé›†ä¸­å¹¶å¼€æºï¼Œä»¥ä¿ƒè¿›ä¹‰åŠ¡æ•™è‚²çš„æ™®åŠå’Œæ¶ˆé™¤åœ°åŒºé—´çš„æ•™è‚²è´«å›°ã€‚\nè¿˜æœ‰ä¸€ä¸ªæœ€é‡è¦çš„åŸå› æ˜¯ï¼Œå¸Œæœ›æµ·å¤–åäººèƒ½å¤Ÿè®©è‡ªå·±çš„å­©å­ç»§ç»­äº†è§£å›½å†…æ•™è‚²ã€‚\nå­¦ä¹ æ•°å­¦\nå¸Œæœ›æœªæ¥å‡ºç°æ›´å¤šä¸æ˜¯ä¸ºäº†è€ƒå­¦è€Œè¯»ä¹¦çš„äººã€‚\nå°å­¦æ•°å­¦\nä¸€å¹´çº§ä¸Šå†Œ\nä¸€å¹´çº§ä¸‹å†Œ\näºŒå¹´çº§ä¸Šå†Œ\näºŒå¹´çº§ä¸‹å†Œ\nä¸‰å¹´çº§ä¸Šå†Œ\nä¸‰å¹´çº§ä¸‹å†Œ\nå››å¹´çº§ä¸Šå†Œ\nå››å¹´çº§ä¸‹å†Œ\näº”å¹´çº§ä¸Šå†Œ\näº”å¹´çº§ä¸‹å†Œ\nå…­å¹´çº§ä¸Šå†Œ\nå…­å¹´çº§ä¸‹å†Œ\nåˆä¸­æ•°å­¦\nåˆä¸€ä¸Šå†Œ\nåˆä¸€ä¸‹å†Œ\nåˆäºŒä¸Šå†Œ\nåˆäºŒä¸‹å†Œ\nåˆä¸‰ä¸Šå†Œ\nåˆä¸‰ä¸‹å†Œ\né«˜ä¸­æ•°å­¦\nç›®å½•\nå¤§å­¦æ•°å­¦\né«˜ç­‰æ•°å­¦\nçº¿æ€§ä»£æ•°\nç¦»æ•£æ•°å­¦\næ¦‚ç‡è®º\næ›´å¤šæ•°å­¦èµ„æ–™-(å¤§å­¦æ•°å­¦ç½‘)\né—®é¢˜ï¼šå¦‚ä½•åˆå¹¶è¢«æ‹†åˆ†çš„æ–‡ä»¶ï¼Ÿ\nç”±äº GitHub å¯¹å•ä¸ªæ–‡ä»¶çš„ä¸Šä¼ æœ‰æœ€å¤§é™åˆ¶ï¼Œè¶…è¿‡ 100MB çš„æ–‡ä»¶ä¼šè¢«æ‹’ç»ä¸Šä¼ ï¼Œè¶…è¿‡ 50MB çš„æ–‡ä»¶ä¸Šä¼ æ—¶ä¼šæ”¶åˆ°è­¦å‘Šã€‚å› æ­¤ï¼Œæ–‡ä»¶å¤§å°è¶…è¿‡ 50MB çš„æ–‡ä»¶ä¼šè¢«æ‹†åˆ†æˆæ¯ä¸ª 35MB çš„å¤šä¸ªæ–‡ä»¶ã€‚\nç¤ºä¾‹\næ–‡ä»¶è¢«æ‹†åˆ†çš„ç¤ºä¾‹ï¼š\nä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ Â· æ•°å­¦ä¸€å¹´çº§ä¸Šå†Œ.pdf.1\nä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ Â· æ•°å­¦ä¸€å¹´çº§ä¸Šå†Œ.pdf.2\nè§£å†³åŠæ³•\nè¦åˆå¹¶è¿™äº›è¢«æ‹†åˆ†çš„æ–‡ä»¶ï¼Œæ‚¨åªéœ€æ‰§è¡Œä»¥ä¸‹æ­¥éª¤(å…¶ä»–æ“ä½œç³»ç»ŸåŒç†)ï¼š\nå°†åˆå¹¶ç¨‹åº\nmergePDFs-windows-amd64.exe\nä¸‹è½½åˆ°åŒ…å« PDF æ–‡ä»¶çš„æ–‡ä»¶å¤¹ä¸­ã€‚\nç¡®ä¿\nmergePDFs-windows-amd64.exe\nå’Œè¢«æ‹†åˆ†çš„ PDF æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚\nåŒå‡»\nmergePDFs-windows-amd64.exe\nç¨‹åºå³å¯è‡ªåŠ¨å®Œæˆæ–‡ä»¶åˆå¹¶ã€‚\nä¸‹è½½æ–¹å¼\næ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹é“¾æ¥ï¼Œä¸‹è½½æ–‡ä»¶åˆå¹¶ç¨‹åºï¼š\nä¸‹è½½æ–‡ä»¶åˆå¹¶ç¨‹åº\næ–‡ä»¶å’Œç¨‹åºç¤ºä¾‹\nmergePDFs-windows-amd64.exe\nä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ Â· æ•°å­¦ä¸€å¹´çº§ä¸Šå†Œ.pdf.1\nä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ Â· æ•°å­¦ä¸€å¹´çº§ä¸Šå†Œ.pdf.2\né‡æ–°ä¸‹è½½\nå¦‚æœæ‚¨ä½äºå†…åœ°ï¼Œå¹¶ä¸”ç½‘ç»œä¸é”™ï¼Œæƒ³é‡æ–°ä¸‹è½½ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨\ntchMaterial-parser\né¡¹ç›®ï¼ˆé¼“åŠ±å¼€æºï¼‰ï¼Œè¿›è¡Œé‡æ–°ä¸‹è½½ã€‚\nå¦‚æœæ‚¨ä½äºå›½å¤–ï¼Œå’Œå†…åœ°ç½‘ç»œé€šä¿¡é€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨æœ¬å­˜å‚¨åº“è¿›è¡Œç­¾å‡ºã€‚\næ•™ææçŒ®\nå¦‚æœè¿™ä¸ªé¡¹ç›®å¸®åŠ©æ‚¨å…è´¹è·å–æ•™è‚²èµ„æºï¼Œè¯·è€ƒè™‘æ”¯æŒæˆ‘ä»¬æ¨å¹¿å¼€æ”¾æ•™è‚²çš„åŠªåŠ›ï¼æ‚¨çš„æçŒ®å°†å¸®åŠ©æˆ‘ä»¬ç»´æŠ¤å’Œæ‰©å±•è¿™ä¸ªèµ„æºåº“ã€‚\nåŠ å…¥æˆ‘ä»¬çš„ Telegram ç¤¾åŒºï¼Œè·å–æœ€æ–°åŠ¨æ€å¹¶åˆ†äº«æ‚¨çš„æƒ³æ³•ï¼š\nhttps://t.me/+1V6WjEq8WEM4MDM1\næ”¯æŒæˆ‘\nå¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œæ‚¨å¯ä»¥æ‰«æä»¥ä¸‹äºŒç»´ç è¿›è¡Œæèµ ï¼š",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 441",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 52,599"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/TapXWorld/ChinaTextbook"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/timelinize/timelinize",
      "title": "timelinize/timelinize",
      "date": null,
      "executive_summary": [
        "Store your data from all your accounts and devices in a single cohesive timeline on your own computer",
        "---",
        "Organize your photos & videos, chats & messages, location history, social media content, contacts, and more into a single cohesive timeline on your own computer where you can keep them alive forever.\nTimelinize lets you import your data from practically anywhere: your computer, phone, online accounts, GPS-enabled radios, various apps and programs, contact lists, cameras, and more.\nJoin our Discord\nto discuss!\nNote\nI am looking for a better name for this project. If you have an idea for a good name that is short, relevant, unique, and available,\nI'd love to hear it!\nScreenshots\nThese were captured using a dev repository of mine filled with a subset of my real data, so I've run Timelinize in obfuscation mode: images and videos are blurred (except profile pictures---need to fix that); names, identifiers, and locations around sensitive areas are all randomized, and text has been replaced with random words so that the string is about the same length.\n(I hope to make a video tour soon.)\nPlease remember this is an early alpha preview, and the software is very much evolving and improving. And you can help!\nWIP dashboard.\nVery\nWIP. The bubble chart is particularly interesting as it shows you what kinds of data are most common at which times of day throughout the years.\nThe classic timeline view is a combination of all data grouped by types and time segments for reconstructing a day or other custom time period.\nViewing an item shows all the information about it, regardless of type: text, photo, live photo, video, location, etc.\nI had to make a custom file picker since browser APIs are too limiting. This is how you'll import most of your data into your timeline, but this flow is being revised soon.\nThe large map view is capable of 3D exploration, showing your memories right where they happened with a color-coded path that represents time.\nBecause Timelinize is entity-aware and supports multiple data sources, it can show data on a map even if it doesn't have geolocation information. That's what the gray dots or pins represent. In this example, a text message was received while at church, even though it doesn't have any geolocation info associated with it directly.\nTimelinize treats entities (people, pets/animals, organizations, etc.) as first-class data points which you can filter and organize.\nTimelinize will automatically recognize the same entity across data sources with enough information, but if it isn't possible automatically, you can manually merge entities with a click.\nConversations are aggregated across data sources that have messaging capabilities. They become emergent from the database by querying relationships between items and entities.\nIn this conversation view, you can see messages exchanged with this person across both Facebook and SMS/text message are displayed together. Reactions are also supported.\nA gallery displays photos and videos, but not just those in your photo library: it includes pictures and memes sent via messages, photos and videos uploaded to social media, and any other photos/videos in your data. You can always filter to drill down.\nHow it works\nObtain your data.\nThis usually involves exporting your data from apps, online accounts, or devices. For example, requesting an archive from Google Takeout. (Apple iCloud, Facebook, Twitter/X, Strava, Instagram, etc. all offer similar functionality for GDPR compliance.) Do this early/soon, because some services take days to provide your data.\nImport your data using Timelinize. You don't need to extract or decompress .tar or .zip archives; Timelinize will attempt to recognize your data in its original format and folder structure. All the data you import is indexed in a SQLite database and stored on disk organized by date -- no obfuscation or proprietary formats; you can simply browse your files if you wish.\nExplore and organize! Timelinize has a UI that portrays data using various projections and filters. It can recall moments from your past and help you view your life more comprehensively. (It's a great living family history tool.)\nRepeat steps 1-3 as often as desired. Timelinize will skip any existing data that is the same and only import new content. You could do this every few weeks or months for busy accounts that are most important to you.\nCaution\nTimelinize is in active development and is still considered unstable. The schema is still changing, necessitating starting over from a clean slate when updating. Always keep your original source data. Expect to delete and recreate your timelines as you upgrade during this alpha development period.\nDownload and run\nDownload the\nlatest release\nfor your platform.\nSee the website for\ninstallation instructions\n.\nDevelop\nSee our\nproject wiki\nfor instructions on\ncompiling from source\n.\nCommand line interface\nTimelinize has a symmetric HTTP API and CLI. When an HTTP API endpoint is created in the code, it automatically adds to the command line as well.\nRun\ntimelinize help\n(or\ngo run main.go help\nif you're running from source) to view the list of commands, which are also HTTP endpoints. JSON or form inputs are converted to command line args/flags that represent the JSON schema or form fields.\nSetup Development Environment\nDev Container setup is provided for easy development using GitHub Codespaces or Visual Studio Code with the DevContainers extension.\nGetting started with VSCode\nMake sure you have the following installed:\nDocker\nDevContainers for VSCode\nOpen this project in VSCode\nGo to the\nRemote Explorer\non Activity Bar\nClick on\nNew Dev Container (+)\nClick on\nOpen Current Folder in Container\nThis sets up a docker container with all the dependencies required for building this project. You can get started with contributing quickly.\nMotivation and vision\n(For roadmap, see\nissues tagged\nlong-term ğŸ”­\n.)\nThe motivation for this project is two-fold. Both press upon me with a sense of urgency, which is why I dedicated some nights and weekends to work on this.\nConnecting with my family -- both living and deceased -- is important to me and my close relatives. But I wish we had more insights into the lives of those who came before us. What was important to them? Where did they live / travel / spend their time? What lessons did they learn? How did global and local events -- or heck, even the weather -- affect them? What hardships did they endure? What would they have wanted to remember? What would it be like to talk to them? A lot of this could not be known unless they wrote it all down. But these days, we have that data for ourselves. What better time than right now to start collecting personal histories from all available sources and develop a rich timeline of our life for our family, or even just for our own reference and nostalgia.\nOur lives are better-documented than any before us, but the record of our life is more ephemeral than any before us, too. We lose control of our data by relying on centralized, proprietary apps and cloud services which are useful today, and gone tomorrow. I wrote Timelinize because now is the time to liberate my data from corporations who don't own it, yet who have the only copy of it. This reality has made me feel uneasy for years, and it's not going away soon. Timelinize makes it bearable.\nImagine being able to pull up a single screen with your data from any and all of your online accounts and services -- while offline. And there you see so many aspects of your life at a glance: your photos and videos, social media posts, locations on a map and how you got there, emails and letters, documents, health and physical activities, mental and emotional wellness, and maybe even music you listened to, for any given day. You can \"zoom out\" and get the big picture. Machine learning algorithms could suggest major clusters based on your content to summarize your days, months, or years, and from that, even recommend printing physical memorabilia. It's like a highly-detailed, automated journal, fully in your control, which you can add to in the app: augment it with your own thoughts like a regular journal.\nThen cross-reference your own timeline with a global public timeline: see how locations you went to changed over time, or what major news events may have affected you, or what the political/social climate -- or the literal climate -- was like at the time. For example, you may wonder, \"Why did the family stay inside so much of the summer one year?\" You could then see, \"Oh, because it was 110 F (43 C) degrees for two months straight.\"\nOr translate the projection sideways, and instead of looking at time cross-sections, look at cross-sections of your timeline by media type: photos, posts, location, sentiment. Look at plots, charts, graphs, of your physical activity.\nOr view projections by space instead of time: view interrelations between items on a map, even items that don't have location data, because the database is entity-aware. So if a person receives a text message and the same person has location information at about the same time from a photo or GPS device, then the text message can appear on a map too, reminding you where you first got the text with the news about your nephew's birth.\nAnd all of this runs on your own computer: no one else has access to it, no one else owns it, but you.\nAnd if everyone had their own timeline, in theory they could be merged into a global supertimeline to become a thorough record of the human race, all without the need for centralizing our data on cloud services that are controlled by greedy corporations.\nHistory\nI've been working on this project since about 2013, even before I conceptualized\nCaddy\n. My initial vision was to create an automated backup of my Picasa albums that I could store on my own hard drive. This project was called Photobak. Picasa eventually became Google Photos, and about the same time I realized I wanted to backup my photos posted to Facebook, Instagram, and Twitter, too. And while I was at it, why not include my Google Location History to augment the location data from the photos. The vision continued to expand as I realized that my family could use this too, so the schema was upgraded to support multiple people/entities as well. This could allow us to merge databases, or timelines, as family members pass, or as they share parts of their timeline around with each other. Timelinize is the mature evolution of the original project that is now designed to be a comprehensive, highly detailed archive of one's life through digital (or\ndigitized\n) content. An authoritative, unified record that is easy to preserve and organize.\nLicense\nThis project is licensed with AGPL. I chose this license because I do not want others to make proprietary or commercial software using this package. The point of this project is liberation of and control over one's own, personal data, and I want to ensure that this project won't be used in anything that would perpetuate the walled garden dilemma we already face today. Even if the future of this project ever has proprietary source code, I can ensure it will stay aligned with my values and the project's original goals.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 336",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 2,387"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/timelinize/timelinize"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/MODSetter/SurfSense",
      "title": "MODSetter/SurfSense",
      "date": null,
      "executive_summary": [
        "Open Source Alternative to NotebookLM / Perplexity, connected to external sources such as Search Engines, Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more. Join our discord: https://discord.gg/ejRNvftDp9",
        "---",
        "SurfSense\nWhile tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar, Luma and more to come.\nVideo\ntemp_demo_v7.mp4\nPodcast Sample\nelon_vs_trump_podcast.mp4\nKey Features\nğŸ’¡\nIdea\n:\nHave your own highly customizable private NotebookLM and Perplexity integrated with external sources.\nğŸ“\nMultiple File Format Uploading Support\nSave content from your own personal files\n(Documents, images, videos and supports\n50+ file extensions\n)\nto your own personal knowledge base .\nğŸ”\nPowerful Search\nQuickly research or find anything in your saved content .\nğŸ’¬\nChat with your Saved Content\nInteract in Natural Language and get cited answers.\nğŸ“„\nCited Answers\nGet Cited answers just like Perplexity.\nğŸ””\nPrivacy & Local LLM Support\nWorks Flawlessly with Ollama local LLMs.\nğŸ \nSelf Hostable\nOpen source and easy to deploy locally.\nğŸ™ï¸ Podcasts\nBlazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)\nConvert your chat conversations into engaging audio content\nSupport for local TTS providers (Kokoro TTS)\nSupport for multiple TTS providers (OpenAI, Azure, Google Vertex AI)\nğŸ“Š\nAdvanced RAG Techniques\nSupports 100+ LLM's\nSupports 6000+ Embedding Models.\nSupports all major Rerankers (Pinecode, Cohere, Flashrank etc)\nUses Hierarchical Indices (2 tiered RAG setup).\nUtilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).\nRAG as a Service API Backend.\nâ„¹ï¸\nExternal Sources\nSearch Engines (Tavily, LinkUp)\nSlack\nLinear\nJira\nClickUp\nConfluence\nNotion\nGmail\nYoutube Videos\nGitHub\nDiscord\nAirtable\nGoogle Calendar\nLuma\nand more to come.....\nğŸ“„\nSupported File Extensions\nNote\n: File format support depends on your ETL service configuration. LlamaCloud supports 50+ formats, Unstructured supports 34+ core formats, and Docling (core formats, local processing, privacy-focused, no API key).\nDocuments & Text\nLlamaCloud\n:\n.pdf\n,\n.doc\n,\n.docx\n,\n.docm\n,\n.dot\n,\n.dotm\n,\n.rtf\n,\n.txt\n,\n.xml\n,\n.epub\n,\n.odt\n,\n.wpd\n,\n.pages\n,\n.key\n,\n.numbers\n,\n.602\n,\n.abw\n,\n.cgm\n,\n.cwk\n,\n.hwp\n,\n.lwp\n,\n.mw\n,\n.mcw\n,\n.pbd\n,\n.sda\n,\n.sdd\n,\n.sdp\n,\n.sdw\n,\n.sgl\n,\n.sti\n,\n.sxi\n,\n.sxw\n,\n.stw\n,\n.sxg\n,\n.uof\n,\n.uop\n,\n.uot\n,\n.vor\n,\n.wps\n,\n.zabw\nUnstructured\n:\n.doc\n,\n.docx\n,\n.odt\n,\n.rtf\n,\n.pdf\n,\n.xml\n,\n.txt\n,\n.md\n,\n.markdown\n,\n.rst\n,\n.html\n,\n.org\n,\n.epub\nDocling\n:\n.pdf\n,\n.docx\n,\n.html\n,\n.htm\n,\n.xhtml\n,\n.adoc\n,\n.asciidoc\nPresentations\nLlamaCloud\n:\n.ppt\n,\n.pptx\n,\n.pptm\n,\n.pot\n,\n.potm\n,\n.potx\n,\n.odp\n,\n.key\nUnstructured\n:\n.ppt\n,\n.pptx\nDocling\n:\n.pptx\nSpreadsheets & Data\nLlamaCloud\n:\n.xlsx\n,\n.xls\n,\n.xlsm\n,\n.xlsb\n,\n.xlw\n,\n.csv\n,\n.tsv\n,\n.ods\n,\n.fods\n,\n.numbers\n,\n.dbf\n,\n.123\n,\n.dif\n,\n.sylk\n,\n.slk\n,\n.prn\n,\n.et\n,\n.uos1\n,\n.uos2\n,\n.wk1\n,\n.wk2\n,\n.wk3\n,\n.wk4\n,\n.wks\n,\n.wq1\n,\n.wq2\n,\n.wb1\n,\n.wb2\n,\n.wb3\n,\n.qpw\n,\n.xlr\n,\n.eth\nUnstructured\n:\n.xls\n,\n.xlsx\n,\n.csv\n,\n.tsv\nDocling\n:\n.xlsx\n,\n.csv\nImages\nLlamaCloud\n:\n.jpg\n,\n.jpeg\n,\n.png\n,\n.gif\n,\n.bmp\n,\n.svg\n,\n.tiff\n,\n.webp\n,\n.html\n,\n.htm\n,\n.web\nUnstructured\n:\n.jpg\n,\n.jpeg\n,\n.png\n,\n.bmp\n,\n.tiff\n,\n.heic\nDocling\n:\n.jpg\n,\n.jpeg\n,\n.png\n,\n.bmp\n,\n.tiff\n,\n.tif\n,\n.webp\nAudio & Video\n(Always Supported)\n.mp3\n,\n.mpga\n,\n.m4a\n,\n.wav\n,\n.mp4\n,\n.mpeg\n,\n.webm\nEmail & Communication\nUnstructured\n:\n.eml\n,\n.msg\n,\n.p7s\nğŸ”– Cross Browser Extension\nThe SurfSense extension can be used to save any webpage you like.\nIts main usecase is to save any webpages protected beyond authentication.\nFEATURE REQUESTS AND FUTURE\nSurfSense is actively being developed.\nWhile it's not yet production-ready, you can help us speed up the process.\nJoin the\nSurfSense Discord\nand help shape the future of SurfSense!\nğŸš€ Roadmap\nStay up to date with our development progress and upcoming features!\nCheck out our public roadmap and contribute your ideas or feedback:\nView the Roadmap:\nSurfSense Roadmap on GitHub Projects\nHow to get started?\nInstallation Options\nSurfSense provides two installation methods:\nDocker Installation\n- The easiest way to get SurfSense up and running with all dependencies containerized.\nIncludes pgAdmin for database management through a web UI\nSupports environment variable customization via\n.env\nfile\nFlexible deployment options (full stack or core services only)\nNo need to manually edit configuration files between environments\nSee\nDocker Setup Guide\nfor detailed instructions\nFor deployment scenarios and options, see\nDeployment Guide\nManual Installation (Recommended)\n- For users who prefer more control over their setup or need to customize their deployment.\nBoth installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.\nBefore installation, make sure to complete the\nprerequisite setup steps\nincluding:\nPGVector setup\nFile Processing ETL Service\n(choose one):\nUnstructured.io API key (supports 34+ formats)\nLlamaIndex API key (enhanced parsing, supports 50+ formats)\nDocling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)\nOther required API keys\nScreenshots\nResearch Agent\nSearch Spaces\nManage Documents\nPodcast Agent\nAgent Chat\nBrowser Extension\nTech Stack\nBackEnd\nFastAPI\n: Modern, fast web framework for building APIs with Python\nPostgreSQL with pgvector\n: Database with vector search capabilities for similarity searches\nSQLAlchemy\n: SQL toolkit and ORM (Object-Relational Mapping) for database interactions\nAlembic\n: A database migrations tool for SQLAlchemy.\nFastAPI Users\n: Authentication and user management with JWT and OAuth support\nLangGraph\n: Framework for developing AI-agents.\nLangChain\n: Framework for developing AI-powered applications.\nLLM Integration\n: Integration with LLM models through LiteLLM\nRerankers\n: Advanced result ranking for improved search relevance\nHybrid Search\n: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)\nVector Embeddings\n: Document and text embeddings for semantic search\npgvector\n: PostgreSQL extension for efficient vector similarity operations\nChonkie\n: Advanced document chunking and embedding library\nUses\nAutoEmbeddings\nfor flexible embedding model selection\nLateChunker\nfor optimized document chunking based on embedding model's max sequence length\nFrontEnd\nNext.js 15.2.3\n: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.\nReact 19.0.0\n: JavaScript library for building user interfaces.\nTypeScript\n: Static type-checking for JavaScript, enhancing code quality and developer experience.\nVercel AI SDK Kit UI Stream Protocol\n: To create scalable chat UI.\nTailwind CSS 4.x\n: Utility-first CSS framework for building custom UI designs.\nShadcn\n: Headless components library.\nLucide React\n: Icon set implemented as React components.\nFramer Motion\n: Animation library for React.\nSonner\n: Toast notification library.\nGeist\n: Font family from Vercel.\nReact Hook Form\n: Form state management and validation.\nZod\n: TypeScript-first schema validation with static type inference.\n@hookform/resolvers\n: Resolvers for using validation libraries with React Hook Form.\n@tanstack/react-table\n: Headless UI for building powerful tables & datagrids.\nDevOps\nDocker\n: Container platform for consistent deployment across environments\nDocker Compose\n: Tool for defining and running multi-container Docker applications\npgAdmin\n: Web-based PostgreSQL administration tool included in Docker setup\nExtension\nManifest v3 on Plasmo\nFuture Work\nAdd More Connectors.\nPatch minor bugs.\nDocument Podcasts\nContribute\nContributions are very welcome! A contribution can be as small as a â­ or even finding and creating issues.\nFine-tuning the Backend is always desired.\nFor detailed contribution guidelines, please see our\nCONTRIBUTING.md\nfile.\nStar History",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 334",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 9,147"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/MODSetter/SurfSense"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/browserbase/stagehand",
      "title": "browserbase/stagehand",
      "date": null,
      "executive_summary": [
        "The AI Browser Automation Framework",
        "---",
        "The AI Browser Automation Framework\nRead the Docs\nIf you're looking for the Python implementation, you can find it\nhere\nVibe code\nStagehand with\nDirector\nWhy Stagehand?\nMost existing browser automation tools either require you to write low-level code in a framework like Selenium, Playwright, or Puppeteer, or use high-level agents that can be unpredictable in production. By letting developers choose what to write in code vs. natural language, Stagehand is the natural choice for browser automations in production.\nChoose when to write code vs. natural language\n: use AI when you want to navigate unfamiliar pages, and use code (\nPlaywright\n) when you know exactly what you want to do.\nPreview and cache actions\n: Stagehand lets you preview AI actions before running them, and also helps you easily cache repeatable actions to save time and tokens.\nComputer use models with one line of code\n: Stagehand lets you integrate SOTA computer use models from OpenAI and Anthropic into the browser with one line of code.\nExample\nHere's how to build a sample browser automation with Stagehand:\n// Use Playwright functions on the page object\nconst\npage\n=\nstagehand\n.\npage\n;\nawait\npage\n.\ngoto\n(\n\"https://github.com/browserbase\"\n)\n;\n// Use act() to execute individual actions\nawait\npage\n.\nact\n(\n\"click on the stagehand repo\"\n)\n;\n// Use Computer Use agents for larger actions\nconst\nagent\n=\nstagehand\n.\nagent\n(\n{\nprovider\n:\n\"openai\"\n,\nmodel\n:\n\"computer-use-preview\"\n,\n}\n)\n;\nawait\nagent\n.\nexecute\n(\n\"Get to the latest PR\"\n)\n;\n// Use extract() to read data from the page\nconst\n{\nauthor\n,\ntitle\n}\n=\nawait\npage\n.\nextract\n(\n{\ninstruction\n:\n\"extract the author and title of the PR\"\n,\nschema\n:\nz\n.\nobject\n(\n{\nauthor\n:\nz\n.\nstring\n(\n)\n.\ndescribe\n(\n\"The username of the PR author\"\n)\n,\ntitle\n:\nz\n.\nstring\n(\n)\n.\ndescribe\n(\n\"The title of the PR\"\n)\n,\n}\n)\n,\n}\n)\n;\nDocumentation\nVisit\ndocs.stagehand.dev\nto view the full documentation.\nGetting Started\nStart with Stagehand with one line of code, or check out our\nQuickstart Guide\nfor more information:\nnpx create-browser-app\nWatch Anirudh demo create-browser-app to create a Stagehand project!\nBuild and Run from Source\ngit clone https://github.com/browserbase/stagehand.git\ncd\nstagehand\npnpm install\npnpm playwright install\npnpm run build\npnpm run example\n#\nrun the blank script at ./examples/example.ts\npnpm run example 2048\n#\nrun the 2048 example at ./examples/2048.ts\npnpm run evals -man\n#\nsee evaluation suite options\nStagehand is best when you have an API key for an LLM provider and Browserbase credentials. To add these to your project, run:\ncp .env.example .env\nnano .env\n#\nEdit the .env file to add API keys\nContributing\nNote\nWe highly value contributions to Stagehand! For questions or support, please join our\nSlack community\n.\nAt a high level, we're focused on improving reliability, speed, and cost in that order of priority. If you're interested in contributing, we strongly recommend reaching out to\nMiguel Gonzalez\nor\nPaul Klein\nin our\nSlack community\nbefore starting to ensure that your contribution aligns with our goals.\nFor more information, please see our\nContributing Guide\n.\nAcknowledgements\nThis project heavily relies on\nPlaywright\nas a resilient backbone to automate the web. It also would not be possible without the awesome techniques and discoveries made by\ntarsier\n,\ngemini-zod\n, and\nfuji-web\n.\nWe'd like to thank the following people for their major contributions to Stagehand:\nPaul Klein\nAnirudh Kamath\nSean McGuire\nMiguel Gonzalez\nSameel Arif\nFilip Michalsky\nJeremy Press\nNavid Pour\nLicense\nLicensed under the MIT License.\nCopyright 2025 Browserbase, Inc.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 248",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 18,145"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/browserbase/stagehand"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/rustdesk/rustdesk",
      "title": "rustdesk/rustdesk",
      "date": null,
      "executive_summary": [
        "An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.",
        "---",
        "Build\nâ€¢\nDocker\nâ€¢\nStructure\nâ€¢\nSnapshot\n[\nĞ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°\n] | [\nÄesky\n] | [\nä¸­æ–‡\n] | [\nMagyar\n] | [\nEspaÃ±ol\n] | [\nÙØ§Ø±Ø³ÛŒ\n] | [\nFranÃ§ais\n] | [\nDeutsch\n] | [\nPolski\n] | [\nIndonesian\n] | [\nSuomi\n] | [\nà´®à´²à´¯à´¾à´³à´‚\n] | [\næ—¥æœ¬èª\n] | [\nNederlands\n] | [\nItaliano\n] | [\nĞ ÑƒÑÑĞºĞ¸Ğ¹\n] | [\nPortuguÃªs (Brasil)\n] | [\nEsperanto\n] | [\ní•œêµ­ì–´\n] | [\nØ§Ù„Ø¹Ø±Ø¨ÙŠ\n] | [\nTiáº¿ng Viá»‡t\n] | [\nDansk\n] | [\nÎ•Î»Î»Î·Î½Î¹ÎºÎ¬\n] | [\nTÃ¼rkÃ§e\n] | [\nNorsk\n]\nWe need your help to translate this README,\nRustDesk UI\nand\nRustDesk Doc\nto your native language\nCaution\nMisuse Disclaimer:\nThe developers of RustDesk do not condone or support any unethical or illegal use of this software. Misuse, such as unauthorized access, control or invasion of privacy, is strictly against our guidelines. The authors are not responsible for any misuse of the application.\nChat with us:\nDiscord\n|\nTwitter\n|\nReddit\n|\nYouTube\nYet another remote desktop solution, written in Rust. Works out of the box with no configuration required. You have full control of your data, with no concerns about security. You can use our rendezvous/relay server,\nset up your own\n, or\nwrite your own rendezvous/relay server\n.\nRustDesk welcomes contribution from everyone. See\nCONTRIBUTING.md\nfor help getting started.\nFAQ\nBINARY DOWNLOAD\nNIGHTLY BUILD\nDependencies\nDesktop versions use Flutter or Sciter (deprecated) for GUI, this tutorial is for Sciter only, since it is easier and more friendly to start. Check out our\nCI\nfor building Flutter version.\nPlease download Sciter dynamic library yourself.\nWindows\n|\nLinux\n|\nmacOS\nRaw Steps to build\nPrepare your Rust development env and C++ build env\nInstall\nvcpkg\n, and set\nVCPKG_ROOT\nenv variable correctly\nWindows: vcpkg install libvpx:x64-windows-static libyuv:x64-windows-static opus:x64-windows-static aom:x64-windows-static\nLinux/macOS: vcpkg install libvpx libyuv opus aom\nrun\ncargo run\nBuild\nHow to Build on Linux\nUbuntu 18 (Debian 10)\nsudo apt install -y zip g++ gcc git curl wget nasm yasm libgtk-3-dev clang libxcb-randr0-dev libxdo-dev \\\n        libxfixes-dev libxcb-shape0-dev libxcb-xfixes0-dev libasound2-dev libpulse-dev cmake make \\\n        libclang-dev ninja-build libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libpam0g-dev\nopenSUSE Tumbleweed\nsudo zypper install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libXfixes-devel cmake alsa-lib-devel gstreamer-devel gstreamer-plugins-base-devel xdotool-devel pam-devel\nFedora 28 (CentOS 8)\nsudo yum -y install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libxdo-devel libXfixes-devel pulseaudio-libs-devel cmake alsa-lib-devel gstreamer1-devel gstreamer1-plugins-base-devel pam-devel\nArch (Manjaro)\nsudo pacman -Syu --needed unzip git cmake gcc curl wget yasm nasm zip make pkg-config clang gtk3 xdotool libxcb libxfixes alsa-lib pipewire\nInstall vcpkg\ngit clone https://github.com/microsoft/vcpkg\ncd\nvcpkg\ngit checkout 2023.04.15\ncd\n..\nvcpkg/bootstrap-vcpkg.sh\nexport\nVCPKG_ROOT=\n$HOME\n/vcpkg\nvcpkg/vcpkg install libvpx libyuv opus aom\nFix libvpx (For Fedora)\ncd\nvcpkg/buildtrees/libvpx/src\ncd\n*\n./configure\nsed -i\n'\ns/CFLAGS+=-I/CFLAGS+=-fPIC -I/g\n'\nMakefile\nsed -i\n'\ns/CXXFLAGS+=-I/CXXFLAGS+=-fPIC -I/g\n'\nMakefile\nmake\ncp libvpx.a\n$HOME\n/vcpkg/installed/x64-linux/lib/\ncd\nBuild\ncurl --proto\n'\n=https\n'\n--tlsv1.2 -sSf https://sh.rustup.rs\n|\nsh\nsource\n$HOME\n/.cargo/env\ngit clone --recurse-submodules https://github.com/rustdesk/rustdesk\ncd\nrustdesk\nmkdir -p target/debug\nwget https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so\nmv libsciter-gtk.so target/debug\nVCPKG_ROOT=\n$HOME\n/vcpkg cargo run\nHow to build with Docker\nBegin by cloning the repository and building the Docker container:\ngit clone https://github.com/rustdesk/rustdesk\ncd\nrustdesk\ngit submodule update --init --recursive\ndocker build -t\n\"\nrustdesk-builder\n\"\n.\nThen, each time you need to build the application, run the following command:\ndocker run --rm -it -v\n$PWD\n:/home/user/rustdesk -v rustdesk-git-cache:/home/user/.cargo/git -v rustdesk-registry-cache:/home/user/.cargo/registry -e PUID=\n\"\n$(\nid -u\n)\n\"\n-e PGID=\n\"\n$(\nid -g\n)\n\"\nrustdesk-builder\nNote that the first build may take longer before dependencies are cached, subsequent builds will be faster. Additionally, if you need to specify different arguments to the build command, you may do so at the end of the command in the\n<OPTIONAL-ARGS>\nposition. For instance, if you wanted to build an optimized release version, you would run the command above followed by\n--release\n. The resulting executable will be available in the target folder on your system, and can be run with:\ntarget/debug/rustdesk\nOr, if you're running a release executable:\ntarget/release/rustdesk\nPlease ensure that you run these commands from the root of the RustDesk repository, or the application may not find the required resources. Also note that other cargo subcommands such as\ninstall\nor\nrun\nare not currently supported via this method as they would install or run the program inside the container instead of the host.\nFile Structure\nlibs/hbb_common\n: video codec, config, tcp/udp wrapper, protobuf, fs functions for file transfer, and some other utility functions\nlibs/scrap\n: screen capture\nlibs/enigo\n: platform specific keyboard/mouse control\nlibs/clipboard\n: file copy and paste implementation for Windows, Linux, macOS.\nsrc/ui\n: obsolete Sciter UI (deprecated)\nsrc/server\n: audio/clipboard/input/video services, and network connections\nsrc/client.rs\n: start a peer connection\nsrc/rendezvous_mediator.rs\n: Communicate with\nrustdesk-server\n, wait for remote direct (TCP hole punching) or relayed connection\nsrc/platform\n: platform specific code\nflutter\n: Flutter code for desktop and mobile\nflutter/web/js\n: JavaScript for Flutter web client\nScreenshots",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 231",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 99,705"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/rustdesk/rustdesk"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/shadcn-ui/ui",
      "title": "shadcn-ui/ui",
      "date": null,
      "executive_summary": [
        "A set of beautifully-designed, accessible components and a code distribution platform. Works with your favorite frameworks. Open Source. Open Code.",
        "---",
        "shadcn/ui\nAccessible and customizable components that you can copy and paste into your apps. Free. Open Source.\nUse this to build your own component library\n.\nDocumentation\nVisit\nhttp://ui.shadcn.com/docs\nto view the documentation.\nContributing\nPlease read the\ncontributing guide\n.\nLicense\nLicensed under the\nMIT license\n.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 231",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 97,333"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/shadcn-ui/ui"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/openai/codex",
      "title": "openai/codex",
      "date": null,
      "executive_summary": [
        "Lightweight coding agent that runs in your terminal",
        "---",
        "npm i -g @openai/codex\nor\nbrew install codex\nCodex CLI\nis a coding agent from OpenAI that runs locally on your computer.\nIf you want Codex in your code editor (VS Code, Cursor, Windsurf),\ninstall in your IDE\nIf you are looking for the\ncloud-based agent\nfrom OpenAI,\nCodex Web\n, go to\nchatgpt.com/codex\nQuickstart\nInstalling and running Codex CLI\nInstall globally with your preferred package manager. If you use npm:\nnpm install -g @openai/codex\nAlternatively, if you use Homebrew:\nbrew install codex\nThen simply run\ncodex\nto get started:\ncodex\nYou can also go to the\nlatest GitHub Release\nand download the appropriate binary for your platform.\nEach GitHub Release contains many executables, but in practice, you likely want one of these:\nmacOS\nApple Silicon/arm64:\ncodex-aarch64-apple-darwin.tar.gz\nx86_64 (older Mac hardware):\ncodex-x86_64-apple-darwin.tar.gz\nLinux\nx86_64:\ncodex-x86_64-unknown-linux-musl.tar.gz\narm64:\ncodex-aarch64-unknown-linux-musl.tar.gz\nEach archive contains a single entry with the platform baked into the name (e.g.,\ncodex-x86_64-unknown-linux-musl\n), so you likely want to rename it to\ncodex\nafter extracting it.\nUsing Codex with your ChatGPT plan\nRun\ncodex\nand select\nSign in with ChatGPT\n. We recommend signing into your ChatGPT account to use Codex as part of your Plus, Pro, Team, Edu, or Enterprise plan.\nLearn more about what's included in your ChatGPT plan\n.\nYou can also use Codex with an API key, but this requires\nadditional setup\n. If you previously used an API key for usage-based billing, see the\nmigration steps\n. If you're having trouble with login, please comment on\nthis issue\n.\nModel Context Protocol (MCP)\nCodex can access MCP servers. To configure them, refer to the\nconfig docs\n.\nConfiguration\nCodex CLI supports a rich set of configuration options, with preferences stored in\n~/.codex/config.toml\n. For full configuration options, see\nConfiguration\n.\nDocs & FAQ\nGetting started\nCLI usage\nRunning with a prompt as input\nExample prompts\nMemory with AGENTS.md\nConfiguration\nSandbox & approvals\nAuthentication\nAuth methods\nLogin on a \"Headless\" machine\nAutomating Codex\nGitHub Action\nTypeScript SDK\nNon-interactive mode (\ncodex exec\n)\nAdvanced\nTracing / verbose logging\nModel Context Protocol (MCP)\nZero data retention (ZDR)\nContributing\nInstall & build\nSystem Requirements\nDotSlash\nBuild from source\nFAQ\nOpen source fund\nLicense\nThis repository is licensed under the\nApache-2.0 License\n.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 188",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 46,874"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/openai/codex"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/Zie619/n8n-workflows",
      "title": "Zie619/n8n-workflows",
      "date": null,
      "executive_summary": [
        "all of the workflows of n8n i could find (also from the site itself)",
        "---",
        "âš¡ N8N Workflow Collection & Documentation\nA professionally organized collection of\n2,057 n8n workflows\nwith a lightning-fast documentation system that provides instant search, analysis, and browsing capabilities.\nâš ï¸\nIMPORTANT NOTICE (Aug 14, 2025):\nRepository history has been rewritten due to DMCA compliance. If you have a fork or local clone, please see\nIssue 85\nfor instructions on syncing your copy.\nSupport My Work\nIf you'd like to say thanks, consider buying me a coffeeâ€”your support helps me keep improving this project!\nğŸš€\nNEW: Public Search Interface & High-Performance Documentation\nğŸŒ\nBrowse workflows online\n- No installation required!\nOr run locally for development with 100x performance improvement:\nOption 1: Online Search (Recommended for Users)\nğŸ”— Visit:\nzie619.github.io/n8n-workflows\nâš¡\nInstant access\n- No setup required\nğŸ”\nSearch 2,057+ workflows\ndirectly in browser\nğŸ“±\nMobile-friendly\ninterface\nğŸ·ï¸\nCategory filtering\nacross 15 categories\nğŸ“¥\nDirect download\nof workflow JSON files\nOption 2: Local Development System\n#\nInstall dependencies\npip install -r requirements.txt\n#\nStart the fast API server\npython run.py\n#\nOpen in browser\nhttp://localhost:8000\nFeatures:\nâš¡\nSub-100ms response times\nwith SQLite FTS5 search\nğŸ”\nInstant full-text search\nwith advanced filtering\nğŸ“±\nResponsive design\n- works perfectly on mobile\nğŸŒ™\nDark/light themes\nwith system preference detection\nğŸ“Š\nLive statistics\n- 365 unique integrations, 29,445 total nodes\nğŸ¯\nSmart categorization\nby trigger type and complexity\nğŸ¯\nUse case categorization\nby service name mapped to categories\nğŸ“„\nOn-demand JSON viewing\nand download\nğŸ”—\nMermaid diagram generation\nfor workflow visualization\nğŸ”„\nReal-time workflow naming\nwith intelligent formatting\nPerformance Comparison\nMetric\nOld System\nNew System\nImprovement\nFile Size\n71MB HTML\n<100KB\n700x smaller\nLoad Time\n10+ seconds\n<1 second\n10x faster\nSearch\nClient-side only\nFull-text with FTS5\nInstant\nMemory Usage\n~2GB RAM\n<50MB RAM\n40x less\nMobile Support\nPoor\nExcellent\nFully responsive\nğŸ“‚ Repository Organization\nWorkflow Collection\n2,057 workflows\nwith meaningful, searchable names\n365 unique integrations\nacross popular platforms\n29,445 total nodes\nwith professional categorization\nQuality assurance\n- All workflows analyzed and categorized\nAdvanced Naming System âœ¨\nOur intelligent naming system converts technical filenames into readable titles:\nBefore\n:\n2051_Telegram_Webhook_Automation_Webhook.json\nAfter\n:\nTelegram Webhook Automation\n100% meaningful names\nwith smart capitalization\nAutomatic integration detection\nfrom node analysis\nUse Case Category âœ¨\nThe search interface includes a dropdown filter that lets you browse 2,057+ workflows by category.\nThe system includes an automated categorization feature that organizes workflows by service categories to make them easier to discover and filter.\nHow Categorization Works\nRun the categorization script\npython create_categories.py\nService Name Recognition\nThe script analyzes each workflow JSON filename to identify recognized service names (e.g., \"Twilio\", \"Slack\", \"Gmail\", etc.)\nCategory Mapping\nEach recognized service name is matched to its corresponding category using the definitions in\ncontext/def_categories.json\n. For example:\nTwilio â†’ Communication & Messaging\nGmail â†’ Communication & Messaging\nAirtable â†’ Data Processing & Analysis\nSalesforce â†’ CRM & Sales\nSearch Categories Generation\nThe script produces a\nsearch_categories.json\nfile that contains the categorized workflow data\nFilter Interface\nUsers can then filter workflows by category in the search interface, making it easier to find workflows for specific use cases\nAvailable Categories\nThe categorization system includes the following main categories:\nAI Agent Development\nBusiness Process Automation\nCloud Storage & File Management\nCommunication & Messaging\nCreative Content & Video Automation\nCreative Design Automation\nCRM & Sales\nData Processing & Analysis\nE-commerce & Retail\nFinancial & Accounting\nMarketing & Advertising Automation\nProject Management\nSocial Media Management\nTechnical Infrastructure & DevOps\nWeb Scraping & Data Extraction\nContribute Categories\nYou can help expand the categorization by adding more service-to-category mappings (e.g., Twilio â†’ Communication & Messaging) in context/defs_categories.json.\nMany workflow JSON files are conveniently named with the service name, often separated by underscores (_).\nğŸ›  Usage Instructions\nOption 1: Modern Fast System (Recommended)\n#\nClone repository\ngit clone\n<\nrepo-url\n>\ncd\nn8n-workflows\n#\nInstall Python dependencies\npip install -r requirements.txt\n#\nStart the documentation server\npython run.py\n#\nBrowse workflows at http://localhost:8000\n#\n- Instant search across 2,057 workflows\n#\n- Professional responsive interface\n#\n- Real-time workflow statistics\nOption 2: Development Mode\n#\nStart with auto-reload for development\npython run.py --dev\n#\nOr specify custom host/port\npython run.py --host 0.0.0.0 --port 3000\n#\nForce database reindexing\npython run.py --reindex\nImport Workflows into n8n\n#\nUse the Python importer (recommended)\npython import_workflows.py\n#\nOr manually import individual workflows:\n#\n1. Open your n8n Editor UI\n#\n2. Click menu (â˜°) â†’ Import workflow\n#\n3. Choose any .json file from the workflows/ folder\n#\n4. Update credentials/webhook URLs before running\nğŸ“Š Workflow Statistics\nCurrent Collection Stats\nTotal Workflows\n: 2,057 automation workflows\nActive Workflows\n: 215 (10.5% active rate)\nTotal Nodes\n: 29,528 (avg 14.4 nodes per workflow)\nUnique Integrations\n: 367 different services and APIs\nDatabase\n: SQLite with FTS5 full-text search\nTrigger Distribution\nComplex\n: 832 workflows (40.4%) - Multi-trigger systems\nWebhook\n: 521 workflows (25.3%) - API-triggered automations\nManual\n: 478 workflows (23.2%) - User-initiated workflows\nScheduled\n: 226 workflows (11.0%) - Time-based executions\nComplexity Analysis\nLow (â‰¤5 nodes)\n: ~35% - Simple automations\nMedium (6-15 nodes)\n: ~45% - Standard workflows\nHigh (16+ nodes)\n: ~20% - Complex enterprise systems\nPopular Integrations\nTop services by usage frequency:\nCommunication\n: Telegram, Discord, Slack, WhatsApp\nCloud Storage\n: Google Drive, Google Sheets, Dropbox\nDatabases\n: PostgreSQL, MySQL, MongoDB, Airtable\nAI/ML\n: OpenAI, Anthropic, Hugging Face\nDevelopment\n: HTTP Request, Webhook, GraphQL\nğŸ” Advanced Search Features\nSmart Search Categories\nOur system automatically categorizes workflows into 15 main categories:\nAvailable Categories:\nAI Agent Development\n: OpenAI, Anthropic, Hugging Face, CalcsLive\nBusiness Process Automation\n: Workflow utilities, scheduling, data processing\nCloud Storage & File Management\n: Google Drive, Dropbox, OneDrive, Box\nCommunication & Messaging\n: Telegram, Discord, Slack, WhatsApp, Email\nCreative Content & Video Automation\n: YouTube, Vimeo, content creation\nCreative Design Automation\n: Canva, Figma, image processing\nCRM & Sales\n: Salesforce, HubSpot, Pipedrive, customer management\nData Processing & Analysis\n: Database operations, analytics, data transformation\nE-commerce & Retail\n: Shopify, Stripe, PayPal, online stores\nFinancial & Accounting\n: Financial tools, payment processing, accounting\nMarketing & Advertising Automation\n: Email marketing, campaigns, lead generation\nProject Management\n: Jira, Trello, Asana, task management\nSocial Media Management\n: LinkedIn, Twitter/X, Facebook, Instagram\nTechnical Infrastructure & DevOps\n: GitHub, deployment, monitoring\nWeb Scraping & Data Extraction\n: HTTP requests, webhooks, data collection\nAPI Usage Examples\n#\nSearch workflows by text\ncurl\n\"\nhttp://localhost:8000/api/workflows?q=telegram+automation\n\"\n#\nFilter by trigger type and complexity\ncurl\n\"\nhttp://localhost:8000/api/workflows?trigger=Webhook&complexity=high\n\"\n#\nFind all messaging workflows\ncurl\n\"\nhttp://localhost:8000/api/workflows/category/messaging\n\"\n#\nGet database statistics\ncurl\n\"\nhttp://localhost:8000/api/stats\n\"\n#\nBrowse available categories\ncurl\n\"\nhttp://localhost:8000/api/categories\n\"\nğŸ— Technical Architecture\nModern Stack\nSQLite Database\n- FTS5 full-text search with 365 indexed integrations\nFastAPI Backend\n- RESTful API with automatic OpenAPI documentation\nResponsive Frontend\n- Modern HTML5 with embedded CSS/JavaScript\nSmart Analysis\n- Automatic workflow categorization and naming\nKey Features\nChange Detection\n- MD5 hashing for efficient re-indexing\nBackground Processing\n- Non-blocking workflow analysis\nCompressed Responses\n- Gzip middleware for optimal speed\nError Handling\n- Graceful degradation and comprehensive logging\nMobile Optimization\n- Touch-friendly interface design\nDatabase Performance\n--\nOptimized schema for lightning-fast queries\nCREATE\nTABLE\nworkflows\n(\n    id\nINTEGER\nPRIMARY KEY\n,\n    filename\nTEXT\nUNIQUE,\n    name\nTEXT\n,\n    active\nBOOLEAN\n,\n    trigger_type\nTEXT\n,\n    complexity\nTEXT\n,\n    node_count\nINTEGER\n,\n    integrations\nTEXT\n,\n--\nJSON array of 365 unique services\ndescription\nTEXT\n,\n    file_hash\nTEXT\n,\n--\nMD5 for change detection\nanalyzed_at\nTIMESTAMP\n);\n--\nFull-text search with ranking\nCREATE VIRTUAL TABLE workflows_fts USING fts5(\n    filename, name, description, integrations, tags,\n    content\n=\n'\nworkflows\n'\n, content_rowid\n=\n'\nid\n'\n);\nğŸ”§ Setup & Requirements\nSystem Requirements\nPython 3.7+\n- For running the documentation system\nModern Browser\n- Chrome, Firefox, Safari, Edge\n50MB Storage\n- For SQLite database and indexes\nn8n Instance\n- For importing and running workflows\nInstallation\n#\nClone repository\ngit clone\n<\nrepo-url\n>\ncd\nn8n-workflows\n#\nInstall dependencies\npip install -r requirements.txt\n#\nStart documentation server\npython run.py\n#\nAccess at http://localhost:8000\nDevelopment Setup\n#\nCreate virtual environment\npython3 -m venv .venv\nsource\n.venv/bin/activate\n#\nLinux/Mac\n#\nor .venv\\Scripts\\activate  # Windows\n#\nInstall dependencies\npip install -r requirements.txt\n#\nRun with auto-reload for development\npython api_server.py --reload\n#\nForce database reindexing\npython workflow_db.py --index --force\nğŸ“‹ Naming Convention\nIntelligent Formatting System\nOur system automatically converts technical filenames to user-friendly names:\n#\nAutomatic transformations:\n2051_Telegram_Webhook_Automation_Webhook.json â†’\n\"\nTelegram Webhook Automation\n\"\n0250_HTTP_Discord_Import_Scheduled.json â†’\n\"\nHTTP Discord Import Scheduled\n\"\n0966_OpenAI_Data_Processing_Manual.json â†’\n\"\nOpenAI Data Processing Manual\n\"\nTechnical Format\n[ID]_[Service1]_[Service2]_[Purpose]_[Trigger].json\nSmart Capitalization Rules\nHTTP\nâ†’ HTTP (not Http)\nAPI\nâ†’ API (not Api)\nwebhook\nâ†’ Webhook\nautomation\nâ†’ Automation\nscheduled\nâ†’ Scheduled\nğŸš€ API Documentation\nCore Endpoints\nGET /\n- Main workflow browser interface\nGET /api/stats\n- Database statistics and metrics\nGET /api/workflows\n- Search with filters and pagination\nGET /api/workflows/{filename}\n- Detailed workflow information\nGET /api/workflows/{filename}/download\n- Download workflow JSON\nGET /api/workflows/{filename}/diagram\n- Generate Mermaid diagram\nAdvanced Search\nGET /api/workflows/category/{category}\n- Search by service category\nGET /api/categories\n- List all available categories\nGET /api/integrations\n- Get integration statistics\nPOST /api/reindex\n- Trigger background reindexing\nResponse Examples\n// GET /api/stats\n{\n\"total\"\n:\n2053\n,\n\"active\"\n:\n215\n,\n\"inactive\"\n:\n1838\n,\n\"triggers\"\n: {\n\"Complex\"\n:\n831\n,\n\"Webhook\"\n:\n519\n,\n\"Manual\"\n:\n477\n,\n\"Scheduled\"\n:\n226\n},\n\"total_nodes\"\n:\n29445\n,\n\"unique_integrations\"\n:\n365\n}\nğŸ¤ Contributing\nğŸ‰ This project solves\nIssue #84\n- providing online access to workflows without requiring local setup!\nAdding New Workflows\nExport workflow\nas JSON from n8n\nName descriptively\nfollowing the established pattern:\n[ID]_[Service]_[Purpose]_[Trigger].json\nAdd to workflows/\ndirectory (create service folder if needed)\nRemove sensitive data\n(credentials, personal URLs)\nAdd tags\nfor better searchability (calculation, automation, etc.)\nGitHub Actions automatically\nupdates the public search interface\nQuality Standards\nâœ… Workflow must be functional and tested\nâœ… Remove all credentials and sensitive data\nâœ… Follow naming convention for consistency\nâœ… Verify compatibility with recent n8n versions\nâœ… Include meaningful description or comments\nâœ… Add relevant tags for search optimization\nCustom Node Workflows\nâœ… Include npm package links in descriptions\nâœ… Document custom node requirements\nâœ… Add installation instructions\nâœ… Use descriptive tags (like CalcsLive example)\nReindexing (for local development)\n#\nForce database reindexing after adding workflows\npython run.py --reindex\n#\nOr update search index only\npython scripts/generate_search_index.py\nâš ï¸\nImportant Notes\nSecurity & Privacy\nReview before use\n- All workflows shared as-is for educational purposes\nUpdate credentials\n- Replace API keys, tokens, and webhooks\nTest safely\n- Verify in development environment first\nCheck permissions\n- Ensure proper access rights for integrations\nCompatibility\nn8n Version\n- Compatible with n8n 1.0+ (most workflows)\nCommunity Nodes\n- Some workflows may require additional node installations\nAPI Changes\n- External services may have updated their APIs since creation\nDependencies\n- Verify required integrations before importing\nğŸ“š Resources & References\nWorkflow Sources\nThis comprehensive collection includes workflows from:\nOfficial n8n.io\n- Documentation and community examples\nGitHub repositories\n- Open source community contributions\nBlog posts & tutorials\n- Real-world automation patterns\nUser submissions\n- Tested and verified workflows\nEnterprise use cases\n- Business process automations\nLearn More\nn8n Documentation\n- Official documentation\nn8n Community\n- Community forum and support\nWorkflow Templates\n- Official template library\nIntegration Docs\n- Service-specific guides\nğŸ† Project Achievements\nRepository Transformation\n2,053 workflows\nprofessionally organized and named\n365 unique integrations\nautomatically detected and categorized\n100% meaningful names\n(improved from basic filename patterns)\nZero data loss\nduring intelligent renaming process\nAdvanced search\nwith 15 service categories\nPerformance Revolution\nSub-100ms search\nwith SQLite FTS5 full-text indexing\nInstant filtering\nacross 29,445 workflow nodes\nMobile-optimized\nresponsive design for all devices\nReal-time statistics\nwith live database queries\nProfessional interface\nwith modern UX principles\nSystem Reliability\nRobust error handling\nwith graceful degradation\nChange detection\nfor efficient database updates\nBackground processing\nfor non-blocking operations\nComprehensive logging\nfor debugging and monitoring\nProduction-ready\nwith proper middleware and security\nThis repository represents the most comprehensive and well-organized collection of n8n workflows available, featuring cutting-edge search technology and professional documentation that makes workflow discovery and usage a delightful experience.\nğŸ¯ Perfect for\n: Developers, automation engineers, business analysts, and anyone looking to streamline their workflows with proven n8n automations.\nä¸­æ–‡",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 186",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 35,868"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/Zie619/n8n-workflows"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/davila7/claude-code-templates",
      "title": "davila7/claude-code-templates",
      "date": null,
      "executive_summary": [
        "CLI tool for configuring and monitoring Claude Code",
        "---",
        "Claude Code Templates (aitmpl.com)\nReady-to-use configurations for Anthropic's Claude Code.\nA comprehensive collection of AI agents, custom commands, settings, hooks, external integrations (MCPs), and project templates to enhance your development workflow.\nBrowse & Install Components and Templates\nBrowse All Templates\n- Interactive web interface to explore and install 100+ agents, commands, settings, hooks, and MCPs.\nğŸš€ Quick Installation\n#\nInstall a complete development stack\nnpx claude-code-templates@latest --agent frontend-developer --command generate-tests --mcp github-integration\n#\nBrowse and install interactively\nnpx claude-code-templates@latest\n#\nInstall specific components\nnpx claude-code-templates@latest --agent security-auditor\nnpx claude-code-templates@latest --command optimize-bundle\nnpx claude-code-templates@latest --setting mcp-timeouts\nnpx claude-code-templates@latest --hook pre-commit-validation\nnpx claude-code-templates@latest --mcp postgresql-integration\nWhat You Get\nComponent\nDescription\nExamples\nğŸ¤– Agents\nAI specialists for specific domains\nSecurity auditor, React performance optimizer, database architect\nâš¡ Commands\nCustom slash commands\n/generate-tests\n,\n/optimize-bundle\n,\n/check-security\nğŸ”Œ MCPs\nExternal service integrations\nGitHub, PostgreSQL, Stripe, AWS, OpenAI\nâš™ï¸ Settings\nClaude Code configurations\nTimeouts, memory settings, output styles\nğŸª Hooks\nAutomation triggers\nPre-commit validation, post-completion actions\nğŸ“¦ Templates\nComplete project configurations with CLAUDE.md, .claude/* files and .mcp.json\nFramework-specific setups, project best practices\nğŸ› ï¸ Additional Tools\nBeyond the template catalog, Claude Code Templates includes powerful development tools:\nğŸ“Š Claude Code Analytics\nMonitor your AI-powered development sessions in real-time with live state detection and performance metrics.\nnpx claude-code-templates@latest --analytics\nğŸ’¬ Conversation Monitor\nMobile-optimized interface to view Claude responses in real-time with secure remote access.\n#\nLocal access\nnpx claude-code-templates@latest --chats\n#\nSecure remote access via Cloudflare Tunnel\nnpx claude-code-templates@latest --chats --tunnel\nğŸ” Health Check\nComprehensive diagnostics to ensure your Claude Code installation is optimized.\nnpx claude-code-templates@latest --health-check\nğŸ“– Documentation\nğŸ“š docs.aitmpl.com\n- Complete guides, examples, and API reference for all components and tools.\nContributing\nWe welcome contributions!\nBrowse existing templates\nto see what's available, then check our\ncontributing guidelines\nto add your own agents, commands, MCPs, settings, or hooks.\nPlease read our\nCode of Conduct\nbefore contributing.\nAttribution\nThis collection includes components from multiple sources:\nAgents Collection:\nwshobson/agents Collection\nby\nwshobson\n- Licensed under MIT License (48 agents)\nCommands Collection:\nawesome-claude-code Commands\nby\nhesreallyhim\n- Licensed under CC0 1.0 Universal (21 commands)\nğŸ“„ License\nThis project is licensed under the MIT License - see the\nLICENSE\nfile for details.\nğŸ”— Links\nğŸŒ Browse Templates\n:\naitmpl.com\nğŸ“š Documentation\n:\ndocs.aitmpl.com\nğŸ’¬ Community\n:\nGitHub Discussions\nğŸ› Issues\n:\nGitHub Issues\nâ­ Star History\nâ­ Found this useful? Give us a star to support the project!",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 186",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 6,984"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/davila7/claude-code-templates"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/anthropics/claude-code",
      "title": "anthropics/claude-code",
      "date": null,
      "executive_summary": [
        "Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.",
        "---",
        "Claude Code\nClaude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows -- all through natural language commands. Use it in your terminal, IDE, or tag @claude on Github.\nLearn more in the\nofficial documentation\n.\nGet started\nInstall Claude Code:\nnpm install -g @anthropic-ai/claude-code\nNavigate to your project directory and run\nclaude\n.\nReporting Bugs\nWe welcome your feedback. Use the\n/bug\ncommand to report issues directly within Claude Code, or file a\nGitHub issue\n.\nConnect on Discord\nJoin the\nClaude Developers Discord\nto connect with other developers using Claude Code. Get help, share feedback, and discuss your projects with the community.\nData collection, usage, and retention\nWhen you use Claude Code, we collect feedback, which includes usage data (such as code acceptance or rejections), associated conversation data, and user feedback submitted via the\n/bug\ncommand.\nHow we use your data\nSee our\ndata usage policies\n.\nPrivacy safeguards\nWe have implemented several safeguards to protect your data, including limited retention periods for sensitive information, restricted access to user session data, and clear policies against using feedback for model training.\nFor full details, please review our\nCommercial Terms of Service\nand\nPrivacy Policy\n.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 177",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 36,195"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/anthropics/claude-code"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/CapSoftware/Cap",
      "title": "CapSoftware/Cap",
      "date": null,
      "executive_summary": [
        "Open source Loom alternative. Beautiful, shareable screen recordings.",
        "---",
        "Cap\nThe open source Loom alternative.\nCap.so Â»\nDownloads for\nmacOS & Windows\nCap is the open source alternative to Loom. It's a video messaging tool that allows you to record, edit and share videos in seconds.\nSelf Hosting\nCap Web is available to self-host using Docker or Railway, see our\nself-hosting docs\nto learn more.\nYou can also use the button below to deploy Cap Web to Railway:\nCap Desktop can connect to your self-hosted Cap Web instance regardless of if you build it yourself or\ndownload from our website\n.\nMonorepo App Architecture\nWe use a combination of Rust, React (Next.js), TypeScript, Tauri, Drizzle (ORM), MySQL, TailwindCSS throughout this Turborepo powered monorepo.\nA note about database: The codebase is currently designed to work with MySQL only. MariaDB or other compatible databases might partially work but are not officially supported.\nApps:\ndesktop\n: A\nTauri\n(Rust) app, using\nSolidStart\non the frontend.\nweb\n: A\nNext.js\nweb app.\nPackages:\nui\n: A\nReact\nShared component library.\nutils\n: A\nReact\nShared utility library.\ntsconfig\n: Shared\ntsconfig\nconfigurations used throughout the monorepo.\ndatabase\n: A\nReact\nand\nDrizzle ORM\nShared database library.\nconfig\n:\neslint\nconfigurations (includes\neslint-config-next\n,\neslint-config-prettier\nother configs used throughout the monorepo).\nLicense:\nPortions of this software are licensed as follows:\nAll code residing in the\ncap-camera*\nand\nscap-*\nfamilies of crates is licensed under the MIT License (see\nlicenses/LICENSE-MIT\n).\nAll third party components are licensed under the original license provided by the owner of the applicable component\nAll other content not mentioned above is available under the AGPLv3 license as defined in\nLICENSE\nContributing\nSee\nCONTRIBUTING.md\nfor more information. This guide is a work in progress, and is updated regularly as the app matures.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 136",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 12,407"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/CapSoftware/Cap"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/open-webui/open-webui",
      "title": "open-webui/open-webui",
      "date": null,
      "executive_summary": [
        "User-friendly AI Interface (Supports Ollama, OpenAI API, ...)",
        "---",
        "Open WebUI ğŸ‘‹\nOpen WebUI is an\nextensible\n, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.\nIt supports various LLM runners like\nOllama\nand\nOpenAI-compatible APIs\n, with\nbuilt-in inference engine\nfor RAG, making it a\npowerful AI deployment solution\n.\nPassionate about open-source AI?\nJoin our team â†’\nTip\nLooking for an\nEnterprise Plan\n?\nâ€“\nSpeak with Our Sales Team Today!\nGet\nenhanced capabilities\n, including\ncustom theming and branding\n,\nService Level Agreement (SLA) support\n,\nLong-Term Support (LTS) versions\n, and\nmore!\nFor more information, be sure to check out our\nOpen WebUI Documentation\n.\nKey Features of Open WebUI â­\nğŸš€\nEffortless Setup\n: Install seamlessly using Docker or Kubernetes (kubectl, kustomize or helm) for a hassle-free experience with support for both\n:ollama\nand\n:cuda\ntagged images.\nğŸ¤\nOllama/OpenAI API Integration\n: Effortlessly integrate OpenAI-compatible APIs for versatile conversations alongside Ollama models. Customize the OpenAI API URL to link with\nLMStudio, GroqCloud, Mistral, OpenRouter, and more\n.\nğŸ›¡ï¸\nGranular Permissions and User Groups\n: By allowing administrators to create detailed user roles and permissions, we ensure a secure user environment. This granularity not only enhances security but also allows for customized user experiences, fostering a sense of ownership and responsibility amongst users.\nğŸ”„\nSCIM 2.0 Support\n: Enterprise-grade user and group provisioning through SCIM 2.0 protocol, enabling seamless integration with identity providers like Okta, Azure AD, and Google Workspace for automated user lifecycle management.\nğŸ“±\nResponsive Design\n: Enjoy a seamless experience across Desktop PC, Laptop, and Mobile devices.\nğŸ“±\nProgressive Web App (PWA) for Mobile\n: Enjoy a native app-like experience on your mobile device with our PWA, providing offline access on localhost and a seamless user interface.\nâœ’ï¸ğŸ”¢\nFull Markdown and LaTeX Support\n: Elevate your LLM experience with comprehensive Markdown and LaTeX capabilities for enriched interaction.\nğŸ¤ğŸ“¹\nHands-Free Voice/Video Call\n: Experience seamless communication with integrated hands-free voice and video call features, allowing for a more dynamic and interactive chat environment.\nğŸ› ï¸\nModel Builder\n: Easily create Ollama models via the Web UI. Create and add custom characters/agents, customize chat elements, and import models effortlessly through\nOpen WebUI Community\nintegration.\nğŸ\nNative Python Function Calling Tool\n: Enhance your LLMs with built-in code editor support in the tools workspace. Bring Your Own Function (BYOF) by simply adding your pure Python functions, enabling seamless integration with LLMs.\nğŸ“š\nLocal RAG Integration\n: Dive into the future of chat interactions with groundbreaking Retrieval Augmented Generation (RAG) support. This feature seamlessly integrates document interactions into your chat experience. You can load documents directly into the chat or add files to your document library, effortlessly accessing them using the\n#\ncommand before a query.\nğŸ”\nWeb Search for RAG\n: Perform web searches using providers like\nSearXNG\n,\nGoogle PSE\n,\nBrave Search\n,\nserpstack\n,\nserper\n,\nSerply\n,\nDuckDuckGo\n,\nTavilySearch\n,\nSearchApi\nand\nBing\nand inject the results directly into your chat experience.\nğŸŒ\nWeb Browsing Capability\n: Seamlessly integrate websites into your chat experience using the\n#\ncommand followed by a URL. This feature allows you to incorporate web content directly into your conversations, enhancing the richness and depth of your interactions.\nğŸ¨\nImage Generation Integration\n: Seamlessly incorporate image generation capabilities using options such as AUTOMATIC1111 API or ComfyUI (local), and OpenAI's DALL-E (external), enriching your chat experience with dynamic visual content.\nâš™ï¸\nMany Models Conversations\n: Effortlessly engage with various models simultaneously, harnessing their unique strengths for optimal responses. Enhance your experience by leveraging a diverse set of models in parallel.\nğŸ”\nRole-Based Access Control (RBAC)\n: Ensure secure access with restricted permissions; only authorized individuals can access your Ollama, and exclusive model creation/pulling rights are reserved for administrators.\nğŸŒğŸŒ\nMultilingual Support\n: Experience Open WebUI in your preferred language with our internationalization (i18n) support. Join us in expanding our supported languages! We're actively seeking contributors!\nğŸ§©\nPipelines, Open WebUI Plugin Support\n: Seamlessly integrate custom logic and Python libraries into Open WebUI using\nPipelines Plugin Framework\n. Launch your Pipelines instance, set the OpenAI URL to the Pipelines URL, and explore endless possibilities.\nExamples\ninclude\nFunction Calling\n, User\nRate Limiting\nto control access,\nUsage Monitoring\nwith tools like Langfuse,\nLive Translation with LibreTranslate\nfor multilingual support,\nToxic Message Filtering\nand much more.\nğŸŒŸ\nContinuous Updates\n: We are committed to improving Open WebUI with regular updates, fixes, and new features.\nWant to learn more about Open WebUI's features? Check out our\nOpen WebUI documentation\nfor a comprehensive overview!\nSponsors ğŸ™Œ\nEmerald\nTailscale\nâ€¢ Connect self-hosted AI to any device with Tailscale\nWarp\nâ€¢ The intelligent terminal for developers\nWe are incredibly grateful for the generous support of our sponsors. Their contributions help us to maintain and improve our project, ensuring we can continue to deliver quality work to our community. Thank you!\nHow to Install ğŸš€\nInstallation via Python pip ğŸ\nOpen WebUI can be installed using pip, the Python package installer. Before proceeding, ensure you're using\nPython 3.11\nto avoid compatibility issues.\nInstall Open WebUI\n:\nOpen your terminal and run the following command to install Open WebUI:\npip install open-webui\nRunning Open WebUI\n:\nAfter installation, you can start Open WebUI by executing:\nopen-webui serve\nThis will start the Open WebUI server, which you can access at\nhttp://localhost:8080\nQuick Start with Docker ğŸ³\nNote\nPlease note that for certain Docker environments, additional configurations might be needed. If you encounter any connection issues, our detailed guide on\nOpen WebUI Documentation\nis ready to assist you.\nWarning\nWhen using Docker to install Open WebUI, make sure to include the\n-v open-webui:/app/backend/data\nin your Docker command. This step is crucial as it ensures your database is properly mounted and prevents any loss of data.\nTip\nIf you wish to utilize Open WebUI with Ollama included or CUDA acceleration, we recommend utilizing our official images tagged with either\n:cuda\nor\n:ollama\n. To enable CUDA, you must install the\nNvidia CUDA container toolkit\non your Linux/WSL system.\nInstallation with Default Configuration\nIf Ollama is on your computer\n, use this command:\ndocker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\nIf Ollama is on a Different Server\n, use this command:\nTo connect to Ollama on another server, change the\nOLLAMA_BASE_URL\nto the server's URL:\ndocker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\nTo run Open WebUI with Nvidia GPU support\n, use this command:\ndocker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda\nInstallation for OpenAI API Usage Only\nIf you're only using OpenAI API\n, use this command:\ndocker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\nInstalling Open WebUI with Bundled Ollama Support\nThis installation method uses a single container image that bundles Open WebUI with Ollama, allowing for a streamlined setup via a single command. Choose the appropriate command based on your hardware setup:\nWith GPU Support\n:\nUtilize GPU resources by running the following command:\ndocker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\nFor CPU Only\n:\nIf you're not using a GPU, use this command instead:\ndocker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\nBoth commands facilitate a built-in, hassle-free installation of both Open WebUI and Ollama, ensuring that you can get everything up and running swiftly.\nAfter installation, you can access Open WebUI at\nhttp://localhost:3000\n. Enjoy! ğŸ˜„\nOther Installation Methods\nWe offer various installation alternatives, including non-Docker native installation methods, Docker Compose, Kustomize, and Helm. Visit our\nOpen WebUI Documentation\nor join our\nDiscord community\nfor comprehensive guidance.\nLook at the\nLocal Development Guide\nfor instructions on setting up a local development environment.\nTroubleshooting\nEncountering connection issues? Our\nOpen WebUI Documentation\nhas got you covered. For further assistance and to join our vibrant community, visit the\nOpen WebUI Discord\n.\nOpen WebUI: Server Connection Error\nIf you're experiencing connection issues, itâ€™s often due to the WebUI docker container not being able to reach the Ollama server at 127.0.0.1:11434 (host.docker.internal:11434) inside the container . Use the\n--network=host\nflag in your docker command to resolve this. Note that the port changes from 3000 to 8080, resulting in the link:\nhttp://localhost:8080\n.\nExample Docker Command\n:\ndocker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main\nKeeping Your Docker Installation Up-to-Date\nIn case you want to update your local Docker installation to the latest version, you can do it with\nWatchtower\n:\ndocker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui\nIn the last part of the command, replace\nopen-webui\nwith your container name if it is different.\nCheck our Updating Guide available in our\nOpen WebUI Documentation\n.\nUsing the Dev Branch ğŸŒ™\nWarning\nThe\n:dev\nbranch contains the latest unstable features and changes. Use it at your own risk as it may have bugs or incomplete features.\nIf you want to try out the latest bleeding-edge features and are okay with occasional instability, you can use the\n:dev\ntag like this:\ndocker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev\nOffline Mode\nIf you are running Open WebUI in an offline environment, you can set the\nHF_HUB_OFFLINE\nenvironment variable to\n1\nto prevent attempts to download models from the internet.\nexport\nHF_HUB_OFFLINE=1\nWhat's Next? ğŸŒŸ\nDiscover upcoming features on our roadmap in the\nOpen WebUI Documentation\n.\nLicense ğŸ“œ\nThis project contains code under multiple licenses. The current codebase includes components licensed under the Open WebUI License with an additional requirement to preserve the \"Open WebUI\" branding, as well as prior contributions under their respective original licenses. For a detailed record of license changes and the applicable terms for each section of the code, please refer to\nLICENSE_HISTORY\n. For complete and updated licensing details, please see the\nLICENSE\nand\nLICENSE_HISTORY\nfiles.\nSupport ğŸ’¬\nIf you have any questions, suggestions, or need assistance, please open an issue or join our\nOpen WebUI Discord community\nto connect with us! ğŸ¤\nStar History\nCreated by\nTimothy Jaeryang Baek\n- Let's make Open WebUI even more amazing together! ğŸ’ª",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 110",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 112,077"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/open-webui/open-webui"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/78/xiaozhi-esp32",
      "title": "78/xiaozhi-esp32",
      "date": null,
      "executive_summary": [
        "An MCP-based chatbot | ä¸€ä¸ªåŸºäºMCPçš„èŠå¤©æœºå™¨äºº",
        "---",
        "An MCP-based Chatbot\nï¼ˆä¸­æ–‡ |\nEnglish\n|\næ—¥æœ¬èª\nï¼‰\nä»‹ç»\nğŸ‘‰\näººç±»ï¼šç»™ AI è£…æ‘„åƒå¤´ vs AIï¼šå½“åœºå‘ç°ä¸»äººä¸‰å¤©æ²¡æ´—å¤´ã€bilibiliã€‘\nğŸ‘‰\næ‰‹å·¥æ‰“é€ ä½ çš„ AI å¥³å‹ï¼Œæ–°æ‰‹å…¥é—¨æ•™ç¨‹ã€bilibiliã€‘\nå°æ™º AI èŠå¤©æœºå™¨äººä½œä¸ºä¸€ä¸ªè¯­éŸ³äº¤äº’å…¥å£ï¼Œåˆ©ç”¨ Qwen / DeepSeek ç­‰å¤§æ¨¡å‹çš„ AI èƒ½åŠ›ï¼Œé€šè¿‡ MCP åè®®å®ç°å¤šç«¯æ§åˆ¶ã€‚\nç‰ˆæœ¬è¯´æ˜\nå½“å‰ v2 ç‰ˆæœ¬ä¸ v1 ç‰ˆæœ¬åˆ†åŒºè¡¨ä¸å…¼å®¹ï¼Œæ‰€ä»¥æ— æ³•ä» v1 ç‰ˆæœ¬é€šè¿‡ OTA å‡çº§åˆ° v2 ç‰ˆæœ¬ã€‚åˆ†åŒºè¡¨è¯´æ˜å‚è§\npartitions/v2/README.md\nã€‚\nä½¿ç”¨ v1 ç‰ˆæœ¬çš„æ‰€æœ‰ç¡¬ä»¶ï¼Œå¯ä»¥é€šè¿‡æ‰‹åŠ¨çƒ§å½•å›ºä»¶æ¥å‡çº§åˆ° v2 ç‰ˆæœ¬ã€‚\nv1 çš„ç¨³å®šç‰ˆæœ¬ä¸º 1.9.2ï¼Œå¯ä»¥é€šè¿‡\ngit checkout v1\næ¥åˆ‡æ¢åˆ° v1 ç‰ˆæœ¬ï¼Œè¯¥åˆ†æ”¯ä¼šæŒç»­ç»´æŠ¤åˆ° 2026 å¹´ 2 æœˆã€‚\nå·²å®ç°åŠŸèƒ½\nWi-Fi / ML307 Cat.1 4G\nç¦»çº¿è¯­éŸ³å”¤é†’\nESP-SR\næ”¯æŒä¸¤ç§é€šä¿¡åè®®ï¼ˆ\nWebsocket\næˆ– MQTT+UDPï¼‰\né‡‡ç”¨ OPUS éŸ³é¢‘ç¼–è§£ç \nåŸºäºæµå¼ ASR + LLM + TTS æ¶æ„çš„è¯­éŸ³äº¤äº’\nå£°çº¹è¯†åˆ«ï¼Œè¯†åˆ«å½“å‰è¯´è¯äººçš„èº«ä»½\n3D Speaker\nOLED / LCD æ˜¾ç¤ºå±ï¼Œæ”¯æŒè¡¨æƒ…æ˜¾ç¤º\nç”µé‡æ˜¾ç¤ºä¸ç”µæºç®¡ç†\næ”¯æŒå¤šè¯­è¨€ï¼ˆä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ï¼‰\næ”¯æŒ ESP32-C3ã€ESP32-S3ã€ESP32-P4 èŠ¯ç‰‡å¹³å°\né€šè¿‡è®¾å¤‡ç«¯ MCP å®ç°è®¾å¤‡æ§åˆ¶ï¼ˆéŸ³é‡ã€ç¯å…‰ã€ç”µæœºã€GPIO ç­‰ï¼‰\né€šè¿‡äº‘ç«¯ MCP æ‰©å±•å¤§æ¨¡å‹èƒ½åŠ›ï¼ˆæ™ºèƒ½å®¶å±…æ§åˆ¶ã€PCæ¡Œé¢æ“ä½œã€çŸ¥è¯†æœç´¢ã€é‚®ä»¶æ”¶å‘ç­‰ï¼‰\nè‡ªå®šä¹‰å”¤é†’è¯ã€å­—ä½“ã€è¡¨æƒ…ä¸èŠå¤©èƒŒæ™¯ï¼Œæ”¯æŒç½‘é¡µç«¯åœ¨çº¿ä¿®æ”¹ (\nè‡ªå®šä¹‰Assetsç”Ÿæˆå™¨\n)\nç¡¬ä»¶\né¢åŒ…æ¿æ‰‹å·¥åˆ¶ä½œå®è·µ\nè¯¦è§é£ä¹¦æ–‡æ¡£æ•™ç¨‹ï¼š\nğŸ‘‰\nã€Šå°æ™º AI èŠå¤©æœºå™¨äººç™¾ç§‘å…¨ä¹¦ã€‹\né¢åŒ…æ¿æ•ˆæœå›¾å¦‚ä¸‹ï¼š\næ”¯æŒ 70 å¤šä¸ªå¼€æºç¡¬ä»¶ï¼ˆä»…å±•ç¤ºéƒ¨åˆ†ï¼‰\nç«‹åˆ›Â·å®æˆ˜æ´¾ ESP32-S3 å¼€å‘æ¿\nä¹é‘« ESP32-S3-BOX3\nM5Stack CoreS3\nM5Stack AtomS3R + Echo Base\nç¥å¥‡æŒ‰é’® 2.4\nå¾®é›ªç”µå­ ESP32-S3-Touch-AMOLED-1.8\nLILYGO T-Circle-S3\nè™¾å“¥ Mini C3\nç’€ç’¨Â·AI åŠå \næ— åç§‘æŠ€ Nologo-æ˜Ÿæ™º-1.54TFT\nSenseCAP Watcher\nESP-HI è¶…ä½æˆæœ¬æœºå™¨ç‹—\nè½¯ä»¶\nå›ºä»¶çƒ§å½•\næ–°æ‰‹ç¬¬ä¸€æ¬¡æ“ä½œå»ºè®®å…ˆä¸è¦æ­å»ºå¼€å‘ç¯å¢ƒï¼Œç›´æ¥ä½¿ç”¨å…å¼€å‘ç¯å¢ƒçƒ§å½•çš„å›ºä»¶ã€‚\nå›ºä»¶é»˜è®¤æ¥å…¥\nxiaozhi.me\nå®˜æ–¹æœåŠ¡å™¨ï¼Œä¸ªäººç”¨æˆ·æ³¨å†Œè´¦å·å¯ä»¥å…è´¹ä½¿ç”¨ Qwen å®æ—¶æ¨¡å‹ã€‚\nğŸ‘‰\næ–°æ‰‹çƒ§å½•å›ºä»¶æ•™ç¨‹\nå¼€å‘ç¯å¢ƒ\nCursor æˆ– VSCode\nå®‰è£… ESP-IDF æ’ä»¶ï¼Œé€‰æ‹© SDK ç‰ˆæœ¬ 5.4 æˆ–ä»¥ä¸Š\nLinux æ¯” Windows æ›´å¥½ï¼Œç¼–è¯‘é€Ÿåº¦å¿«ï¼Œä¹Ÿå…å»é©±åŠ¨é—®é¢˜çš„å›°æ‰°\næœ¬é¡¹ç›®ä½¿ç”¨ Google C++ ä»£ç é£æ ¼ï¼Œæäº¤ä»£ç æ—¶è¯·ç¡®ä¿ç¬¦åˆè§„èŒƒ\nå¼€å‘è€…æ–‡æ¡£\nè‡ªå®šä¹‰å¼€å‘æ¿æŒ‡å—\n- å­¦ä¹ å¦‚ä½•ä¸ºå°æ™º AI åˆ›å»ºè‡ªå®šä¹‰å¼€å‘æ¿\nMCP åè®®ç‰©è”ç½‘æ§åˆ¶ç”¨æ³•è¯´æ˜\n- äº†è§£å¦‚ä½•é€šè¿‡ MCP åè®®æ§åˆ¶ç‰©è”ç½‘è®¾å¤‡\nMCP åè®®äº¤äº’æµç¨‹\n- è®¾å¤‡ç«¯ MCP åè®®çš„å®ç°æ–¹å¼\nMQTT + UDP æ··åˆé€šä¿¡åè®®æ–‡æ¡£\nä¸€ä»½è¯¦ç»†çš„ WebSocket é€šä¿¡åè®®æ–‡æ¡£\nå¤§æ¨¡å‹é…ç½®\nå¦‚æœä½ å·²ç»æ‹¥æœ‰ä¸€ä¸ªå°æ™º AI èŠå¤©æœºå™¨äººè®¾å¤‡ï¼Œå¹¶ä¸”å·²æ¥å…¥å®˜æ–¹æœåŠ¡å™¨ï¼Œå¯ä»¥ç™»å½•\nxiaozhi.me\næ§åˆ¶å°è¿›è¡Œé…ç½®ã€‚\nğŸ‘‰\nåå°æ“ä½œè§†é¢‘æ•™ç¨‹ï¼ˆæ—§ç‰ˆç•Œé¢ï¼‰\nç›¸å…³å¼€æºé¡¹ç›®\nåœ¨ä¸ªäººç”µè„‘ä¸Šéƒ¨ç½²æœåŠ¡å™¨ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹ç¬¬ä¸‰æ–¹å¼€æºçš„é¡¹ç›®ï¼š\nxinnan-tech/xiaozhi-esp32-server\nPython æœåŠ¡å™¨\njoey-zhou/xiaozhi-esp32-server-java\nJava æœåŠ¡å™¨\nAnimeAIChat/xiaozhi-server-go\nGolang æœåŠ¡å™¨\nä½¿ç”¨å°æ™ºé€šä¿¡åè®®çš„ç¬¬ä¸‰æ–¹å®¢æˆ·ç«¯é¡¹ç›®ï¼š\nhuangjunsen0406/py-xiaozhi\nPython å®¢æˆ·ç«¯\nTOM88812/xiaozhi-android-client\nAndroid å®¢æˆ·ç«¯\n100askTeam/xiaozhi-linux\nç™¾é—®ç§‘æŠ€æä¾›çš„ Linux å®¢æˆ·ç«¯\n78/xiaozhi-sf32\næ€æ¾ˆç§‘æŠ€çš„è“ç‰™èŠ¯ç‰‡å›ºä»¶\nQuecPython/solution-xiaozhiAI\nç§»è¿œæä¾›çš„ QuecPython å›ºä»¶\nå…³äºé¡¹ç›®\nè¿™æ˜¯ä¸€ä¸ªç”±è™¾å“¥å¼€æºçš„ ESP32 é¡¹ç›®ï¼Œä»¥ MIT è®¸å¯è¯å‘å¸ƒï¼Œå…è®¸ä»»ä½•äººå…è´¹ä½¿ç”¨ï¼Œä¿®æ”¹æˆ–ç”¨äºå•†ä¸šç”¨é€”ã€‚\næˆ‘ä»¬å¸Œæœ›é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œèƒ½å¤Ÿå¸®åŠ©å¤§å®¶äº†è§£ AI ç¡¬ä»¶å¼€å‘ï¼Œå°†å½“ä¸‹é£é€Ÿå‘å±•çš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨åˆ°å®é™…çš„ç¡¬ä»¶è®¾å¤‡ä¸­ã€‚\nå¦‚æœä½ æœ‰ä»»ä½•æƒ³æ³•æˆ–å»ºè®®ï¼Œè¯·éšæ—¶æå‡º Issues æˆ–åŠ å…¥ QQ ç¾¤ï¼š1011329060\nStar History",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 107",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 19,437"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/78/xiaozhi-esp32"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/evershopcommerce/evershop",
      "title": "evershopcommerce/evershop",
      "date": null,
      "executive_summary": [
        "ğŸ›ï¸ Typescript E-commerce Platform",
        "---",
        "EverShop\nDocumentation\n|\nDemo\nIntroduction\nEverShop is a modern, TypeScript-first eCommerce platform built with GraphQL and React. Designed for developers, it offers essential commerce features in a modular, fully customizable architectureâ€”perfect for building tailored shopping experiences with confidence and speed.\nInstallation Using Docker\nYou can get started with EverShop in minutes by using the Docker image. The Docker image is a great way to get started with EverShop without having to worry about installing dependencies or configuring your environment.\ncurl -sSL https://raw.githubusercontent.com/evershopcommerce/evershop/main/docker-compose.yml\n>\ndocker-compose.yml\ndocker-compose up -d\nFor the full installation guide, please refer to our\nInstallation guide\n.\nDocumentation\nInstallation guide\n.\nExtension development\n.\nTheme development\n.\nDemo\nExplore our demo store.\nDemo user:\nEmail:\ndemo@evershop.io\nPassword: 123456\nSupport\nIf you like my work, feel free to:\nâ­ this repository. It helps.\nabout EverShop. Thank you!\nContributing\nEverShop is an open-source project. We are committed to a fully transparent development process and appreciate highly any contributions. Whether you are helping us fix bugs, proposing new features, improving our documentation or spreading the word - we would love to have you as part of the EverShop community.\nAsk a question about EverShop\nYou can ask questions, and participate in discussions about EverShop-related topics in the EverShop Discord channel.\nCreate a bug report\nIf you see an error message or run into an issue, please\ncreate bug report\n. This effort is valued and it will help all EverShop users.\nSubmit a feature request\nIf you have an idea, or you're missing a capability that would make development easier and more robust, please\nSubmit feature request\n.\nIf a similar feature request already exists, don't forget to leave a \"+1\".\nIf you add some more information such as your thoughts and vision about the feature, your comments will be embraced warmly :)\nPlease refer to our\nContribution Guidelines\nand\nCode of Conduct\n.\nLicense\nGPL-3.0 License",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 99",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 6,383"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/evershopcommerce/evershop"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/rizinorg/cutter",
      "title": "rizinorg/cutter",
      "date": null,
      "executive_summary": [
        "Free and Open Source Reverse Engineering Platform powered by rizin",
        "---",
        "Cutter\nCutter is a free and open-source reverse engineering platform powered by\nrizin\n. It aims at being an advanced and customizable reverse engineering platform while keeping the user experience in mind. Cutter is created by reverse engineers for reverse engineers.\nLearn more at\ncutter.re\n.\nGetting Cutter\nDownload\nCutter release binaries for all major platforms (Linux, macOS, Windows) can be downloaded from\nGitHub Releases\n.\nLinux\n: If your distribution provides it, check for\ncutter\npackage in your package manager (or\ncutter-re\n/\nrz-cutter\n). If not available there, we have setup repositories in\nOBS\nfor some common distributions. Look at\nhttps://software.opensuse.org/package/cutter-re\nand follow the instructions there. Otherwise download the\n.AppImage\nfile from our release, make it executable and run as below or use\nAppImageLauncher\n.\nchmod +x Cutter*.AppImage; ./Cutter*.AppImage\nmacOS\n: Download the\n.dmg\nfile or use\nHomebrew Cask\n:\nbrew install --cask cutter\nWindows\n: Download the\n.zip\narchive, or use either\nChocolatey\nor\nScoop\n:\nchoco install cutter\nscoop bucket add extras\nfollowed by\nscoop install cutter\nBuild from sources\nTo build Cutter from sources, please check the\nBuilding Docs\n.\nDocker image\nTo deploy\ncutter\nusing a pre-built\nDockerfile\n, it's possible to use the\nprovided configuration\n. The corresponding\nREADME.md\nfile also contains instructions on how to get started using the docker image with minimal effort.\nDocumentation\nUser Guide\nContribution Guidelines\nDevelopers Docs\nPlugins\nCutter supports both Python and Native C++ plugins.\nOur community has built many plugins and useful scripts for Cutter such as the native integration of\nGhidra decompiler\nor the plugin to visualize DynamoRIO code coverage. You can find a list of cutter plugins linked below. Feel free to extend it with your own plugins and scripts for Cutter.\nOfficial & Community Plugins\nPlugins Development Guide\nGetting Help\nPlease use the following channels to ask for help from Cutter developers and community:\nTelegram:\nhttps://t.me/cutter_re\nMattermost:\nhttps://im.rizin.re\nIRC:\n#cutter on\nhttps://web.libera.chat/\nTwitter:\n@cutter_re",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 97",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 17,754"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/rizinorg/cutter"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/xyflow/xyflow",
      "title": "xyflow/xyflow",
      "date": null,
      "executive_summary": [
        "React Flow |Â Svelte Flow - Powerful open source libraries for building node-based UIs with React (https://reactflow.dev) or Svelte (https://svelteflow.dev). Ready out-of-the-box and infinitely customizable.",
        "---",
        "Powerful open source libraries for building node-based UIs with React or Svelte. Ready out-of-the-box and infinitely customizable.\nReact Flow\nÂ·\nSvelte Flow\nÂ·\nReact Flow Pro\nÂ·\nDiscord\nThe xyflow mono repo\nThe xyflow repository is the home of four packages:\nReact Flow 12\n@xyflow/react\npackages/react\nReact Flow 11\nreactflow\nv11 branch\nSvelte Flow\n@xyflow/svelte\npackages/svelte\nShared helper library\n@xyflow/system\npackages/system\nCommercial usage\nAre you using React Flow or Svelte Flow for a personal project?\nGreat! No sponsorship needed, you can support us by reporting any bugs you find, sending us screenshots of your projects, and starring us on Github ğŸŒŸ\nAre you using React Flow or Svelte Flow at your organization and making money from it?\nAwesome! We rely on your support to keep our libraries developed and maintained under an MIT License, just how we like it. For React Flow you can do that on the\nReact Flow Pro website\nand for both of our libraries you can do it through\nGithub Sponsors\n.\nGetting started\nThe best way to get started is to check out the\nReact Flow\nor\nSvelte Flow\nlearn section. However if you want to get a sneak peek of how to install and use the libraries you can see it here:\nReact Flow\nbasic usage\nInstallation\nnpm install @xyflow/react\nBasic usage\nimport\n{\nuseCallback\n}\nfrom\n'react'\n;\nimport\n{\nReactFlow\n,\nMiniMap\n,\nControls\n,\nBackground\n,\nuseNodesState\n,\nuseEdgesState\n,\naddEdge\n,\n}\nfrom\n'@xyflow/react'\n;\nimport\n'@xyflow/react/dist/style.css'\n;\nconst\ninitialNodes\n=\n[\n{\nid\n:\n'1'\n,\nposition\n:\n{\nx\n:\n0\n,\ny\n:\n0\n}\n,\ndata\n:\n{\nlabel\n:\n'1'\n}\n}\n,\n{\nid\n:\n'2'\n,\nposition\n:\n{\nx\n:\n0\n,\ny\n:\n100\n}\n,\ndata\n:\n{\nlabel\n:\n'2'\n}\n}\n,\n]\n;\nconst\ninitialEdges\n=\n[\n{\nid\n:\n'e1-2'\n,\nsource\n:\n'1'\n,\ntarget\n:\n'2'\n}\n]\n;\nfunction\nFlow\n(\n)\n{\nconst\n[\nnodes\n,\nsetNodes\n,\nonNodesChange\n]\n=\nuseNodesState\n(\ninitialNodes\n)\n;\nconst\n[\nedges\n,\nsetEdges\n,\nonEdgesChange\n]\n=\nuseEdgesState\n(\ninitialEdges\n)\n;\nconst\nonConnect\n=\nuseCallback\n(\n(\nparams\n)\n=>\nsetEdges\n(\n(\neds\n)\n=>\naddEdge\n(\nparams\n,\neds\n)\n)\n,\n[\nsetEdges\n]\n)\n;\nreturn\n(\n<\nReactFlow\nnodes\n=\n{\nnodes\n}\nedges\n=\n{\nedges\n}\nonNodesChange\n=\n{\nonNodesChange\n}\nonEdgesChange\n=\n{\nonEdgesChange\n}\nonConnect\n=\n{\nonConnect\n}\n>\n<\nMiniMap\n/>\n<\nControls\n/>\n<\nBackground\n/>\n</\nReactFlow\n>\n)\n;\n}\nexport\ndefault\nFlow\n;\nSvelte Flow\nbasic usage\nInstallation\nnpm install @xyflow/svelte\nBasic usage\n<\nscript\nlang\n=\n\"\nts\n\"\n>\nimport\n{\nwritable\n}\nfrom\n'\nsvelte/store\n'\n;\nimport\n{\nSvelteFlow\n,\nControls\n,\nBackground\n,\nBackgroundVariant\n,\nMiniMap\n,\n}\nfrom\n'\n@xyflow/svelte\n'\n;\nimport\n'\n@xyflow/svelte/dist/style.css\n'\nconst\nnodes\n=\nwritable\n([\n{\nid:\n'\n1\n'\n,\ntype:\n'\ninput\n'\n,\ndata: { label:\n'\nInput Node\n'\n},\nposition: { x:\n0\n, y:\n0\n}\n},\n{\nid:\n'\n2\n'\n,\ntype:\n'\ncustom\n'\n,\ndata: { label:\n'\nNode\n'\n},\nposition: { x:\n0\n, y:\n150\n}\n}\n]);\nconst\nedges\n=\nwritable\n([\n{\nid:\n'\n1-2\n'\n,\ntype:\n'\ndefault\n'\n,\nsource:\n'\n1\n'\n,\ntarget:\n'\n2\n'\n,\nlabel:\n'\nEdge Text\n'\n}\n]);\n</\nscript\n>\n\n<\nSvelteFlow\n{\nnodes\n}\n{\nedges\n}\nfitView\non:nodeclick\n={(\nevent\n)\n=>\nconsole\n.\nlog\n(\n'\non node click\n'\n,\nevent\n)}\n>\n<\nControls\n/>\n<\nBackground\nvariant\n={\nBackgroundVariant\n.\nDots\n} />\n<\nMiniMap\n/>\n</\nSvelteFlow\n>\nReleases\nFor releasing packages we are using\nchangesets\nin combination with the\nchangeset Github action\n. The rough idea is:\ncreate PRs for new features, updates and fixes (with a changeset if relevant for changelog)\nmerge into main\nchangset creates a PR that bumps all packages based on the changesets\nmerge changeset PR if you want to release to Github and npm\nBuilt by\nxyflow\nReact Flow and Svelte Flow are maintained by the\nxyflow team\n. If you need help or want to talk to us about a collaboration, reach out through our\ncontact form\nor by joining our\nDiscord Server\n.\nLicense\nReact Flow and Svelte Flow are\nMIT licensed\n.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 92",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 32,452"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/xyflow/xyflow"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/rust-lang/rustfmt",
      "title": "rust-lang/rustfmt",
      "date": null,
      "executive_summary": [
        "Format Rust code",
        "---",
        "rustfmt\nA tool for formatting Rust code according to style guidelines.\nIf you'd like to help out (and you should, it's a fun project!), see\nContributing.md\nand our\nCode of\nConduct\n.\nYou can use rustfmt in Travis CI builds. We provide a minimal Travis CI\nconfiguration (see\nhere\n).\nQuick start\nYou can run\nrustfmt\nwith Rust 1.24 and above.\nOn the Stable toolchain\nTo install:\nrustup component add rustfmt\nTo run on a cargo project in the current working directory:\ncargo fmt\nOn the Nightly toolchain\nFor the latest and greatest\nrustfmt\n, nightly is required.\nTo install:\nrustup component add rustfmt --toolchain nightly\nTo run on a cargo project in the current working directory:\ncargo +nightly fmt\nLimitations\nRustfmt tries to work on as much Rust code as possible. Sometimes, the code\ndoesn't even need to compile! In general, we are looking to limit areas of\ninstability; in particular, post-1.0, the formatting of most code should not\nchange as Rustfmt improves. However, there are some things that Rustfmt can't\ndo or can't do well (and thus where formatting might change significantly,\neven post-1.0). We would like to reduce the list of limitations over time.\nThe following list enumerates areas where Rustfmt does not work or where the\nstability guarantees do not apply (we don't make a distinction between the two\nbecause in the future Rustfmt might work on code where it currently does not):\na program where any part of the program does not parse (parsing is an early\nstage of compilation and in Rust includes macro expansion).\nMacro declarations and uses (current status: some macro declarations and uses\nare formatted).\nComments, including any AST node with a comment 'inside' (Rustfmt does not\ncurrently attempt to format comments, it does format code with comments inside, but that formatting may change in the future).\nRust code in code blocks in comments.\nAny fragment of a program (i.e., stability guarantees only apply to whole\nprograms, even where fragments of a program can be formatted today).\nCode containing non-ascii unicode characters (we believe Rustfmt mostly works\nhere, but do not have the test coverage or experience to be 100% sure).\nBugs in Rustfmt (like any software, Rustfmt has bugs, we do not consider bug\nfixes to break our stability guarantees).\nRunning\nYou can run Rustfmt by just typing\nrustfmt filename\nif you used\ncargo install\n. This runs rustfmt on the given file, if the file includes out of line\nmodules, then we reformat those too. So to run on a whole module or crate, you\njust need to run on the root file (usually mod.rs or lib.rs). Rustfmt can also\nread data from stdin. Alternatively, you can use\ncargo fmt\nto format all\nbinary and library targets of your crate.\nYou can run\nrustfmt --help\nfor information about available arguments.\nThe easiest way to run rustfmt against a project is with\ncargo fmt\n.\ncargo fmt\nworks on both\nsingle-crate projects and\ncargo workspaces\n.\nPlease see\ncargo fmt --help\nfor usage information.\nYou can specify the path to your own\nrustfmt\nbinary for cargo to use by setting the\nRUSTFMT\nenvironment variable. This was added in v1.4.22, so you must have this version or newer to leverage this feature (\ncargo fmt --version\n)\nRunning\nrustfmt\ndirectly\nTo format individual files or arbitrary codes from stdin, the\nrustfmt\nbinary should be used. Some\nexamples follow:\nrustfmt lib.rs main.rs\nwill format \"lib.rs\" and \"main.rs\" in place\nrustfmt\nwill read a code from stdin and write formatting to stdout\necho \"fn     main() {}\" | rustfmt\nwould emit \"fn main() {}\".\nFor more information, including arguments and emit options, see\nrustfmt --help\n.\nVerifying code is formatted\nWhen running with\n--check\n, Rustfmt will exit with\n0\nif Rustfmt would not\nmake any formatting changes to the input, and\n1\nif Rustfmt would make changes.\nIn other modes, Rustfmt will exit with\n1\nif there was some error during\nformatting (for example a parsing or internal error) and\n0\nif formatting\ncompleted without error (whether or not changes were made).\nRunning Rustfmt from your editor\nVim\nEmacs\nSublime Text 3\nAtom\nVisual Studio Code\nIntelliJ or CLion\nChecking style on a CI server\nTo keep your code base consistently formatted, it can be helpful to fail the CI build\nwhen a pull request contains unformatted code. Using\n--check\ninstructs\nrustfmt to exit with an error code if the input is not formatted correctly.\nIt will also print any found differences. (Older versions of Rustfmt don't\nsupport\n--check\n, use\n--write-mode diff\n).\nA minimal Travis setup could look like this (requires Rust 1.31.0 or greater):\nlanguage\n:\nrust\nbefore_script\n:\n-\nrustup component add rustfmt\nscript\n:\n-\ncargo build\n-\ncargo test\n-\ncargo fmt --all -- --check\nSee\nthis blog post\nfor more info.\nHow to build and test\ncargo build\nto build.\ncargo test\nto run all tests.\nTo run rustfmt after this, use\ncargo run --bin rustfmt -- filename\n. See the\nnotes above on running rustfmt.\nConfiguring Rustfmt\nRustfmt is designed to be very configurable. You can create a TOML file called\nrustfmt.toml\nor\n.rustfmt.toml\n, place it in the project or any other parent\ndirectory and it will apply the options in that file. See\nrustfmt --help=config\nfor the options which are available, or if you prefer to see\nvisual style previews,\nGitHub page\n.\nBy default, Rustfmt uses a style which conforms to the\nRust style guide\nthat has been formalized through the\nstyle RFC\nprocess\n.\nConfiguration options are either stable or unstable. Stable options can always\nbe used, while unstable ones are only available on a nightly toolchain, and opt-in.\nSee\nGitHub page\nfor details.\nRust's Editions\nThe\nedition\noption determines the Rust language edition used for parsing the code. This is important for syntax compatibility but does not directly control formatting behavior (see\nStyle Editions\n).\nWhen running\ncargo fmt\n, the\nedition\nis automatically read from the\nCargo.toml\nfile. However, when running\nrustfmt\ndirectly, the\nedition\ndefaults to 2015. For consistent parsing between rustfmt and\ncargo fmt\n, you should configure the\nedition\nin your\nrustfmt.toml\nfile:\nedition\n=\n\"\n2018\n\"\nStyle Editions\nThis option is inferred from the\nedition\nif not specified.\nSee\nRust Style Editions\nfor details on formatting differences between style editions.\nrustfmt has a default style edition of\n2015\nwhile\ncargo fmt\ninfers the style edition from the\nedition\nset in\nCargo.toml\n. This can lead to inconsistencies between\nrustfmt\nand\ncargo fmt\nif the style edition is not explicitly configured.\nTo ensure consistent formatting, it is recommended to specify the\nstyle_edition\nin a\nrustfmt.toml\nconfiguration file. For example:\nstyle_edition\n=\n\"\n2024\n\"\nTips\nTo ensure consistent parsing between\ncargo fmt\nand\nrustfmt\n, you should configure the\nedition\nin your\nrustfmt.toml\nfile.\nTo ensure consistent formatting between\ncargo fmt\nand\nrustfmt\n, you should configure the\nstyle_edition\nin your\nrustfmt.toml\nfile.\nFor things you do not want rustfmt to mangle, use\n#[rustfmt::skip]\nTo prevent rustfmt from formatting a macro or an attribute,\nuse\n#[rustfmt::skip::macros(target_macro_name)]\nor\n#[rustfmt::skip::attributes(target_attribute_name)]\nExample:\n#!\n[\nrustfmt\n::\nskip\n::\nattributes\n(\ncustom_attribute\n)\n]\n#\n[\ncustom_attribute\n(\nformatting\n,\nhere\n,\nshould\n,\nbe\n,\nSkipped\n)\n]\n#\n[\nrustfmt\n::\nskip\n::\nmacros\n(\nhtml\n)\n]\nfn\nmain\n(\n)\n{\nlet\nmacro_result1 =\nhtml\n!\n{\n<div>\nHello\n</div>\n}\n.\nto_string\n(\n)\n;\nWhen you run rustfmt, place a file named\nrustfmt.toml\nor\n.rustfmt.toml\nin\ntarget file directory or its parents to override the default settings of\nrustfmt. You can generate a file containing the default configuration with\nrustfmt --print-config default rustfmt.toml\nand customize as needed.\nAfter successful compilation, a\nrustfmt\nexecutable can be found in the\ntarget directory.\nIf you're having issues compiling Rustfmt (or compile errors when trying to\ninstall), make sure you have the most recent version of Rust installed.\nYou can change the way rustfmt emits the changes with the --emit flag:\nExample:\ncargo fmt -- --emit files\nOptions:\nFlag\nDescription\nNightly Only\nfiles\noverwrites output to files\nNo\nstdout\nwrites output to stdout\nNo\ncoverage\ndisplays how much of the input file was processed\nYes\ncheckstyle\nemits in a checkstyle format\nYes\njson\nemits diffs in a json format\nYes\nLicense\nRustfmt is distributed under the terms of both the MIT license and the\nApache License (Version 2.0).\nSee\nLICENSE-APACHE\nand\nLICENSE-MIT\nfor details.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 81",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 6,583"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/rust-lang/rustfmt"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/tile-ai/tilelang",
      "title": "tile-ai/tilelang",
      "date": null,
      "executive_summary": [
        "Domain-specific language designed to streamline the development of high-performance GPU/CPU/Accelerators kernels",
        "---",
        "Tile Language\nTile Language (\ntile-lang\n) is a concise domain-specific language designed to streamline the development of high-performance GPU/CPU kernels (e.g., GEMM, Dequant GEMM, FlashAttention, LinearAttention). By employing a Pythonic syntax with an underlying compiler infrastructure on top of\nTVM\n, tile-lang allows developers to focus on productivity without sacrificing the low-level optimizations necessary for state-of-the-art performance.\nLatest News\n10/07/2025 ğŸ: Added Apple Metal Device support, check out\nPull Request #799\nfor details.\n09/29/2025  ğŸ‰: Thrilled to announce that â€‹â€‹AscendCâ€‹â€‹ and â€‹Ascendâ€‹NPU IRâ€‹â€‹ backends targeting Huawei Ascend chips are now supported!\nCheck out the preview here:\nğŸ”—\nlink\n.\nThis includes implementations across two branches:\nascendc_pto\nand\nnpuir\n.\nFeel free to explore and share your feedback!\n07/04/2025 ğŸš€: Introduced\nT.gemm_sp\nfor 2:4 sparse tensor core support, check out\nPull Request #526\nfor details.\n06/05/2025 âœ¨: Added\nNVRTC Backend\nto significantly reduce compilation time for cute templates!\n04/14/2025 ğŸš€: Added high-performance FlashMLA implementation for AMD MI300X, achieving performance parity with hand-optimized assembly kernels of Aiter! See\nexample_mla_amd\nfor details.\n03/03/2025 ğŸš€: Added high-performance MLA Decoding support using only 80 lines of Python code, achieving performance on par with FlashMLA on H100 (see\nexample_mla_decode.py\n)! We also provide\ndocumentation\nexplaining how TileLang achieves this.\n02/15/2025 âœ¨: Added WebGPU Codegen support, see\nPull Request #86\n!\n02/12/2025 âœ¨: Excited to announce the release of\nv0.1.0\n!\n02/10/2025 ğŸš€: Added debug tools for TileLangâ€”\nT.print\nfor printing variables/buffers (\ndocs\n) and a memory layout plotter (\nexamples/plot_layout\n).\n01/20/2025 âœ¨: We are excited to announce that tile-lang, a dsl for high performance AI workloads, is now open source and available to the public!\nTested Devices\nAlthough tile-lang aims to be portable across a range of Devices, it has been specifically tested and validated on the following devices: for NVIDIA GPUs, this includes the H100 (with Auto TMA/WGMMA support), A100, V100, RTX 4090, RTX 3090, and RTX A6000; for AMD GPUs, it includes the MI250 (with Auto MatrixCore support) and the MI300X (with Async Copy support).\nOP Implementation Examples\ntile-lang\nprovides the building blocks to implement a wide variety of operators. Some examples include:\nMatrix Multiplication\nDequantization GEMM\nFlash Attention\nFlash Linear Attention\nFlash MLA Decoding\nNative Sparse Attention\nWithin the\nexamples\ndirectory, you will also find additional complex kernelsâ€”such as convolutions, forward/backward passes for FlashAttention, more operators will continuously be added.\nBenchmark Summary\nTileLang achieves exceptional performance across a variety of computational patterns. Comprehensive benchmark scripts and settings are available at\ntilelang-benchmark\n. Below are selected results showcasing its capabilities:\nMLA Decoding Performance on H100\nFlash Attention Performance on H100\nMatmul Performance on GPUs (RTX 4090, A100, H100, MI300X)\nDequantize Matmul Performance on A100\nInstallation\nMethod 1: Install with Pip\nThe quickest way to get started is to install the latest release from PyPI:\npip install tilelang\nAlternatively, you can install directly from the GitHub repository:\npip install git+https://github.com/tile-ai/tilelang\nOr install locally:\n#\ninstall required system dependencies\nsudo apt-get update\nsudo apt-get install -y python3-setuptools gcc libtinfo-dev zlib1g-dev build-essential cmake libedit-dev libxml2-dev\n\npip install -e\n.\n-v\n#\nremove -e option if you don't want to install in editable mode, -v for verbose output\nMethod 2: Build from Source\nWe currently provide three ways to install\ntile-lang\nfrom source:\nInstall from Source (using your own TVM installation)\nInstall from Source (using the bundled TVM submodule)\nInstall Using the Provided Script\nMethod 3: Install with Nightly Version\nFor users who want access to the latest features and improvements before official releases, we provide nightly builds of\ntile-lang\n.\npip install tilelang -f https://tile-ai.github.io/whl/nightly/cu121/\n#\nor pip install tilelang --find-links https://tile-ai.github.io/whl/nightly/cu121/\nNote:\nNightly builds contain the most recent code changes but may be less stable than official releases. They're ideal for testing new features or if you need a specific bugfix that hasn't been released yet.\nQuick Start\nIn this section, you'll learn how to write and execute a straightforward GEMM (matrix multiplication) kernel using tile-lang, followed by techniques for layout optimizations, pipelining, and L2-cacheâ€“friendly swizzling.\nGEMM Example with Annotations (Layout, L2 Cache Swizzling, and Pipelining, etc.)\nBelow is an example that demonstrates more advanced features: layout annotation, parallelized copy, and swizzle for improved L2 cache locality. This snippet shows how to adapt your kernel to maximize performance on complex hardware.\nimport\ntilelang\nimport\ntilelang\n.\nlanguage\nas\nT\n# @tilelang.jit(target=\"cuda\")\n# target currently can be \"cuda\" or \"hip\" or \"cpu\".\n# if not specified, it will be inferred from the input tensors during compile time\n@\ntilelang\n.\njit\ndef\nmatmul\n(\nM\n,\nN\n,\nK\n,\nblock_M\n,\nblock_N\n,\nblock_K\n,\ndtype\n=\n\"float16\"\n,\naccum_dtype\n=\n\"float\"\n):\n@\nT\n.\nprim_func\ndef\nmatmul_relu_kernel\n(\nA\n:\nT\n.\nTensor\n((\nM\n,\nK\n),\ndtype\n),\nB\n:\nT\n.\nTensor\n((\nK\n,\nN\n),\ndtype\n),\nC\n:\nT\n.\nTensor\n((\nM\n,\nN\n),\ndtype\n),\n    ):\n# Initialize Kernel Context\nwith\nT\n.\nKernel\n(\nT\n.\nceildiv\n(\nN\n,\nblock_N\n),\nT\n.\nceildiv\n(\nM\n,\nblock_M\n),\nthreads\n=\n128\n)\nas\n(\nbx\n,\nby\n):\nA_shared\n=\nT\n.\nalloc_shared\n((\nblock_M\n,\nblock_K\n),\ndtype\n)\nB_shared\n=\nT\n.\nalloc_shared\n((\nblock_K\n,\nblock_N\n),\ndtype\n)\nC_local\n=\nT\n.\nalloc_fragment\n((\nblock_M\n,\nblock_N\n),\naccum_dtype\n)\n# Enable rasterization for better L2 cache locality (Optional)\n# T.use_swizzle(panel_size=10, enable=True)\n# Clear local accumulation\nT\n.\nclear\n(\nC_local\n)\nfor\nko\nin\nT\n.\nPipelined\n(\nT\n.\nceildiv\n(\nK\n,\nblock_K\n),\nnum_stages\n=\n3\n):\n# Copy tile of A\n# This is a sugar syntax for parallelized copy\nT\n.\ncopy\n(\nA\n[\nby\n*\nblock_M\n,\nko\n*\nblock_K\n],\nA_shared\n)\n# Copy tile of B\nT\n.\ncopy\n(\nB\n[\nko\n*\nblock_K\n,\nbx\n*\nblock_N\n],\nB_shared\n)\n# Perform a tile-level GEMM on the shared buffers\n# Currently we dispatch to the cute/hip on Nvidia/AMD GPUs\nT\n.\ngemm\n(\nA_shared\n,\nB_shared\n,\nC_local\n)\n# relu\nfor\ni\n,\nj\nin\nT\n.\nParallel\n(\nblock_M\n,\nblock_N\n):\nC_local\n[\ni\n,\nj\n]\n=\nT\n.\nmax\n(\nC_local\n[\ni\n,\nj\n],\n0\n)\n# Copy result back to global memory\nT\n.\ncopy\n(\nC_local\n,\nC\n[\nby\n*\nblock_M\n,\nbx\n*\nblock_N\n])\nreturn\nmatmul_relu_kernel\nM\n=\n1024\n# M = T.symbolic(\"m\") if you want to use dynamic shape\nN\n=\n1024\nK\n=\n1024\nblock_M\n=\n128\nblock_N\n=\n128\nblock_K\n=\n32\n# 1. Define the kernel (matmul) and compile/lower it into an executable module\nmatmul_relu_kernel\n=\nmatmul\n(\nM\n,\nN\n,\nK\n,\nblock_M\n,\nblock_N\n,\nblock_K\n)\n# 3. Test the kernel in Python with PyTorch data\nimport\ntorch\n# Create random input tensors on the GPU\na\n=\ntorch\n.\nrandn\n(\nM\n,\nK\n,\ndevice\n=\n\"cuda\"\n,\ndtype\n=\ntorch\n.\nfloat16\n)\nb\n=\ntorch\n.\nrandn\n(\nK\n,\nN\n,\ndevice\n=\n\"cuda\"\n,\ndtype\n=\ntorch\n.\nfloat16\n)\nc\n=\ntorch\n.\nempty\n(\nM\n,\nN\n,\ndevice\n=\n\"cuda\"\n,\ndtype\n=\ntorch\n.\nfloat16\n)\n# Run the kernel through the Profiler\nmatmul_relu_kernel\n(\na\n,\nb\n,\nc\n)\nprint\n(\nc\n)\n# Reference multiplication using PyTorch\nref_c\n=\ntorch\n.\nrelu\n(\na\n@\nb\n)\n# Validate correctness\ntorch\n.\ntesting\n.\nassert_close\n(\nc\n,\nref_c\n,\nrtol\n=\n1e-2\n,\natol\n=\n1e-2\n)\nprint\n(\n\"Kernel output matches PyTorch reference.\"\n)\n# 4. Retrieve and inspect the generated CUDA source (optional)\n# cuda_source = jit_kernel.get_kernel_source()\n# print(\"Generated CUDA kernel:\\n\", cuda_source)\n# 5.Profile latency with kernel\nprofiler\n=\nmatmul_relu_kernel\n.\nget_profiler\n(\ntensor_supply_type\n=\ntilelang\n.\nTensorSupplyType\n.\nNormal\n)\nlatency\n=\nprofiler\n.\ndo_bench\n()\nprint\n(\nf\"Latency:\n{\nlatency\n}\nms\"\n)\nDive Deep into TileLang Beyond GEMM\nIn addition to GEMM, we provide a variety of examples to showcase the versatility and power of TileLang, including:\nDequantize GEMM\n: Achieve high-performance dequantization by\nfine-grained control over per-thread operations\n, with many features now adopted as default behaviors in\nBitBLAS\n, which utilizing magic layout transformation and intrins to accelerate dequantize gemm.\nFlashAttention\n: Enable cross-operator fusion with simple and intuitive syntax, and we also provide an example of auto tuning.\nLinearAttention\n: Examples include RetNet and Mamba implementations.\nConvolution\n: Implementations of Convolution with IM2Col.\nUpcoming Features\nCheck our\ntilelang v0.2.0 release plan\nfor upcoming features.\nTileLang has now been used in project\nBitBLAS\nand\nAttentionEngine\n.\nJoin the Discussion\nWelcome to join our Discord community for discussions, support, and collaboration!\nAcknowledgements\nWe would like to express our gratitude to the\nTVM\ncommunity for their invaluable contributions. The initial version of this project was mainly developed by\nLeiWang1999\n,\nchengyupku\nand\nnox-410\nwith supervision from Prof.\nZhi Yang\nat Peking University. Part of this work was carried out during an internship at Microsoft Research, where Dr. Lingxiao Ma, Dr. Yuqing Xia, Dr. Jilong Xue, and Dr. Fan Yang offered valuable advice and support. We deeply appreciate their mentorship and contributions.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 73",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 3,425"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/tile-ai/tilelang"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/vllm-project/vllm",
      "title": "vllm-project/vllm",
      "date": null,
      "executive_summary": [
        "A high-throughput and memory-efficient inference and serving engine for LLMs",
        "---",
        "Easy, fast, and cheap LLM serving for everyone\n|\nDocumentation\n|\nBlog\n|\nPaper\n|\nTwitter/X\n|\nUser Forum\n|\nDeveloper Slack\n|\nJoin us at the\nPyTorch Conference, October 22-23\nand\nRay Summit, November 3-5\nin San Francisco for our latest updates on vLLM and to meet the vLLM team! Register now for the largest vLLM community events of the year!\nLatest News\nğŸ”¥\n[2025/09] We hosted\nvLLM Toronto Meetup\nfocused on tackling inference at scale and speculative decoding with speakers from NVIDIA and Red Hat! Please find the meetup slides\nhere\n.\n[2025/08] We hosted\nvLLM Shenzhen Meetup\nfocusing on the ecosystem around vLLM! Please find the meetup slides\nhere\n.\n[2025/08] We hosted\nvLLM Singapore Meetup\n. We shared V1 updates, disaggregated serving and MLLM speedups with speakers from Embedded LLM, AMD, WekaIO, and A*STAR. Please find the meetup slides\nhere\n.\n[2025/08] We hosted\nvLLM Shanghai Meetup\nfocusing on building, developing, and integrating with vLLM! Please find the meetup slides\nhere\n.\n[2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement\nhere\n.\n[2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post\nhere\n.\nPrevious News\n[2025/08] We hosted\nvLLM Korea Meetup\nwith Red Hat and Rebellions! We shared the latest advancements in vLLM along with project spotlights from the vLLM Korea community. Please find the meetup slides\nhere\n.\n[2025/08] We hosted\nvLLM Beijing Meetup\nfocusing on large-scale LLM deployment! Please find the meetup slides\nhere\nand the recording\nhere\n.\n[2025/05] We hosted\nNYC vLLM Meetup\n! Please find the meetup slides\nhere\n.\n[2025/04] We hosted\nAsia Developer Day\n! Please find the meetup slides from the vLLM team\nhere\n.\n[2025/03] We hosted\nvLLM x Ollama Inference Night\n! Please find the meetup slides from the vLLM team\nhere\n.\n[2025/03] We hosted\nthe first vLLM China Meetup\n! Please find the meetup slides from vLLM team\nhere\n.\n[2025/03] We hosted\nthe East Coast vLLM Meetup\n! Please find the meetup slides\nhere\n.\n[2025/02] We hosted\nthe ninth vLLM meetup\nwith Meta! Please find the meetup slides from vLLM team\nhere\nand AMD\nhere\n. The slides from Meta will not be posted.\n[2025/01] We hosted\nthe eighth vLLM meetup\nwith Google Cloud! Please find the meetup slides from vLLM team\nhere\n, and Google Cloud team\nhere\n.\n[2024/12] vLLM joins\npytorch ecosystem\n! Easy, Fast, and Cheap LLM Serving for Everyone!\n[2024/11] We hosted\nthe seventh vLLM meetup\nwith Snowflake! Please find the meetup slides from vLLM team\nhere\n, and Snowflake team\nhere\n.\n[2024/10] We have just created a developer slack (\nslack.vllm.ai\n) focusing on coordinating contributions and discussing features. Please feel free to join us there!\n[2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team\nhere\n. Learn more from the\ntalks\nfrom other vLLM contributors and users!\n[2024/09] We hosted\nthe sixth vLLM meetup\nwith NVIDIA! Please find the meetup slides\nhere\n.\n[2024/07] We hosted\nthe fifth vLLM meetup\nwith AWS! Please find the meetup slides\nhere\n.\n[2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post\nhere\n.\n[2024/06] We hosted\nthe fourth vLLM meetup\nwith Cloudflare and BentoML! Please find the meetup slides\nhere\n.\n[2024/04] We hosted\nthe third vLLM meetup\nwith Roblox! Please find the meetup slides\nhere\n.\n[2024/01] We hosted\nthe second vLLM meetup\nwith IBM! Please find the meetup slides\nhere\n.\n[2023/10] We hosted\nthe first vLLM meetup\nwith a16z! Please find the meetup slides\nhere\n.\n[2023/08] We would like to express our sincere gratitude to\nAndreessen Horowitz\n(a16z) for providing a generous grant to support the open-source development and research of vLLM.\n[2023/06] We officially released vLLM! FastChat-vLLM integration has powered\nLMSYS Vicuna and Chatbot Arena\nsince mid-April. Check out our\nblog post\n.\nAbout\nvLLM is a fast and easy-to-use library for LLM inference and serving.\nOriginally developed in the\nSky Computing Lab\nat UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.\nvLLM is fast with:\nState-of-the-art serving throughput\nEfficient management of attention key and value memory with\nPagedAttention\nContinuous batching of incoming requests\nFast model execution with CUDA/HIP graph\nQuantizations:\nGPTQ\n,\nAWQ\n,\nAutoRound\n, INT4, INT8, and FP8\nOptimized CUDA kernels, including integration with FlashAttention and FlashInfer\nSpeculative decoding\nChunked prefill\nvLLM is flexible and easy to use with:\nSeamless integration with popular Hugging Face models\nHigh-throughput serving with various decoding algorithms, including\nparallel sampling\n,\nbeam search\n, and more\nTensor, pipeline, data and expert parallelism support for distributed inference\nStreaming outputs\nOpenAI-compatible API server\nSupport for NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, and TPU. Additionally, support for diverse hardware plugins such as Intel Gaudi, IBM Spyre and Huawei Ascend.\nPrefix caching support\nMulti-LoRA support\nvLLM seamlessly supports most popular open-source models on HuggingFace, including:\nTransformer-like LLMs (e.g., Llama)\nMixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)\nEmbedding Models (e.g., E5-Mistral)\nMulti-modal LLMs (e.g., LLaVA)\nFind the full list of supported models\nhere\n.\nGetting Started\nInstall vLLM with\npip\nor\nfrom source\n:\npip install vllm\nVisit our\ndocumentation\nto learn more.\nInstallation\nQuickstart\nList of Supported Models\nContributing\nWe welcome and value any contributions and collaborations.\nPlease check out\nContributing to vLLM\nfor how to get involved.\nSponsors\nvLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!\nCash Donations:\na16z\nDropbox\nSequoia Capital\nSkywork AI\nZhenFund\nCompute Resources:\nAlibaba Cloud\nAMD\nAnyscale\nAWS\nCrusoe Cloud\nDatabricks\nDeepInfra\nGoogle Cloud\nIntel\nLambda Lab\nNebius\nNovita AI\nNVIDIA\nReplicate\nRoblox\nRunPod\nTrainy\nUC Berkeley\nUC San Diego\nVolcengine\nSlack Sponsor: Anyscale\nWe also have an official fundraising venue through\nOpenCollective\n. We plan to use the fund to support the development, maintenance, and adoption of vLLM.\nCitation\nIf you use vLLM for your research, please cite our\npaper\n:\n@inproceedings\n{\nkwon2023efficient\n,\ntitle\n=\n{\nEfficient Memory Management for Large Language Model Serving with PagedAttention\n}\n,\nauthor\n=\n{\nWoosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica\n}\n,\nbooktitle\n=\n{\nProceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles\n}\n,\nyear\n=\n{\n2023\n}\n}\nContact Us\nFor technical questions and feature requests, please use GitHub\nIssues\nFor discussing with fellow users, please use the\nvLLM Forum\nFor coordinating contributions and development, please use\nSlack\nFor security disclosures, please use GitHub's\nSecurity Advisories\nfeature\nFor collaborations and partnerships, please contact us at\nvllm-questions@lists.berkeley.edu\nMedia Kit\nIf you wish to use vLLM's logo, please refer to\nour media kit repo",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 70",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 59,787"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/vllm-project/vllm"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/aaPanel/BillionMail",
      "title": "aaPanel/BillionMail",
      "date": null,
      "executive_summary": [
        "BillionMail gives you open-source MailServer, NewsLetter, Email Marketing â€” fully self-hosted, dev-friendly, and free from monthly fees. Join the discord: https://discord.gg/asfXzBUhZr",
        "---",
        "BillionMail ğŸ“§\nAn Open-Source MailServer, NewsLetter, Email Marketing Solution for Smarter Campaigns\nEnglish |\nç®€ä½“ä¸­æ–‡\n|\næ—¥æœ¬èª\n|\nTÃ¼rkÃ§e\nWhat is BillionMail?\nBillionMail is a\nfuture open-source Mail server, Email marketing platform\ndesigned to help businesses and individuals manage their email campaigns with ease. Whether you're sending newsletters, promotional emails, or transactional messages, this tool will provide\nfull control\nover your email marketing efforts. With features like\nadvanced analytics\n, and\ncustomer management\n, you'll be able to create, send, and track emails like a pro.\nJust 3 steps to send a billion emails!\nBillion emails. Any business. Guaranteed.\nStep 1ï¸âƒ£ Install BillionMail:\nâœ… It takes\nonly 8ï¸âƒ£ minutes\nfrom installation to\nâœ… successful email sending\ncd\n/opt\n&&\ngit clone https://github.com/aaPanel/BillionMail\n&&\ncd\nBillionMail\n&&\nbash install.sh\nStep 2ï¸âƒ£: Connect Your Domain\nAdd the sending domain\nVerify DNS records\nAuto-enable free SSL\nStep 3ï¸âƒ£: Build Your Campaign\nWrite or paste your email\nChoose list & tags\nSet send time or send now\nWatch on Youtube\nOther installation methods\nOne-click installation on aaPanel\nğŸ‘‰\nhttps://www.aapanel.com/new/download.html\n(Log in to âœ…aaPanel --> ğŸ³Docker --> 1ï¸âƒ£OneClick install)\nDocker\ncd\n/opt\n&&\ngit clone https://github.com/aaPanel/BillionMail\n&&\ncd\nBillionMail\n&&\ncp env_init .env\n&&\ndocker compose up -d\n||\ndocker-compose up -d\nManagement script\nManagement help\nbm help\nView Login default info\nbm default\nShow domain DNS record\nbm show-record\nUpdate BillionMail\nbm update\nLive Demo\nBillionMail Demo:\nhttps://demo.billionmail.com/billionmail\nUsername:\nbillionmail\nPassword:\nbillionmail\nWebMail\nBillionMail has integrated\nRoundCube\n, you can access WebMail via\n/roundcube/\n.\nWhy BillionMail?\nMost email marketing platforms are either\nexpensive\n,\nclosed-source\n, or\nlack essential features\n. BillionMail aims to be different:\nâœ…\nFully Open-Source\nâ€“ No hidden costs, no vendor lock-in.\nğŸ“Š\nAdvanced Analytics\nâ€“ Track email delivery, open rates, click-through rates, and more.\nğŸ“§\nUnlimited Sending\nâ€“ No restrictions on the number of emails you can send.\nğŸ¨\nCustomizable Templates\nâ€“ Custom professional marketing templates for reuse.\nğŸ”’\nPrivacy-First\nâ€“ Your data stays with you, no third-party tracking.\nğŸš€\nSelf-Hosted\nâ€“ Run it on your own server for complete control.\nHow You Can Help ğŸŒŸ\nBillionMail is a\ncommunity-driven project\n, and we need your support to get started! Here's how you can help:\nStar This Repository\n: Show your interest by starring this repo.\nSpread the Word\n: Share BillionMail with your networkâ€”developers, marketers, and open-source enthusiasts.\nShare Feedback\n: Let us know what features you'd like to see in BillionMail by opening an issue or joining the discussion.\nContribute\n: Once development begins, we'll welcome contributions from the community. Stay tuned for updates!\nğŸ“§\nBillionMail â€“ The Future of Open-Source Email Marketing.\nIssues\nIf you encounter any issues or have feature requests, please\nopen an issue\n. Be sure to include:\nA clear description of the problem or request.\nSteps to reproduce the issue (if applicable).\nScreenshots or error logs (if applicable).\nInstall Now:\nâœ…It takes\nonly 8 minutes\nfrom installation to\nsuccessful email sending\ncd\n/opt\n&&\ngit clone https://github.com/aaPanel/BillionMail\n&&\ncd\nBillionMail\n&&\nbash install.sh\nInstall with Docker:\n(Please install Docker and docker-compose-plugin manually, and modify .env file)\ncd\n/opt\n&&\ngit clone https://github.com/aaPanel/BillionMail\n&&\ncd\nBillionMail\n&&\ncp env_init .env\n&&\ndocker compose up -d\n||\ndocker-compose up -d\nStar History\nLicense\nBillionMail is licensed under the\nAGPLv3 License\n. This means you can:\nâœ… Use the software for free.\nâœ… Modify and distribute the code.\nâœ… Use it privately without restrictions.\nSee the\nLICENSE\nfile for more details.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 65",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 11,612"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/aaPanel/BillionMail"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/supermemoryai/supermemory",
      "title": "supermemoryai/supermemory",
      "date": null,
      "executive_summary": [
        "Memory engine and app that is extremely fast, scalable. The Memory API for the AI era.",
        "---",
        "Features\nCore Functionality\nAdd Memories from Any Content\n: Easily add memories from URLs, PDFs, and plain textâ€”just paste, upload, or link.\nChat with Your Memories\n: Converse with your stored content using natural language chat.\nSupermemory MCP Integration\n: Seamlessly connect with all major AI tools (Claude, Cursor, etc.) via Supermemory MCP.\nHow do i use this?\nGo to\napp.supermemory.ai\nand sign into with your account\nStart Adding Memory with your choice of format (Note, Link, File)\nYou can also Connect to your favourite services (Notion, Google Drive, OneDrive)\nOnce Memories are added, you can chat with Supermemory by clicking on \"Open Chat\" and retrieve info from your saved memories\nAdd MCP to your AI Tools (by clicking on \"Connect to your AI\" and select the AI tool you are trying to integrate)\nSupport\nHave questions or feedback? We're here to help:\nEmail:\ndhravya@supermemory.com\nDocumentation:\ndocs.supermemory.ai\nContributing\nWe welcome contributions from developers of all skill levels! Whether you're fixing bugs, adding features, or improving documentation, your help makes supermemory better for everyone.\nQuick Start for Contributors\nFork and clone\nthe repository\nInstall dependencies\nwith\nbun install\nSet up your environment\nby copying\n.env.example\nto\n.env.local\nStart developing\nwith\nbun run dev\nFor detailed guidelines, development setup, coding standards, and the complete contribution workflow, please see our\nContributing Guide\n.\nWays to Contribute\nğŸ›\nBug fixes\n- Help us squash those pesky issues\nâœ¨\nNew features\n- Add functionality that users will love\nğŸ¨\nUI/UX improvements\n- Make the interface more intuitive\nâš¡\nPerformance optimizations\n- Help us make supermemory faster\nCheck out our\nIssues\npage for\ngood first issue\nand\nhelp wanted\nlabels to get started!\nUpdates & Roadmap\nStay up to date with the latest improvements:\nChangelog\nX\n.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 62",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 11,473"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/supermemoryai/supermemory"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/chen08209/FlClash",
      "title": "chen08209/FlClash",
      "date": null,
      "executive_summary": [
        "A multi-platform proxy client based on ClashMeta,simple and easy to use, open-source and ad-free.",
        "---",
        "ç®€ä½“ä¸­æ–‡\nFlClash\nA multi-platform proxy client based on ClashMeta, simple and easy to use, open-source and ad-free.\non Desktop:\non Mobile:\nFeatures\nâœˆï¸\nMulti-platform: Android, Windows, macOS and Linux\nğŸ’» Adaptive multiple screen sizes, Multiple color themes available\nğŸ’¡ Based on Material You Design,\nSurfboard\n-like UI\nâ˜ï¸ Supports data sync via WebDAV\nâœ¨ Support subscription link, Dark mode\nUse\nLinux\nâš ï¸\nMake sure to install the following dependencies before using them\nsudo apt-get install libayatana-appindicator3-dev\n sudo apt-get install libkeybinder-3.0-dev\nAndroid\nSupport the following actions\ncom.follow.clash.action.START\n \n com.follow.clash.action.STOP\n \n com.follow.clash.action.TOGGLE\nDownload\nBuild\nUpdate submodules\ngit submodule update --init --recursive\nInstall\nFlutter\nand\nGolang\nenvironment\nBuild Application\nandroid\nInstall\nAndroid SDK\n,\nAndroid NDK\nSet\nANDROID_NDK\nenvironment variables\nRun Build script\ndart .\n\\s\netup.dart android\nwindows\nYou need a windows client\nInstall\nGcc\nï¼Œ\nInno Setup\nRun build script\ndart .\n\\s\netup.dart windows --arch\n<\narm64\n|\namd\n64>\nlinux\nYou need a linux client\nRun build script\ndart .\n\\s\netup.dart linux --arch\n<\narm64\n|\namd\n64>\nmacOS\nYou need a macOS client\nRun build script\ndart .\n\\s\netup.dart macos --arch\n<\narm64\n|\namd\n64>\nStar\nThe easiest way to support developers is to click on the star (â­) at the top of the page.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 61",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 23,044"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/chen08209/FlClash"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/jiji262/douyin-downloader",
      "title": "jiji262/douyin-downloader",
      "date": null,
      "executive_summary": [
        "æŠ–éŸ³æ‰¹é‡ä¸‹è½½å·¥å…·ï¼Œå»æ°´å°ï¼Œæ”¯æŒè§†é¢‘ã€å›¾é›†ã€åˆé›†ã€éŸ³ä¹(åŸå£°)ã€‚å…è´¹ï¼å…è´¹ï¼å…è´¹ï¼",
        "---",
        "æŠ–éŸ³ä¸‹è½½å™¨ - æ— æ°´å°æ‰¹é‡ä¸‹è½½å·¥å…·\nä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„æŠ–éŸ³å†…å®¹æ‰¹é‡ä¸‹è½½å·¥å…·ï¼Œæ”¯æŒè§†é¢‘ã€å›¾é›†ã€éŸ³ä¹ã€ç›´æ’­ç­‰å¤šç§å†…å®¹ç±»å‹çš„ä¸‹è½½ã€‚æä¾›ä¸¤ä¸ªç‰ˆæœ¬ï¼šV1.0ï¼ˆç¨³å®šç‰ˆï¼‰å’Œ V2.0ï¼ˆå¢å¼ºç‰ˆï¼‰ã€‚\nğŸ“‹ ç›®å½•\nå¿«é€Ÿå¼€å§‹\nç‰ˆæœ¬è¯´æ˜\nV1.0 ä½¿ç”¨æŒ‡å—\nV2.0 ä½¿ç”¨æŒ‡å—\nCookie é…ç½®å·¥å…·\næ”¯æŒçš„é“¾æ¥ç±»å‹\nå¸¸è§é—®é¢˜\næ›´æ–°æ—¥å¿—\nâš¡ å¿«é€Ÿå¼€å§‹\nç¯å¢ƒè¦æ±‚\nPython 3.9+\næ“ä½œç³»ç»Ÿ\nï¼šWindowsã€macOSã€Linux\nå®‰è£…æ­¥éª¤\nå…‹éš†é¡¹ç›®\ngit clone https://github.com/jiji262/douyin-downloader.git\ncd\ndouyin-downloader\nå®‰è£…ä¾èµ–\npip install -r requirements.txt\né…ç½® Cookie\nï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦ï¼‰\n#\næ–¹å¼1ï¼šè‡ªåŠ¨è·å–ï¼ˆæ¨èï¼‰\npython cookie_extractor.py\n#\næ–¹å¼2ï¼šæ‰‹åŠ¨è·å–\npython get_cookies_manual.py\nğŸ“¦ ç‰ˆæœ¬è¯´æ˜\nV1.0 (DouYinCommand.py) - ç¨³å®šç‰ˆ\nâœ…\nç»è¿‡éªŒè¯\nï¼šç¨³å®šå¯é ï¼Œç»è¿‡å¤§é‡æµ‹è¯•\nâœ…\nç®€å•æ˜“ç”¨\nï¼šé…ç½®æ–‡ä»¶é©±åŠ¨ï¼Œä½¿ç”¨ç®€å•\nâœ…\nåŠŸèƒ½å®Œæ•´\nï¼šæ”¯æŒæ‰€æœ‰å†…å®¹ç±»å‹ä¸‹è½½\nâœ…\nå•ä¸ªè§†é¢‘ä¸‹è½½\nï¼šå®Œå…¨æ­£å¸¸å·¥ä½œ\nâš ï¸\néœ€è¦æ‰‹åŠ¨é…ç½®\nï¼šéœ€è¦æ‰‹åŠ¨è·å–å’Œé…ç½® Cookie\nV2.0 (downloader.py) - å¢å¼ºç‰ˆ\nğŸš€\nè‡ªåŠ¨ Cookie ç®¡ç†\nï¼šæ”¯æŒè‡ªåŠ¨è·å–å’Œåˆ·æ–° Cookie\nğŸš€\nç»Ÿä¸€å…¥å£\nï¼šæ•´åˆæ‰€æœ‰åŠŸèƒ½åˆ°å•ä¸€è„šæœ¬\nğŸš€\nå¼‚æ­¥æ¶æ„\nï¼šæ€§èƒ½æ›´ä¼˜ï¼Œæ”¯æŒå¹¶å‘ä¸‹è½½\nğŸš€\næ™ºèƒ½é‡è¯•\nï¼šè‡ªåŠ¨é‡è¯•å’Œé”™è¯¯æ¢å¤\nğŸš€\nå¢é‡ä¸‹è½½\nï¼šæ”¯æŒå¢é‡æ›´æ–°ï¼Œé¿å…é‡å¤ä¸‹è½½\nâš ï¸\nå•ä¸ªè§†é¢‘ä¸‹è½½\nï¼šç›®å‰ API è¿”å›ç©ºå“åº”ï¼ˆå·²çŸ¥é—®é¢˜ï¼‰\nâœ…\nç”¨æˆ·ä¸»é¡µä¸‹è½½\nï¼šå®Œå…¨æ­£å¸¸å·¥ä½œ\nğŸ¯ V1.0 ä½¿ç”¨æŒ‡å—\né…ç½®æ–‡ä»¶è®¾ç½®\nç¼–è¾‘é…ç½®æ–‡ä»¶\ncp config.example.yml config.yml\n#\nç¼–è¾‘ config.yml æ–‡ä»¶\né…ç½®ç¤ºä¾‹\n#\nä¸‹è½½é“¾æ¥\nlink\n:\n  -\nhttps://v.douyin.com/xxxxx/\n#\nå•ä¸ªè§†é¢‘\n-\nhttps://www.douyin.com/user/xxxxx\n#\nç”¨æˆ·ä¸»é¡µ\n-\nhttps://www.douyin.com/collection/xxxxx\n#\nåˆé›†\n#\nä¿å­˜è·¯å¾„\npath\n:\n./Downloaded/\n#\nCookieé…ç½®ï¼ˆå¿…å¡«ï¼‰\ncookies\n:\nmsToken\n:\nYOUR_MS_TOKEN_HERE\nttwid\n:\nYOUR_TTWID_HERE\nodin_tt\n:\nYOUR_ODIN_TT_HERE\npassport_csrf_token\n:\nYOUR_PASSPORT_CSRF_TOKEN_HERE\nsid_guard\n:\nYOUR_SID_GUARD_HERE\n#\nä¸‹è½½é€‰é¡¹\nmusic\n:\nTrue\n#\nä¸‹è½½éŸ³ä¹\ncover\n:\nTrue\n#\nä¸‹è½½å°é¢\navatar\n:\nTrue\n#\nä¸‹è½½å¤´åƒ\njson\n:\nTrue\n#\nä¿å­˜JSONæ•°æ®\n#\nä¸‹è½½æ¨¡å¼\nmode\n:\n  -\npost\n#\nä¸‹è½½å‘å¸ƒçš„ä½œå“\n#\n- like     # ä¸‹è½½å–œæ¬¢çš„ä½œå“\n#\n- mix      # ä¸‹è½½åˆé›†\n#\nä¸‹è½½æ•°é‡ï¼ˆ0è¡¨ç¤ºå…¨éƒ¨ï¼‰\nnumber\n:\npost\n:\n0\n#\nå‘å¸ƒä½œå“æ•°é‡\nlike\n:\n0\n#\nå–œæ¬¢ä½œå“æ•°é‡\nallmix\n:\n0\n#\nåˆé›†æ•°é‡\nmix\n:\n0\n#\nå•ä¸ªåˆé›†å†…ä½œå“æ•°é‡\n#\nå…¶ä»–è®¾ç½®\nthread\n:\n5\n#\nä¸‹è½½çº¿ç¨‹æ•°\ndatabase\n:\nTrue\n#\nä½¿ç”¨æ•°æ®åº“è®°å½•\nè¿è¡Œç¨‹åº\n#\nä½¿ç”¨é…ç½®æ–‡ä»¶è¿è¡Œ\npython DouYinCommand.py\n#\næˆ–è€…ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°\npython DouYinCommand.py --cmd False\nä½¿ç”¨ç¤ºä¾‹\n#\nä¸‹è½½å•ä¸ªè§†é¢‘\n#\nåœ¨ config.yml ä¸­è®¾ç½® link ä¸ºå•ä¸ªè§†é¢‘é“¾æ¥\npython DouYinCommand.py\n#\nä¸‹è½½ç”¨æˆ·ä¸»é¡µ\n#\nåœ¨ config.yml ä¸­è®¾ç½® link ä¸ºç”¨æˆ·ä¸»é¡µé“¾æ¥\npython DouYinCommand.py\n#\nä¸‹è½½åˆé›†\n#\nåœ¨ config.yml ä¸­è®¾ç½® link ä¸ºåˆé›†é“¾æ¥\npython DouYinCommand.py\nğŸš€ V2.0 ä½¿ç”¨æŒ‡å—\nå‘½ä»¤è¡Œä½¿ç”¨\n#\nä¸‹è½½å•ä¸ªè§†é¢‘ï¼ˆéœ€è¦å…ˆé…ç½® Cookieï¼‰\npython downloader.py -u\n\"\nhttps://v.douyin.com/xxxxx/\n\"\n#\nä¸‹è½½ç”¨æˆ·ä¸»é¡µï¼ˆæ¨èï¼‰\npython downloader.py -u\n\"\nhttps://www.douyin.com/user/xxxxx\n\"\n#\nè‡ªåŠ¨è·å– Cookie å¹¶ä¸‹è½½\npython downloader.py --auto-cookie -u\n\"\nhttps://www.douyin.com/user/xxxxx\n\"\n#\næŒ‡å®šä¿å­˜è·¯å¾„\npython downloader.py -u\n\"\né“¾æ¥\n\"\n--path\n\"\n./my_videos/\n\"\n#\nä½¿ç”¨é…ç½®æ–‡ä»¶\npython downloader.py --config\né…ç½®æ–‡ä»¶ä½¿ç”¨\nåˆ›å»ºé…ç½®æ–‡ä»¶\ncp config.example.yml config_simple.yml\né…ç½®ç¤ºä¾‹\n#\nä¸‹è½½é“¾æ¥\nlink\n:\n  -\nhttps://www.douyin.com/user/xxxxx\n#\nä¿å­˜è·¯å¾„\npath\n:\n./Downloaded/\n#\nè‡ªåŠ¨ Cookie ç®¡ç†\nauto_cookie\n:\ntrue\n#\nä¸‹è½½é€‰é¡¹\nmusic\n:\ntrue\ncover\n:\ntrue\navatar\n:\ntrue\njson\n:\ntrue\n#\nä¸‹è½½æ¨¡å¼\nmode\n:\n  -\npost\n#\nä¸‹è½½æ•°é‡\nnumber\n:\npost\n:\n10\n#\nå¢é‡ä¸‹è½½\nincrease\n:\npost\n:\nfalse\n#\næ•°æ®åº“\ndatabase\n:\ntrue\nè¿è¡Œç¨‹åº\npython downloader.py --config\nå‘½ä»¤è¡Œå‚æ•°\npython downloader.py [é€‰é¡¹] [é“¾æ¥...]\n\né€‰é¡¹ï¼š\n  -u, --url URL          ä¸‹è½½é“¾æ¥\n  -p, --path PATH        ä¿å­˜è·¯å¾„\n  -c, --config           ä½¿ç”¨é…ç½®æ–‡ä»¶\n  --auto-cookie          è‡ªåŠ¨è·å– Cookie\n  --cookies COOKIES      æ‰‹åŠ¨æŒ‡å®š Cookie\n  -h, --help            æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯\nğŸª Cookie é…ç½®å·¥å…·\n1. cookie_extractor.py - è‡ªåŠ¨è·å–å·¥å…·\nåŠŸèƒ½\nï¼šä½¿ç”¨ Playwright è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼Œè‡ªåŠ¨è·å– Cookie\nä½¿ç”¨æ–¹å¼\nï¼š\n#\nå®‰è£… Playwright\npip install playwright\nplaywright install chromium\n#\nè¿è¡Œè‡ªåŠ¨è·å–\npython cookie_extractor.py\nç‰¹ç‚¹\nï¼š\nâœ… è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨\nâœ… æ”¯æŒæ‰«ç ç™»å½•\nâœ… è‡ªåŠ¨æ£€æµ‹ç™»å½•çŠ¶æ€\nâœ… è‡ªåŠ¨ä¿å­˜åˆ°é…ç½®æ–‡ä»¶\nâœ… æ”¯æŒå¤šç§ç™»å½•æ–¹å¼\nä½¿ç”¨æ­¥éª¤\nï¼š\nè¿è¡Œ\npython cookie_extractor.py\né€‰æ‹©æå–æ–¹å¼ï¼ˆæ¨èé€‰æ‹©1ï¼‰\nåœ¨æ‰“å¼€çš„æµè§ˆå™¨ä¸­å®Œæˆç™»å½•\nç¨‹åºè‡ªåŠ¨æå–å¹¶ä¿å­˜ Cookie\n2. get_cookies_manual.py - æ‰‹åŠ¨è·å–å·¥å…·\nåŠŸèƒ½\nï¼šé€šè¿‡æµè§ˆå™¨å¼€å‘è€…å·¥å…·æ‰‹åŠ¨è·å– Cookie\nä½¿ç”¨æ–¹å¼\nï¼š\npython get_cookies_manual.py\nç‰¹ç‚¹\nï¼š\nâœ… æ— éœ€å®‰è£… Playwright\nâœ… è¯¦ç»†çš„æ“ä½œæ•™ç¨‹\nâœ… æ”¯æŒ Cookie éªŒè¯\nâœ… è‡ªåŠ¨ä¿å­˜åˆ°é…ç½®æ–‡ä»¶\nâœ… æ”¯æŒå¤‡ä»½å’Œæ¢å¤\nä½¿ç”¨æ­¥éª¤\nï¼š\nè¿è¡Œ\npython get_cookies_manual.py\né€‰æ‹©\"è·å–æ–°çš„Cookie\"\næŒ‰ç…§æ•™ç¨‹åœ¨æµè§ˆå™¨ä¸­è·å– Cookie\nç²˜è´´ Cookie å†…å®¹\nç¨‹åºè‡ªåŠ¨è§£æå¹¶ä¿å­˜\nCookie è·å–æ•™ç¨‹\næ–¹æ³•ä¸€ï¼šæµè§ˆå™¨å¼€å‘è€…å·¥å…·\næ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—®\næŠ–éŸ³ç½‘é¡µç‰ˆ\nç™»å½•ä½ çš„æŠ–éŸ³è´¦å·\næŒ‰\nF12\næ‰“å¼€å¼€å‘è€…å·¥å…·\nåˆ‡æ¢åˆ°\nNetwork\næ ‡ç­¾é¡µ\nåˆ·æ–°é¡µé¢ï¼Œæ‰¾åˆ°ä»»æ„è¯·æ±‚\nåœ¨è¯·æ±‚å¤´ä¸­æ‰¾åˆ°\nCookie\nå­—æ®µ\nå¤åˆ¶ä»¥ä¸‹å…³é”® cookie å€¼ï¼š\nmsToken\nttwid\nodin_tt\npassport_csrf_token\nsid_guard\næ–¹æ³•äºŒï¼šä½¿ç”¨è‡ªåŠ¨å·¥å…·\n#\næ¨èä½¿ç”¨è‡ªåŠ¨å·¥å…·\npython cookie_extractor.py\nğŸ“‹ æ”¯æŒçš„é“¾æ¥ç±»å‹\nğŸ¬ è§†é¢‘å†…å®¹\nå•ä¸ªè§†é¢‘åˆ†äº«é“¾æ¥\nï¼š\nhttps://v.douyin.com/xxxxx/\nå•ä¸ªè§†é¢‘ç›´é“¾\nï¼š\nhttps://www.douyin.com/video/xxxxx\nå›¾é›†ä½œå“\nï¼š\nhttps://www.douyin.com/note/xxxxx\nğŸ‘¤ ç”¨æˆ·å†…å®¹\nç”¨æˆ·ä¸»é¡µ\nï¼š\nhttps://www.douyin.com/user/xxxxx\næ”¯æŒä¸‹è½½ç”¨æˆ·å‘å¸ƒçš„æ‰€æœ‰ä½œå“\næ”¯æŒä¸‹è½½ç”¨æˆ·å–œæ¬¢çš„ä½œå“ï¼ˆéœ€è¦æƒé™ï¼‰\nğŸ“š åˆé›†å†…å®¹\nç”¨æˆ·åˆé›†\nï¼š\nhttps://www.douyin.com/collection/xxxxx\néŸ³ä¹åˆé›†\nï¼š\nhttps://www.douyin.com/music/xxxxx\nğŸ”´ ç›´æ’­å†…å®¹\nç›´æ’­é—´\nï¼š\nhttps://live.douyin.com/xxxxx\nğŸ”§ å¸¸è§é—®é¢˜\nQ: ä¸ºä»€ä¹ˆå•ä¸ªè§†é¢‘ä¸‹è½½å¤±è´¥ï¼Ÿ\nA\n:\nV1.0ï¼šè¯·æ£€æŸ¥ Cookie æ˜¯å¦æœ‰æ•ˆï¼Œç¡®ä¿åŒ…å«å¿…è¦çš„å­—æ®µ\nV2.0ï¼šç›®å‰å·²çŸ¥é—®é¢˜ï¼ŒAPI è¿”å›ç©ºå“åº”ï¼Œå»ºè®®ä½¿ç”¨ç”¨æˆ·ä¸»é¡µä¸‹è½½\nQ: Cookie è¿‡æœŸæ€ä¹ˆåŠï¼Ÿ\nA\n:\nä½¿ç”¨\npython cookie_extractor.py\né‡æ–°è·å–\næˆ–ä½¿ç”¨\npython get_cookies_manual.py\næ‰‹åŠ¨è·å–\nQ: ä¸‹è½½é€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ\nA\n:\nè°ƒæ•´\nthread\nå‚æ•°å¢åŠ å¹¶å‘æ•°\næ£€æŸ¥ç½‘ç»œè¿æ¥\né¿å…åŒæ—¶ä¸‹è½½è¿‡å¤šå†…å®¹\nQ: å¦‚ä½•æ‰¹é‡ä¸‹è½½ï¼Ÿ\nA\n:\nV1.0ï¼šåœ¨\nconfig.yml\nä¸­æ·»åŠ å¤šä¸ªé“¾æ¥\nV2.0ï¼šä½¿ç”¨å‘½ä»¤è¡Œä¼ å…¥å¤šä¸ªé“¾æ¥æˆ–ä½¿ç”¨é…ç½®æ–‡ä»¶\nQ: æ”¯æŒå“ªäº›æ ¼å¼ï¼Ÿ\nA\n:\nè§†é¢‘ï¼šMP4 æ ¼å¼ï¼ˆæ— æ°´å°ï¼‰\nå›¾ç‰‡ï¼šJPG æ ¼å¼\néŸ³é¢‘ï¼šMP3 æ ¼å¼\næ•°æ®ï¼šJSON æ ¼å¼\nğŸ“ æ›´æ–°æ—¥å¿—\nV2.0 (2025-08)\nâœ…\nç»Ÿä¸€å…¥å£\nï¼šæ•´åˆæ‰€æœ‰åŠŸèƒ½åˆ°\ndownloader.py\nâœ…\nè‡ªåŠ¨ Cookie ç®¡ç†\nï¼šæ”¯æŒè‡ªåŠ¨è·å–å’Œåˆ·æ–°\nâœ…\nå¼‚æ­¥æ¶æ„\nï¼šæ€§èƒ½ä¼˜åŒ–ï¼Œæ”¯æŒå¹¶å‘ä¸‹è½½\nâœ…\næ™ºèƒ½é‡è¯•\nï¼šè‡ªåŠ¨é‡è¯•å’Œé”™è¯¯æ¢å¤\nâœ…\nå¢é‡ä¸‹è½½\nï¼šæ”¯æŒå¢é‡æ›´æ–°\nâœ…\nç”¨æˆ·ä¸»é¡µä¸‹è½½\nï¼šå®Œå…¨æ­£å¸¸å·¥ä½œ\nâš ï¸\nå•ä¸ªè§†é¢‘ä¸‹è½½\nï¼šAPI è¿”å›ç©ºå“åº”ï¼ˆå·²çŸ¥é—®é¢˜ï¼‰\nV1.0 (2024-12)\nâœ…\nç¨³å®šå¯é \nï¼šç»è¿‡å¤§é‡æµ‹è¯•éªŒè¯\nâœ…\nåŠŸèƒ½å®Œæ•´\nï¼šæ”¯æŒæ‰€æœ‰å†…å®¹ç±»å‹\nâœ…\nå•ä¸ªè§†é¢‘ä¸‹è½½\nï¼šå®Œå…¨æ­£å¸¸å·¥ä½œ\nâœ…\né…ç½®æ–‡ä»¶é©±åŠ¨\nï¼šç®€å•æ˜“ç”¨\nâœ…\næ•°æ®åº“æ”¯æŒ\nï¼šè®°å½•ä¸‹è½½å†å²\nâš–ï¸ æ³•å¾‹å£°æ˜\næœ¬é¡¹ç›®ä»…ä¾›\nå­¦ä¹ äº¤æµ\nä½¿ç”¨\nè¯·éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„å’Œå¹³å°æœåŠ¡æ¡æ¬¾\nä¸å¾—ç”¨äºå•†ä¸šç”¨é€”æˆ–ä¾µçŠ¯ä»–äººæƒç›Š\nä¸‹è½½å†…å®¹è¯·å°Šé‡åŸä½œè€…ç‰ˆæƒ\nğŸ¤ è´¡çŒ®æŒ‡å—\næ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼\næŠ¥å‘Šé—®é¢˜\nä½¿ç”¨\nIssues\næŠ¥å‘Š bug\nè¯·æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œå¤ç°æ­¥éª¤\nåŠŸèƒ½å»ºè®®\nåœ¨ Issues ä¸­æå‡ºæ–°åŠŸèƒ½å»ºè®®\nè¯¦ç»†æè¿°åŠŸèƒ½éœ€æ±‚å’Œä½¿ç”¨åœºæ™¯\nğŸ“„ è®¸å¯è¯\næœ¬é¡¹ç›®é‡‡ç”¨\nMIT License\nå¼€æºè®¸å¯è¯ã€‚\nå¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª â­ Star æ”¯æŒä¸€ä¸‹ï¼\nğŸ› æŠ¥å‘Šé—®é¢˜\nâ€¢\nğŸ’¡ åŠŸèƒ½å»ºè®®\nâ€¢\nğŸ“– æŸ¥çœ‹æ–‡æ¡£\nMade with â¤ï¸ by\njiji262",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 60",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 5,265"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/jiji262/douyin-downloader"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/wmjordan/PDFPatcher",
      "title": "wmjordan/PDFPatcher",
      "date": null,
      "executive_summary": [
        "PDFè¡¥ä¸ä¸â€”â€”PDFå·¥å…·ç®±ï¼Œå¯ä»¥ç¼–è¾‘ä¹¦ç­¾ã€å‰ªè£æ—‹è½¬é¡µé¢ã€è§£é™¤é™åˆ¶ã€æå–æˆ–åˆå¹¶æ–‡æ¡£ï¼Œæ¢æŸ¥æ–‡æ¡£ç»“æ„ï¼Œæå–å›¾ç‰‡ã€è½¬æˆå›¾ç‰‡ç­‰ç­‰",
        "---",
        "PDF è¡¥ä¸ä¸ï¼ˆPDFPatcherï¼‰\næ„Ÿè°¢æ‚¨å…³æ³¨ PDF è¡¥ä¸ä¸ï¼Œè¯·åœ¨ä½¿ç”¨è½¯ä»¶æˆ–æºä»£ç å‰é˜…è¯»æœ¬è¯´æ˜å’Œæˆæƒåè®®ã€‚æœ¬è½¯ä»¶åŠæºä»£ç é‡‡ç”¨ AGPLï¼‹â€œ\nè‰¯å¿ƒæˆæƒ\nâ€åè®®â€”â€”\nç”¨æˆ·æ¯æ¬¡ä½¿ç”¨æœ¬è½¯ä»¶åå¦‚æœ‰æ‰€è·ç›Šï¼Œåº”è¡Œä¸€å–„äº‹ï¼›å¦‚ä½¿ç”¨æºä»£ç å¼€å‘äº†æ–°çš„è½¯ä»¶å¹¶è·å¾—æ”¶ç›Šï¼Œåº”å°†æ”¶ç›Šä¸­ä¸ä½äºåƒåˆ†ä¹‹ä¸€çš„é‡‘é¢æèµ ç»™ç¤¾ä¼šçš„å¼±åŠ¿ç¾¤ä½“\nã€‚\nåŠŸèƒ½ç®€ä»‹\nPDF è¡¥ä¸ä¸æ˜¯ä¸€ä¸ª PDF å¤„ç†å·¥å…·ã€‚å®ƒå…·æœ‰ä»¥ä¸‹åŠŸèƒ½ï¼š\nä¿®æ”¹ PDF æ–‡æ¡£ï¼šä¿®æ”¹æ–‡æ¡£å±æ€§ã€é¡µç ç¼–å·ã€é¡µé¢é“¾æ¥ï¼›ç»Ÿä¸€é¡µé¢å°ºå¯¸ï¼›åˆ é™¤è‡ªåŠ¨æ‰“å¼€ç½‘é¡µç­‰åŠ¨ä½œï¼›å»é™¤å¤åˆ¶åŠæ‰“å°é™åˆ¶ï¼›è®¾ç½®é˜…è¯»å™¨åˆå§‹æ¨¡å¼ï¼›æ¸…ç†æ–‡æ¡£éšè—åƒåœ¾æ•°æ®ï¼›é‡æ–°å‹ç¼©é»‘ç™½å›¾ç‰‡ï¼›æ—‹è½¬é¡µé¢ã€‚\nè´´å¿ƒ PDF ä¹¦ç­¾ç¼–è¾‘å™¨ï¼šå¸¦æœ‰é˜…è¯»ç•Œé¢ï¼ˆå…·æœ‰ä¾¿äºé˜…è¯»ç«–æ’æ–‡æ¡£çš„ä»å³åˆ°å·¦é˜…è¯»æ–¹å¼ï¼‰ï¼Œå¯æ‰¹é‡ä¿®æ”¹ PDF ä¹¦ç­¾å±æ€§ï¼ˆé¢œè‰²ã€æ ·å¼ã€ç›®æ ‡é¡µç ã€ç¼©æ”¾æ¯”ä¾‹ç­‰ï¼‰ï¼Œä¹¦ç­¾å¯ç²¾ç¡®å®šä½åˆ°é¡µé¢ä¸­é—´ï¼›åœ¨ä¹¦ç­¾ä¸­æ‰§è¡ŒæŸ¥æ‰¾æ›¿æ¢ï¼ˆæ”¯æŒæ­£åˆ™è¡¨è¾¾å¼åŠ XPath åŒ¹é…ã€å¯å¿«é€Ÿé€‰æ‹©ç¯‡ã€ç« ã€èŠ‚ä¹¦ç­¾ï¼‰ï¼Œ\nè‡ªåŠ¨å¿«é€Ÿç”Ÿæˆæ–‡æ¡£ä¹¦ç­¾\nã€‚\nåˆ¶ä½œ PDF æ–‡ä»¶ï¼šåˆå¹¶å·²æœ‰ PDF æ–‡ä»¶æˆ–å›¾ç‰‡ï¼Œç”Ÿæˆæ–°çš„ PDF æ–‡ä»¶ï¼›åˆå¹¶åçš„ PDF æ–‡æ¡£å¸¦æœ‰åŸæ–‡æ¡£çš„ä¹¦ç­¾ï¼Œè¿˜å¯æŒ‚ä¸Šæ–°ä¹¦ç­¾ï¼ˆæˆ–æ ¹æ®æ–‡ä»¶åç”Ÿæˆï¼‰ï¼Œæ–°ä¹¦ç­¾æ–‡æœ¬å’Œæ ·å¼å¯è‡ªå®šä¹‰ï¼›åˆå¹¶çš„ PDF æ–‡æ¡£å¯æŒ‡å®šç»Ÿä¸€çš„é¡µé¢å°ºå¯¸ï¼Œä»¥ä¾¿æ‰“å°å’Œé˜…è¯»ã€‚\næ‹†åˆ†æˆ–åˆå¹¶ PDF æ–‡ä»¶ï¼Œå¹¶ä¿ç•™åŸæ–‡ä»¶çš„ä¹¦ç­¾æˆ–æŒ‚ä¸Šæ–°çš„ä¹¦ç­¾ã€‚\né«˜é€Ÿæ— æŸå¯¼å‡º PDF æ–‡æ¡£çš„å›¾ç‰‡ã€‚\nå°† PDF é¡µé¢è½¬æ¢ä¸ºå›¾ç‰‡ã€‚\næå–æˆ–åˆ é™¤ PDF æ–‡æ¡£ä¸­æŒ‡å®šçš„é¡µé¢ï¼Œè°ƒæ•´ PDF æ–‡æ¡£çš„é¡µé¢é¡ºåºã€‚\næ ¹æ® PDF æ–‡æ¡£å…ƒæ•°æ®é‡å‘½å PDF æ–‡ä»¶åã€‚\nè°ƒç”¨å¾®è½¯ Office çš„å›¾åƒè¯†åˆ«å¼•æ“åˆ†æ PDF æ–‡æ¡£å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼›å°†å›¾ç‰‡ PDF çš„ç›®å½•é¡µè½¬æ¢ä¸º PDF ä¹¦ç­¾ã€‚è¯†åˆ«ç»“æœå¯å†™å…¥ PDF æ–‡ä»¶ã€‚\næ›¿æ¢å­—ä½“ï¼šæ›¿æ¢æ–‡æ¡£ä¸­ä½¿ç”¨çš„å­—ä½“ï¼›åµŒå…¥å­—åº“åˆ° PDF æ–‡æ¡£ï¼Œæ¶ˆé™¤å¤åˆ¶æ–‡æœ¬æ—¶çš„ä¹±ç ï¼Œä½¿ä¹‹å¯åœ¨æ²¡æœ‰å­—åº“çš„è®¾å¤‡ï¼ˆå¦‚ Kindle ç­‰ç”µå­ä¹¦é˜…è¯»å™¨ï¼‰ä¸Šé˜…è¯»ã€‚\nåˆ†ææ–‡æ¡£ç»“æ„ï¼šä»¥æ ‘è§†å›¾æ˜¾ç¤º PDF æ–‡æ¡£ç»“æ„ï¼Œå¯ç¼–è¾‘ä¿®æ”¹ PDF æ–‡æ¡£èŠ‚ç‚¹ï¼Œæˆ–å°† PDF æ–‡æ¡£å¯¼å‡ºæˆ XML æ–‡ä»¶ï¼Œä¾› PDF çˆ±å¥½è€…åˆ†æã€è°ƒè¯•ä¹‹ç”¨ã€‚\næ°¸ä¹…å…è´¹ï¼Œç»ä¸è¿‡æœŸï¼Œæ— å¹¿å‘Šï¼Œæ— å¼¹å‡ºåºŸè¯å¯¹è¯æ¡†ï¼Œä¸çª¥æ¢éšç§ã€‚\næˆæƒåè®®\nã€ŠPDF è¡¥ä¸ä¸ã€‹è½¯ä»¶ï¼ˆä»¥ä¸‹ç®€ç§°æœ¬è½¯ä»¶ï¼‰å—è‘—ä½œæƒæ³•åŠå›½é™…æ¡çº¦æ¡æ¬¾å’Œå…¶å®ƒçŸ¥è¯†äº§æƒæ³•åŠæ¡çº¦çš„ä¿æŠ¤ã€‚\næœ¬è½¯ä»¶å¯¹äºæœ€ç»ˆç”¨æˆ·å…è´¹ã€‚ç”±äºæœ¬è½¯ä»¶ä½¿ç”¨äº†å¸¦æœ‰ AGPL æ¡æ¬¾çš„ç¬¬ä¸‰æ–¹å¼€æºç»„ä»¶ï¼Œå› æ­¤ï¼Œæœ¬è½¯ä»¶åŠå…¶æºä»£ç çš„ä½¿ç”¨åè®®ä¹ŸåŸºäº AGPLã€‚å¦å¤–è¿˜å¸¦æœ‰å¦‚ä¸‹é™„åŠ æ¡ä»¶ã€‚åœ¨éµå®ˆæœ¬è½¯ä»¶çš„å‰ææ¡ä»¶ä¸‹ï¼Œä½ å¯ä»¥åœ¨éµå¾ªæœ¬åè®®çš„åŸºç¡€ä¸Šè‡ªç”±çš„ä½¿ç”¨å’Œä¼ æ’­å®ƒï¼Œä½ ä¸€æ—¦å®‰è£…ã€å¤åˆ¶æˆ–ä½¿ç”¨æœ¬è½¯ä»¶ï¼Œåˆ™è¡¨ç¤ºæ‚¨å·²ç»åŒæ„æœ¬åè®®æ¡æ¬¾ã€‚å¦‚æœä½ ä¸åŒæ„æœ¬åè®®ï¼Œè¯·ä¸è¦å®‰è£…ä½¿ç”¨æœ¬è½¯ä»¶ï¼Œä¹Ÿä¸åº”åˆ©ç”¨å…¶æºä»£ç ã€‚\né™„åŠ æ¡ä»¶ï¼š\næ¯ä¸€ä¸ªä½¿ç”¨æœ¬è½¯ä»¶çš„ç”¨æˆ·ï¼Œå¦‚æœæœ¬è½¯ä»¶å¸®åŠ©äº†æ‚¨ï¼Œæ¯ä½¿ç”¨æœ¬è½¯ä»¶åï¼Œæ‚¨åº”å½“åš 1 ä»¶å–„äº‹ã€‚å–„äº‹æ— åˆ†å¤§å°ï¼Œæœ‰å¿ƒåˆ™è¡Œã€‚ä¾‹å¦‚ï¼š\nå¦‚æœæ‚¨çš„çˆ¶æ¯åœ¨èº«è¾¹ï¼Œä½ å¯ä»¥ä¸ºæ‚¨çš„çˆ¶æ¯åšä¸€é¡¿ç¾å‘³çš„é¥­èœï¼Œæˆ–è€…ä¸ºä»–ä»¬æŒ‰æ‘©ã€æ´—è„šï¼›å¦‚æœä»–ä»¬èº«å¤„è¿œæ–¹ï¼Œä½ å¯ä»¥å‘ä»–ä»¬å‘èµ·é€šè¯ï¼Œé—®å€™ä»–ä»¬çš„å¥åº·å’Œç”Ÿæ´»ã€‚\nåœ¨å¤§é›¨æ»‚æ²±çš„æ—¶å€™ï¼Œå¦‚æœæ‚¨æœ‰é›¨ä¼ï¼Œå¯ä¸åŒè·¯çš„äººå…±äº«ï¼›åœ¨çƒˆæ—¥å½“ç©ºçš„æ—¶èŠ‚ï¼Œå¦‚æœæ‚¨çœ‹åˆ°ç¯å«å·¥äººå¤ªé˜³ä¸‹å·¥ä½œï¼Œæ‚¨å¯ä»¥ä¸ºä»–ä»¬ä¹°ä¸€ç“¶æ°´é€ç»™ä»–ä»¬ï¼›åœ¨æ‹¥æŒ¤çš„å…¬å…±äº¤é€šå·¥å…·ä¸Šï¼Œæˆ–åœ¨å…¬å…±åœºåˆæ’é˜Ÿç­‰å€™ä¹‹é™…ï¼Œå¦‚æœæ‚¨æœ‰åº§ä½ï¼Œå¯ä»¥è®©ç»™è€äººã€å­•å¦‡æˆ–æç€é‡ç‰©çš„äººå°±åã€‚\næ‚¨å¯ä»¥ç”¨æ‚¨æ“…é•¿çš„æŠ€èƒ½ï¼Œä¸ºèº«è¾¹çš„äººæ’éš¾è§£å›°ï¼›æ‚¨å¯ä»¥å°†æ‚¨çš„çŸ¥è¯†ï¼Œåˆ†äº«ç»™å…¶ä»–äººï¼Œè®©ä»–ä»¬æœ‰æ‰€è·ç›Šï¼›æ‚¨å¯ä»¥å‘æ¯”æ‚¨å›°éš¾çš„äººæèµ„èµ ç‰©ã€‚\nå¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªè½¯ä»¶çœŸçš„å¥½ç”¨ï¼Œè¯·å°†å®ƒçš„ä½¿ç”¨æ–¹æ³•ä»‹ç»ç»™åˆ«äººï¼Œè®©åˆ«äººä¹Ÿé€šè¿‡ä½¿ç”¨æœ¬è½¯ä»¶è€Œå¾—åˆ°å¥½å¤„ï¼›æˆ–è€…å°†å…¶å®ƒæ‚¨è§‰å¾—å¥½ç”¨çš„è½¯ä»¶ä»‹ç»ç»™åˆ«äººã€‚\nå¦‚æœæ‚¨æ— æ³•åšåˆ°ä½¿ç”¨æœ¬è½¯ä»¶ååš 1 ä»¶å–„äº‹ï¼Œè¯·è®°åœ¨å¿ƒä¸­ã€‚åœ¨æœ‰æœºä¼šçš„æ—¶å€™ï¼Œå¤šè¡Œå–„ç§¯å¾·ã€‚æœ¬ç”¨æˆ·åè®®ä¹‹éµå¾ªä¸å¦ï¼Œå…¨åœ¨äºæ‚¨çš„è‰¯å¿ƒã€‚æ˜¯ä¸ºâ€œ\nè‰¯å¿ƒæˆæƒ\nâ€ã€‚\nç›¸å…³å®šä¹‰ï¼š\nè½¯ä»¶ï¼šè½¯ä»¶æ˜¯æŒ‡ã€ŠPDF è¡¥ä¸ä¸ã€‹è½¯ä»¶ä»¥åŠå®ƒçš„æ›´æ–°ã€äº§å“æ‰‹å†Œï¼Œä»¥åŠåœ¨çº¿æ–‡æ¡£ç­‰ç›¸å…³è½½ä½“ã€‚\né™åˆ¶ï¼šä½ å¯ä»¥ä½¿ç”¨æœ¬è½¯ä»¶çš„æºä»£ç å¼€å‘åº”ç”¨ç¨‹åºï¼ˆè‡ªç”±ã€å…±äº«æˆ–å•†ç”¨ï¼‰ï¼Œä¹Ÿå¯ä»¥ä»»æ„æ–¹å¼åˆ†å‘æ•°é‡ä¸é™çš„æœ¬è½¯ä»¶çš„å®Œæ•´æ‹·è´ï¼Œä½†å‰ææ˜¯ï¼š\nâ‘  ä½ åˆ†å‘è½¯ä»¶æ—¶å¿…é¡»æä¾›æœ¬è½¯ä»¶çš„å®Œæ•´ç‰ˆæœ¬ï¼Œæœªç»è®¸å¯ä¸å¾—å¯¹è½¯ä»¶ä¹ƒè‡³å®ƒçš„å®‰è£…ç¨‹åºåšä»»ä½•ä¿®æ”¹ï¼›\nâ‘¡ ä½ åˆ†å‘è½¯ä»¶æ—¶ä¸èƒ½æ›´æ”¹æœ¬æˆæƒåè®®ï¼›\nâ‘¢ ä½ å¦‚æœåœ¨å•†ä¸šæ€§å®£ä¼ æ´»åŠ¨ã€äº§å“ä¸­é™„åŠ æœ¬è½¯ä»¶ï¼Œåº”å½“è·å¾—è‘—ä½œæƒäººçš„ä¹¦é¢è®¸å¯ï¼›\nâ‘£ ä½ å¦‚æœåˆ©ç”¨æœ¬è½¯ä»¶çš„æºä»£ç ç¼–å†™äº†å…¶å®ƒè½¯ä»¶ï¼Œå¹¶ä¸”äº§ç”Ÿäº†é”€å”®æ”¶å…¥ï¼Œåº”å½“å°†è¯¥è½¯ä»¶é”€å”®æ”¶å…¥ä¸ä½äºåƒåˆ†ä¹‹ä¸€çš„é‡‘é¢æçŒ®ç»™ç¤¾ä¼šä¸Šçš„å¼±åŠ¿ç¾¤ä½“ã€‚\næ”¯æŒï¼šè½¯ä»¶ä¼šç”±äºç”¨æˆ·çš„éœ€æ±‚è€Œä¸æ–­æ›´æ–°ï¼Œè‘—ä½œæƒäººå°†æä¾›åŒ…æ‹¬ç”¨æˆ·æ‰‹å†Œã€ç”µå­é‚®ä»¶ç­‰å„ç§ç›¸å…³ä¿¡æ¯æ”¯æŒï¼Œä½†è½¯ä»¶ä¸ç¡®ä¿æ”¯æŒå†…å®¹å’ŒåŠŸèƒ½ä¸å‘ç”Ÿå˜æ›´ã€‚\nç»ˆæ­¢ï¼šå½“ä½ ä¸åŒæ„æˆ–è€…è¿èƒŒæœ¬åè®®çš„æ—¶å€™ï¼Œåè®®å°†è‡ªåŠ¨ç»ˆæ­¢ï¼Œä½ å¿…é¡»ç«‹å³åˆ é™¤æœ¬è½¯ä»¶äº§å“ã€‚\nç‰ˆæƒï¼šæœ¬è½¯ä»¶åŠæºä»£ç å—è‘—ä½œæƒæ³•åŠå›½é™…æ¡çº¦æ¡æ¬¾å’Œå…¶å®ƒçŸ¥è¯†äº§æƒæ³•åŠæ¡çº¦çš„ä¿æŠ¤ã€‚\nå…è´£ï¼šå¯¹äºæœ¬è½¯ä»¶å®‰è£…ã€å¤åˆ¶ã€ä½¿ç”¨ä¸­å¯¼è‡´çš„ä»»ä½•æŸå¤±ï¼Œæœ¬è½¯ä»¶åŠè‘—ä½œæƒäººä¸è´Ÿè´£ä»»ã€‚\nå¸¸ç”¨çš„ PDF å¼€æºç»„ä»¶ç®€ä»‹\nPDF æ–‡æ¡£çš„è§„èŒƒï¼ˆISO 32000-1:2008 ã€ŠDocument management â€” Portable document format â€” Part 1:PDF 1.7ã€‹ï¼‰å¯ä»ç½‘ä¸Šæ‰¾åˆ°ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œå®ƒæ˜¯ PDF å¤„ç†ç¨‹åºå¼€å‘è€…çš„å¿…è¯»æ–‡çŒ®ã€‚\nPDF æ–‡æ¡£æ ¼å¼ä¸­æ¶‰åŠå°åˆ·é¢†åŸŸçš„å¤šé¡¹æŠ€æœ¯ï¼Œå¹¶æœ‰å…¶ç‹¬ç‰¹çš„æ–‡æ¡£ç»“æ„ï¼Œè¿˜ä½¿ç”¨äº†å¤šç§æ•°æ®å‹ç¼©ç®—æ³•ã€‚è¦ä»é›¶å¼€å§‹ç¼–å†™ PDF æ–‡æ¡£çš„å¤„ç†ç¨‹åºï¼Œå¯¹äºä¸€èˆ¬äººè€Œè¨€ï¼Œé€šå¸¸æ˜¯å›°éš¾è€Œä¸å¤ªç°å®çš„ã€‚PDF è¡¥ä¸ä¸ä½¿ç”¨ .NET Framework å¼€å‘ï¼Œä¸»è¦é‡‡ç”¨ iText å’Œ MuPDF è¿™ä¸¤ä¸ªå¼€æ”¾æºä»£ç çš„ç»„ä»¶åº“æ¥å¤„ç† PDF æ–‡æ¡£ã€‚\nå‰è€…æ˜¯ .NET ç»„ä»¶ï¼Œä¸ PDF ä¸»ç¨‹åºå…·æœ‰è¾ƒå¥½çš„äº’æ“ä½œæ€§ï¼Œå¹¶ä¸”åœ¨è§£æã€ç”Ÿæˆå’Œä¿®æ”¹ PDF æ–‡æ¡£ï¼Œä»¥åŠåµŒå…¥ TTF å­—ä½“å­é›†è¿™äº›åŠŸèƒ½ä¸Šï¼Œä¼˜èƒœäºåè€…ã€‚\nåè€…é‡‡ç”¨ C è¯­è¨€å¼€å‘å¹¶ç¼–è¯‘ï¼Œä¸å‰è€…ç›¸æ¯”ï¼Œå…¶æœ€å¤§çš„ä¼˜ç‚¹æ˜¯å…·æœ‰æ¸²æŸ“ PDF æ–‡æ¡£ä¸ºä½å›¾çš„åŠŸèƒ½ã€‚MuPDF ç¼–è¯‘å‡ºæ¥çš„åŠ¨æ€ç»„ä»¶åº“å¯åœ¨ä½œè€…å¦ä¸€ä¸ªå¼€æ”¾æºä»£ç åº“\nSharpMuPDF\nä¸‹è½½ã€‚PDF è¡¥ä¸ä¸é€šè¿‡ P/Invoke æŠ€æœ¯è°ƒç”¨è¯¥ç»„ä»¶åº“çš„åŠŸèƒ½ã€‚\né™¤äº† PDF å¼€æºç»„ä»¶ä¹‹å¤–ï¼Œç¨‹åºè¿˜ä½¿ç”¨äº†å…¶å®ƒä¼˜ç§€å¼€æºç»„ä»¶ã€‚ä¾‹å¦‚ ObjectListView è¿™ä¸ªå¼ºå¤§çš„åˆ—è¡¨æ§ä»¶ã€FreeImage æ¥è¯»å–å’Œè§£ç å„ç§ç±»å‹çš„ç‚¹é˜µå›¾åƒæ–‡ä»¶ã€Cyotek çš„ ImageBox ç”¨äºæ˜¾ç¤ºæ¸²æŸ“å¥½çš„ PDF æ–‡æ¡£é¡µé¢ã€TabControlExtra ç”¨äºæ„å»ºé€‰é¡¹å¡å¼æ–‡æ¡£ç•Œé¢ã€HTMLRenderer ç”¨äºæ˜¾ç¤º HTML ç½‘é¡µç•Œé¢ç­‰ç­‰ã€‚\næºä»£ç çš„ç»“æ„\nApp ç›®å½•ï¼šPDF è¡¥ä¸ä¸ä¸»ç¨‹åº\nCommonï¼šä¸€äº›å¸¸ç”¨çš„å·¥å…·ç±»\nFunctionsï¼šç”¨äºå‘ˆç°è½¯ä»¶å„ç±»åŠŸèƒ½çš„çª—ä½“å’Œæ§ä»¶\nLibï¼šç¨‹åºä½¿ç”¨çš„ç¬¬ä¸‰æ–¹ç»„ä»¶\nModelï¼šç¼–è¾‘æ–‡æ¡£æ—¶æ‰€ç”¨çš„é«˜çº§æ¨¡å‹ï¼ˆåŸºç¡€æ•°æ®æ¨¡å‹ç”± iText å’Œ MuPDF çš„ç±»å®ç°ï¼‰\nOptionsï¼šç¨‹åºçš„é€‰é¡¹\nProcessorï¼šå¤„ç† PDF æ–‡æ¡£çš„ç®—æ³•ï¼ˆå…¶ä¸­ Mupdf ç›®å½•é‡Œæ”¾ç½®äº† P/Invoke è°ƒç”¨ MuPDF çš„ç±»ï¼‰\ndoc ç›®å½•ï¼šæ”¾ç½®ç¨‹åºçš„ä½¿ç”¨æ–‡æ¡£\nJBig2 ç›®å½•ï¼šæ”¾ç½® JBIG2 å›¾åƒçš„ç¼–ç å’Œè§£ç åº“ä»£ç \nè¿è¡Œç¯å¢ƒ\nWindows 7 ä»¥ä¸Šç‰ˆæœ¬çš„æ“ä½œç³»ç»Ÿã€‚\n.NET Framework 4.0 åˆ° 4.8 ç‰ˆæœ¬ã€‚\nä½¿ç”¨æ–‡å­—è¯†åˆ«åŠŸèƒ½éœ€è¦å®‰è£… Microsoft Office 2003ï¼ˆæˆ– 2007ï¼‰çš„ Document Imaging ç»„ä»¶ï¼ˆMODIï¼‰ã€‚\nç¼–è¯‘ç¨‹åºæºä»£ç ï¼Œå»ºè®®ä½¿ç”¨ Visual Studio 2022 æˆ–æ›´æ–°ç‰ˆæœ¬ï¼Œå¹¶å®‰è£…â€œ.NET æ¡Œé¢å¼€å‘â€ï¼ˆç”¨äºç¼–è¯‘ PDF è¡¥ä¸ä¸æºä»£ç ï¼‰å’Œâ€œC++ æ¡Œé¢å¼€å‘â€ï¼ˆç”¨äºç¼–è¯‘ JBIG2 ç¼–ç ç»„ä»¶ï¼‰ä¸¤ä¸ªå·¥ä½œè´Ÿè½½ã€‚å¯èƒ½ä¼šé‡åˆ°é¡¹ç›®â€œé¢å‘ä¸å†å—æ”¯æŒçš„ .NET Frameworkâ€ã€éœ€è¦â€œå°†ç›®æ ‡æ›´æ–°ä¸º .NET Framework 4.8â€çš„é—®é¢˜ã€‚ç®€å•æ–¹æ³•æ˜¯å°†ç›®æ ‡æ›´æ–°ä¸º .NET Framework 4.8ï¼Œå¦‚ä¸æ›´æ–°ç›®æ ‡ï¼Œè¯·å‚è€ƒ\nè¿™ç¯‡æ–‡ç« ä»‹ç»çš„æ–¹æ³•\nã€‚\nè”ç³»ä½œè€…\né™¤ç¬¬ä¸‰æ–¹ç»„ä»¶å¤–ï¼Œæœ¬è½¯ä»¶çš„æºä»£ç å®Œå…¨å¼€æ”¾ï¼š\nhttps://github.com/wmjordan/PDFPatcher\nhttps://gitee.com/wmjordan/pdfpatcher\nå»ºè®®é€šè¿‡å¼€æ”¾æºä»£ç ç½‘ç«™é€šè¿‡æäº¤ issue çš„æ–¹å¼æäº¤æ‚¨çš„å»ºè®®æˆ–éœ€æ±‚ã€‚å› æ—¥å¸¸å·¥ä½œç¹å¿™ï¼Œæš‚ä¸æä¾›åŠ  QQ æˆ–å¾®ä¿¡å’¨è¯¢çš„æœåŠ¡ï¼Œæ•¬è¯·è°…è§£ã€‚\nåœ¨é‚®ä»¶æˆ–æ¶ˆæ¯ä¸­ï¼Œè¯·æ³¨æ˜ä½ çš„ç‰ˆæœ¬å·ï¼Œé™„ä¸Šæˆªå›¾å’Œé™„ä»¶ï¼Œè¯¦ç»†è¯´æ˜ä½ é‡åˆ°çš„é—®é¢˜ã€‚\nå¦‚é‡åˆ°éœ€è¦æä¾›é™„ä»¶çš„æƒ…å†µï¼Œè¯·æŠŠå®ƒæå°ä¸€ç‚¹ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæœ€å¥½ä¸è¦å‘é€è¶…è¿‡ 10M çš„é™„ä»¶ã€‚\nå¯¹äº PDF æ–‡ä»¶ï¼Œå¯ç”¨â€œæå–é¡µé¢â€åŠŸèƒ½æå–æœ‰ä»£è¡¨æ€§çš„é¡µé¢ã€‚\nå¯¹äºå›¾ç‰‡æ–‡ä»¶ï¼Œè¯·å‹ç¼©æºæ–‡ä»¶ï¼Œæˆ–æä¾›æœ‰ä»£è¡¨æ€§çš„ä¸€ä¸¤é¡µå›¾ç‰‡ã€‚",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 56",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 10,561"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/wmjordan/PDFPatcher"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/Mentra-Community/MentraOS",
      "title": "Mentra-Community/MentraOS",
      "date": null,
      "executive_summary": [
        "The open-source OS for smart glasses with dozens of apps. Get captions, AI assistant, notifications, translation, and more. Devs now write 1 app that runs on any pair of smart glases.",
        "---",
        "MentraOS\nThe open source operating system for smart glasses\nWebsite\nâ€¢\nDocumentation\nâ€¢\nDeveloper Console\nâ€¢\nMentra Store\nSupported Smart Glasses\nWorks with Even Realities G1, Mentra Mach 1, Mentra Live. See\nsmart glasses compatibility list here\n.\nApps on Mentra Store\nThe Mentra Store already has a ton of useful apps that real users are running everyday. Here are some apps already published by developers on the Mentra Store:\nLive Captions\nLink\nMerge\nNotes\nCalendar\nDash\nTranslation\nâ†’ Browse All Apps\nWrite Once, Run on Any Smart Glasses\nMentraOS is how developers build smart glasses apps.\nWe handle the pairing, connection, data streaming, and cross-compatibility, so you can focus on creating amazing apps. Every component is 100% open source (MIT license).\nWhy Build with MentraOS?\nCross Compatibility\n: Your app runs on any pair of smart glasses\nSpeed\n: TypeScript SDK means you're making apps in minutes, not months\nControl\n: Access smart glasses I/O - displays, microphones, cameras, speakers\nDistribution\n: Get your app in front of everyone using smart glasses\nMentraOS Community\nThe MentraOS Community is a group of developers, companies, and users dedicated to ensuring the next personal computer is open, cross-compatible, and user-controlled. That's why we're building MentraOS.\nTo get involved, join the\nMentraOS Community Discord server\n.\nContact\nHave questions or ideas? We'd love to hear from you!\nEmail\n:\nteam@mentra.glass\nDiscord\n:\nJoin our community\nTwitter\n:\nFollow @mentralabs\nContributing\nMentraOS is made by a community and we welcome PRs. Here's the Contributors Guide:\ndocs.mentra.glass/contributing\nLicense\nMIT License Copyright 2025 MentraOS Community\nÂ© 2025 Mentra Labs",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 55",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 1,316"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/Mentra-Community/MentraOS"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/rustfs/rustfs",
      "title": "rustfs/rustfs",
      "date": null,
      "executive_summary": [
        "ğŸš€ High-performance distributed object storage for MinIO alternative.",
        "---",
        "RustFS is a high-performance distributed object storage software built using Rust\nGetting Started\nÂ·\nDocs\nÂ·\nBug reports\nÂ·\nDiscussions\nEnglish |\nç®€ä½“ä¸­æ–‡\n|\nDeutsch\n|\nEspaÃ±ol\n|\nfranÃ§ais\n|\næ—¥æœ¬èª\n|\ní•œêµ­ì–´\n|\nPortuguÃªs\n|\nĞ ÑƒÑÑĞºĞ¸Ğ¹\nRustFS is a high-performance distributed object storage software built using Rust, one of the most popular languages worldwide. Along with MinIO, it shares a range of advantages such as simplicity, S3 compatibility, open-source nature, support for data lakes, AI, and big data. Furthermore, it has a better and more user-friendly open-source license in comparison to other storage systems, being constructed under the Apache license. As Rust serves as its foundation, RustFS provides faster speed and safer distributed features for high-performance object storage.\nâš ï¸\nRustFS is under rapid development. Do NOT use in production environments!\nFeatures\nHigh Performance\n: Built with Rust, ensuring speed and efficiency.\nDistributed Architecture\n: Scalable and fault-tolerant design for large-scale deployments.\nS3 Compatibility\n: Seamless integration with existing S3-compatible applications.\nData Lake Support\n: Optimized for big data and AI workloads.\nOpen Source\n: Licensed under Apache 2.0, encouraging community contributions and transparency.\nUser-Friendly\n: Designed with simplicity in mind, making it easy to deploy and manage.\nRustFS vs MinIO\nStress test server parameters\nType\nparameter\nRemark\nCPU\n2 Core\nIntel Xeon(Sapphire Rapids) Platinum 8475B , 2.7/3.2 GHz\nMemory\n4GB\nNetwork\n15Gbp\nDriver\n40GB x 4\nIOPS 3800 / Driver\nrustfs.mp4\nRustFS vs Other object storage\nRustFS\nOther object storage\nPowerful Console\nSimple and useless Console\nDeveloped based on Rust language, memory is safer\nDeveloped in Go or C, with potential issues like memory GC/leaks\nDoes not report logs to third-party countries\nReporting logs to other third countries may violate national security laws\nLicensed under Apache, more business-friendly\nAGPL V3 License and other License, polluted open source and License traps, infringement of intellectual property rights\nComprehensive S3 support, works with domestic and international cloud providers\nFull support for S3, but no local cloud vendor support\nRust-based development, strong support for secure and innovative devices\nPoor support for edge gateways and secure innovative devices\nStable commercial prices, free community support\nHigh pricing, with costs up to $250,000 for 1PiB\nNo risk\nIntellectual property risks and risks of prohibited uses\nQuickstart\nTo get started with RustFS, follow these steps:\nOne-click installation script (Option 1)â€‹â€‹\ncurl -O  https://rustfs.com/install_rustfs.sh\n&&\nbash install_rustfs.sh\nDocker Quick Start (Option 2)â€‹â€‹\n#\ncreate data and logs directories\nmkdir -p data logs\n#\nusing latest alpha version\ndocker run -d -p 9000:9000 -v\n$(\npwd\n)\n/data:/data -v\n$(\npwd\n)\n/logs:/logs rustfs/rustfs:alpha\n#\nSpecific version\ndocker run -d -p 9000:9000 -v\n$(\npwd\n)\n/data:/data -v\n$(\npwd\n)\n/logs:/logs rustfs/rustfs:1.0.0.alpha.45\nFor docker installation, you can also run the container with docker compose. With the\ndocker-compose.yml\nfile under root directory, running the command:\ndocker compose --profile observability up -d\nNOTE\n: You should be better to have a look for\ndocker-compose.yaml\nfile. Because, several services contains in the file. Grafan,prometheus,jaeger containers will be launched using docker compose file, which is helpful for rustfs observability. If you want to start redis as well as nginx container, you can specify the corresponding profiles.\nBuild from Source (Option 3) - Advanced Users\nFor developers who want to build RustFS Docker images from source with multi-architecture support:\n#\nBuild multi-architecture images locally\n./docker-buildx.sh --build-arg RELEASE=latest\n#\nBuild and push to registry\n./docker-buildx.sh --push\n#\nBuild specific version\n./docker-buildx.sh --release v1.0.0 --push\n#\nBuild for custom registry\n./docker-buildx.sh --registry your-registry.com --namespace yourname --push\nThe\ndocker-buildx.sh\nscript supports:\nMulti-architecture builds\n:\nlinux/amd64\n,\nlinux/arm64\nAutomatic version detection\n: Uses git tags or commit hashes\nRegistry flexibility\n: Supports Docker Hub, GitHub Container Registry, etc.\nBuild optimization\n: Includes caching and parallel builds\nYou can also use Make targets for convenience:\nmake docker-buildx\n#\nBuild locally\nmake docker-buildx-push\n#\nBuild and push\nmake docker-buildx-version VERSION=v1.0.0\n#\nBuild specific version\nmake help-docker\n#\nShow all Docker-related commands\nAccess the Console\n: Open your web browser and navigate to\nhttp://localhost:9000\nto access the RustFS console, default username and password is\nrustfsadmin\n.\nCreate a Bucket\n: Use the console to create a new bucket for your objects.\nUpload Objects\n: You can upload files directly through the console or use S3-compatible APIs to interact with your RustFS instance.\nNOTE\n: If you want to access RustFS instance with\nhttps\n, you can refer to\nTLS configuration docs\n.\nDocumentation\nFor detailed documentation, including configuration options, API references, and advanced usage, please visit our\nDocumentation\n.\nGetting Help\nIf you have any questions or need assistance, you can:\nCheck the\nFAQ\nfor common issues and solutions.\nJoin our\nGitHub Discussions\nto ask questions and share your experiences.\nOpen an issue on our\nGitHub Issues\npage for bug reports or feature requests.\nLinks\nDocumentation\n- The manual you should read\nChangelog\n- What we broke and fixed\nGitHub Discussions\n- Where the community lives\nContact\nBugs\n:\nGitHub Issues\nBusiness\n:\nhello@rustfs.com\nJobs\n:\njobs@rustfs.com\nGeneral Discussion\n:\nGitHub Discussions\nContributing\n:\nCONTRIBUTING.md\nContributors\nRustFS is a community-driven project, and we appreciate all contributions. Check out the\nContributors\npage to see the amazing people who have helped make RustFS better.\nLicense\nApache 2.0\nRustFS\nis a trademark of RustFS, Inc. All other trademarks are the property of their respective owners.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 52",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 8,877"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/rustfs/rustfs"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/krahets/hello-algo",
      "title": "krahets/hello-algo",
      "date": null,
      "executive_summary": [
        "ã€ŠHello ç®—æ³•ã€‹ï¼šåŠ¨ç”»å›¾è§£ã€ä¸€é”®è¿è¡Œçš„æ•°æ®ç»“æ„ä¸ç®—æ³•æ•™ç¨‹ã€‚æ”¯æŒ Python, Java, C++, C, C#, JS, Go, Swift, Rust, Ruby, Kotlin, TS, Dart ä»£ç ã€‚ç®€ä½“ç‰ˆå’Œç¹ä½“ç‰ˆåŒæ­¥æ›´æ–°ï¼ŒEnglish version in translation",
        "---",
        "åŠ¨ç”»å›¾è§£ã€ä¸€é”®è¿è¡Œçš„æ•°æ®ç»“æ„ä¸ç®—æ³•æ•™ç¨‹\nç®€ä½“ä¸­æ–‡\n  ï½œ\nç¹é«”ä¸­æ–‡\nï½œ\nEnglish\nå…³äºæœ¬ä¹¦\næœ¬é¡¹ç›®æ—¨åœ¨æ‰“é€ ä¸€æœ¬å¼€æºå…è´¹ã€æ–°æ‰‹å‹å¥½çš„æ•°æ®ç»“æ„ä¸ç®—æ³•å…¥é—¨æ•™ç¨‹ã€‚\nå…¨ä¹¦é‡‡ç”¨åŠ¨ç”»å›¾è§£ï¼Œå†…å®¹æ¸…æ™°æ˜“æ‡‚ã€å­¦ä¹ æ›²çº¿å¹³æ»‘ï¼Œå¼•å¯¼åˆå­¦è€…æ¢ç´¢æ•°æ®ç»“æ„ä¸ç®—æ³•çš„çŸ¥è¯†åœ°å›¾ã€‚\næºä»£ç å¯ä¸€é”®è¿è¡Œï¼Œå¸®åŠ©è¯»è€…åœ¨ç»ƒä¹ ä¸­æå‡ç¼–ç¨‹æŠ€èƒ½ï¼Œäº†è§£ç®—æ³•å·¥ä½œåŸç†å’Œæ•°æ®ç»“æ„åº•å±‚å®ç°ã€‚\næå€¡è¯»è€…äº’åŠ©å­¦ä¹ ï¼Œæ¬¢è¿å¤§å®¶åœ¨è¯„è®ºåŒºæå‡ºé—®é¢˜ä¸åˆ†äº«è§è§£ï¼Œåœ¨äº¤æµè®¨è®ºä¸­å…±åŒè¿›æ­¥ã€‚\nè‹¥æœ¬ä¹¦å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œè¯·åœ¨é¡µé¢å³ä¸Šè§’ç‚¹ä¸ª Star â­ æ”¯æŒä¸€ä¸‹ï¼Œè°¢è°¢ï¼\næ¨èè¯­\nâ€œä¸€æœ¬é€šä¿—æ˜“æ‡‚çš„æ•°æ®ç»“æ„ä¸ç®—æ³•å…¥é—¨ä¹¦ï¼Œå¼•å¯¼è¯»è€…æ‰‹è„‘å¹¶ç”¨åœ°å­¦ä¹ ï¼Œå¼ºçƒˆæ¨èç®—æ³•åˆå­¦è€…é˜…è¯»ã€‚â€\nâ€”â€” é‚“ä¿Šè¾‰ï¼Œæ¸…åå¤§å­¦è®¡ç®—æœºç³»æ•™æˆ\nâ€œå¦‚æœæˆ‘å½“å¹´å­¦æ•°æ®ç»“æ„ä¸ç®—æ³•çš„æ—¶å€™æœ‰ã€ŠHello ç®—æ³•ã€‹ï¼Œå­¦èµ·æ¥åº”è¯¥ä¼šç®€å• 10 å€ï¼â€\nâ€”â€” ææ²ï¼Œäºšé©¬é€Šèµ„æ·±é¦–å¸­ç§‘å­¦å®¶\né¸£è°¢\nWarp is built for coding with multiple AI agents.\nå¼ºçƒˆæ¨è Warp ç»ˆç«¯ï¼Œé«˜é¢œå€¼ + å¥½ç”¨çš„ AIï¼Œä½“éªŒéå¸¸æ£’ï¼\nè´¡çŒ®\næœ¬å¼€æºä¹¦ä»åœ¨æŒç»­æ›´æ–°ä¹‹ä¸­ï¼Œæ¬¢è¿æ‚¨å‚ä¸æœ¬é¡¹ç›®ï¼Œä¸€åŒä¸ºè¯»è€…æä¾›æ›´ä¼˜è´¨çš„å­¦ä¹ å†…å®¹ã€‚\nå†…å®¹ä¿®æ­£\nï¼šè¯·æ‚¨ååŠ©ä¿®æ­£æˆ–åœ¨è¯„è®ºåŒºæŒ‡å‡ºè¯­æ³•é”™è¯¯ã€å†…å®¹ç¼ºå¤±ã€æ–‡å­—æ­§ä¹‰ã€æ— æ•ˆé“¾æ¥æˆ–ä»£ç  bug ç­‰é—®é¢˜ã€‚\nä»£ç è½¬è¯‘\nï¼šæœŸå¾…æ‚¨è´¡çŒ®å„ç§è¯­è¨€ä»£ç ï¼Œå·²æ”¯æŒ Pythonã€Javaã€C++ã€Goã€JavaScript ç­‰ 12 é—¨ç¼–ç¨‹è¯­è¨€ã€‚\nä¸­è¯‘è‹±\nï¼šè¯šé‚€æ‚¨åŠ å…¥æˆ‘ä»¬çš„ç¿»è¯‘å°ç»„ï¼Œæˆå‘˜ä¸»è¦æ¥è‡ªè®¡ç®—æœºç›¸å…³ä¸“ä¸šã€è‹±è¯­ä¸“ä¸šå’Œè‹±æ–‡æ¯è¯­è€…ã€‚\næ¬¢è¿æ‚¨æå‡ºå®è´µæ„è§å’Œå»ºè®®ï¼Œå¦‚æœ‰ä»»ä½•é—®é¢˜è¯·æäº¤ Issues æˆ–å¾®ä¿¡è”ç³»\nkrahets-jyd\nã€‚\næ„Ÿè°¢æœ¬å¼€æºä¹¦çš„æ¯ä¸€ä½æ’°ç¨¿äººï¼Œæ˜¯ä»–ä»¬çš„æ— ç§å¥‰çŒ®è®©è¿™æœ¬ä¹¦å˜å¾—æ›´å¥½ï¼Œä»–ä»¬æ˜¯ï¼š\nLicense\nThe texts, code, images, photos, and videos in this repository are licensed under\nCC BY-NC-SA 4.0\n.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 51",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 117,629"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/krahets/hello-algo"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/coollabsio/coolify",
      "title": "coollabsio/coolify",
      "date": null,
      "executive_summary": [
        "An open-source & self-hostable Heroku / Netlify / Vercel alternative.",
        "---",
        "About the Project\nCoolify is an open-source & self-hostable alternative to Heroku / Netlify / Vercel / etc.\nIt helps you manage your servers, applications, and databases on your own hardware; you only need an SSH connection. You can manage VPS, Bare Metal, Raspberry PIs, and anything else.\nImagine having the ease of a cloud but with your own servers. That is\nCoolify\n.\nNo vendor lock-in, which means that all the configurations for your applications/databases/etc are saved to your server. So, if you decide to stop using Coolify (oh nooo), you could still manage your running resources. You lose the automations and all the magic. ğŸª„ï¸\nFor more information, take a look at our landing page at\ncoolify.io\n.\nInstallation\ncurl -fsSL https://cdn.coollabs.io/coolify/install.sh\n|\nbash\nYou can find the installation script source\nhere\n.\nNote\nPlease refer to the\ndocs\nfor more information about the installation.\nSupport\nContact us at\ncoolify.io/docs/contact\n.\nCloud\nIf you do not want to self-host Coolify, there is a paid cloud version available:\napp.coolify.io\nFor more information & pricing, take a look at our landing page\ncoolify.io\n.\nWhy should I use the Cloud version?\nThe recommended way to use Coolify is to have one server for Coolify and one (or more) for the resources you are deploying. A server is around 4-5$/month.\nBy subscribing to the cloud version, you get the Coolify server for the same price, but with:\nHigh-availability\nFree email notifications\nBetter support\nLess maintenance for you\nDonations\nTo stay completely free and open-source, with no feature behind the paywall and evolve the project, we need your help. If you like Coolify, please consider donating to help us fund the project's future development.\ncoolify.io/sponsorships\nThank you so much!\nBig Sponsors\nCubePath\n- Dedicated Servers & Instant Deploy\nGlueOps\n- DevOps automation and infrastructure management\nAlgora\n- Open source contribution platform\nUbicloud\n- Open source cloud infrastructure platform\nLiquidWeb\n- Premium managed hosting solutions\nConvex\n- Open-source reactive database for web app developers\nArcjet\n- Advanced web security and performance solutions\nSaasyKit\n- Complete SaaS starter kit for developers\nSupaGuide\n- Your comprehensive guide to Supabase\nLogto\n- The better identity infrastructure for developers\nTrieve\n- AI-powered search and analytics\nSupadata AI\n- Scrape YouTube, web, and files. Get AI-ready, clean data\nDarweb\n- Design. Develop. Deliver. Specialized in 3D CPQ Solutions\nHetzner\n- Server, cloud, hosting, and data center solutions\nCOMIT\n- New York Times awardâ€“winning contractor\nBlacksmith\n- Infrastructure automation platform\nWZ-IT\n- German agency for customised cloud solutions\nBC Direct\n- Your trusted technology consulting partner\nTigris\n- Modern developer data platform\nHostinger\n- Web hosting and VPS solutions\nQuantCDN\n- Enterprise-grade content delivery network\nPFGLabs\n- Build Real Projects with Golang\nJobsCollider\n- 30,000+ remote jobs for developers\nJuxtdigital\n- Digital PR & AI Authority Building Agency\nCloudify.ro\n- Cloud hosting solutions\nCodeRabbit\n- Cut Code Review Time & Bugs in Half\nAmerican Cloud\n- US-based cloud infrastructure services\nMassiveGrid\n- Enterprise cloud hosting solutions\nSyntax.fm\n- Podcast for web developers\nTolgee\n- The open source localization platform\nCompAI\n- Open source compliance automation platform\nGoldenVM\n- Premium virtual machine hosting solutions\nGozunga\n- Seriously Simple Cloud Infrastructure\nMacarne\n- Best IP Transit & Carrier Ethernet Solutions for Simplified Network Connectivity\nSmall Sponsors\n...and many more at\nGitHub Sponsors\nRecognitions\nCore Maintainers\nAndras Bacsai\nğŸ”ï¸ Peak\nRepo Activity\nStar History",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 49",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 46,176"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/coollabsio/coolify"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/hyprwm/Hyprland",
      "title": "hyprwm/Hyprland",
      "date": null,
      "executive_summary": [
        "Hyprland is an independent, highly customizable, dynamic tiling Wayland compositor that doesn't sacrifice on its looks.",
        "---",
        "Hyprland is a 100% independent, dynamic tiling Wayland compositor that doesn't sacrifice on its looks.\nIt provides the latest Wayland features, is highly customizable, has all the eyecandy, the most powerful plugins,\neasy IPC, much more QoL stuff than other compositors and more...\nInstall\nQuick Start\nConfigure\nContribute\nFeatures\nAll of the eyecandy: gradient borders, blur, animations, shadows and much more\nA lot of customization\n100% independent, no wlroots, no libweston, no kwin, no mutter.\nCustom bezier curves for the best animations\nPowerful plugin support\nBuilt-in plugin manager\nTearing support for better gaming performance\nEasily expandable and readable codebase\nFast and active development\nNot afraid to provide bleeding-edge features\nConfig reloaded instantly upon saving\nFully dynamic workspaces\nTwo built-in layouts and more available as plugins\nGlobal keybinds passed to your apps of choice\nTiling/pseudotiling/floating/fullscreen windows\nSpecial workspaces (scratchpads)\nWindow groups (tabbed mode)\nPowerful window/monitor/layer rules\nSocket-based IPC\nNative IME and Input Panels Support\nand much more...\nGallery\nSpecial Thanks\nwlroots\n-\nFor powering Hyprland in the past\ntinywl\n-\nFor showing how 2 do stuff\nSway\n-\nFor showing how 2 do stuff the overkill way\nVivarium\n-\nFor showing how 2 do stuff the simple way\ndwl\n-\nFor showing how 2 do stuff the hacky way\nWayfire\n-\nFor showing how 2 do some graphics stuff",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 49",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 31,051"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/hyprwm/Hyprland"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/tangyoha/telegram_media_downloader",
      "title": "tangyoha/telegram_media_downloader",
      "date": null,
      "executive_summary": [
        "åŸºäºDineshkarthikçš„é¡¹ç›®ï¼Œ ç”µæŠ¥è§†é¢‘ä¸‹è½½ï¼Œç”µæŠ¥èµ„æºä¸‹è½½ï¼Œè·¨å¹³å°ï¼Œæ”¯æŒwebæŸ¥çœ‹ä¸‹è½½è¿›åº¦ ï¼Œæ”¯æŒbotä¸‹å‘æŒ‡ä»¤ä¸‹è½½ï¼Œæ”¯æŒä¸‹è½½å·²ç»åŠ å…¥çš„ç§æœ‰ç¾¤ä½†æ˜¯é™åˆ¶ä¸‹è½½çš„èµ„æºï¼Œ telegram media download,Download media files from a telegram conversation/chat/channel up to 2GiB per file",
        "---",
        "Telegram Media Downloader\nä¸­æ–‡\nÂ·\nFeature request\nÂ·\nReport a bug\nÂ·\nSupport:\nDiscussions\n&\nTelegram Community\nOverview\nSupport two default running\nThe robot is running, and the command\ndownload\nor\nforward\nis issued from the robot\nDownload as a one-time download tool\nUI\nWeb page\nAfter running, open a browser and visit\nlocalhost:5000\nIf it is a remote machine, you need to configure web_host: 0.0.0.0\nRobot\nNeed to configure bot_token, please refer to\nDocumentation\nSupport\nCategory\nSupport\nLanguage\nPython 3.7\nand above\nDownload media types\naudio, document, photo, video, video_note, voice\nVersion release plan\nv2.2.0\nInstallation\nFor *nix os distributions with\nmake\navailability\ngit clone https://github.com/tangyoha/telegram_media_downloader.git\ncd\ntelegram_media_downloader\nmake install\nFor Windows which doesn't have\nmake\ninbuilt\ngit clone https://github.com/tangyoha/telegram_media_downloader.git\ncd\ntelegram_media_downloader\npip3 install -r requirements.txt\nDocker\nFor more detailed installation tutorial, please check the wiki\nMake sure you have\ndocker\nand\ndocker-compose\ninstalled\ndocker pull tangyoha/telegram_media_downloader:latest\nmkdir -p\n~\n/app\n&&\nmkdir -p\n~\n/app/log/\n&&\ncd\n~\n/app\nwget https://raw.githubusercontent.com/tangyoha/telegram_media_downloader/master/docker-compose.yaml -O docker-compose.yaml\nwget https://raw.githubusercontent.com/tangyoha/telegram_media_downloader/master/config.yaml -O config.yaml\nwget https://raw.githubusercontent.com/tangyoha/telegram_media_downloader/master/data.yaml -O data.yaml\n#\nvi config.yaml and docker-compose.yaml\nvi config.yaml\n#\nThe first time you need to start the foreground\n#\nenter your phone number and code, then exit(ctrl + c)\ndocker-compose run --rm telegram_media_downloader\n#\nAfter performing the above operations, all subsequent startups will start in the background\ndocker-compose up -d\n#\nUpgrade\ndocker pull tangyoha/telegram_media_downloader:latest\ncd\n~\n/app\ndocker-compose down\ndocker-compose up -d\nUpgrade installation\ncd\ntelegram_media_downloader\npip3 install -r requirements.txt\nConfiguration\nAll the configurations are  passed to the Telegram Media Downloader via\nconfig.yaml\nfile.\nGetting your API Keys:\nThe very first step requires you to obtain a valid Telegram API key (API id/hash pair):\nVisit\nhttps://my.telegram.org/apps\nand log in with your Telegram Account.\nFill out the form to register a new Telegram application.\nDone! The API key consists of two parts:\napi_id\nand\napi_hash\n.\nGetting chat id:\n1. Using web telegram:\nOpen\nhttps://web.telegram.org/?legacy=1#/im\nNow go to the chat/channel and you will see the URL as something like\nhttps://web.telegram.org/?legacy=1#/im?p=u853521067_2449618633394\nhere\n853521067\nis the chat id.\nhttps://web.telegram.org/?legacy=1#/im?p=@somename\nhere\nsomename\nis the chat id.\nhttps://web.telegram.org/?legacy=1#/im?p=s1301254321_6925449697188775560\nhere take\n1301254321\nand add\n-100\nto the start of the id =>\n-1001301254321\n.\nhttps://web.telegram.org/?legacy=1#/im?p=c1301254321_6925449697188775560\nhere take\n1301254321\nand add\n-100\nto the start of the id =>\n-1001301254321\n.\n2. Using bot:\nUse\n@username_to_id_bot\nto get the chat_id of\nalmost any telegram user: send username to the bot or just forward their message to the bot\nany chat: send chat username or copy and send its joinchat link to the bot\npublic or private channel: same as chats, just copy and send to the bot\nid of any telegram bot\nconfig.yaml\napi_hash\n:\nyour_api_hash\napi_id\n:\nyour_api_id\nchat\n:\n-\nchat_id\n:\ntelegram_chat_id\nlast_read_message_id\n:\n0\ndownload_filter\n:\nmessage_date >= 2022-12-01 00:00:00 and message_date <= 2023-01-17 00:00:00\n-\nchat_id\n:\ntelegram_chat_id_2\nlast_read_message_id\n:\n0\n#\nnote we remove ids_to_retry to data.yaml\nids_to_retry\n:\n[]\nmedia_types\n:\n-\naudio\n-\ndocument\n-\nphoto\n-\nvideo\n-\nvoice\n-\nanimation\n#\ngif\nfile_formats\n:\naudio\n:\n  -\nall\ndocument\n:\n  -\npdf\n-\nepub\nvideo\n:\n  -\nmp4\nsave_path\n:\nD:\\telegram_media_downloader\nfile_path_prefix\n:\n-\nchat_title\n-\nmedia_datetime\nupload_drive\n:\n#\nrequired\nenable_upload_file\n:\ntrue\n#\nrequired\nremote_dir\n:\ndrive:/telegram\n#\nrequired\nupload_adapter\n:\nrclone\n#\noption,when config upload_adapter rclone then this config are required\nrclone_path\n:\nD:\\rclone\\rclone.exe\n#\noption\nbefore_upload_file_zip\n:\nTrue\n#\noption\nafter_upload_file_delete\n:\nTrue\nhide_file_name\n:\ntrue\nfile_name_prefix\n:\n-\nmessage_id\n-\nfile_name\nfile_name_prefix_split\n:\n'\n-\n'\nmax_download_task\n:\n5\nweb_host\n:\n127.0.0.1\nweb_port\n:\n5000\nlanguage\n:\nEN\nweb_login_secret\n:\n123\nallowed_user_ids\n:\n-\n'\nme\n'\ndate_format\n:\n'\n%Y_%m\n'\nenable_download_txt\n:\nfalse\napi_hash\n- The api_hash you got from telegram apps\napi_id\n- The api_id you got from telegram apps\nbot_token\n- Your bot token\nchat\n- Chat list\nchat_id\n-  The id of the chat/channel you want to download media. Which you get from the above-mentioned steps.\ndownload_filter\n- Download filter, see\nHow to use Filter\nlast_read_message_id\n- If it is the first time you are going to read the channel let it be\n0\nor if you have already used this script to download media it will have some numbers which are auto-updated after the scripts successful execution. Don't change it.\nids_to_retry\n-\nLeave it as it is.\nThis is used by the downloader script to keep track of all skipped downloads so that it can be downloaded during the next execution of the script.\nmedia_types\n- Type of media to download, you can update which type of media you want to download it can be one or any of the available types.\nfile_formats\n- File types to download for supported media types which are\naudio\n,\ndocument\nand\nvideo\n. Default format is\nall\n, downloads all files.\nsave_path\n- The root directory where you want to store downloaded files.\nfile_path_prefix\n- Store file subfolders, the order of the list is not fixed, can be randomly combined.\nchat_title\n- Channel or group title, it will be chat id if not exist title.\nmedia_datetime\n- Media date.\nmedia_type\n- Media type, also see\nmedia_types\n.\nupload_drive\n- You can upload file to cloud drive.\nenable_upload_file\n- Enable upload file, default\nfalse\n.\nremote_dir\n- Where you upload, like\ndrive_id/drive_name\n.\nupload_adapter\n- Upload file adapter, which can be\nrclone\n,\naligo\n. If it is\nrclone\n, it supports all\nrclone\nservers that support uploading. If it is\naligo\n, it supports uploading\nAli cloud disk\n.\nrclone_path\n- RClone exe path, see\nHow to use rclone\nbefore_upload_file_zip\n- Zip file before upload, default\nfalse\n.\nafter_upload_file_delete\n- Delete file after upload success, default\nfalse\n.\nfile_name_prefix\n- Custom file name, use the same as\nfile_path_prefix\nmessage_id\n- Message id\nfile_name\n- File name (may be empty)\ncaption\n- The title of the message (may be empty)\nfile_name_prefix_split\n- Custom file name prefix symbol, the default is\n-\nmax_download_task\n- The maximum number of task download tasks, the default is 5.\nhide_file_name\n- Whether to hide the web interface file name, default\nfalse\nweb_host\n- Web host\nweb_port\n- Web port\nlanguage\n- Application language, the default is English (\nEN\n), optional\nZH\n(Chinese),\nRU\n,\nUA\nweb_login_secret\n- Web page login password, if not configured, no login is required to access the web page\nlog_level\n- see\nlogging._nameToLevel\n.\nforward_limit\n- Limit the number of forwards per minute, the default is 33, please do not modify this parameter by default.\nallowed_user_ids\n- Who is allowed to use the robot? The default login account can be used. Please add single quotes to the name with @.\ndate_format\nSupport custom configuration of media_datetime format in file_path_prefix.see\npython-datetime\nenable_download_txt\nEnable download txt file, default\nfalse\nExecution\npython3 media_downloader.py\nAll downloaded media will be stored at the root of\nsave_path\n.\nThe specific location reference is as follows:\nThe complete directory of video download is:\nsave_path\n/\nchat_title\n/\nmedia_datetime\n/\nmedia_type\n.\nThe order of the list is not fixed and can be randomly combined.\nIf the configuration is empty, all files are saved under\nsave_path\n.\nProxy\nsocks4, socks5, http\nproxies are supported in this project currently. To use it, add the following to the bottom of your\nconfig.yaml\nfile\nproxy\n:\nscheme\n:\nsocks5\nhostname\n:\n127.0.0.1\nport\n:\n1234\nusername\n:\nyour_username(delete the line if none)\npassword\n:\nyour_password(delete the line if none)\nIf your proxy doesnâ€™t require authorization you can omit username and password. Then the proxy will automatically be enabled.\nContributing\nContributing Guidelines\nRead through our\ncontributing guidelines\nto learn about our submission process, coding rules and more.\nWant to Help?\nWant to file a bug, contribute some code, or improve documentation? Excellent! Read up on our guidelines for\ncontributing\n.\nCode of Conduct\nHelp us keep Telegram Media Downloader open and inclusive. Please read and follow our\nCode of Conduct\n.\nSponsor\nPayPal",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 48",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 4,112"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/tangyoha/telegram_media_downloader"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/DioxusLabs/dioxus",
      "title": "DioxusLabs/dioxus",
      "date": null,
      "executive_summary": [
        "Fullstack app framework for web, desktop, and mobile.",
        "---",
        "Website\n|\nExamples\n|\nGuide\n|\nä¸­æ–‡\n|\nPT-BR\n|\næ—¥æœ¬èª\n|\nTÃ¼rkÃ§e\n|\ní•œêµ­ì–´\nâœ¨ Dioxus 0.7 is in alpha - test it out! âœ¨\nBuild for web, desktop, and mobile, and more with a single codebase. Zero-config setup, integrated hot-reloading, and signals-based state management. Add backend functionality with Server Functions and bundle with our CLI.\nfn\napp\n(\n)\n->\nElement\n{\nlet\nmut\ncount =\nuse_signal\n(\n||\n0\n)\n;\nrsx\n!\n{\nh1\n{\n\"High-Five counter: {count}\"\n}\nbutton\n{\nonclick\n:\nmove |_| count +=\n1\n,\n\"Up high!\"\n}\nbutton\n{\nonclick\n:\nmove |_| count -=\n1\n,\n\"Down low!\"\n}\n}\n}\nâ­ï¸ Unique features:\nCross-platform apps in three lines of code (web, desktop, mobile, server, and more)\nErgonomic state management\ncombines the best of React, Solid, and Svelte\nBuilt-in featureful, type-safe, fullstack web framework\nIntegrated bundler for deploying to the web, macOS, Linux, and Windows\nSubsecond Rust hot-patching and asset hot-reloading\nAnd more!\nTake a tour of Dioxus\n.\nInstant hot-reloading\nWith one command,\ndx serve\nand your app is running. Edit your markup, styles, and see changes in milliseconds. Use our experimental\ndx serve --hotpatch\nto update Rust code in real time.\nBuild Beautiful Apps\nDioxus apps are styled with HTML and CSS. Use the built-in TailwindCSS support or load your favorite CSS library. Easily call into native code (objective-c, JNI, Web-Sys) for a perfect native touch.\nTruly fullstack applications\nDioxus deeply integrates with\naxum\nto provide powerful fullstack capabilities for both clients and servers. Pick from a wide array of built-in batteries like WebSockets, SSE, Streaming, File Upload/Download, Server-Side-Rendering, Forms, Middleware, and Hot-Reload, or go fully custom and integrate your existing axum backend.\nExperimental Native Renderer\nRender using web-sys, webview, server-side-rendering, liveview, or even with our experimental WGPU-based renderer. Embed Dioxus in Bevy, WGPU, or even run on embedded Linux!\nFirst-party primitive components\nGet started quickly with a complete set of primitives modeled after shadcn/ui and Radix-Primitives.\nFirst-class Android and iOS support\nDioxus is the fastest way to build native mobile apps with Rust. Simply run\ndx serve --platform android\nand your app is running in an emulator or on device in seconds. Call directly into JNI and Native APIs.\nBundle for web, desktop, and mobile\nSimply run\ndx bundle\nand your app will be built and bundled with maximization optimizations. On the web, take advantage of\n.avif\ngeneration,\n.wasm\ncompression, minification\n, and more. Build WebApps weighing\nless than 50kb\nand desktop/mobile apps less than 5mb.\nFantastic documentation\nWe've put a ton of effort into building clean, readable, and comprehensive documentation. All html elements and listeners are documented with MDN docs, and our Docs runs continuous integration with Dioxus itself to ensure that the docs are always up to date. Check out the\nDioxus website\nfor guides, references, recipes, and more. Fun fact: we use the Dioxus website as a testbed for new Dioxus features -\ncheck it out!\nModular and Customizable\nBuild your own renderer, or use a community renderer like\nFreya\n. Use our modular components like RSX, VirtualDom, Blitz, Taffy, and Subsecond.\nCommunity\nDioxus is a community-driven project, with a very active\nDiscord\nand\nGitHub\ncommunity. We're always looking for help, and we're happy to answer questions and help you get started.\nOur SDK\nis community-run and we even have a\nGitHub organization\nfor the best Dioxus crates that receive free upgrades and support.\nFull-time core team\nDioxus has grown from a side project to a small team of fulltime engineers. Thanks to the generous support of FutureWei, Satellite.im, the GitHub Accelerator program, we're able to work on Dioxus full-time. Our long term goal is for Dioxus to become self-sustaining by providing paid high-quality enterprise tools. If your company is interested in adopting Dioxus and would like to work with us, please reach out!\nSupported Platforms\nWeb\nRender directly to the DOM using WebAssembly\nPre-render with SSR and rehydrate on the client\nSimple \"hello world\" at about 50kb, comparable to React\nBuilt-in dev server and hot reloading for quick iteration\nDesktop\nRender using Webview or - experimentally - with WGPU or\nFreya\n(Skia)\nZero-config setup. Simply `cargo run` or `dx serve` to build your app\nFull support for native system access without IPC\nSupports macOS, Linux, and Windows. Portable <3mb binaries\nMobile\nRender using Webview or - experimentally - with WGPU or Skia\nBuild .ipa and .apk files for iOS and Android\nCall directly into Java and Objective-C with minimal overhead\nFrom \"hello world\" to running on device in seconds\nServer-side Rendering\nSuspense, hydration, and server-side rendering\nQuickly drop in backend functionality with server functions\nExtractors, middleware, and routing integrations\nStatic-site generation and incremental regeneration\nRunning the examples\nThe examples in the main branch of this repository target the git version of dioxus and the CLI. If you are looking for examples that work with the latest stable release of dioxus, check out the\n0.6 branch\n.\nThe examples in the top level of this repository can be run with:\ncargo run --example\n<\nexample\n>\nHowever, we encourage you to download the dioxus-cli to test out features like hot-reloading. To install the most recent binary CLI, you can use cargo binstall.\ncargo binstall dioxus-cli@0.7.0-rc.1 --force\nIf this CLI is out-of-date, you can install it directly from git\ncargo install --git https://github.com/DioxusLabs/dioxus dioxus-cli --locked\nWith the CLI, you can also run examples with the web platform. You will need to disable the default desktop feature and enable the web feature with this command:\ndx serve --example\n<\nexample\n>\n--platform web -- --no-default-features\nContributing\nCheck out the website\nsection on contributing\n.\nReport issues on our\nissue tracker\n.\nJoin\nthe discord and ask questions!\nLicense\nThis project is licensed under either the\nMIT license\nor the\nApache-2 License\n.\nUnless you explicitly state otherwise, any contribution intentionally submitted\nfor inclusion in Dioxus by you, shall be licensed as MIT or Apache-2, without any additional\nterms or conditions.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 47",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 31,009"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/DioxusLabs/dioxus"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/WECENG/ticket-purchase",
      "title": "WECENG/ticket-purchase",
      "date": null,
      "executive_summary": [
        "å¤§éº¦è‡ªåŠ¨æŠ¢ç¥¨ï¼Œæ”¯æŒäººå‘˜ã€åŸå¸‚ã€æ—¥æœŸåœºæ¬¡ã€ä»·æ ¼é€‰æ‹©",
        "---",
        "å¤§éº¦æŠ¢ç¥¨è„šæœ¬ V1.0\nç‰¹å¾\nè‡ªåŠ¨æ— å»¶æ—¶æŠ¢ç¥¨\næ”¯æŒäººå‘˜ã€åŸå¸‚ã€æ—¥æœŸåœºæ¬¡ã€ä»·æ ¼é€‰æ‹©\nåŠŸèƒ½ä»‹ç»\né€šè¿‡seleniumæ‰“å¼€é¡µé¢è¿›è¡Œç™»å½•ï¼Œæ¨¡æ‹Ÿç”¨æˆ·è´­ç¥¨æµç¨‹è‡ªåŠ¨è´­ç¥¨\nå…¶æµç¨‹å›¾å¦‚ä¸‹:\nå‡†å¤‡å·¥ä½œ\n1. é…ç½®ç¯å¢ƒ\n1.1å®‰è£…python3ç¯å¢ƒ\nWindows\nè®¿é—®Pythonå®˜æ–¹ç½‘ç«™ï¼š\nhttps://www.python.org/downloads/windows/\nä¸‹è½½æœ€æ–°çš„Python 3.9+ç‰ˆæœ¬çš„å®‰è£…ç¨‹åºã€‚\nè¿è¡Œå®‰è£…ç¨‹åºã€‚\nåœ¨å®‰è£…ç¨‹åºä¸­ï¼Œç¡®ä¿å‹¾é€‰ \"Add Python X.X to PATH\" é€‰é¡¹ï¼Œè¿™å°†è‡ªåŠ¨å°†Pythonæ·»åŠ åˆ°ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­ï¼Œæ–¹ä¾¿åœ¨å‘½ä»¤è¡Œä¸­ä½¿ç”¨Pythonã€‚\nå®Œæˆå®‰è£…åï¼Œä½ å¯ä»¥åœ¨å‘½ä»¤æç¤ºç¬¦æˆ–PowerShellä¸­è¾“å…¥\npython3\næ¥å¯åŠ¨Pythonè§£é‡Šå™¨ã€‚\nmacOS\nä½ å¯ä»¥ä½¿ç”¨Homebrewæ¥å®‰è£…Python 3ã€‚\nå®‰è£…Homebrewï¼ˆå¦‚æœæœªå®‰è£…ï¼‰ï¼šæ‰“å¼€ç»ˆç«¯å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n/bin/bash -c\n\"\n$(\ncurl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh\n)\n\"\nå®‰è£…Python 3ï¼šè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…Python 3ï¼š\nbrew install python@3\n1.2 å®‰è£…æ‰€éœ€è¦çš„ç¯å¢ƒ\nåœ¨å‘½ä»¤çª—å£è¾“å…¥å¦‚ä¸‹æŒ‡ä»¤\npip3 install selenium\n1.3 ä¸‹è½½google chromeæµè§ˆå™¨\nä¸‹è½½åœ°å€:\nhttps://www.google.cn/intl/zh-CN/chrome/?brand=YTUH&gclid=Cj0KCQjwj5mpBhDJARIsAOVjBdoV_1sBwdqKGHV3rUU1vJmNKZdy5QNzbRT8F5O0-_jq1WHXurE8a7MaAkWrEALw_wcB&gclsrc=aw.ds\n2. ä¿®æ”¹é…ç½®æ–‡ä»¶\nåœ¨è¿è¡Œç¨‹åºä¹‹å‰ï¼Œéœ€è¦å…ˆä¿®æ”¹\nconfig.json\næ–‡ä»¶ã€‚è¯¥æ–‡ä»¶ç”¨äºæŒ‡å®šç”¨æˆ·éœ€è¦æŠ¢ç¥¨çš„ç›¸å…³ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¼”å”±ä¼šçš„åœºæ¬¡ã€è§‚æ¼”çš„äººå‘˜ã€åŸå¸‚ã€æ—¥æœŸã€ä»·æ ¼ç­‰ã€‚æ–‡ä»¶ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n2.1 æ–‡ä»¶å†…å®¹è¯´æ˜\nindex_url\nä¸ºå¤§éº¦ç½‘çš„åœ°å€ï¼Œ\næ— éœ€ä¿®æ”¹\nlogin_url\nä¸ºå¤§éº¦ç½‘çš„ç™»å½•åœ°å€ï¼Œ\næ— éœ€ä¿®æ”¹\ntarget_url\nä¸ºç”¨æˆ·éœ€è¦æŠ¢çš„æ¼”å”±ä¼šç¥¨çš„ç›®æ ‡åœ°å€ï¼Œ\nå¾…ä¿®æ”¹\nusers\nä¸ºè§‚æ¼”äººçš„å§“åï¼Œ\nè§‚æ¼”äººéœ€è¦ç”¨æˆ·åœ¨æ‰‹æœºå¤§éº¦APPä¸­å…ˆå¡«å†™å¥½ï¼Œç„¶åå†å¡«å…¥è¯¥é…ç½®æ–‡ä»¶ä¸­\nï¼Œ\nå¾…ä¿®æ”¹\ncity\nä¸ºåŸå¸‚ï¼Œ\nå¦‚æœç”¨æˆ·éœ€è¦æŠ¢çš„æ¼”å”±ä¼šç¥¨éœ€è¦é€‰æ‹©åŸå¸‚ï¼Œè¯·æŠŠåŸå¸‚å¡«å…¥æ­¤å¤„ã€‚å¦‚æ— éœ€é€‰æ‹©ï¼Œåˆ™ä¸å¡«\ndate\nä¸ºåœºæ¬¡æ—¥æœŸï¼Œ\nå¾…ä¿®æ”¹ï¼Œå¯å¤šé€‰\nprice\nä¸ºç¥¨æ¡£çš„ä»·æ ¼ï¼Œ\nå¾…ä¿®æ”¹ï¼Œå¯å¤šé€‰\nif_commit_order\nä¸ºæ˜¯å¦è¦è‡ªåŠ¨æäº¤è®¢å•ï¼Œ\næ”¹æˆ true\nif_listenä¸ºæ˜¯å¦å›æµç›‘å¬ï¼Œ\næ”¹æˆtrue\n2.2 ç¤ºä¾‹è¯´æ˜\nè¿›å…¥å¤§éº¦ç½‘\nhttps://www.damai.cn/ï¼Œé€‰æ‹©ä½ éœ€è¦æŠ¢ç¥¨çš„æ¼”å”±ä¼šã€‚å‡è®¾å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\næ¥ä¸‹æ¥æŒ‰ç…§ä¸‹å›¾çš„æ ‡æ³¨å¯¹é…ç½®æ–‡ä»¶è¿›è¡Œä¿®æ”¹ï¼š\næœ€ç»ˆ\nconfig.json\nçš„æ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š\n{\n\"index_url\"\n:\n\"\nhttps://www.damai.cn/\n\"\n,\n\"login_url\"\n:\n\"\nhttps://passport.damai.cn/login?ru=https%3A%2F%2Fwww.damai.cn%2F\n\"\n,\n\"target_url\"\n:\n\"\nhttps://detail.damai.cn/item.htm?spm=a2oeg.home.card_0.ditem_1.591b23e1JQGWHg&id=740680932762\n\"\n,\n\"users\"\n: [\n\"\nåå­—1\n\"\n,\n\"\nåå­—2\n\"\n],\n\"city\"\n:\n\"\nå¹¿å·\n\"\n,\n\"date\"\n:\n\"\n2023-10-28\n\"\n,\n\"price\"\n:\n\"\n1039\n\"\n,\n\"if_listen\"\n:\ntrue\n,\n\"if_commit_order\"\n:\ntrue\n}\n3.è¿è¡Œç¨‹åº\nè¿è¡Œç¨‹åºå¼€å§‹æŠ¢ç¥¨ï¼Œè¿›å…¥å‘½ä»¤çª—å£ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š\ncd\ndamai\npython3 damai.py\nå¤§éº¦appæŠ¢ç¥¨\nå¤§éº¦appæŠ¢ç¥¨è„šæœ¬éœ€è¦ä¾èµ–appiumï¼Œå› æ­¤éœ€è¦ç°åœ¨å®‰è£…appium server&clientç¯å¢ƒï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š\nappium server\nä¸‹è½½\nå…ˆå®‰è£…å¥½nodeç¯å¢ƒï¼ˆå…·å¤‡npmï¼‰nodeç‰ˆæœ¬å·18.0.0\nå…ˆä¸‹è½½å¹¶å®‰è£…å¥½android sdkï¼Œå¹¶é…ç½®ç¯å¢ƒå˜é‡ï¼ˆappium serverè¿è¡Œéœ€ä¾èµ–android sdk)\nä¸‹è½½appium\nnpm install -g appium\næŸ¥çœ‹appiumæ˜¯å¦å®‰è£…æˆåŠŸ\nappium -v\nä¸‹è½½UiAutomator2é©±åŠ¨\nnpm install appium-uiautomator2-driver\nâ€‹\t\tå¯èƒ½ä¼šé‡åˆ°å¦‚ä¸‹é”™è¯¯ï¼š\nâœ  xcode git:(master) âœ— npm install appium-uiautomator2-driver\n\nnpm ERR! code 1\nnpm ERR! path /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/appium-chromedriver\nnpm ERR! command failed\nnpm ERR! command sh -c node install-npm.js\nnpm ERR! [11:57:54] Error installing Chromedriver: Request failed with status code 404\nnpm ERR! [11:57:54] AxiosError: Request failed with status code 404\nnpm ERR!     at settle (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/core/settle.js:19:12)\nnpm ERR!     at IncomingMessage.handleStreamEnd (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/adapters/http.js:572:11)\nnpm ERR!     at IncomingMessage.emit (node:events:539:35)\nnpm ERR!     at endReadableNT (node:internal/streams/readable:1344:12)\nnpm ERR!     at processTicksAndRejections (node:internal/process/task_queues:82:21)\nnpm ERR! [11:57:54] Downloading Chromedriver can be skipped by setting the'APPIUM_SKIP_CHROMEDRIVER_INSTALL' environment variable.\n\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     /Users/chenweicheng/.npm/_logs/2023-10-26T03_57_35_950Z-debug-0.log\nâ€‹\t\tè§£å†³åŠæ³•ï¼ˆæ·»åŠ ç¯å¢ƒå˜é‡ï¼Œé”™è¯¯åŸå› æ˜¯æ²¡æœ‰æ‰¾åˆ°chromeæµè§ˆå™¨é©±åŠ¨ï¼Œå¿½ç•¥å³å¯ï¼‰\nexport\nAPPIUM_SKIP_CHROMEDRIVER_INSTALL=true\nå¯åŠ¨\nå¯åŠ¨appium serverå¹¶ä½¿ç”¨uiautomator2é©±åŠ¨\nappium --use-plugins uiautomator2\nå¯åŠ¨æˆåŠŸå°†å‡ºç°å¦‚ä¸‹ä¿¡æ¯ï¼š\n[Appium] Welcome to Appium v2.2.1 (REV 2176894a5be5da17a362bf3f20678641a78f4b69)\n[Appium] Non-default server args:\n[Appium] {\n[Appium]   usePlugins: [\n[Appium]     'uiautomator2'\n[Appium]   ]\n[Appium] }\n[Appium] Attempting to load driver uiautomator2...\n[Appium] Requiring driver at /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver\n[Appium] Appium REST http interface listener started on http://0.0.0.0:4723\n[Appium] You can provide the following URLs in your client code to connect to this server:\n[Appium] \thttp://127.0.0.1:4723/ (only accessible from the same host)\n[Appium] \thttp://172.31.102.45:4723/\n[Appium] \thttp://198.18.0.1:4723/\n[Appium] Available drivers:\n[Appium]   - uiautomator2@2.32.3 (automationName 'UiAutomator2')\n[Appium] No plugins have been installed. Use the \"appium plugin\" command to install the one(s) you want to use.\nå…¶ä¸­\n[Appium] \thttp://127.0.0.1:4723/ (only accessible from the same host) [Appium] \thttp://172.31.102.45:4723/ [Appium] \thttp://198.18.0.1:4723/\nä¸ºappium serverè¿æ¥åœ°å€\nappium client\nå…ˆä¸‹è½½å¹¶å®‰è£…å¥½python3å’Œpip3\nå®‰è£…\npip3 install appium-python-client\nåœ¨ä»£ç ä¸­å¼•å…¥å¹¶ä½¿ç”¨appium\nfrom\nappium\nimport\nwebdriver\nfrom\nappium\n.\noptions\n.\ncommon\n.\nbase\nimport\nAppiumOptions\ndevice_app_info\n=\nAppiumOptions\n()\ndevice_app_info\n.\nset_capability\n(\n'platformName'\n,\n'Android'\n)\ndevice_app_info\n.\nset_capability\n(\n'platformVersion'\n,\n'10'\n)\ndevice_app_info\n.\nset_capability\n(\n'deviceName'\n,\n'YourDeviceName'\n)\ndevice_app_info\n.\nset_capability\n(\n'appPackage'\n,\n'cn.damai'\n)\ndevice_app_info\n.\nset_capability\n(\n'appActivity'\n,\n'.launcher.splash.SplashMainActivity'\n)\ndevice_app_info\n.\nset_capability\n(\n'unicodeKeyboard'\n,\nTrue\n)\ndevice_app_info\n.\nset_capability\n(\n'resetKeyboard'\n,\nTrue\n)\ndevice_app_info\n.\nset_capability\n(\n'noReset'\n,\nTrue\n)\ndevice_app_info\n.\nset_capability\n(\n'newCommandTimeout'\n,\n6000\n)\ndevice_app_info\n.\nset_capability\n(\n'automationName'\n,\n'UiAutomator2'\n)\n# è¿æ¥appium serverï¼Œserveråœ°å€æŸ¥çœ‹appiumå¯åŠ¨ä¿¡æ¯\ndriver\n=\nwebdriver\n.\nRemote\n(\n'http://127.0.0.1:4723'\n,\noptions\n=\ndevice_app_info\n)\nå¯åŠ¨è„šæœ¬ç¨‹åº\ncd\ndamai_appium\npython3 damai_appium.py",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 46",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 4,482"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/WECENG/ticket-purchase"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/coze-dev/coze-studio",
      "title": "coze-dev/coze-studio",
      "date": null,
      "executive_summary": [
        "An AI agent development platform with all-in-one visual tools, simplifying agent creation, debugging, and deployment like never before. Coze your way to AI Agent creation.",
        "---",
        "Coze Studio\nâ€¢\nFeature list\nâ€¢\nQuickstart\nâ€¢\nDeveloper Guide\nEnglish |\nä¸­æ–‡\nWhat is Coze Studio?\nCoze Studio\nis an all-in-one AI agent development tool. Providing the latest large models and tools, various development modes and frameworks, Coze Studio offers the most convenient AI agent development environment, from development to deployment.\nProvides all core technologies needed for AI agent development\n: prompt, RAG, plugin, workflow, enabling developers to focus on creating the core value of AI.\nReady to use for professional AI agent development at the lowest cost\n: Coze Studio provides developers with complete app templates and build frameworks, allowing you to quickly construct various AI agents and turn creative ideas into reality.\nCoze Studio, derived from the \"Coze Development Platform\" which has served tens of thousands of enterprises and millions of developers, we have made its core engine completely open. It is a one-stop visual development tool for AI Agents that makes creating, debugging, and deploying AI Agents unprecedentedly simple. Through Coze Studio's visual design and build tools, developers can quickly create and debug agents, apps, and workflows using no-code or low-code approaches, enabling powerful AI app development and more customized business logic. It's an ideal choice for building low-code AI products tailored . Coze Studio aims to lower the threshold for AI agent development and application, encouraging community co-construction and sharing for deeper exploration and practice in the AI field.\nThe backend of Coze Studio is developed using Golang, the frontend uses React + TypeScript, and the overall architecture is based on microservices and built following domain-driven design (DDD) principles. Provide developers with a high-performance, highly scalable, and easy-to-customize underlying framework to help them address complex business needs.\nFeature list\nModule\nFeature\nModel service\nManage the model list, integrate services such as OpenAI and Volcengine\nBuild agent\n* Build, publish, and manage agent\n* Support configuring workflows, knowledge bases, and other resources\nBuild apps\n* Create and publish apps\n* Build business logic through workflows\nBuild a workflow\nCreate, modify, publish, and delete workflows\nDevelop resources\nSupport creating and managing the following resources:\n* Plugins\n* Knowledge bases\n* Databases\n* Prompts\nAPI and SDK\n* Create conversations, initiate chats, and other OpenAPI\n* Integrate agents or apps into your own app through Chat SDK\nQuickstart\nLearn how to obtain and deploy the open-source version of Coze Studio, quickly build projects, and experience Coze Studio's open-source version.\nEnvironment requirements:\nBefore installing Coze Studio, please ensure that your machine meets the following minimum system requirements: 2 Coreã€4 GB\nPre-install Docker and Docker Compose, and start the Docker service.\nDeployment steps:\nRetrieve the source code.\n#\nClone code\ngit clone https://github.com/coze-dev/coze-studio.git\nConfigure the model.\nCopy the template files of the doubao-seed-1.6 model from the template directory and paste them into the configuration file directory.\ncd\ncoze-studio\n#\nCopy model configuration template\ncp backend/conf/model/template/model_template_ark_doubao-seed-1.6.yaml backend/conf/model/ark_doubao-seed-1.6.yaml\nModify the template file in the configuration file directory.\nEnter the directory\nbackend/conf/model\n. Open the file\nark_doubao-seed-1.6.yaml\n.\nSet the fields\nid\n,\nmeta.conn_config.api_key\n,\nmeta.conn_config.model\n, and save the file.\nid\n: The model ID in Coze Studio, defined by the developer, must be a non-zero integer and globally unique. Agents or workflows call models based on model IDs. For models that have already been launched, do not modify their IDs; otherwise, it may result in model call failures.\nmeta.conn_config.api_key\n: The API Key for the model service. In this example, it is the API Key for Ark API Key. For more information, see\nGet Volcengine Ark API Key\nor\nGet BytePlus ModelArk API Key\n.\nmeta.conn_config.model\n: The Model name for the model service. In this example, it refers to the Model ID or Endpoint ID of Ark. For more information, see\nGet Volcengine Ark Model ID\n/\nGet Volcengine Ark Endpoint ID\nor\nGet BytePlus ModelArk Model ID\n/\nGet BytePlus ModelArk Endpoint ID\n.\nFor users in China, you may use Volcengine Ark; for users outside China, you may use BytePlus ModelArk instead.\nDeploy and start the service.\nWhen deploying and starting Coze Studio for the first time, it may take a while to retrieve images and build local images. Please be patient. During deployment, you will see the following log information. If you see the message \"Container coze-server Started,\" it means the Coze Studio service has started successfully.\n#\nStart the service\ncd\ndocker\ncp .env.example .env\ndocker compose up -d\nFor common startup failure issues,\nplease refer to the\nFAQ\n.\nAfter starting the service, you can open Coze Studio by accessing\nhttp://localhost:8888/\nthrough your browser.\nWarning\nIf you want to deploy Coze Studio in a public network environment, it is recommended to assess security risks before you begin, and take corresponding protection measures. Possible security risks include account registration functions, Python execution environments in workflow code nodes, Coze Server listening address configurations, SSRF (Server - Side Request Forgery), and some horizontal privilege escalations in APIs.  For more details, refer to\nQuickstart\n.\nDeveloper Guide\nProject Configuration\n:\nModel Configuration\n: Before deploying the open-source version of Coze Studio, you must configure the model service. Otherwise, you cannot select models when building agents, workflows, and apps.\nPlugin Configuration\n: To use official plugins from the plugin store, you must first configure the plugins and add the authentication keys for third-party services.\nBasic Component Configuration\n: Learn how to configure components such as image uploaders to use functions like image uploading in Coze Studio .\nAPI Reference\n: The Coze Studio Community Edition API and Chat SDK are authenticated using Personal Access Token, providing APIs for conversations and workflows.\nDevelopment Guidelines\n:\nProject Architecture\n: Learn about the technical architecture and core components of the open-source version of Coze Studio.\nCode Development and Testing\n: Learn how to perform secondary development and testing based on the open-source version of Coze Studio.\nTroubleshooting\n: Learn how to view container states and system logs.\nUsing the open-source version of Coze Studio\nRegarding how to use Coze Studio, refer to the\nCoze Development Platform Official Documentation Center\nfor more information. Please note that certain features, such as tone customization, are limited to the commercial version. Differences between the open-source and commercial versions can be found in the\nFeature List\n.\nQuick Start\n: Quickly build an AI assistant agent with Coze Studio.\nDeveloping Agents\n: Learn how to create, build, publish, and manage agents. You can use functions such as knowledge, plugins, etc., to resolve model hallucination and lack of expertise in professional fields. In addition, Coze Studio provides rich memory features that enable agents to generate more accurate responses based on a personal user's historical conversations during interactions.\nDevelop workflows\n: A workflow is a set of executable instructions used to implement business logic or complete specific tasks. It structures data flow and task processing for apps or agents. Coze Studio provides a visual canvas where you can quickly build workflows by dragging and dropping nodes.\nResources such as plugins\n: In Coze Studio, workflows, plugins, databases, knowledge bases, and variables are collectively referred to as resources.\nAPI & SDK\n: Coze Studio supports\nAPI related to chat and workflows\n, and you can also integrate agents or apps with local business systems through\nChat SDK\n.\nTutorials for practice\n: Learn how to use Coze Studio to implement various AI scenarios, such as building web-based online customer service using Chat SDK.\nLicense\nThis project uses the Apache 2.0 license. For details, please refer to the\nLICENSE\nfile.\nCommunity contributions\nWe welcome community contributions. For contribution guidelines, please refer to\nCONTRIBUTING\nand\nCode of conduct\n. We look forward to your contributions!\nSecurity and privacy\nIf you discover potential security issues in the project, or believe you may have found a security issue, please notify the ByteDance security team through our\nsecurity center\nor\nvulnerability reporting email\n.\nPlease\ndo not\ncreate public GitHub Issues.\nJoin Community\nWe are committed to building an open and friendly developer community. All developers interested in AI Agent development are welcome to join us!\nğŸ› Issue Reports & Feature Requests\nTo efficiently track and resolve issues while ensuring transparency and collaboration, we recommend participating through:\nGitHub Issues\n:\nSubmit bug reports or feature requests\nPull Requests\n:\nContribute code or documentation improvements\nğŸ’¬ Technical Discussion & Communication\nJoin our technical discussion groups to share experiences with other developers and stay updated with the latest project developments:\nFeishu Group Chat\nScan the QR code below with Feishu mobile app to join:\nDiscord Server\nClick to join:\nCoze Community\nTelegram Group\nClick to join: Telegram Group\nCoze\nAcknowledgments\nThank you to all the developers and community members who have contributed to the Coze Studio project. Special thanks:\nThe\nEino\nframework team - providing powerful support for Coze Studio's agent and workflow runtime engines, model abstractions and implementations, and knowledge base indexing and retrieval\nThe\nFlowGram\nteam - providing a high-quality workflow building engine for Coze Studio's frontend workflow canvas editor\nThe\nHertz\nteam - Go HTTP framework with high-performance and strong-extensibility for building micro-services\nAll users who participated in testing and feedback",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 44",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 17,538"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/coze-dev/coze-studio"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/jgraph/drawio-desktop",
      "title": "jgraph/drawio-desktop",
      "date": null,
      "executive_summary": [
        "Official electron build of draw.io",
        "---",
        "About\ndrawio-desktop\nis a diagramming desktop app based on\nElectron\nthat wraps the\ncore draw.io editor\n.\nDownload built binaries from the\nreleases section\n.\nCan I use this app for free?\nYes, under the apache 2.0 license. If you don't change the code and accept it is provided \"as-is\", you can use it for any purpose.\nSecurity\ndraw.io Desktop is designed to be completely isolated from the Internet, apart from the update process. This checks github.com at startup for a newer version and downloads it from an AWS S3 bucket owned by Github. All JavaScript files are self-contained, the Content Security Policy forbids running remotely loaded JavaScript.\nNo diagram data is ever sent externally, nor do we send any analytics about app usage externally. There is a Content Security Policy in place on the web part of the interface to ensure external transmission cannot happen, even by accident.\nSecurity and isolating the app are the primarily objectives of draw.io desktop. If you ask for anything that involves external connections enabled in the app by default, the answer will be no.\nSupport\nSupport is provided on a reasonable business constraints basis, but without anything contractually binding. All support is provided via this repo. There is no private ticketing support for non-paying users.\nPurchasing draw.io for Confluence or Jira does not entitle you to commercial support for draw.io desktop.\nDeveloping\ndraw.io\nis a git submodule of\ndrawio-desktop\n. To get both you need to clone recursively:\ngit clone --recursive https://github.com/jgraph/drawio-desktop.git\nTo run this:\nnpm install\n(in the root directory of this repo)\n[internal use only] export DRAWIO_ENV=dev if you want to develop/debug in dev mode.\nnpm start\nin the root directory of this repo\nruns the app. For debugging, use\nnpm start --enable-logging\n.\nNote: If a symlink is used to refer to drawio repo (instead of the submodule), then symlink the\nnode_modules\ndirectory inside\ndrawio/src/main/webapp\nalso.\nTo release:\nUpdate the draw.io sub-module and push the change. Add version tag before pushing to origin.\nWait for the builds to complete (\nhttps://travis-ci.org/jgraph/drawio-desktop\nand\nhttps://ci.appveyor.com/project/davidjgraph/drawio-desktop\n)\nGo to\nhttps://github.com/jgraph/drawio-desktop/releases\n, edit the preview release.\nDownload the windows exe and windows portable, sign them using\nsigntool sign /a /tr http://rfc3161timestamp.globalsign.com/advanced /td SHA256 c:/path/to/your/file.exe\nRe-upload signed file as\ndraw.io-windows-installer-x.y.z.exe\nand\ndraw.io-windows-no-installer-x.y.z.exe\nAdd release notes\nPublish release\nNote\n: In Windows release, when using both x64 and is32 as arch, the result is one big file with both archs. This is why we split them.\nLocal Storage and Session Storage is stored in the AppData folder:\nmacOS:\n~/Library/Application Support/draw.io\nWindows:\nC:\\Users\\<USER-NAME>\\AppData\\Roaming\\draw.io\\\nNot open-contribution\ndraw.io is closed to contributions (unless a maintainer permits it, which is extremely rare).\nThe level of complexity of this project means that even simple changes\ncan break a\nlot\nof other moving parts. The amount of testing required\nis far more than it first seems. If we were to receive a PR, we'd have\nto basically throw it away and write it how we want it to be implemented.\nWe are grateful for community involvement, bug reports, & feature requests. We do\nnot wish to come off as anything but welcoming, however, we've\nmade the decision to keep this project closed to contributions for\nthe long term viability of the project.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 42",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 57,217"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/jgraph/drawio-desktop"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/microsoft/RD-Agent",
      "title": "microsoft/RD-Agent",
      "date": null,
      "executive_summary": [
        "Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through R&D-Agent, which lets AI drive data-driven AI. ğŸ”—https://aka.ms/RD-Agent-Tech-Report",
        "---",
        "ğŸ–¥ï¸ Live Demo\n|\nğŸ¥ Demo Video\nâ–¶ï¸\nYouTube\n|\nğŸ“– Documentation\n|\nğŸ“„ Tech Report\n|\nğŸ“ƒ Papers\nğŸ“° News\nğŸ—ï¸ News\nğŸ“ Description\nNeurIPS 2025 Acceptance\nWe are thrilled to announce that our paper\nR&D-Agent-Quant\nhas been accepted to NeurIPS 2025\nTechnical Report Release\nOverall framework description and results on MLE-bench\nR&D-Agent-Quant Release\nApply R&D-Agent to quant trading\nMLE-Bench Results Released\nR&D-Agent currently leads as the\ntop-performing machine learning engineering agent\non MLE-bench\nSupport LiteLLM Backend\nWe now fully support\nLiteLLM\nas our default backend for integration with multiple LLM providers.\nGeneral Data Science Agent\nData Science Agent\nKaggle Scenario release\nWe release\nKaggle Agent\n, try the new features!\nOfficial WeChat group release\nWe created a WeChat group, welcome to join! (ğŸ—ª\nQR Code\n)\nOfficial Discord release\nWe launch our first chatting channel in Discord (ğŸ—ª\n)\nFirst release\nR&D-Agent\nis released on GitHub\nğŸ† The Best Machine Learning Engineering Agent!\nMLE-bench\nis a comprehensive benchmark evaluating the performance of AI agents on machine learning engineering tasks. Utilizing datasets from 75 Kaggle competitions, MLE-bench provides robust assessments of AI systems' capabilities in real-world ML engineering scenarios.\nR&D-Agent currently leads as the top-performing machine learning engineering agent on MLE-bench:\nAgent\nLow == Lite (%)\nMedium (%)\nHigh (%)\nAll (%)\nR&D-Agent o3(R)+GPT-4.1(D)\n51.52 Â± 6.9\n19.3 Â± 5.5\n26.67 Â± 0\n30.22 Â± 1.5\nR&D-Agent o1-preview\n48.18 Â± 2.49\n8.95 Â± 2.36\n18.67 Â± 2.98\n22.4 Â± 1.1\nAIDE o1-preview\n34.3 Â± 2.4\n8.8 Â± 1.1\n10.0 Â± 1.9\n16.9 Â± 1.1\nNotes:\nO3(R)+GPT-4.1(D)\n: This version is designed to both reduce average time per loop and leverage a cost-effective combination of backend LLMs by seamlessly integrating Research Agent (o3) with Development Agent (GPT-4.1).\nAIDE o1-preview\n: Represents the previously best public result on MLE-bench as reported in the original MLE-bench paper.\nAverage and standard deviation results for R&D-Agent o1-preview is based on a independent of 5 seeds and for R&D-Agent o3(R)+GPT-4.1(D) is based on 6 seeds.\nAccording to MLE-Bench, the 75 competitions are categorized into three levels of complexity:\nLow==Lite\nif we estimate that an experienced ML engineer can produce a sensible solution in under 2 hours, excluding the time taken to train any models;\nMedium\nif it takes between 2 and 10 hours; and\nHigh\nif it takes more than 10 hours.\nYou can inspect the detailed runs of the above results online.\nR&D-Agent o1-preview detailed runs\nR&D-Agent o3(R)+GPT-4.1(D) detailed runs\nFor running R&D-Agent on MLE-bench, refer to\nMLE-bench Guide: Running ML Engineering via MLE-bench\nğŸ¥‡ The First Data-Centric Quant Multi-Agent Framework!\nR&D-Agent for Quantitative Finance, in short\nRD-Agent(Q)\n, is the first data-centric, multi-agent framework designed to automate the full-stack research and development of quantitative strategies via coordinated factor-model co-optimization.\nExtensive experiments in real stock markets show that, at a cost under $10, RD-Agent(Q) achieves approximately 2Ã— higher ARR than benchmark factor libraries while using over 70% fewer factors. It also surpasses state-of-the-art deep time-series models under smaller resource budgets. Its alternating factorâ€“model optimization further delivers excellent trade-off between predictive accuracy and strategy robustness.\nYou can learn more details about\nRD-Agent(Q)\nthrough the\npaper\nand reproduce it through the\ndocumentation\n.\nData Science Agent Preview\nCheck out our demo video showcasing the current progress of our Data Science Agent under development:\nDS.Agent.Preview.mp4\nğŸŒŸ Introduction\nR&D-Agent aims to automate the most critical and valuable aspects of the industrial R&D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data.\nMethodologically, we have identified a framework with two key components: 'R' for proposing new ideas and 'D' for implementing them.\nWe believe that the automatic evolution of R&D will lead to solutions of significant industrial value.\nR&D is a very general scenario. The advent of R&D-Agent can be your\nğŸ’°\nAutomatic Quant Factory\n(\nğŸ¥Demo Video\n|\nâ–¶ï¸\nYouTube\n)\nğŸ¤–\nData Mining Agent:\nIteratively proposing data & models (\nğŸ¥Demo Video 1\n|\nâ–¶ï¸\nYouTube\n) (\nğŸ¥Demo Video 2\n|\nâ–¶ï¸\nYouTube\n)  and implementing them by gaining knowledge from data.\nğŸ¦¾\nResearch Copilot:\nAuto read research papers (\nğŸ¥Demo Video\n|\nâ–¶ï¸\nYouTube\n) / financial reports (\nğŸ¥Demo Video\n|\nâ–¶ï¸\nYouTube\n) and implement model structures or building datasets.\nğŸ¤–\nKaggle Agent:\nAuto Model Tuning and Feature Engineering(\nğŸ¥Demo Video Coming Soon...\n) and implementing them to achieve more in competitions.\n...\nYou can click the links above to view the demo. We're continuously adding more methods and scenarios to the project to enhance your R&D processes and boost productivity.\nAdditionally, you can take a closer look at the examples in our\nğŸ–¥ï¸ Live Demo\n.\nâš¡ Quick start\nRD-Agent currently only supports Linux.\nYou can try above demos by running the following command:\nğŸ³ Docker installation.\nUsers must ensure Docker is installed before attempting most scenarios. Please refer to the\nofficial ğŸ³Docker page\nfor installation instructions.\nEnsure the current user can run Docker commands\nwithout using sudo\n. You can verify this by executing\ndocker run hello-world\n.\nğŸ Create a Conda Environment\nCreate a new conda environment with Python (3.10 and 3.11 are well-tested in our CI):\nconda create -n rdagent python=3.10\nActivate the environment:\nconda activate rdagent\nğŸ› ï¸ Install the R&D-Agent\nFor Users\nYou can directly install the R&D-Agent package from PyPI:\npip install rdagent\nFor Developers\nIf you want to try the latest version or contribute to RD-Agent, you can install it from the source and follow the development setup:\ngit clone https://github.com/microsoft/RD-Agent\ncd\nRD-Agent\nmake dev\nMore details can be found in the\ndevelopment setup\n.\nğŸ’Š Health check\nrdagent provides a health check that currently checks two things.\nwhether the docker installation was successful.\nwhether the default port used by the\nrdagent ui\nis occupied.\nrdagent health_check --no-check-env\nâš™ï¸ Configuration\nThe demos requires following ability:\nChatCompletion\njson_mode\nembedding query\nYou can set your Chat Model and Embedding Model in the following ways:\nğŸ”¥ Attention\n: We now provide experimental support for\nDeepSeek\nmodels! You can use DeepSeek's official API for cost-effective and high-performance inference. See the configuration example below for DeepSeek setup.\nUsing LiteLLM (Default)\n: We now support LiteLLM as a backend for integration with multiple LLM providers. You can configure in multiple ways:\nOption 1: Unified API base for both models\nConfiguration Example:\nOpenAI\nSetup :\ncat\n<<\nEOF\n> .env\n# Set to any model supported by LiteLLM.\nCHAT_MODEL=gpt-4o\nEMBEDDING_MODEL=text-embedding-3-small\n# Configure unified API base\nOPENAI_API_BASE=<your_unified_api_base>\nOPENAI_API_KEY=<replace_with_your_openai_api_key>\nConfiguration Example:\nAzure OpenAI\nSetup :\nBefore using this configuration, please confirm in advance that your\nAzure OpenAI API key\nsupports\nembedded models\n.\ncat\n<<\nEOF\n> .env\nEMBEDDING_MODEL=azure/<Model deployment supporting embedding>\nCHAT_MODEL=azure/<your deployment name>\nAZURE_API_KEY=<replace_with_your_openai_api_key>\nAZURE_API_BASE=<your_unified_api_base>\nAZURE_API_VERSION=<azure api version>\nOption 2: Separate API bases for Chat and Embedding models\ncat\n<<\nEOF\n> .env\n# Set to any model supported by LiteLLM.\n# Configure separate API bases for chat and embedding\n# CHAT MODEL:\nCHAT_MODEL=gpt-4o\nOPENAI_API_BASE=<your_chat_api_base>\nOPENAI_API_KEY=<replace_with_your_openai_api_key>\n# EMBEDDING MODEL:\n# TAKE siliconflow as an example, you can use other providers.\n# Note: embedding requires litellm_proxy prefix\nEMBEDDING_MODEL=litellm_proxy/BAAI/bge-large-en-v1.5\nLITELLM_PROXY_API_KEY=<replace_with_your_siliconflow_api_key>\nLITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1\nConfiguration Example:\nDeepSeek\nSetup :\nSince many users encounter configuration errors when setting up DeepSeek. Here's a complete working example for DeepSeek Setup:\ncat\n<<\nEOF\n> .env\n# CHAT MODEL: Using DeepSeek Official API\nCHAT_MODEL=deepseek/deepseek-chat\nDEEPSEEK_API_KEY=<replace_with_your_deepseek_api_key>\n# EMBEDDING MODEL: Using SiliconFlow for embedding since deepseek has no embedding model.\n# Note: embedding requires litellm_proxy prefix\nEMBEDDING_MODEL=litellm_proxy/BAAI/bge-m3\nLITELLM_PROXY_API_KEY=<replace_with_your_siliconflow_api_key>\nLITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1\nNotice: If you are using reasoning models that include thought processes in their responses (such as <think> tags), you need to set the following environment variable:\nREASONING_THINK_RM=True\nYou can also use a deprecated backend if you only use\nOpenAI API\nor\nAzure OpenAI\ndirectly. For this deprecated setting and more configuration information, please refer to the\ndocumentation\n.\nIf your environment configuration is complete, please execute the following commands to check if your configuration is valid. This step is necessary.\nrdagent health_check\nğŸš€ Run the Application\nThe\nğŸ–¥ï¸ Live Demo\nis implemented by the following commands(each item represents one demo, you can select the one you prefer):\nRun the\nAutomated Quantitative Trading & Iterative Factors Model Joint Evolution\n:\nQlib\nself-loop factor & model proposal and implementation application\nrdagent fin_quant\nRun the\nAutomated Quantitative Trading & Iterative Factors Evolution\n:\nQlib\nself-loop factor proposal and implementation application\nrdagent fin_factor\nRun the\nAutomated Quantitative Trading & Iterative Model Evolution\n:\nQlib\nself-loop model proposal and implementation application\nrdagent fin_model\nRun the\nAutomated Quantitative Trading & Factors Extraction from Financial Reports\n:  Run the\nQlib\nfactor extraction and implementation application based on financial reports\n#\n1. Generally, you can run this scenario using the following command:\nrdagent fin_factor_report --report-folder=\n<\nYour financial reports folder path\n>\n#\n2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:\nwget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip\nunzip all_reports.zip -d git_ignore_folder/reports\nrdagent fin_factor_report --report-folder=git_ignore_folder/reports\nRun the\nAutomated Model Research & Development Copilot\n: model extraction and implementation application\n#\n1. Generally, you can run your own papers/reports with the following command:\nrdagent general_model\n<\nYour paper URL\n>\n#\n2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:\nrdagent general_model\n\"\nhttps://arxiv.org/pdf/2210.09789\n\"\nRun the\nAutomated Medical Prediction Model Evolution\n: Medical self-loop model proposal and implementation application\n#\nGenerally, you can run the data science program with the following command:\nrdagent data_science --competition\n<\nyour competition name\n>\n#\nSpecifically, you need to create a folder for storing competition files (e.g., competition description file, competition datasets, etc.), and configure the path to the folder in your environment. In addition, you need to use chromedriver when you download the competition descriptors, which you can follow for this specific example:\n#\n1. Download the dataset, extract it to the target folder.\nwget https://github.com/SunsetWolf/rdagent_resource/releases/download/ds_data/arf-12-hours-prediction-task.zip\nunzip arf-12-hours-prediction-task.zip -d ./git_ignore_folder/ds_data/\n#\n2. Configure environment variables in the `.env` file\ndotenv\nset\nDS_LOCAL_DATA_PATH\n\"\n$(\npwd\n)\n/git_ignore_folder/ds_data\n\"\ndotenv\nset\nDS_CODER_ON_WHOLE_PIPELINE True\ndotenv\nset\nDS_IF_USING_MLE_DATA False\ndotenv\nset\nDS_SAMPLE_DATA_BY_LLM False\ndotenv\nset\nDS_SCEN rdagent.scenarios.data_science.scen.DataScienceScen\n#\n3. run the application\nrdagent data_science --competition arf-12-hours-prediction-task\nNOTE:\nFor more information about the dataset, please refer to the\ndocumentation\n.\nRun the\nAutomated Kaggle Model Tuning & Feature Engineering\n:  self-loop model proposal and feature engineering implementation application\nUsing\ntabular-playground-series-dec-2021\nas an example.\nRegister and login on the\nKaggle\nwebsite.\nConfiguring the Kaggle API.\n(1) Click on the avatar (usually in the top right corner of the page) ->\nSettings\n->\nCreate New Token\n, A file called\nkaggle.json\nwill be downloaded.\n(2) Move\nkaggle.json\nto\n~/.config/kaggle/\n(3) Modify the permissions of the kaggle.json file. Reference command:\nchmod 600 ~/.config/kaggle/kaggle.json\nJoin the competition: Click\nJoin the competition\n->\nI Understand and Accept\nat the bottom of the\ncompetition details page\n.\n#\nGenerally, you can run the Kaggle competition program with the following command:\nrdagent data_science --competition\n<\nyour competition name\n>\n#\n1. Configure environment variables in the `.env` file\nmkdir -p ./git_ignore_folder/ds_data\ndotenv\nset\nDS_LOCAL_DATA_PATH\n\"\n$(\npwd\n)\n/git_ignore_folder/ds_data\n\"\ndotenv\nset\nDS_CODER_ON_WHOLE_PIPELINE True\ndotenv\nset\nDS_IF_USING_MLE_DATA True\ndotenv\nset\nDS_SAMPLE_DATA_BY_LLM True\ndotenv\nset\nDS_SCEN rdagent.scenarios.data_science.scen.KaggleScen\n#\n2. run the application\nrdagent data_science --competition tabular-playground-series-dec-2021\nğŸ–¥ï¸ Monitor the Application Results\nYou can run the following command for our demo program to see the run logs.\nrdagent ui --port 19899 --log-dir\n<\nyour log folder like\n\"\nlog/\n\"\n>\n--data-science\nAbout the\ndata_science\nparameter: If you want to see the logs of the data science scenario, set the\ndata_science\nparameter to\nTrue\n; otherwise set it to\nFalse\n.\nAlthough port 19899 is not commonly used, but before you run this demo, you need to check if port 19899 is occupied. If it is, please change it to another port that is not occupied.\nYou can check if a port is occupied by running the following command.\nrdagent health_check --no-check-env --no-check-docker\nğŸ­ Scenarios\nWe have applied R&D-Agent to multiple valuable data-driven industrial scenarios.\nğŸ¯ Goal: Agent for Data-driven R&D\nIn this project, we are aiming to build an Agent to automate Data-Driven R&D that can\nğŸ“„ Read real-world material (reports, papers, etc.) and\nextract\nkey formulas, descriptions of interested\nfeatures\nand\nmodels\n, which are the key components of data-driven R&D .\nğŸ› ï¸\nImplement\nthe extracted formulas (e.g., features, factors, and models) in runnable codes.\nDue to the limited ability of LLM in implementing at once, build an evolving process for the agent to improve performance by learning from feedback and knowledge.\nğŸ’¡ Propose\nnew ideas\nbased on current knowledge and observations.\nğŸ“ˆ Scenarios/Demos\nIn the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: ğŸ¦¾Copilot and ğŸ¤–Agent.\nThe ğŸ¦¾Copilot follows human instructions to automate repetitive tasks.\nThe ğŸ¤–Agent, being more autonomous, actively proposes ideas for better results in the future.\nThe supported scenarios are listed below:\nScenario/Target\nModel Implementation\nData Building\nğŸ’¹ Finance\nğŸ¤–\nIteratively Proposing Ideas & Evolving\nâ–¶ï¸\nYouTube\nğŸ¤–\nIteratively Proposing Ideas & Evolving\nâ–¶ï¸\nYouTube\nğŸ¦¾\nAuto reports reading & implementation\nâ–¶ï¸\nYouTube\nğŸ©º Medical\nğŸ¤–\nIteratively Proposing Ideas & Evolving\nâ–¶ï¸\nYouTube\n-\nğŸ­ General\nğŸ¦¾\nAuto paper reading & implementation\nâ–¶ï¸\nYouTube\nğŸ¤– Auto Kaggle Model Tuning\nğŸ¤–Auto Kaggle feature Engineering\nRoadMap\n: Currently, we are working hard to add new features to the Kaggle scenario.\nDifferent scenarios vary in entrance and configuration. Please check the detailed setup tutorial in the scenarios documents.\nHere is a gallery of\nsuccessful explorations\n(5 traces showed in\nğŸ–¥ï¸ Live Demo\n). You can download and view the execution trace using\nthis command\nfrom the documentation.\nPlease refer to\nğŸ“–readthedocs_scen\nfor more details of the scenarios.\nâš™ï¸ Framework\nAutomating the R&D process in data science is a highly valuable yet underexplored area in industry. We propose a framework to push the boundaries of this important research field.\nThe research questions within this framework can be divided into three main categories:\nResearch Area\nPaper/Work List\nBenchmark the R&D abilities\nBenchmark\nIdea proposal:\nExplore new ideas or refine existing ones\nResearch\nAbility to realize ideas:\nImplement and execute ideas\nDevelopment\nWe believe that the key to delivering high-quality solutions lies in the ability to evolve R&D capabilities. Agents should learn like human experts, continuously improving their R&D skills.\nMore documents can be found in the\nğŸ“– readthedocs\n.\nğŸ“ƒ Paper/Work list\nOverall Technical Report\nR&D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution\n@misc\n{\nyang2024rdagent\n,\ntitle\n=\n{\nR\\&D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution\n}\n,\nauthor\n=\n{\nXu Yang and Xiao Yang and Shikai Fang and Bowen Xian and Yuante Li and Jian Wang and Minrui Xu and Haoran Pan and Xinpeng Hong and Weiqing Liu and Yelong Shen and Weizhu Chen and Jiang Bian\n}\n,\nyear\n=\n{\n2025\n}\n,\neprint\n=\n{\n2505.14738\n}\n,\narchivePrefix\n=\n{\narXiv\n}\n,\nprimaryClass\n=\n{\ncs.AI\n}\n,\nurl\n=\n{\nhttps://arxiv.org/abs/2505.14738\n}\n}\nğŸ“Š Benchmark\nTowards Data-Centric Automatic R&D\n@misc\n{\nchen2024datacentric\n,\ntitle\n=\n{\nTowards Data-Centric Automatic R&D\n}\n,\nauthor\n=\n{\nHaotian Chen and Xinjie Shen and Zeqi Ye and Wenjun Feng and Haoxue Wang and Xiao Yang and Xu Yang and Weiqing Liu and Jiang Bian\n}\n,\nyear\n=\n{\n2024\n}\n,\neprint\n=\n{\n2404.11276\n}\n,\narchivePrefix\n=\n{\narXiv\n}\n,\nprimaryClass\n=\n{\ncs.AI\n}\n}\nğŸ” Research\nIn a data mining expert's daily research and development process, they propose a hypothesis (e.g., a model structure like RNN can capture patterns in time-series data), design experiments (e.g., finance data contains time-series and we can verify the hypothesis in this scenario), implement the experiment as code (e.g., Pytorch model structure), and then execute the code to get feedback (e.g., metrics, loss curve, etc.). The experts learn from the feedback and improve in the next iteration.\nBased on the principles above, we have established a basic method framework that continuously proposes hypotheses, verifies them, and gets feedback from the real-world practice. This is the first scientific research automation framework that supports linking with real-world verification.\nFor more detail, please refer to our\nğŸ–¥ï¸ Live Demo page\n.\nğŸ› ï¸ Development\nCollaborative Evolving Strategy for Automatic Data-Centric Development\n@misc\n{\nyang2024collaborative\n,\ntitle\n=\n{\nCollaborative Evolving Strategy for Automatic Data-Centric Development\n}\n,\nauthor\n=\n{\nXu Yang and Haotian Chen and Wenjun Feng and Haoxue Wang and Zeqi Ye and Xinjie Shen and Xiao Yang and Shizhao Sun and Weiqing Liu and Jiang Bian\n}\n,\nyear\n=\n{\n2024\n}\n,\neprint\n=\n{\n2407.18690\n}\n,\narchivePrefix\n=\n{\narXiv\n}\n,\nprimaryClass\n=\n{\ncs.AI\n}\n}\nDeep Application in Diverse Scenarios\nR&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization\n@misc\n{\nli2025rdagentquant\n,\ntitle\n=\n{\nR\\&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization\n}\n,\nauthor\n=\n{\nYuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian\n}\n,\nyear\n=\n{\n2025\n}\n,\neprint\n=\n{\n2505.15155\n}\n,\narchivePrefix\n=\n{\narXiv\n}\n,\nprimaryClass\n=\n{\ncs.AI\n}\n}\nğŸ¤ Contributing\nWe welcome contributions and suggestions to improve R&D-Agent. Please refer to the\nContributing Guide\nfor more details on how to contribute.\nBefore submitting a pull request, ensure that your code passes the automatic CI checks.\nğŸ“ Guidelines\nThis project welcomes contributions and suggestions.\nContributing to this project is straightforward and rewarding. Whether it's solving an issue, addressing a bug, enhancing documentation, or even correcting a typo, every contribution is valuable and helps improve R&D-Agent.\nTo get started, you can explore the issues list, or search for\nTODO:\ncomments in the codebase by running the command\ngrep -r \"TODO:\"\n.\nBefore we released R&D-Agent as an open-source project on GitHub, it was an internal project within our group. Unfortunately, the internal commit history was not preserved when we removed some confidential code. As a result, some contributions from our group members, including Haotian Chen, Wenjun Feng, Haoxue Wang, Zeqi Ye, Xinjie Shen, and Jinhui Li, were not included in the public commits.\nâš–ï¸ Legal disclaimer\nThe RD-agent is provided â€œas isâ€, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. The RD-agent is aimed to facilitate research and development process in the financial industry and not ready-to-use for any financial investment or advice. Users shall independently assess and test the risks of the RD-agent in a specific use scenario, ensure the responsible use of AI technology, including but not limited to developing and integrating risk mitigation measures, and comply with all applicable laws and regulations in all applicable jurisdictions. The RD-agent does not provide financial opinions or reflect the opinions of Microsoft, nor is it designed to replace the role of qualified financial professionals in formulating, assessing, and approving finance products. The inputs and outputs of the RD-agent belong to the users and users shall assume all liability under any theory of liability, whether in contract, torts, regulatory, negligence, products liability, or otherwise, associated with use of the RD-agent and any inputs and outputs thereof.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 42",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 8,357"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/microsoft/RD-Agent"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/JannisX11/blockbench",
      "title": "JannisX11/blockbench",
      "date": null,
      "executive_summary": [
        "Blockbench - A low poly 3D model editor",
        "---",
        "Blockbench\nBlockbench is a free and open source model editor for low-poly models with pixel art textures.\nModels can be exported into standardized formats, to be shared, rendered, 3D-printed, or used in game engines. There are also multiple dedicated formats for Minecraft Java and Bedrock Edition with format-specific features.\nBlockbench features a modern and beginner friendly interface, but also offers lots of customization and advanced features for experienced 3D artists. Plugins can extend the functionality of the program even further.\nWebsite and download:\nblockbench.net\nContribution\nCheck out the\nContribution Guidelines\n.\nLaunching Blockbench\nTo launch Blockbench from source, you can clone the repository, navigate to the correct branch and launch the program in development mode using the instructions below. If you just want to use the latest version, please download the app from the website.\nInstall\nNodeJS\n.\nThen install all dependencies via\nnpm install\nBundle the code via\nnpm run bundle\nFinally, launch Blockbench using\nnpm run dev\nPlugins\nBlockbench supports Javascript-based plugins. Learn more about creating plugins on\nhttps://www.blockbench.net/wiki/docs/plugin\n.\nLicense\nThe Blockbench source-code is licensed under the GPL license version 3. See\nLICENSE.MD\n.\nModifications to the source code can be made under the terms of that license.\nBlockbench plugins (external scripts) and themes (theme files to customize the design) that interact with the Blockbench API are an exception. Plugins and themes can be created and/or published as open source, proprietary or paid software.\nAll assets created with Blockbench (models, textures, animations, screenshots etc.) are your own!",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 39",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 4,437"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/JannisX11/blockbench"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/fatedier/frp",
      "title": "fatedier/frp",
      "date": null,
      "executive_summary": [
        "A fast reverse proxy to help you expose a local server behind a NAT or firewall to the internet.",
        "---",
        "frp\nREADME\n|\nä¸­æ–‡æ–‡æ¡£\nSponsors\nfrp is an open source project with its ongoing development made possible entirely by the support of our awesome sponsors. If you'd like to join them, please consider\nsponsoring frp's development\n.\nGold Sponsors\nRecall.ai - API for meeting recordings\nIf you're looking for a meeting recording API, consider checking out\nRecall.ai\n,\nan API that records Zoom, Google Meet, Microsoft Teams, in-person meetings, and more.\nWarp, built for collaborating with AI Agents\nAvailable for macOS, Linux and Windows\nThe complete IDE crafted for professional Go developers\nSecure and Elastic Infrastructure for Running Your AI-Generated Code\nThe sovereign cloud that puts you in control\nAn open source, self-hosted alternative to public clouds, built for data ownership and privacy\nWhat is frp?\nfrp is a fast reverse proxy that allows you to expose a local server located behind a NAT or firewall to the Internet. It currently supports\nTCP\nand\nUDP\n, as well as\nHTTP\nand\nHTTPS\nprotocols, enabling requests to be forwarded to internal services via domain name.\nfrp also offers a P2P connect mode.\nTable of Contents\nDevelopment Status\nAbout V2\nArchitecture\nExample Usage\nAccess your computer in a LAN network via SSH\nMultiple SSH services sharing the same port\nAccessing Internal Web Services with Custom Domains in LAN\nForward DNS query requests\nForward Unix Domain Socket\nExpose a simple HTTP file server\nEnable HTTPS for a local HTTP(S) service\nExpose your service privately\nP2P Mode\nFeatures\nConfiguration Files\nUsing Environment Variables\nSplit Configures Into Different Files\nServer Dashboard\nClient Admin UI\nMonitor\nPrometheus\nAuthenticating the Client\nToken Authentication\nOIDC Authentication\nEncryption and Compression\nTLS\nHot-Reloading frpc configuration\nGet proxy status from client\nOnly allowing certain ports on the server\nPort Reuse\nBandwidth Limit\nFor Each Proxy\nTCP Stream Multiplexing\nSupport KCP Protocol\nSupport QUIC Protocol\nConnection Pooling\nLoad balancing\nService Health Check\nRewriting the HTTP Host Header\nSetting other HTTP Headers\nGet Real IP\nHTTP X-Forwarded-For\nProxy Protocol\nRequire HTTP Basic Auth (Password) for Web Services\nCustom Subdomain Names\nURL Routing\nTCP Port Multiplexing\nConnecting to frps via PROXY\nPort range mapping\nClient Plugins\nServer Manage Plugins\nSSH Tunnel Gateway\nVirtual Network (VirtualNet)\nFeature Gates\nAvailable Feature Gates\nEnabling Feature Gates\nFeature Lifecycle\nRelated Projects\nContributing\nDonation\nGitHub Sponsors\nPayPal\nDevelopment Status\nfrp is currently under development. You can try the latest release version in the\nmaster\nbranch, or use the\ndev\nbranch to access the version currently in development.\nWe are currently working on version 2 and attempting to perform some code refactoring and improvements. However, please note that it will not be compatible with version 1.\nWe will transition from version 0 to version 1 at the appropriate time and will only accept bug fixes and improvements, rather than big feature requests.\nAbout V2\nThe complexity and difficulty of the v2 version are much higher than anticipated. I can only work on its development during fragmented time periods, and the constant interruptions disrupt productivity significantly. Given this situation, we will continue to optimize and iterate on the current version until we have more free time to proceed with the major version overhaul.\nThe concept behind v2 is based on my years of experience and reflection in the cloud-native domain, particularly in K8s and ServiceMesh. Its core is a modernized four-layer and seven-layer proxy, similar to envoy. This proxy itself is highly scalable, not only capable of implementing the functionality of intranet penetration but also applicable to various other domains. Building upon this highly scalable core, we aim to implement all the capabilities of frp v1 while also addressing the functionalities that were previously unachievable or difficult to implement in an elegant manner. Furthermore, we will maintain efficient development and iteration capabilities.\nIn addition, I envision frp itself becoming a highly extensible system and platform, similar to how we can provide a range of extension capabilities based on K8s. In K8s, we can customize development according to enterprise needs, utilizing features such as CRD, controller mode, webhook, CSI, and CNI. In frp v1, we introduced the concept of server plugins, which implemented some basic extensibility. However, it relies on a simple HTTP protocol and requires users to start independent processes and manage them on their own. This approach is far from flexible and convenient, and real-world demands vary greatly. It is unrealistic to expect a non-profit open-source project maintained by a few individuals to meet everyone's needs.\nFinally, we acknowledge that the current design of modules such as configuration management, permission verification, certificate management, and API management is not modern enough. While we may carry out some optimizations in the v1 version, ensuring compatibility remains a challenging issue that requires a considerable amount of effort to address.\nWe sincerely appreciate your support for frp.\nArchitecture\nExample Usage\nTo begin, download the latest program for your operating system and architecture from the\nRelease\npage.\nNext, place the\nfrps\nbinary and server configuration file on Server A, which has a public IP address.\nFinally, place the\nfrpc\nbinary and client configuration file on Server B, which is located on a LAN that cannot be directly accessed from the public internet.\nSome antiviruses improperly mark frpc as malware and delete it. This is due to frp being a networking tool capable of creating reverse proxies. Antiviruses sometimes flag reverse proxies due to their ability to bypass firewall port restrictions. If you are using antivirus, then you may need to whitelist/exclude frpc in your antivirus settings to avoid accidental quarantine/deletion. See\nissue 3637\nfor more details.\nAccess your computer in a LAN network via SSH\nModify\nfrps.toml\non server A by setting the\nbindPort\nfor frp clients to connect to:\n#\nfrps.toml\nbindPort\n=\n7000\nStart\nfrps\non server A:\n./frps -c ./frps.toml\nModify\nfrpc.toml\non server B and set the\nserverAddr\nfield to the public IP address of your frps server:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nssh\n\"\ntype\n=\n\"\ntcp\n\"\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n22\nremotePort\n=\n6000\nNote that the\nlocalPort\n(listened on the client) and\nremotePort\n(exposed on the server) are used for traffic going in and out of the frp system, while the\nserverPort\nis used for communication between frps and frpc.\nStart\nfrpc\non server B:\n./frpc -c ./frpc.toml\nTo access server B from another machine through server A via SSH (assuming the username is\ntest\n), use the following command:\nssh -oPort=6000 test@x.x.x.x\nMultiple SSH services sharing the same port\nThis example implements multiple SSH services exposed through the same port using a proxy of type tcpmux. Similarly, as long as the client supports the HTTP Connect proxy connection method, port reuse can be achieved in this way.\nDeploy frps on a machine with a public IP and modify the frps.toml file. Here is a simplified configuration:\nbindPort\n=\n7000\ntcpmuxHTTPConnectPort\n=\n5002\nDeploy frpc on the internal machine A with the following configuration:\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nssh1\n\"\ntype\n=\n\"\ntcpmux\n\"\nmultiplexer\n=\n\"\nhttpconnect\n\"\ncustomDomains\n= [\n\"\nmachine-a.example.com\n\"\n]\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n22\nDeploy another frpc on the internal machine B with the following configuration:\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nssh2\n\"\ntype\n=\n\"\ntcpmux\n\"\nmultiplexer\n=\n\"\nhttpconnect\n\"\ncustomDomains\n= [\n\"\nmachine-b.example.com\n\"\n]\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n22\nTo access internal machine A using SSH ProxyCommand, assuming the username is \"test\":\nssh -o 'proxycommand socat - PROXY:x.x.x.x:%h:%p,proxyport=5002' test@machine-a.example.com\nTo access internal machine B, the only difference is the domain name, assuming the username is \"test\":\nssh -o 'proxycommand socat - PROXY:x.x.x.x:%h:%p,proxyport=5002' test@machine-b.example.com\nAccessing Internal Web Services with Custom Domains in LAN\nSometimes we need to expose a local web service behind a NAT network to others for testing purposes with our own domain name.\nUnfortunately, we cannot resolve a domain name to a local IP. However, we can use frp to expose an HTTP(S) service.\nModify\nfrps.toml\nand set the HTTP port for vhost to 8080:\n#\nfrps.toml\nbindPort\n=\n7000\nvhostHTTPPort\n=\n8080\nIf you want to configure an https proxy, you need to set up the\nvhostHTTPSPort\n.\nStart\nfrps\n:\n./frps -c ./frps.toml\nModify\nfrpc.toml\nand set\nserverAddr\nto the IP address of the remote frps server. Specify the\nlocalPort\nof your web service:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nweb\n\"\ntype\n=\n\"\nhttp\n\"\nlocalPort\n=\n80\ncustomDomains\n= [\n\"\nwww.example.com\n\"\n]\nStart\nfrpc\n:\n./frpc -c ./frpc.toml\nMap the A record of\nwww.example.com\nto either the public IP of the remote frps server or a CNAME record pointing to your original domain.\nVisit your local web service using url\nhttp://www.example.com:8080\n.\nForward DNS query requests\nModify\nfrps.toml\n:\n#\nfrps.toml\nbindPort\n=\n7000\nStart\nfrps\n:\n./frps -c ./frps.toml\nModify\nfrpc.toml\nand set\nserverAddr\nto the IP address of the remote frps server. Forward DNS query requests to the Google Public DNS server\n8.8.8.8:53\n:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\ndns\n\"\ntype\n=\n\"\nudp\n\"\nlocalIP\n=\n\"\n8.8.8.8\n\"\nlocalPort\n=\n53\nremotePort\n=\n6000\nStart frpc:\n./frpc -c ./frpc.toml\nTest DNS resolution using the\ndig\ncommand:\ndig @x.x.x.x -p 6000 www.google.com\nForward Unix Domain Socket\nExpose a Unix domain socket (e.g. the Docker daemon socket) as TCP.\nConfigure\nfrps\nas above.\nStart\nfrpc\nwith the following configuration:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nunix_domain_socket\n\"\ntype\n=\n\"\ntcp\n\"\nremotePort\n=\n6000\n[\nproxies\n.\nplugin\n]\ntype\n=\n\"\nunix_domain_socket\n\"\nunixPath\n=\n\"\n/var/run/docker.sock\n\"\nTest the configuration by getting the docker version using\ncurl\n:\ncurl http://x.x.x.x:6000/version\nExpose a simple HTTP file server\nExpose a simple HTTP file server to access files stored in the LAN from the public Internet.\nConfigure\nfrps\nas described above, then:\nStart\nfrpc\nwith the following configuration:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\ntest_static_file\n\"\ntype\n=\n\"\ntcp\n\"\nremotePort\n=\n6000\n[\nproxies\n.\nplugin\n]\ntype\n=\n\"\nstatic_file\n\"\nlocalPath\n=\n\"\n/tmp/files\n\"\nstripPrefix\n=\n\"\nstatic\n\"\nhttpUser\n=\n\"\nabc\n\"\nhttpPassword\n=\n\"\nabc\n\"\nVisit\nhttp://x.x.x.x:6000/static/\nfrom your browser and specify correct username and password to view files in\n/tmp/files\non the\nfrpc\nmachine.\nEnable HTTPS for a local HTTP(S) service\nYou may substitute\nhttps2https\nfor the plugin, and point the\nlocalAddr\nto a HTTPS endpoint.\nStart\nfrpc\nwith the following configuration:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\ntest_https2http\n\"\ntype\n=\n\"\nhttps\n\"\ncustomDomains\n= [\n\"\ntest.example.com\n\"\n]\n\n[\nproxies\n.\nplugin\n]\ntype\n=\n\"\nhttps2http\n\"\nlocalAddr\n=\n\"\n127.0.0.1:80\n\"\ncrtPath\n=\n\"\n./server.crt\n\"\nkeyPath\n=\n\"\n./server.key\n\"\nhostHeaderRewrite\n=\n\"\n127.0.0.1\n\"\nrequestHeaders.set.x-from-where\n=\n\"\nfrp\n\"\nVisit\nhttps://test.example.com\n.\nExpose your service privately\nTo mitigate risks associated with exposing certain services directly to the public network, STCP (Secret TCP) mode requires a preshared key to be used for access to the service from other clients.\nConfigure\nfrps\nsame as above.\nStart\nfrpc\non machine B with the following config. This example is for exposing the SSH service (port 22), and note the\nsecretKey\nfield for the preshared key, and that the\nremotePort\nfield is removed here:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nsecret_ssh\n\"\ntype\n=\n\"\nstcp\n\"\nsecretKey\n=\n\"\nabcdefg\n\"\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n22\nStart another\nfrpc\n(typically on another machine C) with the following config to access the SSH service with a security key (\nsecretKey\nfield):\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nvisitors\n]]\nname\n=\n\"\nsecret_ssh_visitor\n\"\ntype\n=\n\"\nstcp\n\"\nserverName\n=\n\"\nsecret_ssh\n\"\nsecretKey\n=\n\"\nabcdefg\n\"\nbindAddr\n=\n\"\n127.0.0.1\n\"\nbindPort\n=\n6000\nOn machine C, connect to SSH on machine B, using this command:\nssh -oPort=6000 127.0.0.1\nP2P Mode\nxtcp\nis designed to transmit large amounts of data directly between clients. A frps server is still needed, as P2P here only refers to the actual data transmission.\nNote that it may not work with all types of NAT devices. You might want to fallback to stcp if xtcp doesn't work.\nStart\nfrpc\non machine B, and expose the SSH port. Note that the\nremotePort\nfield is removed:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n#\nset up a new stun server if the default one is not available.\n#\nnatHoleStunServer = \"xxx\"\n[[\nproxies\n]]\nname\n=\n\"\np2p_ssh\n\"\ntype\n=\n\"\nxtcp\n\"\nsecretKey\n=\n\"\nabcdefg\n\"\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n22\nStart another\nfrpc\n(typically on another machine C) with the configuration to connect to SSH using P2P mode:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n#\nset up a new stun server if the default one is not available.\n#\nnatHoleStunServer = \"xxx\"\n[[\nvisitors\n]]\nname\n=\n\"\np2p_ssh_visitor\n\"\ntype\n=\n\"\nxtcp\n\"\nserverName\n=\n\"\np2p_ssh\n\"\nsecretKey\n=\n\"\nabcdefg\n\"\nbindAddr\n=\n\"\n127.0.0.1\n\"\nbindPort\n=\n6000\n#\nwhen automatic tunnel persistence is required, set it to true\nkeepTunnelOpen\n=\nfalse\nOn machine C, connect to SSH on machine B, using this command:\nssh -oPort=6000 127.0.0.1\nFeatures\nConfiguration Files\nSince v0.52.0, we support TOML, YAML, and JSON for configuration. Please note that INI is deprecated and will be removed in future releases. New features will only be available in TOML, YAML, or JSON. Users wanting these new features should switch their configuration format accordingly.\nRead the full example configuration files to find out even more features not described here.\nExamples use TOML format, but you can still use YAML or JSON.\nThese configuration files is for reference only. Please do not use this configuration directly to run the program as it may have various issues.\nFull configuration file for frps (Server)\nFull configuration file for frpc (Client)\nUsing Environment Variables\nEnvironment variables can be referenced in the configuration file, using Go's standard format:\n#\nfrpc.toml\nserverAddr\n=\n\"\n{{ .Envs.FRP_SERVER_ADDR }}\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nssh\n\"\ntype\n=\n\"\ntcp\n\"\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n22\nremotePort\n= {{ .Envs.FRP_SSH_REMOTE_PORT }\n}\nWith the config above, variables can be passed into\nfrpc\nprogram like this:\nexport FRP_SERVER_ADDR=x.x.x.x\nexport FRP_SSH_REMOTE_PORT=6000\n./frpc -c ./frpc.toml\nfrpc\nwill render configuration file template using OS environment variables. Remember to prefix your reference with\n.Envs\n.\nSplit Configures Into Different Files\nYou can split multiple proxy configs into different files and include them in the main file.\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\nincludes\n= [\n\"\n./confd/*.toml\n\"\n]\n#\n./confd/test.toml\n[[\nproxies\n]]\nname\n=\n\"\nssh\n\"\ntype\n=\n\"\ntcp\n\"\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n22\nremotePort\n=\n6000\nServer Dashboard\nCheck frp's status and proxies' statistics information by Dashboard.\nConfigure a port for dashboard to enable this feature:\n#\nThe default value is 127.0.0.1. Change it to 0.0.0.0 when you want to access it from a public network.\nwebServer.addr\n=\n\"\n0.0.0.0\n\"\nwebServer.port\n=\n7500\n#\ndashboard's username and password are both optional\nwebServer.user\n=\n\"\nadmin\n\"\nwebServer.password\n=\n\"\nadmin\n\"\nThen visit\nhttp://[serverAddr]:7500\nto see the dashboard, with username and password both being\nadmin\n.\nAdditionally, you can use HTTPS port by using your domains wildcard or normal SSL certificate:\nwebServer.port\n=\n7500\n#\ndashboard's username and password are both optional\nwebServer.user\n=\n\"\nadmin\n\"\nwebServer.password\n=\n\"\nadmin\n\"\nwebServer.tls.certFile\n=\n\"\nserver.crt\n\"\nwebServer.tls.keyFile\n=\n\"\nserver.key\n\"\nThen visit\nhttps://[serverAddr]:7500\nto see the dashboard in secure HTTPS connection, with username and password both being\nadmin\n.\nClient Admin UI\nThe Client Admin UI helps you check and manage frpc's configuration.\nConfigure an address for admin UI to enable this feature:\nwebServer.addr\n=\n\"\n127.0.0.1\n\"\nwebServer.port\n=\n7400\nwebServer.user\n=\n\"\nadmin\n\"\nwebServer.password\n=\n\"\nadmin\n\"\nThen visit\nhttp://127.0.0.1:7400\nto see admin UI, with username and password both being\nadmin\n.\nMonitor\nWhen web server is enabled, frps will save monitor data in cache for 7 days. It will be cleared after process restart.\nPrometheus is also supported.\nPrometheus\nEnable dashboard first, then configure\nenablePrometheus = true\nin\nfrps.toml\n.\nhttp://{dashboard_addr}/metrics\nwill provide prometheus monitor data.\nAuthenticating the Client\nThere are 2 authentication methods to authenticate frpc with frps.\nYou can decide which one to use by configuring\nauth.method\nin\nfrpc.toml\nand\nfrps.toml\n, the default one is token.\nConfiguring\nauth.additionalScopes = [\"HeartBeats\"]\nwill use the configured authentication method to add and validate authentication on every heartbeat between frpc and frps.\nConfiguring\nauth.additionalScopes = [\"NewWorkConns\"]\nwill do the same for every new work connection between frpc and frps.\nToken Authentication\nWhen specifying\nauth.method = \"token\"\nin\nfrpc.toml\nand\nfrps.toml\n- token based authentication will be used.\nMake sure to specify the same\nauth.token\nin\nfrps.toml\nand\nfrpc.toml\nfor frpc to pass frps validation\nToken Source\nfrp supports reading authentication tokens from external sources using the\ntokenSource\nconfiguration. Currently, file-based token source is supported.\nFile-based token source:\n#\nfrpc.toml\nauth.method\n=\n\"\ntoken\n\"\nauth.tokenSource.type\n=\n\"\nfile\n\"\nauth.tokenSource.file.path\n=\n\"\n/path/to/token/file\n\"\nThe token will be read from the specified file at startup. This is useful for scenarios where tokens are managed by external systems or need to be kept separate from configuration files for security reasons.\nOIDC Authentication\nWhen specifying\nauth.method = \"oidc\"\nin\nfrpc.toml\nand\nfrps.toml\n- OIDC based authentication will be used.\nOIDC stands for OpenID Connect, and the flow used is called\nClient Credentials Grant\n.\nTo use this authentication type - configure\nfrpc.toml\nand\nfrps.toml\nas follows:\n#\nfrps.toml\nauth.method\n=\n\"\noidc\n\"\nauth.oidc.issuer\n=\n\"\nhttps://example-oidc-issuer.com/\n\"\nauth.oidc.audience\n=\n\"\nhttps://oidc-audience.com/.default\n\"\n#\nfrpc.toml\nauth.method\n=\n\"\noidc\n\"\nauth.oidc.clientID\n=\n\"\n98692467-37de-409a-9fac-bb2585826f18\n\"\n#\nReplace with OIDC client ID\nauth.oidc.clientSecret\n=\n\"\noidc_secret\n\"\nauth.oidc.audience\n=\n\"\nhttps://oidc-audience.com/.default\n\"\nauth.oidc.tokenEndpointURL\n=\n\"\nhttps://example-oidc-endpoint.com/oauth2/v2.0/token\n\"\nEncryption and Compression\nThe features are off by default. You can turn on encryption and/or compression:\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nssh\n\"\ntype\n=\n\"\ntcp\n\"\nlocalPort\n=\n22\nremotePort\n=\n6000\ntransport.useEncryption\n=\ntrue\ntransport.useCompression\n=\ntrue\nTLS\nSince v0.50.0, the default value of\ntransport.tls.enable\nand\ntransport.tls.disableCustomTLSFirstByte\nhas been changed to true, and tls is enabled by default.\nFor port multiplexing, frp sends a first byte\n0x17\nto dial a TLS connection. This only takes effect when you set\ntransport.tls.disableCustomTLSFirstByte\nto false.\nTo\nenforce\nfrps\nto only accept TLS connections - configure\ntransport.tls.force = true\nin\nfrps.toml\n.\nThis is optional.\nfrpc\nTLS settings:\ntransport.tls.enable\n=\ntrue\ntransport.tls.certFile\n=\n\"\ncertificate.crt\n\"\ntransport.tls.keyFile\n=\n\"\ncertificate.key\n\"\ntransport.tls.trustedCaFile\n=\n\"\nca.crt\n\"\nfrps\nTLS settings:\ntransport.tls.force\n=\ntrue\ntransport.tls.certFile\n=\n\"\ncertificate.crt\n\"\ntransport.tls.keyFile\n=\n\"\ncertificate.key\n\"\ntransport.tls.trustedCaFile\n=\n\"\nca.crt\n\"\nYou will need\na root CA cert\nand\nat least one SSL/TLS certificate\n. It\ncan\nbe self-signed or regular (such as Let's Encrypt or another SSL/TLS certificate provider).\nIf you using\nfrp\nvia IP address and not hostname, make sure to set the appropriate IP address in the Subject Alternative Name (SAN) area when generating SSL/TLS Certificates.\nGiven an example:\nPrepare openssl config file. It exists at\n/etc/pki/tls/openssl.cnf\nin Linux System and\n/System/Library/OpenSSL/openssl.cnf\nin MacOS, and you can copy it to current path, like\ncp /etc/pki/tls/openssl.cnf ./my-openssl.cnf\n. If not, you can build it by yourself, like:\ncat > my-openssl.cnf << EOF\n[ ca ]\ndefault_ca = CA_default\n[ CA_default ]\nx509_extensions = usr_cert\n[ req ]\ndefault_bits        = 2048\ndefault_md          = sha256\ndefault_keyfile     = privkey.pem\ndistinguished_name  = req_distinguished_name\nattributes          = req_attributes\nx509_extensions     = v3_ca\nstring_mask         = utf8only\n[ req_distinguished_name ]\n[ req_attributes ]\n[ usr_cert ]\nbasicConstraints       = CA:FALSE\nnsComment              = \"OpenSSL Generated Certificate\"\nsubjectKeyIdentifier   = hash\nauthorityKeyIdentifier = keyid,issuer\n[ v3_ca ]\nsubjectKeyIdentifier   = hash\nauthorityKeyIdentifier = keyid:always,issuer\nbasicConstraints       = CA:true\nEOF\nbuild ca certificates:\nopenssl genrsa -out ca.key 2048\nopenssl req -x509 -new -nodes -key ca.key -subj \"/CN=example.ca.com\" -days 5000 -out ca.crt\nbuild frps certificates:\nopenssl genrsa -out server.key 2048\n\nopenssl req -new -sha256 -key server.key \\\n    -subj \"/C=XX/ST=DEFAULT/L=DEFAULT/O=DEFAULT/CN=server.com\" \\\n    -reqexts SAN \\\n    -config <(cat my-openssl.cnf <(printf \"\\n[SAN]\\nsubjectAltName=DNS:localhost,IP:127.0.0.1,DNS:example.server.com\")) \\\n    -out server.csr\n\nopenssl x509 -req -days 365 -sha256 \\\n\t-in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\\n\t-extfile <(printf \"subjectAltName=DNS:localhost,IP:127.0.0.1,DNS:example.server.com\") \\\n\t-out server.crt\nbuild frpc certificatesï¼š\nopenssl genrsa -out client.key 2048\nopenssl req -new -sha256 -key client.key \\\n    -subj \"/C=XX/ST=DEFAULT/L=DEFAULT/O=DEFAULT/CN=client.com\" \\\n    -reqexts SAN \\\n    -config <(cat my-openssl.cnf <(printf \"\\n[SAN]\\nsubjectAltName=DNS:client.com,DNS:example.client.com\")) \\\n    -out client.csr\n\nopenssl x509 -req -days 365 -sha256 \\\n    -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\\n\t-extfile <(printf \"subjectAltName=DNS:client.com,DNS:example.client.com\") \\\n\t-out client.crt\nHot-Reloading frpc configuration\nThe\nwebServer\nfields are required for enabling HTTP API:\n#\nfrpc.toml\nwebServer.addr\n=\n\"\n127.0.0.1\n\"\nwebServer.port\n=\n7400\nThen run command\nfrpc reload -c ./frpc.toml\nand wait for about 10 seconds to let\nfrpc\ncreate or update or remove proxies.\nNote that global client parameters won't be modified except 'start'.\nYou can run command\nfrpc verify -c ./frpc.toml\nbefore reloading to check if there are config errors.\nGet proxy status from client\nUse\nfrpc status -c ./frpc.toml\nto get status of all proxies. The\nwebServer\nfields are required for enabling HTTP API.\nOnly allowing certain ports on the server\nallowPorts\nin\nfrps.toml\nis used to avoid abuse of ports:\n#\nfrps.toml\nallowPorts\n= [\n  {\nstart\n=\n2000\n,\nend\n=\n3000\n},\n  {\nsingle\n=\n3001\n},\n  {\nsingle\n=\n3003\n},\n  {\nstart\n=\n4000\n,\nend\n=\n50000\n}\n]\nPort Reuse\nvhostHTTPPort\nand\nvhostHTTPSPort\nin frps can use same port with\nbindPort\n. frps will detect the connection's protocol and handle it correspondingly.\nWhat you need to pay attention to is that if you want to configure\nvhostHTTPSPort\nand\nbindPort\nto the same port, you need to first set\ntransport.tls.disableCustomTLSFirstByte\nto false.\nWe would like to try to allow multiple proxies bind a same remote port with different protocols in the future.\nBandwidth Limit\nFor Each Proxy\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nssh\n\"\ntype\n=\n\"\ntcp\n\"\nlocalPort\n=\n22\nremotePort\n=\n6000\ntransport.bandwidthLimit\n=\n\"\n1MB\n\"\nSet\ntransport.bandwidthLimit\nin each proxy's configure to enable this feature. Supported units are\nMB\nand\nKB\n.\nSet\ntransport.bandwidthLimitMode\nto\nclient\nor\nserver\nto limit bandwidth on the client or server side. Default is\nclient\n.\nTCP Stream Multiplexing\nfrp supports tcp stream multiplexing since v0.10.0 like HTTP2 Multiplexing, in which case all logic connections to the same frpc are multiplexed into the same TCP connection.\nYou can disable this feature by modify\nfrps.toml\nand\nfrpc.toml\n:\n#\nfrps.toml and frpc.toml, must be same\ntransport.tcpMux\n=\nfalse\nSupport KCP Protocol\nKCP is a fast and reliable protocol that can achieve the transmission effect of a reduction of the average latency by 30% to 40% and reduction of the maximum delay by a factor of three, at the cost of 10% to 20% more bandwidth wasted than TCP.\nKCP mode uses UDP as the underlying transport. Using KCP in frp:\nEnable KCP in frps:\n#\nfrps.toml\nbindPort\n=\n7000\n#\nSpecify a UDP port for KCP.\nkcpBindPort\n=\n7000\nThe\nkcpBindPort\nnumber can be the same number as\nbindPort\n, since\nbindPort\nfield specifies a TCP port.\nConfigure\nfrpc.toml\nto use KCP to connect to frps:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\n#\nSame as the 'kcpBindPort' in frps.toml\nserverPort\n=\n7000\ntransport.protocol\n=\n\"\nkcp\n\"\nSupport QUIC Protocol\nQUIC is a new multiplexed transport built on top of UDP.\nUsing QUIC in frp:\nEnable QUIC in frps:\n#\nfrps.toml\nbindPort\n=\n7000\n#\nSpecify a UDP port for QUIC.\nquicBindPort\n=\n7000\nThe\nquicBindPort\nnumber can be the same number as\nbindPort\n, since\nbindPort\nfield specifies a TCP port.\nConfigure\nfrpc.toml\nto use QUIC to connect to frps:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\n#\nSame as the 'quicBindPort' in frps.toml\nserverPort\n=\n7000\ntransport.protocol\n=\n\"\nquic\n\"\nConnection Pooling\nBy default, frps creates a new frpc connection to the backend service upon a user request. With connection pooling, frps keeps a certain number of pre-established connections, reducing the time needed to establish a connection.\nThis feature is suitable for a large number of short connections.\nConfigure the limit of pool count each proxy can use in\nfrps.toml\n:\n#\nfrps.toml\ntransport.maxPoolCount\n=\n5\nEnable and specify the number of connection pool:\n#\nfrpc.toml\ntransport.poolCount\n=\n1\nLoad balancing\nLoad balancing is supported by\ngroup\n.\nThis feature is only available for types\ntcp\n,\nhttp\n,\ntcpmux\nnow.\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\ntest1\n\"\ntype\n=\n\"\ntcp\n\"\nlocalPort\n=\n8080\nremotePort\n=\n80\nloadBalancer.group\n=\n\"\nweb\n\"\nloadBalancer.groupKey\n=\n\"\n123\n\"\n[[\nproxies\n]]\nname\n=\n\"\ntest2\n\"\ntype\n=\n\"\ntcp\n\"\nlocalPort\n=\n8081\nremotePort\n=\n80\nloadBalancer.group\n=\n\"\nweb\n\"\nloadBalancer.groupKey\n=\n\"\n123\n\"\nloadBalancer.groupKey\nis used for authentication.\nConnections to port 80 will be dispatched to proxies in the same group randomly.\nFor type\ntcp\n,\nremotePort\nin the same group should be the same.\nFor type\nhttp\n,\ncustomDomains\n,\nsubdomain\n,\nlocations\nshould be the same.\nService Health Check\nHealth check feature can help you achieve high availability with load balancing.\nAdd\nhealthCheck.type = \"tcp\"\nor\nhealthCheck.type = \"http\"\nto enable health check.\nWith health check type\ntcp\n, the service port will be pinged (TCPing):\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\ntest1\n\"\ntype\n=\n\"\ntcp\n\"\nlocalPort\n=\n22\nremotePort\n=\n6000\n#\nEnable TCP health check\nhealthCheck.type\n=\n\"\ntcp\n\"\n#\nTCPing timeout seconds\nhealthCheck.timeoutSeconds\n=\n3\n#\nIf health check failed 3 times in a row, the proxy will be removed from frps\nhealthCheck.maxFailed\n=\n3\n#\nA health check every 10 seconds\nhealthCheck.intervalSeconds\n=\n10\nWith health check type\nhttp\n, an HTTP request will be sent to the service and an HTTP 2xx OK response is expected:\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nweb\n\"\ntype\n=\n\"\nhttp\n\"\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n80\ncustomDomains\n= [\n\"\ntest.example.com\n\"\n]\n#\nEnable HTTP health check\nhealthCheck.type\n=\n\"\nhttp\n\"\n#\nfrpc will send a GET request to '/status'\n#\nand expect an HTTP 2xx OK response\nhealthCheck.path\n=\n\"\n/status\n\"\nhealthCheck.timeoutSeconds\n=\n3\nhealthCheck.maxFailed\n=\n3\nhealthCheck.intervalSeconds\n=\n10\nRewriting the HTTP Host Header\nBy default frp does not modify the tunneled HTTP requests at all as it's a byte-for-byte copy.\nHowever, speaking of web servers and HTTP requests, your web server might rely on the\nHost\nHTTP header to determine the website to be accessed. frp can rewrite the\nHost\nheader when forwarding the HTTP requests, with the\nhostHeaderRewrite\nfield:\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nweb\n\"\ntype\n=\n\"\nhttp\n\"\nlocalPort\n=\n80\ncustomDomains\n= [\n\"\ntest.example.com\n\"\n]\nhostHeaderRewrite\n=\n\"\ndev.example.com\n\"\nThe HTTP request will have the\nHost\nheader rewritten to\nHost: dev.example.com\nwhen it reaches the actual web server, although the request from the browser probably has\nHost: test.example.com\n.\nSetting other HTTP Headers\nSimilar to\nHost\n, You can override other HTTP request and response headers with proxy type\nhttp\n.\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nweb\n\"\ntype\n=\n\"\nhttp\n\"\nlocalPort\n=\n80\ncustomDomains\n= [\n\"\ntest.example.com\n\"\n]\nhostHeaderRewrite\n=\n\"\ndev.example.com\n\"\nrequestHeaders.set.x-from-where\n=\n\"\nfrp\n\"\nresponseHeaders.set.foo\n=\n\"\nbar\n\"\nIn this example, it will set header\nx-from-where: frp\nin the HTTP request and\nfoo: bar\nin the HTTP response.\nGet Real IP\nHTTP X-Forwarded-For\nThis feature is for\nhttp\nproxies or proxies with the\nhttps2http\nand\nhttps2https\nplugins enabled.\nYou can get user's real IP from HTTP request headers\nX-Forwarded-For\n.\nProxy Protocol\nfrp supports Proxy Protocol to send user's real IP to local services.\nHere is an example for https service:\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nweb\n\"\ntype\n=\n\"\nhttps\n\"\nlocalPort\n=\n443\ncustomDomains\n= [\n\"\ntest.example.com\n\"\n]\n#\nnow v1 and v2 are supported\ntransport.proxyProtocolVersion\n=\n\"\nv2\n\"\nYou can enable Proxy Protocol support in nginx to expose user's real IP in HTTP header\nX-Real-IP\n, and then read\nX-Real-IP\nheader in your web service for the real IP.\nRequire HTTP Basic Auth (Password) for Web Services\nAnyone who can guess your tunnel URL can access your local web server unless you protect it with a password.\nThis enforces HTTP Basic Auth on all requests with the username and password specified in frpc's configure file.\nIt can only be enabled when proxy type is http.\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nweb\n\"\ntype\n=\n\"\nhttp\n\"\nlocalPort\n=\n80\ncustomDomains\n= [\n\"\ntest.example.com\n\"\n]\nhttpUser\n=\n\"\nabc\n\"\nhttpPassword\n=\n\"\nabc\n\"\nVisit\nhttp://test.example.com\nin the browser and now you are prompted to enter the username and password.\nCustom Subdomain Names\nIt is convenient to use\nsubdomain\nconfigure for http and https types when many people share one frps server.\n#\nfrps.toml\nsubDomainHost\n=\n\"\nfrps.com\n\"\nResolve\n*.frps.com\nto the frps server's IP. This is usually called a Wildcard DNS record.\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nweb\n\"\ntype\n=\n\"\nhttp\n\"\nlocalPort\n=\n80\nsubdomain\n=\n\"\ntest\n\"\nNow you can visit your web service on\ntest.frps.com\n.\nNote that if\nsubdomainHost\nis not empty,\ncustomDomains\nshould not be the subdomain of\nsubdomainHost\n.\nURL Routing\nfrp supports forwarding HTTP requests to different backend web services by url routing.\nlocations\nspecifies the prefix of URL used for routing. frps first searches for the most specific prefix location given by literal strings regardless of the listed order.\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nweb01\n\"\ntype\n=\n\"\nhttp\n\"\nlocalPort\n=\n80\ncustomDomains\n= [\n\"\nweb.example.com\n\"\n]\nlocations\n= [\n\"\n/\n\"\n]\n\n[[\nproxies\n]]\nname\n=\n\"\nweb02\n\"\ntype\n=\n\"\nhttp\n\"\nlocalPort\n=\n81\ncustomDomains\n= [\n\"\nweb.example.com\n\"\n]\nlocations\n= [\n\"\n/news\n\"\n,\n\"\n/about\n\"\n]\nHTTP requests with URL prefix\n/news\nor\n/about\nwill be forwarded to\nweb02\nand other requests to\nweb01\n.\nTCP Port Multiplexing\nfrp supports receiving TCP sockets directed to different proxies on a single port on frps, similar to\nvhostHTTPPort\nand\nvhostHTTPSPort\n.\nThe only supported TCP port multiplexing method available at the moment is\nhttpconnect\n- HTTP CONNECT tunnel.\nWhen setting\ntcpmuxHTTPConnectPort\nto anything other than 0 in frps, frps will listen on this port for HTTP CONNECT requests.\nThe host of the HTTP CONNECT request will be used to match the proxy in frps. Proxy hosts can be configured in frpc by configuring\ncustomDomains\nand / or\nsubdomain\nunder\ntcpmux\nproxies, when\nmultiplexer = \"httpconnect\"\n.\nFor example:\n#\nfrps.toml\nbindPort\n=\n7000\ntcpmuxHTTPConnectPort\n=\n1337\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nproxy1\n\"\ntype\n=\n\"\ntcpmux\n\"\nmultiplexer\n=\n\"\nhttpconnect\n\"\ncustomDomains\n= [\n\"\ntest1\n\"\n]\nlocalPort\n=\n80\n[[\nproxies\n]]\nname\n=\n\"\nproxy2\n\"\ntype\n=\n\"\ntcpmux\n\"\nmultiplexer\n=\n\"\nhttpconnect\n\"\ncustomDomains\n= [\n\"\ntest2\n\"\n]\nlocalPort\n=\n8080\nIn the above configuration - frps can be contacted on port 1337 with a HTTP CONNECT header such as:\nCONNECT test1 HTTP/1.1\\r\\n\\r\\n\nand the connection will be routed to\nproxy1\n.\nConnecting to frps via PROXY\nfrpc can connect to frps through proxy if you set OS environment variable\nHTTP_PROXY\n, or if\ntransport.proxyURL\nis set in frpc.toml file.\nIt only works when protocol is tcp.\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\ntransport.proxyURL\n=\n\"\nhttp://user:pwd@192.168.1.128:8080\n\"\nPort range mapping\nAdded in v0.56.0\nWe can use the range syntax of Go template combined with the built-in\nparseNumberRangePair\nfunction to achieve port range mapping.\nThe following example, when run, will create 8 proxies named\ntest-6000, test-6001 ... test-6007\n, each mapping the remote port to the local port.\n{{- range $_, $v := parseNumberRangePair \"6000-6006,6007\" \"6000-6006,6007\" }}\n[[proxies]]\nname = \"tcp-{{ $v.First }}\"\ntype = \"tcp\"\nlocalPort = {{ $v.First }}\nremotePort = {{ $v.Second }}\n{{- end }}\nClient Plugins\nfrpc only forwards requests to local TCP or UDP ports by default.\nPlugins are used for providing rich features. There are built-in plugins such as\nunix_domain_socket\n,\nhttp_proxy\n,\nsocks5\n,\nstatic_file\n,\nhttp2https\n,\nhttps2http\n,\nhttps2https\nand you can see\nexample usage\n.\nUsing plugin\nhttp_proxy\n:\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nhttp_proxy\n\"\ntype\n=\n\"\ntcp\n\"\nremotePort\n=\n6000\n[\nproxies\n.\nplugin\n]\ntype\n=\n\"\nhttp_proxy\n\"\nhttpUser\n=\n\"\nabc\n\"\nhttpPassword\n=\n\"\nabc\n\"\nhttpUser\nand\nhttpPassword\nare configuration parameters used in\nhttp_proxy\nplugin.\nServer Manage Plugins\nRead the\ndocument\n.\nFind more plugins in\ngofrp/plugin\n.\nSSH Tunnel Gateway\nadded in v0.53.0\nfrp supports listening to an SSH port on the frps side and achieves TCP protocol proxying through the SSH -R protocol, without relying on frpc.\n#\nfrps.toml\nsshTunnelGateway.bindPort\n=\n2200\nWhen running\n./frps -c frps.toml\n, a private key file named\n.autogen_ssh_key\nwill be automatically created in the current working directory. This generated private key file will be used by the SSH server in frps.\nExecuting the command\nssh -R :80:127.0.0.1:8080 v0@{frp address} -p 2200 tcp --proxy_name\n\"\ntest-tcp\n\"\n--remote_port 9090\nsets up a proxy on frps that forwards the local 8080 service to the port 9090.\nfrp (via SSH) (Ctrl+C to quit)\n\nUser:\nProxyName: test-tcp\nType: tcp\nRemoteAddress: :9090\nThis is equivalent to:\nfrpc tcp --proxy_name\n\"\ntest-tcp\n\"\n--local_ip 127.0.0.1 --local_port 8080 --remote_port 9090\nPlease refer to this\ndocument\nfor more information.\nVirtual Network (VirtualNet)\nAlpha feature added in v0.62.0\nThe VirtualNet feature enables frp to create and manage virtual network connections between clients and visitors through a TUN interface. This allows for IP-level routing between machines, extending frp beyond simple port forwarding to support full network connectivity.\nFor detailed information about configuration and usage, please refer to the\nVirtualNet documentation\n.\nFeature Gates\nfrp supports feature gates to enable or disable experimental features. This allows users to try out new features before they're considered stable.\nAvailable Feature Gates\nName\nStage\nDefault\nDescription\nVirtualNet\nALPHA\nfalse\nVirtual network capabilities for frp\nEnabling Feature Gates\nTo enable an experimental feature, add the feature gate to your configuration:\nfeatureGates\n= {\nVirtualNet\n=\ntrue\n}\nFeature Lifecycle\nFeatures typically go through three stages:\nALPHA\n: Disabled by default, may be unstable\nBETA\n: May be enabled by default, more stable but still evolving\nGA (Generally Available)\n: Enabled by default, ready for production use\nRelated Projects\ngofrp/plugin\n- A repository for frp plugins that contains a variety of plugins implemented based on the frp extension mechanism, meeting the customization needs of different scenarios.\ngofrp/tiny-frpc\n- A lightweight version of the frp client (around 3.5MB at minimum) implemented using the ssh protocol, supporting some of the most commonly used features, suitable for devices with limited resources.\nContributing\nInterested in getting involved? We would like to help you!\nTake a look at our\nissues list\nand consider sending a Pull Request to\ndev branch\n.\nIf you want to add a new feature, please create an issue first to describe the new feature, as well as the implementation approach. Once a proposal is accepted, create an implementation of the new features and submit it as a pull request.\nSorry for my poor English. Improvements for this document are welcome, even some typo fixes.\nIf you have great ideas, send an email to\nfatedier@gmail.com\n.\nNote: We prefer you to give your advise in\nissues\n, so others with a same question can search it quickly and we don't need to answer them repeatedly.\nDonation\nIf frp helps you a lot, you can support us by:\nGitHub Sponsors\nSupport us by\nGithub Sponsors\n.\nYou can have your company's logo placed on README file of this project.\nPayPal\nDonate money by\nPayPal\nto my account\nfatedier@gmail.com\n.",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 36",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 99,464"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/fatedier/frp"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/NaiboWang/EasySpider",
      "title": "NaiboWang/EasySpider",
      "date": null,
      "executive_summary": [
        "A visual no-code/code-free web crawler/spideræ˜“é‡‡é›†ï¼šä¸€ä¸ªå¯è§†åŒ–æµè§ˆå™¨è‡ªåŠ¨åŒ–æµ‹è¯•/æ•°æ®é‡‡é›†/çˆ¬è™«è½¯ä»¶ï¼Œå¯ä»¥æ— ä»£ç å›¾å½¢åŒ–çš„è®¾è®¡å’Œæ‰§è¡Œçˆ¬è™«ä»»åŠ¡ã€‚åˆ«åï¼šServiceWrapperé¢å‘Webåº”ç”¨çš„æ™ºèƒ½åŒ–æœåŠ¡å°è£…ç³»ç»Ÿã€‚",
        "---",
        "æ˜“é‡‡é›†/EasySpider: Visual Code-Free Web Crawler\nä¸€ä¸ª\nå®Œå…¨å…è´¹\nï¼ˆ\nåŒ…æ‹¬å•†ä¸šä½¿ç”¨å’ŒäºŒæ¬¡å¼€å‘\nï¼‰çš„å¯è§†åŒ–æµè§ˆå™¨è‡ªåŠ¨åŒ–æµ‹è¯•/æ•°æ®é‡‡é›†/çˆ¬è™«è½¯ä»¶ï¼Œå¯ä»¥ä½¿ç”¨å›¾å½¢åŒ–ç•Œé¢ï¼Œæ— ä»£ç å¯è§†åŒ–çš„è®¾è®¡å’Œæ‰§è¡Œä»»åŠ¡ã€‚åªéœ€è¦åœ¨ç½‘é¡µä¸Šé€‰æ‹©è‡ªå·±æƒ³è¦æ“ä½œçš„å†…å®¹å¹¶æ ¹æ®æç¤ºæ¡†æ“ä½œå³å¯å®Œæˆä»»åŠ¡çš„è®¾è®¡å’Œæ‰§è¡Œã€‚åŒæ—¶è½¯ä»¶è¿˜å¯ä»¥å•ç‹¬ä»¥å‘½ä»¤è¡Œçš„æ–¹å¼è¿›è¡Œæ‰§è¡Œï¼Œä»è€Œå¯ä»¥å¾ˆæ–¹ä¾¿çš„åµŒå…¥åˆ°å…¶ä»–ç³»ç»Ÿä¸­ã€‚\nA\ncompletely free (including for commercial use and secondary development)\nvisual browser automation test/data collection/crawler software, which can be used to design and execute tasks in a code-free visual way. You only need to select the content you want to operate on the web page and follow the prompts to complete the design and execution of the task. At the same time, the software can also be executed separately in the command line, so that it can be easily embedded into other systems.\nä¸‹è½½æ˜“é‡‡é›†/Download EasySpider\nè¿›å…¥\nReleases Page\nä¸‹è½½æœ€æ–°ç‰ˆæœ¬ã€‚å¦‚æœä¸‹è½½é€Ÿåº¦æ…¢ï¼Œå¯ä»¥è€ƒè™‘ä¸­å›½å¢ƒå†…ä¸‹è½½åœ°å€ï¼š\nä¸­å›½å¢ƒå†…ä¸‹è½½åœ°å€\nã€‚\nRefer to the\nReleases Page\nto download the latest version of EasySpider.\nèµåŠ©è€…/Sponsors\näº®æ•°æ®BrightData\næ˜¯ä»£ç†å¸‚åœºé¢†å¯¼è€…ï¼Œè¦†ç›–å…¨çƒçš„7200ä¸‡IPï¼Œæä¾›çœŸäººä½å®…IPï¼Œå³æ—¶æ‰¹é‡é‡‡é›†ç½‘ç»œå…¬å¼€æ•°æ®ï¼ŒæˆåŠŸç‡äº²æµ‹æœ‰ä¿è¯ã€‚éœ€è¦æ€§ä»·æ¯”é«˜ä»£ç†IPçš„å¯\nç‚¹å‡»ä¸Šæ–¹å›¾ç‰‡æ³¨å†Œ\nåè”ç³»ä¸­æ–‡å®¢æœï¼Œå¼€é€šåå…è´¹è¯•ç”¨ï¼Œ\nç°åœ¨æœ‰é¦–å……å¤šå°‘å°±é€å¤šå°‘çš„æ´»åŠ¨\nã€‚BrightDataå¯é…åˆEasySpiderè¿›è¡Œæ•°æ®é‡‡é›†ã€‚\nBestProxy\nå…¨çƒç‹¬äº«ä¸“å±èµ„æºæ± ï¼Œä¼˜é€‰æµ·å¤–195+å›½å®¶/åœ°åŒºé«˜è´¨é‡ä½å®…IPï¼Œæœ¬åœ°ISPåŸç”ŸIPï¼Œä¸é™é‡ä½å®…ä»£ç†ã€é•¿æ•ˆISPä»£ç†ã€é™æ€æ•°æ®ä¸­å¿ƒä»£ç†ã€ç½‘é¡µçˆ¬è™«APIï¼ŒåŸå¸‚çº§ç²¾å‡†å®šä½ï¼Œæ”¯æŒHTTP(S)å’ŒSOCKS5åè®®ï¼Œä½æ£€æµ‹é£é™©ï¼Œå…¨æ–¹ä½ä»£ç†æœåŠ¡è§£å†³æ–¹æ¡ˆï¼ŒåŠ©åŠ›å„ç§åœºæ™¯ä¸šåŠ¡IPä»£ç†éœ€æ±‚ã€‚$0.66/Gèµ·æŒ‰éœ€ä»˜è´¹å’Œé•¿æœŸå¥—é¤ï¼Œé€‚åˆä¸åŒé¢„ç®—éœ€æ±‚ï¼Œ24/7å¤šè¯­è¨€æ”¯æŒï¼Œè”ç³»å®¢æœå…è´¹è¯•ç”¨500Mã€‚å¯ä¸EasySpiderå·¥å…·é…åˆä½¿ç”¨ï¼Œé«˜æ•ˆé‡‡é›†ç½‘ç»œæ•°æ®ã€‚\nIPdodo\nä¸“æ³¨ä¸ºè·¨å¢ƒç”¨æˆ·ï¼Œæä¾›ç‹¬äº«/çº¯å‡€/å®¶å®½/åŸç”Ÿ/åŒISPçš„å…¨çƒä»£ç†IPï¼Œä¸é™æµé‡ã€‚å…¨çƒ8000ä¸‡çœŸå®ä½å®…IPï¼Œè¦†ç›–200+å›½å®¶/åœ°åŒºï¼Œ99.9%åŒ¿åä¿æŠ¤ï¼Œä¸”æ”¯æŒHttp/Https/Socks5åè®®ï¼Œæ»¡è¶³çˆ¬è™«ã€æ•°æ®é‡‡é›†ã€è·¨å¢ƒç”µå•†ã€tk/fbæµåª’ä½“ç­‰ä¸šåŠ¡åœºæ™¯ã€‚ç°åœ¨å‰å¾€IPdodoæ³¨å†Œï¼Œæ”¯æŒå…è´¹è¯•ç”¨ã€‚\nå®˜æ–¹ç½‘ç«™/Official Website\nè®¿é—®æ˜“é‡‡é›†å®˜ç½‘ï¼š\nwww.easyspider.cn\nVisit the official website of EasySpider:\nwww.easyspider.net\nè½¯ä»¶ä½¿ç”¨ç¤ºä¾‹/Software Usage Example\nç¤ºä¾‹1/Example 1\nï¼ˆå³é”®ï¼‰é€‰ä¸­ä¸€ä¸ªå¤§å•†å“å— -> è½¯ä»¶è‡ªåŠ¨æ£€æµ‹åˆ°åŒç±»å‹å•†å“å— -> ç‚¹å‡»â€œé€‰ä¸­å…¨éƒ¨â€é€‰é¡¹ -> ç‚¹å‡»â€œé€‰ä¸­å­å…ƒç´ â€é€‰é¡¹ -> ç‚¹å‡»â€œé‡‡é›†æ•°æ®â€é€‰é¡¹ï¼Œå³å¯é‡‡é›†åˆ°æ‰€æœ‰å•†å“çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå¹¶åˆ†æˆä¸åŒå­—æ®µä¿å­˜ã€‚\n(Right click) Select a large product block -> The software will automatically detect similar blocks -> Click the 'Select All' option -> Click the 'Select Child Elements' option -> Click the 'Collect Data' option, you can collect the information of all products, and will be saved by sub-field.\nç¤ºä¾‹2/Example 2\nï¼ˆå³é”®ï¼‰é€‰ä¸­ä¸€ä¸ªå•†å“æ ‡é¢˜ï¼ŒåŒç±»å‹æ ‡é¢˜ä¼šè¢«è‡ªåŠ¨åŒ¹é…ï¼Œç‚¹å‡»â€œé€‰ä¸­å…¨éƒ¨â€é€‰é¡¹ -> ç‚¹å‡»â€œé‡‡é›†æ•°æ®â€é€‰é¡¹ï¼Œå³å¯é‡‡é›†åˆ°æ‰€æœ‰å•†å“çš„æ ‡é¢˜ä¿¡æ¯ã€‚\nåŒæ—¶ï¼Œé€‰ä¸­å…¨éƒ¨åå¦‚æœé€‰æ‹©â€œå¾ªç¯ç‚¹å‡»æ¯ä¸ªå…ƒç´ â€é€‰é¡¹ï¼Œå³å¯è‡ªåŠ¨æ‰“å¼€æ¯ä¸ªå•†å“çš„è¯¦æƒ…é¡µï¼Œç„¶åå¯ä»¥å†ç»§ç»­è®¾ç½®é‡‡é›†è¯¦æƒ…é¡µçš„ä¿¡æ¯ã€‚\n(Right Click) Select a product title, the same type of title will be automatically matched, click the 'Select All' option -> Click the 'Collect Data' option, you can collect the title information of all products.\nAt the same time, if you select the 'Loop-click every element' option after selecting all, you can automatically open the details page of each product, and then can set to collect the information of the details page.\næ›´å¤šç‰¹æ€§/More Features\næ›´å¤šç‰¹æ€§è¯·ç¿»åˆ°é¡µé¢åº•éƒ¨æŸ¥çœ‹ã€‚\nMore features please scroll to the bottom of this page to view.\næ”¯æŒä½œè€…/Support Author\næ˜“é‡‡é›†EasySpideræ˜¯ä¸€æ¬¾å®Œå…¨å…è´¹ä¸”ä½¿ç”¨ä¸­æ— å¹¿å‘Šçš„å¼€æºè½¯ä»¶ï¼Œè½¯ä»¶å¼€å‘å’Œç»´æŠ¤å…¨é ä½œè€…ç”¨çˆ±å‘ç”µï¼Œå› æ­¤æ‚¨å¯ä»¥é€‰æ‹©æ”¯æŒä½œè€…è®©ä½œè€…æœ‰æ›´å¤šçš„çƒ­æƒ…å’Œç²¾åŠ›ç»´æŠ¤æ­¤è½¯ä»¶ï¼Œæˆ–è€…æ‚¨ä½¿ç”¨äº†æ­¤è½¯ä»¶è¿›è¡Œäº†ç›ˆåˆ©ï¼Œæ¬¢è¿æ‚¨é€šè¿‡ä¸‹é¢çš„æ–¹å¼æ”¯æŒä½œè€…ï¼š\nGithub Sponsorï¼šç›´æ¥ç‚¹å‡»å³ä¾§\nSponsor\næŒ‰é’®èµåŠ©ã€‚\næ”¯ä»˜å®è´¦å·ï¼š\nnaibowang@foxmail.com\nï¼Œä¹Ÿå¯ä»¥æ‰«æä¸‹æ–¹äºŒç»´ç ã€‚\nå¾®ä¿¡æ”¶æ¬¾ï¼šæ‰«æä¸‹æ–¹äºŒç»´ç ã€‚\nPayPalè´¦å·ï¼šnaibowangï¼Œä¹Ÿå¯ä»¥æ‰«æä¸‹æ–¹äºŒç»´ç ã€‚\nYou can support the author by clicking the\nSponsor\nbutton at right side or pay via paypal: naibowang.\næ–‡æ¡£/Documentation\nè¯·ç‚¹æ­¤è¿›å…¥\næ•™ç¨‹æ–‡æ¡£\nï¼Œå¦‚æœ‰è‹±æ–‡å¯æš‚æ—¶ç¿»è¯‘ä¸€ä¸‹ï¼Œæˆ–çœ‹ä½œè€…çš„\nç¡•å£«æ¯•ä¸šè®ºæ–‡\nï¼ˆä¸»è¦çœ‹ç¬¬ä¸‰ç« å’Œç¬¬äº”ç« ï¼‰ã€‚\nEbayæ ·ä¾‹åšå®¢ï¼š\nhttps://blog.csdn.net/ihero/article/details/130805504\nã€‚\nDocumentation can be found from\nGitHub Wiki\n.\nè§†é¢‘æ•™ç¨‹/Video Tutorials\nBilibili/Bç«™è§†é¢‘æ•™ç¨‹:\nEasySpiderä»‹ç» - ä¸­å›½åœ°éœ‡å°ç½‘é‡‡é›†æ¡ˆä¾‹\nè®¾ç½®é¡µé¢å‘ä¸‹æ»šåŠ¨\nå¦‚ä½•æ— ä»£ç å¯è§†åŒ–çš„çˆ¬å–éœ€è¦ç™»å½•æ‰èƒ½çˆ¬çš„ç½‘ç«™ - çŸ¥ä¹ç½‘ç«™æ¡ˆä¾‹\nå¾ªç¯ç‚¹å‡»åˆ—è¡¨ä¸­æ¯ä¸ªé“¾æ¥è¿›å…¥è¯¦æƒ…é¡µé‡‡é›†è¯¦æƒ…é¡µå†…å®¹+è®¾è®¡æ—¶åŠ¨æ€è°ƒè¯•+åŠ¨æ€JS\nå®æˆ˜é‡‡é›†æ±½è½¦ç½‘æ–‡ç« å†…å®¹å¹¶ä¸‹è½½æ–‡ç« å†…å›¾ç‰‡\nå®šæ—¶æ‰§è¡Œä»»åŠ¡+é€‰ä¸­å­å…ƒç´ å¤šç§æ¨¡å¼+å°†æå–å€¼ä½œä¸ºå˜é‡è¾“å…¥\nã€é‡è¦ã€‘è‡ªå®šä¹‰æ¡ä»¶åˆ¤æ–­ä¹‹ä½¿ç”¨å¾ªç¯é¡¹å†…çš„JSå‘½ä»¤è¿”å›å€¼ - ç¬¬äºŒå¼¹\næµç¨‹å›¾æ‰§è¡Œé€»è¾‘è§£æ - 58åŒåŸæˆ¿æºæè¿°é‡‡é›†æ¡ˆä¾‹\nMacOSç³»ç»Ÿè®¾è®¡å’Œæ‰§è¡ŒeBayç½‘ç«™çˆ¬è™«ä»»åŠ¡æ•™ç¨‹\nå¦‚ä½•æ‰§è¡Œè‡ªå·±å†™çš„JSä»£ç å’Œç³»ç»Ÿä»£ç  ï¼ˆè‡ªå®šä¹‰æ“ä½œï¼‰\nå¦‚ä½•è‡ªå®šä¹‰å¾ªç¯å’Œåˆ¤æ–­æ¡ä»¶ - ç¬¬ä¸€å¼¹\nå¦‚ä½•å¯¹å…ƒç´ å’Œç½‘é¡µæˆªå›¾åŠå‘½ä»¤è¡Œæ‰§è¡ŒæŒ‡å—\nOCRè¯†åˆ«å…ƒç´ å†…å®¹åŠŸèƒ½ï¼ˆå¸¸ç”¨äºæ–‡å­—éªŒè¯ç ï¼‰\nå¦‚ä½•çˆ¬éœ€è¦è¾“å…¥éªŒè¯ç çš„ç½‘ç«™\nå¦‚ä½•åˆ‡æ¢IPæ± å’Œä½¿ç”¨éš§é“IP - æ‰“å¼€è¯¦æƒ…é¡µé‡‡é›†æ¡ˆä¾‹\nå¦‚ä½•åŒæ—¶æ‰§è¡Œå¤šä¸ªä»»åŠ¡ï¼ˆå¹¶è¡Œå¤šå¼€ï¼‰\nPythonä»£ç è¿ç®—åçš„ç»“æœä½œä¸ºæ–‡æœ¬æ¡†çš„è¾“å…¥\nå®ä¾‹ - åäººç±»ç½‘ç«™æ–‡ç« é‡‡é›†å’Œä»£ç è°ƒè¯•\nå†™å…¥MySQLæ•°æ®åº“æ•™ç¨‹\nä»æºä»£ç ç¼–è¯‘ç¨‹åºå¹¶è®¾è®¡è¿è¡Œå’Œè°ƒè¯•ä»»åŠ¡æŒ‡å—ï¼ˆåŸºäºUbuntu24.04ï¼‰\nRefer to\nYoutube Playlist\nto see the video tutorials of EasySpider.\næ ·ä¾‹ä»»åŠ¡/Sample Tasks\nä»æœ¬é¡¹ç›®çš„\nExamples\næ–‡ä»¶å¤¹ä¸­ä¸‹è½½æ ·ä¾‹ä»»åŠ¡ï¼Œæ›´åä¸ºå¤§äº0çš„æ•°å­—ï¼Œå¯¼å…¥åˆ°EasySpiderä¸­çš„\ntasks\næ–‡ä»¶å¤¹ä¸­ï¼Œç„¶ååœ¨EasySpiderä¸­æ‰“å¼€å³å¯ã€‚\nDownload sample tasks from the\nExamples\nfolder of this project, rename them to numbers greater than 0, import them into the\ntasks\nfolder in EasySpider, and then open them in EasySpider.\nå£°æ˜/Declaration\næœ¬è½¯ä»¶ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œ\nä¸¥ç¦ä½¿ç”¨è½¯ä»¶è¿›è¡Œä»»ä½•è¿æ³•è¿è§„çš„æ“ä½œï¼Œå¦‚çˆ¬å–ä¸å…è®¸çˆ¬å–çš„æ”¿åºœ/å†›äº‹æœºå…³ç½‘ç«™ç­‰\nã€‚ä½¿ç”¨æœ¬è½¯ä»¶æ‰€é€ æˆçš„\nä¸€åˆ‡åæœç”±ä½¿ç”¨è€…è‡ªè´Ÿ\nï¼Œä¸ä½œè€…æœ¬äººæ— å…³ï¼Œ\nä½œè€…ä¸ä¼šæ‰¿æ‹…ä»»ä½•è´£ä»»\nã€‚\nThis software is for learning and communication only.\nIt is strictly forbidden to use the software for any illegal operations, such as crawling government/military websites that are not allowed to be crawled.\nAll consequences caused by the use of this software are\nat the user's own risk, and the author is not responsible for any consequences\n.\nå¯¹äºæ”¿åºœå’Œå†›äº‹æœºå…³ç­‰ç½‘ç«™çš„çˆ¬è™«æ“ä½œï¼Œ\nä½œè€…å°†ä¸ä¼šè¿›è¡Œä»»ä½•ç­”ç–‘\nï¼Œä»¥å…è¿åå›½å®¶ç›¸å…³æ³•å¾‹æ³•è§„å’Œæ”¿ç­–ã€‚\nFor the crawler operations of government and military websites,\nthe author will not answer any questions\nin order to avoid violating relevant national laws, regulations and policies.\nEasySpideréµå¾ªAGPL-3.0åè®®ï¼Œ\nä»»ä½•ä¸ªäººå’Œä¼ä¸šéƒ½å¯ä»¥å…è´¹ä½¿ç”¨è½¯ä»¶æœ¬èº«æˆ–ä½¿ç”¨æºä»£ç è¿›è¡ŒäºŒæ¬¡å¼€å‘ï¼Œæ— éœ€è”ç³»ä½œè€…è¿›è¡Œå•†ä¸šï¼ˆä¸“åˆ©ï¼‰æˆæƒ\nï¼Œä½†éœ€è¦æ³¨æ„AGPL-3.0åè®®çš„ç›¸å…³è§„åˆ™ï¼š\nEasySpider complies with the AGPL-3.0 agreement.\nAny individual or enterprise can use the software for free and use the software source code for secondary development without contacting the author for commercial (patent) authorization.\nHowever, it is necessary to pay attention to the related rules of the AGPL-3.0 agreement:\n1. Copyleftï¼ˆä¼ æŸ“æ€§ï¼‰ / Copyleft (Viral Clause)\nè¡ç”Ÿä½œå“ / Derivative Works\nä»»ä½•åŸºäº AGPL ä»£ç çš„ä¿®æ”¹æˆ–è¡ç”Ÿä½œå“ï¼Œå¿…é¡»\nä»¥ç›¸åŒè®¸å¯è¯ï¼ˆAGPL-3.0ï¼‰å‘å¸ƒ\nã€‚\nAny modifications or derivative works based on AGPL code must be\nlicensed under AGPL-3.0\n.\nè”åŠ¨èŒƒå›´ / Scope of Copyleft\nè‹¥ AGPL ä»£ç ä¸å…¶ä»–ä»£ç ç»“åˆï¼ˆå¦‚é™æ€é“¾æ¥ã€ç´§å¯†é›†æˆï¼‰ï¼Œæ•´ä¸ªä½œå“éœ€éµå®ˆ AGPLã€‚\nIf AGPL code is combined with other code (e.g., static linking), the entire work must comply with AGPL.\n2. ç½‘ç»œä½¿ç”¨æ¡æ¬¾ / Network Use Clause\nSaaS è§¦å‘å¼€æºä¹‰åŠ¡ / SaaS Trigger\nè‹¥è½¯ä»¶ä»¥æœåŠ¡å½¢å¼æä¾›ï¼ˆå¦‚ç½‘ç«™ã€APIï¼‰ï¼Œå¿…é¡»å‘æ‰€æœ‰ç”¨æˆ·å…¬å¼€\nå®Œæ•´å¯¹åº”æºä»£ç \nï¼ˆåŒ…æ‹¬ä¿®æ”¹åçš„ä»£ç ï¼‰ã€‚\nIf the software is provided as a service (e.g., website, API), the\nfull corresponding source code\n(including modifications) must be made available to all users.\nç”¨æˆ·æƒåˆ© / User Rights\næœåŠ¡çš„æ¥æ”¶è€…å¯é€šè¿‡ä¸‹è½½æˆ–ä¹¦é¢è¯·æ±‚è·å–æºç ã€‚\nService recipients may obtain the source code via download or written request.\n3. æºç æä¾›è¦æ±‚ / Source Code Provision\näºŒè¿›åˆ¶åˆ†å‘ / Binary Distribution\nå¿…é¡»é™„å¸¦æºç æˆ–æä¾›è·å–æ¸ é“ï¼ˆå¦‚ä¸‹è½½é“¾æ¥ï¼‰ã€‚\nSource code must be included or a download link provided.\nç½‘ç»œæœåŠ¡åœºæ™¯ / Network Service Scenario\néœ€é€šè¿‡æœåŠ¡ç•Œé¢\næ˜¾å¼æä¾›æºç é“¾æ¥\nï¼Œæˆ–å‘ç”¨æˆ·ä¹¦é¢æ‰¿è¯ºæä¾›æºç ã€‚\nThe service interface must\nexplicitly provide a source code link\nor offer a written offer for source code.\n4. ä¸“åˆ©æˆæƒ / Patent Grant\nè´¡çŒ®è€…è‡ªåŠ¨æˆäºˆç”¨æˆ·ä¸è½¯ä»¶ç›¸å…³çš„ä¸“åˆ©è®¸å¯ï¼Œç¦æ­¢ä¸“åˆ©è¯‰è®¼ã€‚\nContributors automatically grant users patent rights related to the software, and prohibit patent litigation.\n5. å…è´£å£°æ˜ / Disclaimer\nè½¯ä»¶æŒ‰â€œåŸæ ·â€æä¾›ï¼Œä½œè€…\nä¸æ‰¿æ‹…ä»»ä½•è´£ä»»\nï¼ˆæ— æ‹…ä¿æ¡æ¬¾ï¼‰ã€‚\nThe software is provided \"as is\" with\nno warranties or liabilities\n.\nç­”ç–‘QQç¾¤\nç¾¤å·ï¼š\n682921940\nï¼Œå»ºè®®é€šè¿‡GithubæIssueçš„æ–¹å¼ç­”ç–‘ï¼Œå¦‚æœå®åœ¨æœ‰éœ€è¦æ‰è¯·åŠ QQç¾¤ï¼Œå› ä¸ºç¾¤äººæ•°æœ‰ä¸Šé™ï¼Œ\nQQç¾¤ä¸æä¾›è½¯ä»¶ä¸‹è½½åŠŸèƒ½\nã€‚\nå‡ºç‰ˆç‰©/Publications\nThis software has been accepted by The Web Conference (WWW) 2023 (ä¸­å›½è®¡ç®—æœºå­¦ä¼šé¡¶çº§ä¼šè®®ï¼ŒCCF A):\nEasySpider: A No-Code Visual System for Crawling the Web\n, April 2023.\nä¸­å›½å›½å®¶çŸ¥è¯†äº§æƒå±€å‘æ˜ä¸“åˆ©ï¼Œ\nä¸€ç§è‡ªå®šä¹‰æå–æµç¨‹çš„æœåŠ¡å°è£…ç³»ç»Ÿ\nï¼Œ 2022å¹´5æœˆã€‚\næµ™æ±Ÿå¤§å­¦ç¡•å£«è®ºæ–‡\nï¼Œ\né¢å‘WEBåº”ç”¨çš„æ™ºèƒ½åŒ–æœåŠ¡å°è£…ç³»ç»Ÿè®¾è®¡ä¸å®ç°\nï¼Œ2020å¹´6æœˆã€‚\nç¼–è¯‘è¯´æ˜/Compilation Instructions\næŸ¥çœ‹\nç¼–è¯‘è¯´æ˜\nã€‚\nRefer to\nCompilation Instructions\n.\næ”¯æŒç‰¹æ€§/Supported Features\nä¸­æ–‡ç•Œé¢æˆªå›¾\nè½¯ä»¶ç•Œé¢ç¤ºä¾‹\nå—å’Œå­å—åŠè¡¨å•å®šä¹‰\nå·²é€‰ä¸­å’Œå¾…é€‰æ‹©ç¤ºä¾‹\näº¬ä¸œå•†å“å—é€‰æ‹©ç¤ºä¾‹ï¼š\näº¬ä¸œå•†å“æ ‡é¢˜è‡ªåŠ¨åŒ¹é…é€‰æ‹©ç¤ºä¾‹\nåˆ†å—é€‰æ‹©æ‰€æœ‰å­å…ƒç´ ç¤ºä¾‹\nåŒç±»å‹å…ƒç´ è‡ªåŠ¨å’Œæ‰‹åŠ¨åŒ¹é…ç¤ºä¾‹\nå››ç§é€‰æ‹©æ–¹å¼ç¤ºä¾‹\nè¾“å…¥æ–‡å­—ç¤ºä¾‹\nå¾ªç¯ç‚¹å‡»58åŒåŸæˆ¿å±‹æ ‡é¢˜ä»¥è¿›å…¥è¯¦æƒ…é¡µé‡‡é›†ç¤ºä¾‹\né‡‡é›†å…ƒç´ æ–‡æœ¬ç¤ºä¾‹\næµç¨‹å›¾ç•Œé¢ä»‹ç»\nå¾ªç¯é€‰é¡¹ç¤ºä¾‹\nå¾ªç¯ç‚¹å‡»ä¸‹ä¸€é¡µç¤ºä¾‹\næ¡ä»¶åˆ†æ”¯ç¤ºä¾‹\nå®Œæ•´é‡‡é›†æµç¨‹å›¾ç¤ºä¾‹\nå®Œæ•´é‡‡é›†æµç¨‹å›¾è½¬æ¢ä¸ºå¸¸è§„æµç¨‹å›¾ç¤ºä¾‹\næœåŠ¡ä¿¡æ¯ç¤ºä¾‹\næœåŠ¡è°ƒç”¨ç¤ºä¾‹\n58 åŒåŸæˆ¿æºä¿¡æ¯é‡‡é›†æœåŠ¡éƒ¨åˆ†é‡‡é›†ç»“æœå±•ç¤º",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 36",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 42,881"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/NaiboWang/EasySpider"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/katanemo/archgw",
      "title": "katanemo/archgw",
      "date": null,
      "executive_summary": [
        "The smart edge and AI gateway for agents. Arch is a high-performance proxy server that handles the low-level work in building agents: like applying guardrails, routing prompts to the right agent, and unifying access to LLMs, etc. Natively designed to handle and process prompts, Arch helps you build agents faster.",
        "---",
        "Arch is a modular ai-native edge and AI gateway for agents.\nArch handles the\npesky low-level work\nin building agentic apps â€” like applying guardrails, clarifying vague user input, routing prompts to the right agent, and unifying access to any LLM. Itâ€™s a language and framework friendly infrastructure layer designed to help you build and ship agentic apps faster.\nQuickstart\nâ€¢\nDemos\nâ€¢\nRoute LLMs\nâ€¢\nBuild agentic apps with Arch\nâ€¢\nDocumentation\nâ€¢\nContact\nAbout The Latest Release:\n[0.3.15]\nPreference-aware multi LLM routing for Claude Code 2.0\nOverview\nAI demos are easy to hack. But once you move past a prototype, youâ€™re stuck building and maintaining low-level plumbing code that slows down real innovation. For example:\nRouting & orchestration.\nPut routing in code and youâ€™ve got two choices: maintain it yourself or live with a frameworkâ€™s baked-in logic. Either way, keeping routing consistent means pushing code changes across all your agents, slowing iteration and turning every policy tweak into a refactor instead of a config flip.\nModel integration churn.\nFrameworks wire LLM integrations directly into code abstractions, making it hard to add or swap models without touching application code â€” meaning youâ€™ll have to do codewide search/replace every time you want to experiment with a new model or version.\nObservability & governance.\nLogging, tracing, and guardrails are baked in as tightly coupled features, so bringing in best-of-breed solutions is painful and often requires digging through the guts of a framework.\nPrompt engineering overhead\n. Input validation, clarifying vague user input, and coercing outputs into the right schema all pile up, turning what should be design work into low-level plumbing work.\nBrittle upgrades\n. Every change (new model, new guardrail, new trace format) means patching and redeploying application servers. Contrast that with bouncing a central proxyâ€”one upgrade, instantly consistent everywhere.\nWith Arch, you can move faster by focusing on higher-level objectives in a language and framework agnostic way.\nArch\nwas built by the contributors of\nEnvoy Proxy\nwith the belief that:\nPrompts are nuanced and opaque user requests, which require the same capabilities as traditional HTTP requests including secure handling, intelligent routing, robust observability, and integration with backend (API) systems to improve speed and accuracy for common agentic scenarios  â€“ all outside core application logic.*\nCore Features\n:\nğŸš¦ Route to Agents\n: Engineered with purpose-built\nLLMs\nfor fast (<100ms) agent routing and hand-off\nğŸ”— Route to LLMs\n: Unify access to LLMs with support for\nthree routing strategies\n.\nâ›¨ Guardrails\n: Centrally configure and prevent harmful outcomes and ensure safe user interactions\nâš¡ Tools Use\n: For common agentic scenarios let Arch instantly clarify and convert prompts to tools/API calls\nğŸ•µ Observability\n: W3C compatible request tracing and LLM metrics that instantly plugin with popular tools\nğŸ§± Built on Envoy\n: Arch runs alongside app servers as a containerized process, and builds on top of\nEnvoy's\nproven HTTP management and scalability features to handle ingress and egress traffic related to prompts and LLMs.\nHigh-Level Sequence Diagram\n:\nJump to our\ndocs\nto learn how you can use Arch to improve the speed, security and personalization of your GenAI apps.\nImportant\nToday, the function calling LLM (Arch-Function) designed for the agentic and RAG scenarios is hosted free of charge in the US-central region. To offer consistent latencies and throughput, and to manage our expenses, we will enable access to the hosted version via developers keys soon, and give you the option to run that LLM locally. For more details see this issue\n#258\nContact\nTo get in touch with us, please join our\ndiscord server\n. We will be monitoring that actively and offering support there.\nDemos\nSample App: Weather Forecast Agent\n- A sample agentic weather forecasting app that highlights core function calling capabilities of Arch.\nSample App: Network Operator Agent\n- A simple network device switch operator agent that can retrive device statistics and reboot them.\nUser Case: Connecting to SaaS APIs\n- Connect 3rd party SaaS APIs to your agentic chat experience.\nQuickstart\nFollow this quickstart guide to use Arch as a router for local or hosted LLMs, including dynamic routing. Later in the section we will see how you can Arch to build highly capable agentic applications, and to provide e2e observability.\nPrerequisites\nBefore you begin, ensure you have the following:\nDocker System\n(v24)\nDocker compose\n(v2.29)\nPython\n(v3.13)\nArch's CLI allows you to manage and interact with the Arch gateway efficiently. To install the CLI, simply run the following command:\nTip\nWe recommend that developers create a new Python virtual environment to isolate dependencies before installing Arch. This ensures that archgw and its dependencies do not interfere with other packages on your system.\n$\npython3.12 -m venv venv\n$\nsource\nvenv/bin/activate\n#\nOn Windows, use: venv\\Scripts\\activate\n$\npip install archgw==0.3.15\nUse Arch as a LLM Router\nArch supports three powerful routing strategies for LLMs: model-based routing, alias-based routing, and preference-based routing. Each strategy offers different levels of abstraction and control for managing your LLM infrastructure.\nModel-based Routing\nModel-based routing allows you to configure specific models with static routing. This is ideal when you need direct control over which models handle specific requests. Arch supports 11+ LLM providers including OpenAI, Anthropic, DeepSeek, Mistral, Groq, and more.\nversion\n:\nv0.1.0\nlisteners\n:\negress_traffic\n:\naddress\n:\n0.0.0.0\nport\n:\n12000\nmessage_format\n:\nopenai\ntimeout\n:\n30s\nllm_providers\n:\n  -\nmodel\n:\nopenai/gpt-4o\naccess_key\n:\n$OPENAI_API_KEY\ndefault\n:\ntrue\n-\nmodel\n:\nanthropic/claude-3-5-sonnet-20241022\naccess_key\n:\n$ANTHROPIC_API_KEY\nYou can then route to specific models using any OpenAI-compatible client:\nfrom\nopenai\nimport\nOpenAI\nclient\n=\nOpenAI\n(\nbase_url\n=\n\"http://127.0.0.1:12000/v1\"\n,\napi_key\n=\n\"test\"\n)\n# Route to specific model\nresponse\n=\nclient\n.\nchat\n.\ncompletions\n.\ncreate\n(\nmodel\n=\n\"anthropic/claude-3-5-sonnet-20241022\"\n,\nmessages\n=\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Explain quantum computing\"\n}]\n)\nAlias-based Routing\nAlias-based routing lets you create semantic model names that map to underlying providers. This approach decouples your application code from specific model names, making it easy to experiment with different models or handle provider changes.\nversion\n:\nv0.1.0\nlisteners\n:\negress_traffic\n:\naddress\n:\n0.0.0.0\nport\n:\n12000\nmessage_format\n:\nopenai\ntimeout\n:\n30s\nllm_providers\n:\n  -\nmodel\n:\nopenai/gpt-4o\naccess_key\n:\n$OPENAI_API_KEY\n-\nmodel\n:\nanthropic/claude-3-5-sonnet-20241022\naccess_key\n:\n$ANTHROPIC_API_KEY\nmodel_aliases\n:\n#\nModel aliases - friendly names that map to actual model names\nfast-model\n:\ntarget\n:\ngpt-4o-mini\nreasoning-model\n:\ntarget\n:\ngpt-4o\ncreative-model\n:\ntarget\n:\nclaude-3-5-sonnet-20241022\nUse semantic aliases in your application code:\n# Your code uses semantic names instead of provider-specific ones\nresponse\n=\nclient\n.\nchat\n.\ncompletions\n.\ncreate\n(\nmodel\n=\n\"reasoning-model\"\n,\n# Routes to best available reasoning model\nmessages\n=\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Solve this complex problem...\"\n}]\n)\nPreference-aligned Routing\nPreference-aligned routing provides intelligent, dynamic model selection based on natural language descriptions of tasks and preferences. Instead of hardcoded routing logic, you describe what each model is good at using plain English.\nversion\n:\nv0.1.0\nlisteners\n:\negress_traffic\n:\naddress\n:\n0.0.0.0\nport\n:\n12000\nmessage_format\n:\nopenai\ntimeout\n:\n30s\nllm_providers\n:\n  -\nmodel\n:\nopenai/gpt-4o\naccess_key\n:\n$OPENAI_API_KEY\nrouting_preferences\n:\n      -\nname\n:\ncomplex_reasoning\ndescription\n:\ndeep analysis, mathematical problem solving, and logical reasoning\n-\nname\n:\ncreative_writing\ndescription\n:\nstorytelling, creative content, and artistic writing\n-\nmodel\n:\ndeepseek/deepseek-coder\naccess_key\n:\n$DEEPSEEK_API_KEY\nrouting_preferences\n:\n      -\nname\n:\ncode_generation\ndescription\n:\ngenerating new code, writing functions, and creating scripts\n-\nname\n:\ncode_review\ndescription\n:\nanalyzing existing code for bugs, improvements, and optimization\nArch uses a lightweight 1.5B autoregressive model to intelligently map user prompts to these preferences, automatically selecting the best model for each request. This approach adapts to intent drift, supports multi-turn conversations, and avoids brittle embedding-based classifiers or manual if/else chains. No retraining required when adding models or updating policies â€” routing is governed entirely by human-readable rules.\nLearn More\n: Check our\ndocumentation\nfor comprehensive provider setup guides and routing strategies. You can learn more about the design, benchmarks, and methodology behind preference-based routing in our paper:\nBuild Agentic Apps with Arch\nIn following quickstart we will show you how easy it is to build AI agent with Arch gateway. We will build a currency exchange agent using following simple steps. For this demo we will use\nhttps://api.frankfurter.dev/\nto fetch latest price for currencies and assume USD as base currency.\nStep 1. Create arch config file\nCreate\narch_config.yaml\nfile with following content,\nversion\n:\nv0.1.0\nlisteners\n:\ningress_traffic\n:\naddress\n:\n0.0.0.0\nport\n:\n10000\nmessage_format\n:\nopenai\ntimeout\n:\n30s\nllm_providers\n:\n  -\naccess_key\n:\n$OPENAI_API_KEY\nmodel\n:\nopenai/gpt-4o\nsystem_prompt\n:\n|\nYou are a helpful assistant.\nprompt_guards\n:\ninput_guards\n:\njailbreak\n:\non_exception\n:\nmessage\n:\nLooks like you're curious about my abilities, but I can only provide assistance for currency exchange.\nprompt_targets\n:\n  -\nname\n:\ncurrency_exchange\ndescription\n:\nGet currency exchange rate from USD to other currencies\nparameters\n:\n      -\nname\n:\ncurrency_symbol\ndescription\n:\nthe currency that needs conversion\nrequired\n:\ntrue\ntype\n:\nstr\nin_path\n:\ntrue\nendpoint\n:\nname\n:\nfrankfurther_api\npath\n:\n/v1/latest?base=USD&symbols={currency_symbol}\nsystem_prompt\n:\n|\nYou are a helpful assistant. Show me the currency symbol you want to convert from USD.\n-\nname\n:\nget_supported_currencies\ndescription\n:\nGet list of supported currencies for conversion\nendpoint\n:\nname\n:\nfrankfurther_api\npath\n:\n/v1/currencies\nendpoints\n:\nfrankfurther_api\n:\nendpoint\n:\napi.frankfurter.dev:443\nprotocol\n:\nhttps\nStep 2. Start arch gateway with currency conversion config\n$ archgw up arch_config.yaml\n2024-12-05 16:56:27,979 - cli.main - INFO - Starting archgw cli version: 0.3.15\n2024-12-05 16:56:28,485 - cli.utils - INFO - Schema validation successful\n!\n2024-12-05 16:56:28,485 - cli.main - INFO - Starting arch model server and arch gateway\n2024-12-05 16:56:51,647 - cli.core - INFO - Container is healthy\n!\nOnce the gateway is up you can start interacting with at port 10000 using openai chat completion API.\nSome of the sample queries you can ask could be\nwhat is currency rate for gbp?\nor\nshow me list of currencies for conversion\n.\nStep 3. Interacting with gateway using curl command\nHere is a sample curl command you can use to interact,\n$ curl --header\n'\nContent-Type: application/json\n'\n\\\n  --data\n'\n{\"messages\": [{\"role\": \"user\",\"content\": \"what is exchange rate for gbp\"}], \"model\": \"none\"}\n'\n\\\n  http://localhost:10000/v1/chat/completions\n|\njq\n\"\n.choices[0].message.content\n\"\n\"\nAs of the date provided in your context, December 5, 2024, the exchange rate for GBP (British Pound) from USD (United States Dollar) is 0.78558. This means that 1 USD is equivalent to 0.78558 GBP.\n\"\nAnd to get list of supported currencies,\n$ curl --header\n'\nContent-Type: application/json\n'\n\\\n  --data\n'\n{\"messages\": [{\"role\": \"user\",\"content\": \"show me list of currencies that are supported for conversion\"}], \"model\": \"none\"}\n'\n\\\n  http://localhost:10000/v1/chat/completions\n|\njq\n\"\n.choices[0].message.content\n\"\n\"\nHere is a list of the currencies that are supported for conversion from USD, along with their symbols:\\n\\n1. AUD - Australian Dollar\\n2. BGN - Bulgarian Lev\\n3. BRL - Brazilian Real\\n4. CAD - Canadian Dollar\\n5. CHF - Swiss Franc\\n6. CNY - Chinese Renminbi Yuan\\n7. CZK - Czech Koruna\\n8. DKK - Danish Krone\\n9. EUR - Euro\\n10. GBP - British Pound\\n11. HKD - Hong Kong Dollar\\n12. HUF - Hungarian Forint\\n13. IDR - Indonesian Rupiah\\n14. ILS - Israeli New Sheqel\\n15. INR - Indian Rupee\\n16. ISK - Icelandic KrÃ³na\\n17. JPY - Japanese Yen\\n18. KRW - South Korean Won\\n19. MXN - Mexican Peso\\n20. MYR - Malaysian Ringgit\\n21. NOK - Norwegian Krone\\n22. NZD - New Zealand Dollar\\n23. PHP - Philippine Peso\\n24. PLN - Polish ZÅ‚oty\\n25. RON - Romanian Leu\\n26. SEK - Swedish Krona\\n27. SGD - Singapore Dollar\\n28. THB - Thai Baht\\n29. TRY - Turkish Lira\\n30. USD - United States Dollar\\n31. ZAR - South African Rand\\n\\nIf you want to convert USD to any of these currencies, you can select the one you are interested in.\n\"\nObservability\nArch is designed to support best-in class observability by supporting open standards. Please read our\ndocs\non observability for more details on tracing, metrics, and logs. The screenshot below is from our integration with Signoz (among others)\nDebugging\nWhen debugging issues / errors application logs and access logs provide key information to give you more context on whats going on with the system. Arch gateway runs in info log level and following is a typical output you could see in a typical interaction between developer and arch gateway,\n$ archgw up --service archgw --foreground\n...\n[2025-03-26 18:32:01.350][26][info] prompt_gateway: on_http_request_body: sending request to model server\n[2025-03-26 18:32:01.851][26][info] prompt_gateway: on_http_call_response: model server response received\n[2025-03-26 18:32:01.852][26][info] prompt_gateway: on_http_call_response: dispatching api call to developer endpoint: weather_forecast_service, path: /weather, method: POST\n[2025-03-26 18:32:01.882][26][info] prompt_gateway: on_http_call_response: developer api call response received: status code: 200\n[2025-03-26 18:32:01.882][26][info] prompt_gateway: on_http_call_response: sending request to upstream llm\n[2025-03-26 18:32:01.883][26][info] llm_gateway: on_http_request_body: provider: gpt-4o-mini, model requested: None, model selected: gpt-4o-mini\n[2025-03-26 18:32:02.818][26][info] llm_gateway: on_http_response_body: time to first token: 1468ms\n[2025-03-26 18:32:04.532][26][info] llm_gateway: on_http_response_body: request latency: 3183ms\n...\nLog level can be changed to debug to get more details. To enable debug logs edit (supervisord.conf)[arch/supervisord.conf], change the log level\n--component-log-level wasm:info\nto\n--component-log-level wasm:debug\n. And after that you need to rebuild docker image and restart the arch gateway using following set of commands,\n# make sure you are at the root of the repo\n$ archgw build\n# go to your service that has arch_config.yaml file and issue following command,\n$ archgw up --service archgw --foreground\nContribution\nWe would love feedback on our\nRoadmap\nand we welcome contributions to\nArch\n!\nWhether you're fixing bugs, adding new features, improving documentation, or creating tutorials, your help is much appreciated.\nPlease visit our\nContribution Guide\nfor more details",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 36",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 3,928"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/katanemo/archgw"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/gkd-kit/gkd",
      "title": "gkd-kit/gkd",
      "date": null,
      "executive_summary": [
        "åŸºäºæ— éšœç¢ï¼Œé«˜çº§é€‰æ‹©å™¨ï¼Œè®¢é˜…è§„åˆ™çš„è‡ªå®šä¹‰å±å¹•ç‚¹å‡» Android åº”ç”¨ | An Android APP with custom screen tapping based on Accessibility, Advanced Selectors, and Subscription Rules",
        "---",
        "gkd\nåŸºäº\né«˜çº§é€‰æ‹©å™¨\n+\nè®¢é˜…è§„åˆ™\n+\nå¿«ç…§å®¡æŸ¥\nçš„è‡ªå®šä¹‰å±å¹•ç‚¹å‡» Android åº”ç”¨\né€šè¿‡è‡ªå®šä¹‰è§„åˆ™ï¼Œåœ¨æŒ‡å®šç•Œé¢ï¼Œæ»¡è¶³æŒ‡å®šæ¡ä»¶(å¦‚å±å¹•ä¸Šå­˜åœ¨ç‰¹å®šæ–‡å­—)æ—¶ï¼Œç‚¹å‡»ç‰¹å®šçš„èŠ‚ç‚¹æˆ–ä½ç½®æˆ–æ‰§è¡Œå…¶ä»–æ“ä½œ\nå¿«æ·æ“ä½œ\nå¸®åŠ©ä½ ç®€åŒ–ä¸€äº›é‡å¤çš„æµç¨‹, å¦‚æŸäº›è½¯ä»¶è‡ªåŠ¨ç¡®è®¤ç”µè„‘ç™»å½•\nè·³è¿‡æµç¨‹\næŸäº›è½¯ä»¶å¯èƒ½åœ¨å¯åŠ¨æ—¶å­˜åœ¨ä¸€äº›çƒ¦äººçš„æµç¨‹, è¿™ä¸ªè½¯ä»¶å¯ä»¥å¸®åŠ©ä½ ç‚¹å‡»è·³è¿‡è¿™ä¸ªæµç¨‹\nå…è´£å£°æ˜\næœ¬é¡¹ç›®éµå¾ª\nGPL-3.0\nå¼€æºï¼Œé¡¹ç›®ä»…ä¾›å­¦ä¹ äº¤æµï¼Œç¦æ­¢ç”¨äºå•†ä¸šæˆ–éæ³•ç”¨é€”\nå®‰è£…\nå¦‚é‡é—®é¢˜è¯·å…ˆæŸ¥çœ‹\nç–‘éš¾è§£ç­”\næˆªå›¾\nè®¢é˜…\nGKD\né»˜è®¤ä¸æä¾›è§„åˆ™\nï¼Œéœ€è‡ªè¡Œæ·»åŠ æœ¬åœ°è§„åˆ™ï¼Œæˆ–è€…é€šè¿‡è®¢é˜…é“¾æ¥çš„æ–¹å¼è·å–è¿œç¨‹è§„åˆ™\nä¹Ÿå¯é€šè¿‡\nsubscription-template\nå¿«é€Ÿæ„å»ºè‡ªå·±çš„è¿œç¨‹è®¢é˜…\nç¬¬ä¸‰æ–¹è®¢é˜…åˆ—è¡¨å¯åœ¨\nhttps://github.com/topics/gkd-subscription\næŸ¥çœ‹\nè¦åŠ å…¥æ­¤åˆ—è¡¨, éœ€ç‚¹å‡»ä»“åº“ä¸»é¡µå³ä¸Šè§’è®¾ç½®å›¾æ ‡ååœ¨ Topics ä¸­æ·»åŠ \ngkd-subscription\nç¤ºä¾‹å›¾ç‰‡ - æ·»åŠ è‡³ Topics (ç‚¹å‡»å±•å¼€)\né€‰æ‹©å™¨\nä¸€ä¸ªç±»ä¼¼ CSS é€‰æ‹©å™¨çš„é€‰æ‹©å™¨, èƒ½è”ç³»èŠ‚ç‚¹ä¸Šä¸‹æ–‡ä¿¡æ¯, æ›´å®¹æ˜“ä¹Ÿæ›´ç²¾ç¡®æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹\nhttps://gkd.li/guide/selector\n@[vid=\"menu\"] < [vid=\"menu_container\"] - [vid=\"dot_text_layout\"] > [text^=\"å¹¿å‘Š\"]\nç¤ºä¾‹å›¾ç‰‡ - é€‰æ‹©å™¨è·¯å¾„è§†å›¾ (ç‚¹å‡»å±•å¼€)\næèµ \nå¦‚æœ GKD å¯¹ä½ æœ‰ç”¨, å¯ä»¥é€šè¿‡ä»¥ä¸‹é“¾æ¥æ”¯æŒè¯¥é¡¹ç›®\nhttps://github.com/lisonge/sponsor\næˆ–å‰å¾€\nGoogle Play\nç»™ä¸ªå¥½è¯„\nStar History",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 35",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 31,388"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/gkd-kit/gkd"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/keycloak/keycloak",
      "title": "keycloak/keycloak",
      "date": null,
      "executive_summary": [
        "Open Source Identity and Access Management For Modern Applications and Services",
        "---",
        "Open Source Identity and Access Management\nAdd authentication to applications and secure services with minimum effort. No need to deal with storing users or authenticating users.\nKeycloak provides user federation, strong authentication, user management, fine-grained authorization, and more.\nHelp and Documentation\nDocumentation\nUser Mailing List\n- Mailing list for help and general questions about Keycloak\nJoin\n#keycloak\nfor general questions, or\n#keycloak-dev\non Slack for design and development discussions, by creating an account at\nhttps://slack.cncf.io/\n.\nReporting Security Vulnerabilities\nIf you have found a security vulnerability, please look at the\ninstructions on how to properly report it\n.\nReporting an issue\nIf you believe you have discovered a defect in Keycloak, please open\nan issue\n.\nPlease remember to provide a good summary, description as well as steps to reproduce the issue.\nGetting started\nTo run Keycloak, download the distribution from our\nwebsite\n. Unzip and run:\nbin/kc.[sh|bat] start-dev\nAlternatively, you can use the Docker image by running:\ndocker run quay.io/keycloak/keycloak start-dev\nFor more details refer to the\nKeycloak Documentation\n.\nBuilding from Source\nTo build from source, refer to the\nbuilding and working with the code base\nguide.\nTesting\nTo run tests, refer to the\nrunning tests\nguide.\nWriting Tests\nTo write tests, refer to the\nwriting tests\nguide.\nContributing\nBefore contributing to Keycloak, please read our\ncontributing guidelines\n. Participation in the Keycloak project is governed by the\nCNCF Code of Conduct\n.\nJoining a\ncommunity meeting\nis a great way to get involved and help shape the future of Keycloak.\nOther Keycloak Projects\nKeycloak\n- Keycloak Server and Java adapters\nKeycloak QuickStarts\n- QuickStarts for getting started with Keycloak\nKeycloak Node.js Connect\n- Node.js adapter for Keycloak\nLicense\nApache License, Version 2.0",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 35",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 30,074"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/keycloak/keycloak"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/dataease/SQLBot",
      "title": "dataease/SQLBot",
      "date": null,
      "executive_summary": [
        "ğŸ”¥ åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿã€‚Text-to-SQL Generation via LLMs using RAG.",
        "---",
        "åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿ\nSQLBot æ˜¯ä¸€æ¬¾åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿã€‚SQLBot çš„ä¼˜åŠ¿åŒ…æ‹¬ï¼š\nå¼€ç®±å³ç”¨\n: åªéœ€é…ç½®å¤§æ¨¡å‹å’Œæ•°æ®æºå³å¯å¼€å¯é—®æ•°ä¹‹æ—…ï¼Œé€šè¿‡å¤§æ¨¡å‹å’Œ RAG çš„ç»“åˆæ¥å®ç°é«˜è´¨é‡çš„ text2sqlï¼›\næ˜“äºé›†æˆ\n: æ”¯æŒå¿«é€ŸåµŒå…¥åˆ°ç¬¬ä¸‰æ–¹ä¸šåŠ¡ç³»ç»Ÿï¼Œä¹Ÿæ”¯æŒè¢« n8nã€MaxKBã€Difyã€Coze ç­‰ AI åº”ç”¨å¼€å‘å¹³å°é›†æˆè°ƒç”¨ï¼Œè®©å„ç±»åº”ç”¨å¿«é€Ÿæ‹¥æœ‰æ™ºèƒ½é—®æ•°èƒ½åŠ›ï¼›\nå®‰å…¨å¯æ§\n: æä¾›åŸºäºå·¥ä½œç©ºé—´çš„èµ„æºéš”ç¦»æœºåˆ¶ï¼Œèƒ½å¤Ÿå®ç°ç»†ç²’åº¦çš„æ•°æ®æƒé™æ§åˆ¶ã€‚\nå·¥ä½œåŸç†\nå¿«é€Ÿå¼€å§‹\nå®‰è£…éƒ¨ç½²\nå‡†å¤‡ä¸€å° Linux æœåŠ¡å™¨ï¼Œå®‰è£…å¥½\nDocker\nï¼Œæ‰§è¡Œä»¥ä¸‹ä¸€é”®å®‰è£…è„šæœ¬ï¼š\ndocker run -d \\\n  --name sqlbot \\\n  --restart unless-stopped \\\n  -p 8000:8000 \\\n  -p 8001:8001 \\\n  -v ./data/sqlbot/excel:/opt/sqlbot/data/excel \\\n  -v ./data/sqlbot/file:/opt/sqlbot/data/file \\\n  -v ./data/sqlbot/images:/opt/sqlbot/images \\\n  -v ./data/sqlbot/logs:/opt/sqlbot/app/logs \\\n  -v ./data/postgresql:/var/lib/postgresql/data \\\n  --privileged=true \\\n  dataease/sqlbot\nä½ ä¹Ÿå¯ä»¥é€šè¿‡\n1Panel åº”ç”¨å•†åº—\nå¿«é€Ÿéƒ¨ç½² SQLBotã€‚\nå¦‚æœæ˜¯å†…ç½‘ç¯å¢ƒï¼Œä½ å¯ä»¥é€šè¿‡\nç¦»çº¿å®‰è£…åŒ…æ–¹å¼\néƒ¨ç½² SQLBotã€‚\nè®¿é—®æ–¹å¼\nåœ¨æµè§ˆå™¨ä¸­æ‰“å¼€: http://<ä½ çš„æœåŠ¡å™¨IP>:8000/\nç”¨æˆ·å: admin\nå¯†ç : SQLBot@123456\nè”ç³»æˆ‘ä»¬\nå¦‚ä½ æœ‰æ›´å¤šé—®é¢˜ï¼Œå¯ä»¥åŠ å…¥æˆ‘ä»¬çš„æŠ€æœ¯äº¤æµç¾¤ä¸æˆ‘ä»¬äº¤æµã€‚\nUI å±•ç¤º\nStar History\né£è‡´äº‘æ——ä¸‹çš„å…¶ä»–æ˜æ˜Ÿé¡¹ç›®\nDataEase\n- äººäººå¯ç”¨çš„å¼€æº BI å·¥å…·\n1Panel\n- ç°ä»£åŒ–ã€å¼€æºçš„ Linux æœåŠ¡å™¨è¿ç»´ç®¡ç†é¢æ¿\nMaxKB\n- å¼ºå¤§æ˜“ç”¨çš„ä¼ä¸šçº§æ™ºèƒ½ä½“å¹³å°\nJumpServer\n- å¹¿å—æ¬¢è¿çš„å¼€æºå ¡å’æœº\nCordys CRM\n- æ–°ä¸€ä»£çš„å¼€æº AI CRM ç³»ç»Ÿ\nHalo\n- å¼ºå¤§æ˜“ç”¨çš„å¼€æºå»ºç«™å·¥å…·\nMeterSphere\n- æ–°ä¸€ä»£çš„å¼€æºæŒç»­æµ‹è¯•å·¥å…·\nLicense\næœ¬ä»“åº“éµå¾ª\nFIT2CLOUD Open Source License\nå¼€æºåè®®ï¼Œè¯¥è®¸å¯è¯æœ¬è´¨ä¸Šæ˜¯ GPLv3ï¼Œä½†æœ‰ä¸€äº›é¢å¤–çš„é™åˆ¶ã€‚\nä½ å¯ä»¥åŸºäº SQLBot çš„æºä»£ç è¿›è¡ŒäºŒæ¬¡å¼€å‘ï¼Œä½†æ˜¯éœ€è¦éµå®ˆä»¥ä¸‹è§„å®šï¼š\nä¸èƒ½æ›¿æ¢å’Œä¿®æ”¹ SQLBot çš„ Logo å’Œç‰ˆæƒä¿¡æ¯ï¼›\näºŒæ¬¡å¼€å‘åçš„è¡ç”Ÿä½œå“å¿…é¡»éµå®ˆ GPL V3 çš„å¼€æºä¹‰åŠ¡ã€‚\nå¦‚éœ€å•†ä¸šæˆæƒï¼Œè¯·è”ç³»\nsupport@fit2cloud.com\nã€‚",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 33",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 3,711"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/dataease/SQLBot"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/intruder-io/autoswagger",
      "title": "intruder-io/autoswagger",
      "date": null,
      "executive_summary": [
        "Autoswagger by Intruder - detect API auth weaknesses",
        "---",
        "Autoswagger\nby\nIntruder\nAutoswagger\nis a command-line tool designed to discover, parse, and test for unauthenticated endpoints using\nSwagger/OpenAPI\ndocumentation. It helps identify potential security issues in unprotected endpoints of APIs, such as PII leaks and common secret exposures.\nPlease note that this initial release of Autoswagger is by no means complete, and there are some types of specification which the tool does not currently handle. Please feel free to use it as you wish, and extend its detection capabilities or add detection regexes to cover your specific use-case!\nTable of Contents\nIntroduction\nKey Features\nInstallation & Usage\nDiscovery Phases\nEndpoint Testing\nPII Detection\nOutput Examples\nStats & Reporting\nAcknowledgments\nIntroduction\nAutoswagger automates the process of finding\nOpenAPI/Swagger\nspecifications, extracting API endpoints, and systematically testing them for\nPII\nexposure,\nsecrets\n, and large or interesting responses. It leverages\nPresidio\nfor PII recognition and\nregex\nfor sensitive key/token detection.\nKey Features\nMultiple Discovery Phases\nDiscovers OpenAPI specs in three ways:\nDirect Spec\n: If a full URL with a path ending in\n.json\n,\n.yaml\n, or\n.yml\nis provided, parse that file directly.\nSwagger UI\n: Parse known paths of Swagger UI (e.g.\n/swagger-ui.html\n), and extract spec from HTML or JavaScript.\nDirect Spec by Bruteforce\n: Attempt discovery using common OpenAPI schema locations (\n/swagger.json\n,\n/openapi.json\n, etc.). Only attempt this if 1. and 2. did not yield a result.\nParallel Endpoint Testing\nMulti-threaded concurrent testing of many endpoints, respecting a configurable rate limit (\n-rate\n).\nBrute-Force of Parameter Values\nIf\n-b\nor\n--brute\nis used, try using various data types with a few example values in an attempt to bypass parameter-specific validations.\nPresidio PII Detection\nCheck output for phone numbers, emails, addresses, and names (with context validation to reduce false positives). Also parse CSV rows and naive â€œkey: valueâ€ lines.\nSecrets Detection\nLeverages a set of regex patterns to detect tokens, keys, and debugging artifacts (like environment variables).\nCommand Line or JSON Output\nIn default mode, displays results in a table. With\n-json\n, output a JSON structure.\n-product\nmode filters output to only show those that contain PII, secrets, or large responses.\nInstallation & Usage\nClone\nor\ndownload\nthe repository containing Autoswagger.\ngit clone git@github.com:intruder-io/autoswagger.git\nInstall dependencies\n(e.g., using Python 3.7+):\npip install -r requirements.txt\n(It's recommended to use a virtual environment for this:\npython3 -m venv venv;source venv/bin/activate\n)\nCheck installation, show help:\npython3 autoswagger.py -h\nFlags\nFlag\nDescription\nurls\nList of base URLs or direct spec URLs.\n-v, --verbose\nEnables verbose logging. Creates a log file under\n~/.autoswagger/logs\n.\n-risk\nIncludes non-GET methods (POST, PUT, PATCH, DELETE) in testing.\n-all\nIncludes 200 and 404 endpoints in output (excludes 401/403).\n-product\nOutputs only endpoints with PII or large responses, in JSON format.\n-stats\nDisplays scan statistics (e.g. requests, RPS, hosts with PII).\n-rate <N>\nThrottles requests to N requests per second. Default is 30. Use 0 to disable rate limiting.\n-b, --brute\nEnables brute-forcing of parameter values (multiple test combos).\n-json\nOutputs results in JSON format instead of a Rich table in default mode.\nHelp\n/   | __  __/ /_____  ______      ______ _____ _____ ____  _____\n     / /| |/ / / / __/ __ \\/ ___/ | /| / / __ `/ __ `/ __ `/ _ \\/ ___/\n    / ___ / /_/ / /_/ /_/ (__  )| |/ |/ / /_/ / /_/ / /_/ /  __/ /\n    /_/  |_\\__,_/\\__/\\____/____/ |__/|__/_\\__,_/\\__, /\\__, /\\___/_/\n                                              /____//____/\n                              https://intruder.io\n                          Find unauthenticated endpoints\n\nusage: autoswagger.py [-h] [-v] [-risk] [-all] [-product] [-stats] [-rate RATE] [-b] [-json] [urls ...]\n\nAutoswagger: Detect unauthenticated access control issues via Swagger/OpenAPI documentation.\n\npositional arguments:\n  urls           Base URL(s) or spec URL(s) of the target API(s)\n\noptions:\n  -h, --help     show this help message and exit\n  -v, --verbose  Enable verbose output\n  -risk          Include non-GET requests in testing\n  -all           Include all HTTP status codes in the results, excluding 401 and 403\n  -product       Output all endpoints in JSON, flagging those that contain PII or have large responses.\n  -stats         Display scan statistics. Included in JSON if -product or -json is used.\n  -rate RATE     Set the rate limit in requests per second (default: 30). Use 0 to disable rate limiting.\n  -b, --brute    Enable exhaustive testing of parameter values.\n  -json          Output results in JSON format in default mode.\n\nExample usage:\n  python autoswagger.py https://api.example.com -v\nDiscovery Phases\nDirect Spec\nIf a provided URL ends with\n.json/.yaml/.yml\n, Autoswagger\ndirectly\nattempts to parse the OpenAPI schema.\nSwagger-UI Detection\nTries known UI paths (e.g.,\n/swagger-ui.html\n).\nIf found, parses the HTML or local JavaScript files for a\nswagger.json\nor\nopenapi.json\n.\nCan detect embedded configs like\nwindow.swashbuckleConfig\n.\nDirect Spec by Bruteforce\nIf no spec is found so far, Autoswagger attempts a list of default endpoints like\n/swagger.json\n,\n/openapi.json\n, etc.\nStops when a valid spec is discovered or none are found.\nEndpoint Testing\nCollect Endpoints\nAfter loading a spec, Autoswagger extracts each path and method under the\npaths\nkey.\nHTTP Methods\nBy default, tests\nGET\nonly.\nUse\n-risk\nto include other methods (\nPOST\n,\nPUT\n,\nPATCH\n,\nDELETE\n).\nParameter Values\nFill path/query parameters with defaults or values to enumerate.\nOptionally builds request bodies from the specâ€™s\nrequestBody\n(OpenAPI 3) or body parameters (Swagger 2).\nRate Limiting & Concurrency\nSupports threading with a cap on requests per second (\n-rate\n).\nEach endpoint is tested in a dedicated job.\nResponse Analysis\nDecodes responses, checks for PII, secrets, and large content.\nLogs relevant findings.\nPII Detection\nPresidio-Based Analysis\nSearches for phone numbers, emails, addresses, names.\nContext-based scanning (e.g., CSV headers, key-value lines).\nSecrets & Debug Info\nTruffleHog-like regex checks for API keys, tokens, environment variables.\nMerges any matches into the PII data structure for final reporting.\nLarge Response Check\nFlags responses with 100+ JSON elements or large XML structures as â€œinteresting.â€\nAlso checks raw size threshold (e.g., >100k bytes).\nOutput\nBy default, output is shown in a table.\n-json\nproduces JSON objects, grouping results by endpoint.\n-product\nfilters down to only â€œinterestingâ€ endpoints (PII, large responses and responses with secrets).\nInterpreting Results\nFor most use cases, interpreting results involves looking at the output (endpoints resulting in Status Code 200s), and paying particular attention to endpoints which are marked as 'PII or Secret Detected'. These endpoints are the ones that contain impactful exposures, but they should be manually checked to confirm. You may also wish to look at other 200s that do not contain PII, and determine whether it's intended for these endpoints to be public or not.\nSimple GET endpoints can be triaged using command line tools like curl, but we would recommend using your usual API testing suite (tools such as Postman or Burp Suite) to replay requests and read responses to confirm whether an exposure is present.\nStats & Reporting\n-stats\nappends or prints overall statistics, such as:\nHosts with valid specs\nHosts with PII\nTotal requests sent, average RPS\nPercentage of endpoints responding with 2xx or 4xx\nShown in either a Rich table in default mode or embedded in JSON if\n-json\nor\n-product\nis used.\nAcknowledgments\nAutoswagger is maintained and owned by\nIntruder\n. It was primarily developed by Cale Anderson",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 32",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 1,473"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/intruder-io/autoswagger"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/microsoft/edit",
      "title": "microsoft/edit",
      "date": null,
      "executive_summary": [
        "We all edit.",
        "---",
        "Edit\nA simple editor for simple needs.\nThis editor pays homage to the classic\nMS-DOS Editor\n, but with a modern interface and input controls similar to VS Code. The goal is to provide an accessible editor that even users largely unfamiliar with terminals can easily use.\nInstallation\nYou can also download binaries from\nour Releases page\n.\nWindows\nYou can install the latest version with WinGet:\nwinget install Microsoft.Edit\nBuild Instructions\nInstall Rust\nInstall the nightly toolchain:\nrustup install nightly\nAlternatively, set the environment variable\nRUSTC_BOOTSTRAP=1\nClone the repository\nFor a release build, run:\ncargo build --config .cargo/release.toml --release\nBuild Configuration\nDuring compilation you can set various environment variables to configure the build. The following table lists the available configuration options:\nEnvironment variable\nDescription\nEDIT_CFG_ICU*\nSee\nICU library name (SONAME)\nfor details.\nEDIT_CFG_LANGUAGES\nA comma-separated list of languages to include in the build. See\ni18n/edit.toml\nfor available languages.\nNotes to Package Maintainers\nPackage Naming\nThe canonical executable name is \"edit\" and the alternative name is \"msedit\".\nWe're aware of the potential conflict of \"edit\" with existing commands and recommend alternatively naming packages and executables \"msedit\".\nNames such as \"ms-edit\" should be avoided.\nAssigning an \"edit\" alias is recommended, if possible.\nICU library name (SONAME)\nThis project\noptionally\ndepends on the ICU library for its Search and Replace functionality.\nBy default, the project will look for a SONAME without version suffix:\nWindows:\nicuuc.dll\nmacOS:\nlibicuuc.dylib\nUNIX, and other OS:\nlibicuuc.so\nIf your installation uses a different SONAME, please set the following environment variable at build time:\nEDIT_CFG_ICUUC_SONAME\n:\nFor instance,\nlibicuuc.so.76\n.\nEDIT_CFG_ICUI18N_SONAME\n:\nFor instance,\nlibicui18n.so.76\n.\nAdditionally, this project assumes that the ICU exports are exported without\n_\nprefix and without version suffix, such as\nu_errorName\n.\nIf your installation uses versioned exports, please set:\nEDIT_CFG_ICU_CPP_EXPORTS\n:\nIf set to\ntrue\n, it'll look for C++ symbols such as\n_u_errorName\n.\nEnabled by default on macOS.\nEDIT_CFG_ICU_RENAMING_VERSION\n:\nIf set to a version number, such as\n76\n, it'll look for symbols such as\nu_errorName_76\n.\nFinally, you can set the following environment variables:\nEDIT_CFG_ICU_RENAMING_AUTO_DETECT\n:\nIf set to\ntrue\n, the executable will try to detect the\nEDIT_CFG_ICU_RENAMING_VERSION\nvalue at runtime.\nThe way it does this is not officially supported by ICU and as such is not recommended to be relied upon.\nEnabled by default on UNIX (excluding macOS) if no other options are set.\nTo test your settings, run\ncargo test\nagain but with the\n--ignored\nflag. For instance:\ncargo\ntest\n-- --ignored",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 31",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 12,384"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/microsoft/edit"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/YunaiV/ruoyi-vue-pro",
      "title": "YunaiV/ruoyi-vue-pro",
      "date": null,
      "executive_summary": [
        "ğŸ”¥ å®˜æ–¹æ¨è ğŸ”¥ RuoYi-Vue å…¨æ–° Pro ç‰ˆæœ¬ï¼Œä¼˜åŒ–é‡æ„æ‰€æœ‰åŠŸèƒ½ã€‚åŸºäº Spring Boot + MyBatis Plus + Vue & Element å®ç°çš„åå°ç®¡ç†ç³»ç»Ÿ + å¾®ä¿¡å°ç¨‹åºï¼Œæ”¯æŒ RBAC åŠ¨æ€æƒé™ã€æ•°æ®æƒé™ã€SaaS å¤šç§Ÿæˆ·ã€Flowable å·¥ä½œæµã€ä¸‰æ–¹ç™»å½•ã€æ”¯ä»˜ã€çŸ­ä¿¡ã€å•†åŸã€CRMã€ERPã€AI å¤§æ¨¡å‹ç­‰åŠŸèƒ½ã€‚ä½ çš„ â­ï¸ Star â­ï¸ï¼Œæ˜¯ä½œè€…ç”Ÿå‘çš„åŠ¨åŠ›ï¼",
        "---",
        "ä¸¥è‚ƒå£°æ˜ï¼šç°åœ¨ã€æœªæ¥éƒ½ä¸ä¼šæœ‰å•†ä¸šç‰ˆæœ¬ï¼Œæ‰€æœ‰ä»£ç å…¨éƒ¨å¼€æº!ï¼\nã€Œæˆ‘å–œæ¬¢å†™ä»£ç ï¼Œä¹æ­¤ä¸ç–²ã€\nã€Œæˆ‘å–œæ¬¢åšå¼€æºï¼Œä»¥æ­¤ä¸ºä¹ã€\næˆ‘ ğŸ¶ åœ¨ä¸Šæµ·è‰°è‹¦å¥‹æ–—ï¼Œæ—©ä¸­æ™šåœ¨ top3 å¤§å‚è®¤çœŸæ¬ç –ï¼Œå¤œé‡Œä¸ºå¼€æºåšè´¡çŒ®ã€‚\nå¦‚æœè¿™ä¸ªé¡¹ç›®è®©ä½ æœ‰æ‰€æ”¶è·ï¼Œè®°å¾— Star å…³æ³¨å“¦ï¼Œè¿™å¯¹æˆ‘æ˜¯éå¸¸ä¸é”™çš„é¼“åŠ±ä¸æ”¯æŒã€‚\nğŸ¶ æ–°æ‰‹å¿…è¯»\næ¼”ç¤ºåœ°å€ã€Vue3 + element-plusã€‘ï¼š\nhttp://dashboard-vue3.yudao.iocoder.cn\næ¼”ç¤ºåœ°å€ã€Vue3 + vben(ant-design-vue)ã€‘ï¼š\nhttp://dashboard-vben.yudao.iocoder.cn\næ¼”ç¤ºåœ°å€ã€Vue2 + element-uiã€‘ï¼š\nhttp://dashboard.yudao.iocoder.cn\nå¯åŠ¨æ–‡æ¡£ï¼š\nhttps://doc.iocoder.cn/quick-start/\nè§†é¢‘æ•™ç¨‹ï¼š\nhttps://doc.iocoder.cn/video/\nğŸ° ç‰ˆæœ¬è¯´æ˜\nç‰ˆæœ¬\nJDK 8 + Spring Boot 2.7\nJDK 17/21 + Spring Boot 3.2\nã€å®Œæ•´ç‰ˆã€‘\nruoyi-vue-pro\nmaster\nåˆ†æ”¯\nmaster-jdk17\nåˆ†æ”¯\nã€ç²¾ç®€ç‰ˆã€‘\nyudao-boot-mini\nmaster\nåˆ†æ”¯\nmaster-jdk17\nåˆ†æ”¯\nã€å®Œæ•´ç‰ˆã€‘ï¼šåŒ…æ‹¬ç³»ç»ŸåŠŸèƒ½ã€åŸºç¡€è®¾æ–½ã€ä¼šå‘˜ä¸­å¿ƒã€æ•°æ®æŠ¥è¡¨ã€å·¥ä½œæµç¨‹ã€å•†åŸç³»ç»Ÿã€å¾®ä¿¡å…¬ä¼—å·ã€CRMã€ERP ç­‰åŠŸèƒ½\nã€ç²¾ç®€ç‰ˆã€‘ï¼šåªåŒ…æ‹¬ç³»ç»ŸåŠŸèƒ½ã€åŸºç¡€è®¾æ–½åŠŸèƒ½ï¼Œä¸åŒ…æ‹¬ä¼šå‘˜ä¸­å¿ƒã€æ•°æ®æŠ¥è¡¨ã€å·¥ä½œæµç¨‹ã€å•†åŸç³»ç»Ÿã€å¾®ä¿¡å…¬ä¼—å·ã€CRMã€ERP ç­‰åŠŸèƒ½\nå¯å‚è€ƒ\nã€Šè¿ç§»æ–‡æ¡£ã€‹\nï¼Œåªéœ€è¦ 5-10 åˆ†é’Ÿï¼Œå³å¯å°†ã€å®Œæ•´ç‰ˆã€‘æŒ‰éœ€è¿ç§»åˆ°ã€ç²¾ç®€ç‰ˆã€‘\nğŸ¯ å¹³å°ç®€ä»‹\nèŠ‹é“\nï¼Œä»¥å¼€å‘è€…ä¸ºä¸­å¿ƒï¼Œæ‰“é€ ä¸­å›½ç¬¬ä¸€æµçš„å¿«é€Ÿå¼€å‘å¹³å°ï¼Œå…¨éƒ¨å¼€æºï¼Œä¸ªäººä¸ä¼ä¸šå¯ 100% å…è´¹ä½¿ç”¨ã€‚\næœ‰ä»»ä½•é—®é¢˜ï¼Œæˆ–è€…æƒ³è¦çš„åŠŸèƒ½ï¼Œå¯ä»¥åœ¨\nIssues\nä¸­æç»™è‰¿è‰¿ã€‚\nğŸ˜œ ç»™é¡¹ç›®ç‚¹ç‚¹ Star å§ï¼Œè¿™å¯¹æˆ‘ä»¬çœŸçš„å¾ˆé‡è¦ï¼\nJava åç«¯ï¼š\nmaster\nåˆ†æ”¯ä¸º JDK 8 + Spring Boot 2.7ï¼Œ\nmaster-jdk17\nåˆ†æ”¯ä¸º JDK 17/21 + Spring Boot 3.2\nç®¡ç†åå°çš„ç”µè„‘ç«¯ï¼šVue3 æä¾›\nelement-plus\nã€\nvben(ant-design-vue)\nä¸¤ä¸ªç‰ˆæœ¬ï¼ŒVue2 æä¾›\nelement-ui\nç‰ˆæœ¬\nç®¡ç†åå°çš„ç§»åŠ¨ç«¯ï¼šé‡‡ç”¨\nuni-app\næ–¹æ¡ˆï¼Œä¸€ä»½ä»£ç å¤šç»ˆç«¯é€‚é…ï¼ŒåŒæ—¶æ”¯æŒ APPã€å°ç¨‹åºã€H5ï¼\nåç«¯é‡‡ç”¨ Spring Boot å¤šæ¨¡å—æ¶æ„ã€MySQL + MyBatis Plusã€Redis + Redisson\næ•°æ®åº“å¯ä½¿ç”¨ MySQLã€Oracleã€PostgreSQLã€SQL Serverã€MariaDBã€å›½äº§è¾¾æ¢¦ DMã€TiDB ç­‰\næ¶ˆæ¯é˜Ÿåˆ—å¯ä½¿ç”¨ Eventã€Redisã€RabbitMQã€Kafkaã€RocketMQ ç­‰\næƒé™è®¤è¯ä½¿ç”¨ Spring Security & Token & Redisï¼Œæ”¯æŒå¤šç»ˆç«¯ã€å¤šç§ç”¨æˆ·çš„è®¤è¯ç³»ç»Ÿï¼Œæ”¯æŒ SSO å•ç‚¹ç™»å½•\næ”¯æŒåŠ è½½åŠ¨æ€æƒé™èœå•ï¼ŒæŒ‰é’®çº§åˆ«æƒé™æ§åˆ¶ï¼ŒRedis ç¼“å­˜æå‡æ€§èƒ½\næ”¯æŒ SaaS å¤šç§Ÿæˆ·ï¼Œå¯è‡ªå®šä¹‰æ¯ä¸ªç§Ÿæˆ·çš„æƒé™ï¼Œæä¾›é€æ˜åŒ–çš„å¤šç§Ÿæˆ·åº•å±‚å°è£…\nå·¥ä½œæµä½¿ç”¨ Flowableï¼Œæ”¯æŒåŠ¨æ€è¡¨å•ã€åœ¨çº¿è®¾è®¡æµç¨‹ã€ä¼šç­¾ / æˆ–ç­¾ã€å¤šç§ä»»åŠ¡åˆ†é…æ–¹å¼\né«˜æ•ˆç‡å¼€å‘ï¼Œä½¿ç”¨ä»£ç ç”Ÿæˆå™¨å¯ä»¥ä¸€é”®ç”Ÿæˆ Javaã€Vue å‰åç«¯ä»£ç ã€SQL è„šæœ¬ã€æ¥å£æ–‡æ¡£ï¼Œæ”¯æŒå•è¡¨ã€æ ‘è¡¨ã€ä¸»å­è¡¨\nå®æ—¶é€šä¿¡ï¼Œé‡‡ç”¨ Spring WebSocket å®ç°ï¼Œå†…ç½® Token èº«ä»½æ ¡éªŒï¼Œæ”¯æŒ WebSocket é›†ç¾¤\né›†æˆå¾®ä¿¡å°ç¨‹åºã€å¾®ä¿¡å…¬ä¼—å·ã€ä¼ä¸šå¾®ä¿¡ã€é’‰é’‰ç­‰ä¸‰æ–¹ç™»é™†ï¼Œé›†æˆæ”¯ä»˜å®ã€å¾®ä¿¡ç­‰æ”¯ä»˜ä¸é€€æ¬¾\né›†æˆé˜¿é‡Œäº‘ã€è…¾è®¯äº‘ç­‰çŸ­ä¿¡æ¸ é“ï¼Œé›†æˆ MinIOã€é˜¿é‡Œäº‘ã€è…¾è®¯äº‘ã€ä¸ƒç‰›äº‘ç­‰äº‘å­˜å‚¨æœåŠ¡\né›†æˆæŠ¥è¡¨è®¾è®¡å™¨ã€å¤§å±è®¾è®¡å™¨ï¼Œé€šè¿‡æ‹–æ‹½å³å¯ç”Ÿæˆé…·ç‚«çš„æŠ¥è¡¨ä¸å¤§å±\nğŸ³ é¡¹ç›®å…³ç³»\nä¸‰ä¸ªé¡¹ç›®çš„åŠŸèƒ½å¯¹æ¯”ï¼Œå¯è§ç¤¾åŒºå…±åŒæ•´ç†çš„\nå›½äº§å¼€æºé¡¹ç›®å¯¹æ¯”\nè¡¨æ ¼ã€‚\nåç«¯é¡¹ç›®\né¡¹ç›®\nStar\nç®€ä»‹\nruoyi-vue-pro\nåŸºäº Spring Boot å¤šæ¨¡å—æ¶æ„\nyudao-cloud\nåŸºäº Spring Cloud å¾®æœåŠ¡æ¶æ„\nSpring-Boot-Labs\nç³»ç»Ÿå­¦ä¹  Spring Boot & Cloud ä¸“æ \nå‰ç«¯é¡¹ç›®\né¡¹ç›®\nStar\nç®€ä»‹\nyudao-ui-admin-vue3\nåŸºäº Vue3 + element-plus å®ç°çš„ç®¡ç†åå°\nyudao-ui-admin-vben\nåŸºäº Vue3 + vben(ant-design-vue) å®ç°çš„ç®¡ç†åå°\nyudao-mall-uniapp\nåŸºäº uni-app å®ç°çš„å•†åŸå°ç¨‹åº\nyudao-ui-admin-vue2\nåŸºäº Vue2 + element-ui å®ç°çš„ç®¡ç†åå°\nyudao-ui-admin-uniapp\nåŸºäº Vue2 + element-ui å®ç°çš„ç®¡ç†åå°\nyudao-ui-go-view\nåŸºäº Vue3 + naive-ui å®ç°çš„å¤§å±æŠ¥è¡¨\nğŸ˜ å¼€æºåè®®\nä¸ºä»€ä¹ˆæ¨èä½¿ç”¨æœ¬é¡¹ç›®ï¼Ÿ\nâ‘  æœ¬é¡¹ç›®é‡‡ç”¨æ¯” Apache 2.0 æ›´å®½æ¾çš„\nMIT License\nå¼€æºåè®®ï¼Œä¸ªäººä¸ä¼ä¸šå¯ 100% å…è´¹ä½¿ç”¨ï¼Œä¸ç”¨ä¿ç•™ç±»ä½œè€…ã€Copyright ä¿¡æ¯ã€‚\nâ‘¡ ä»£ç å…¨éƒ¨å¼€æºï¼Œä¸ä¼šåƒå…¶ä»–é¡¹ç›®ä¸€æ ·ï¼Œåªå¼€æºéƒ¨åˆ†ä»£ç ï¼Œè®©ä½ æ— æ³•äº†è§£æ•´ä¸ªé¡¹ç›®çš„æ¶æ„è®¾è®¡ã€‚\nå›½äº§å¼€æºé¡¹ç›®å¯¹æ¯”\nâ‘¢ ä»£ç æ•´æ´ã€æ¶æ„æ•´æ´ï¼Œéµå¾ªã€Šé˜¿é‡Œå·´å·´ Java å¼€å‘æ‰‹å†Œã€‹è§„èŒƒï¼Œä»£ç æ³¨é‡Šè¯¦ç»†ï¼Œ113770 è¡Œ Java ä»£ç ï¼Œ42462 è¡Œä»£ç æ³¨é‡Šã€‚\nğŸ¤ é¡¹ç›®å¤–åŒ…\næˆ‘ä»¬ä¹Ÿæ˜¯æ¥å¤–åŒ…æ»´ï¼Œå¦‚æœä½ æœ‰é¡¹ç›®æƒ³è¦å¤–åŒ…ï¼Œå¯ä»¥å¾®ä¿¡è”ç³»ã€\nAix9975\nã€‘ã€‚\nå›¢é˜ŸåŒ…å«ä¸“ä¸šçš„é¡¹ç›®ç»ç†ã€æ¶æ„å¸ˆã€å‰ç«¯å·¥ç¨‹å¸ˆã€åç«¯å·¥ç¨‹å¸ˆã€æµ‹è¯•å·¥ç¨‹å¸ˆã€è¿ç»´å·¥ç¨‹å¸ˆï¼Œå¯ä»¥æä¾›å…¨æµç¨‹çš„å¤–åŒ…æœåŠ¡ã€‚\né¡¹ç›®å¯ä»¥æ˜¯å•†åŸã€SCRM ç³»ç»Ÿã€OA ç³»ç»Ÿã€ç‰©æµç³»ç»Ÿã€ERP ç³»ç»Ÿã€CMS ç³»ç»Ÿã€HIS ç³»ç»Ÿã€æ”¯ä»˜ç³»ç»Ÿã€IM èŠå¤©ã€å¾®ä¿¡å…¬ä¼—å·ã€å¾®ä¿¡å°ç¨‹åºç­‰ç­‰ã€‚\nğŸ¼ å†…ç½®åŠŸèƒ½\nç³»ç»Ÿå†…ç½®å¤šç§å¤šç§ä¸šåŠ¡åŠŸèƒ½ï¼Œå¯ä»¥ç”¨äºå¿«é€Ÿä½ çš„ä¸šåŠ¡ç³»ç»Ÿï¼š\né€šç”¨æ¨¡å—ï¼ˆå¿…é€‰ï¼‰ï¼šç³»ç»ŸåŠŸèƒ½ã€åŸºç¡€è®¾æ–½\né€šç”¨æ¨¡å—ï¼ˆå¯é€‰ï¼‰ï¼šå·¥ä½œæµç¨‹ã€æ”¯ä»˜ç³»ç»Ÿã€æ•°æ®æŠ¥è¡¨ã€ä¼šå‘˜ä¸­å¿ƒ\nä¸šåŠ¡ç³»ç»Ÿï¼ˆæŒ‰éœ€ï¼‰ï¼šERP ç³»ç»Ÿã€CRM ç³»ç»Ÿã€å•†åŸç³»ç»Ÿã€å¾®ä¿¡å…¬ä¼—å·ã€AI å¤§æ¨¡å‹\nå‹æƒ…æç¤ºï¼šæœ¬é¡¹ç›®åŸºäº RuoYi-Vue ä¿®æ”¹ï¼Œ\né‡æ„ä¼˜åŒ–\nåç«¯çš„ä»£ç ï¼Œ\nç¾åŒ–\nå‰ç«¯çš„ç•Œé¢ã€‚\né¢å¤–æ–°å¢çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬ä½¿ç”¨ ğŸš€ æ ‡è®°ã€‚\né‡æ–°å®ç°çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬ä½¿ç”¨ â­ï¸ æ ‡è®°ã€‚\nğŸ™‚ æ‰€æœ‰åŠŸèƒ½ï¼Œéƒ½é€šè¿‡\nå•å…ƒæµ‹è¯•\nä¿è¯é«˜è´¨é‡ã€‚\nç³»ç»ŸåŠŸèƒ½\nåŠŸèƒ½\næè¿°\nç”¨æˆ·ç®¡ç†\nç”¨æˆ·æ˜¯ç³»ç»Ÿæ“ä½œè€…ï¼Œè¯¥åŠŸèƒ½ä¸»è¦å®Œæˆç³»ç»Ÿç”¨æˆ·é…ç½®\nâ­ï¸\nåœ¨çº¿ç”¨æˆ·\nå½“å‰ç³»ç»Ÿä¸­æ´»è·ƒç”¨æˆ·çŠ¶æ€ç›‘æ§ï¼Œæ”¯æŒæ‰‹åŠ¨è¸¢ä¸‹çº¿\nè§’è‰²ç®¡ç†\nè§’è‰²èœå•æƒé™åˆ†é…ã€è®¾ç½®è§’è‰²æŒ‰æœºæ„è¿›è¡Œæ•°æ®èŒƒå›´æƒé™åˆ’åˆ†\nèœå•ç®¡ç†\né…ç½®ç³»ç»Ÿèœå•ã€æ“ä½œæƒé™ã€æŒ‰é’®æƒé™æ ‡è¯†ç­‰ï¼Œæœ¬åœ°ç¼“å­˜æä¾›æ€§èƒ½\néƒ¨é—¨ç®¡ç†\né…ç½®ç³»ç»Ÿç»„ç»‡æœºæ„ï¼ˆå…¬å¸ã€éƒ¨é—¨ã€å°ç»„ï¼‰ï¼Œæ ‘ç»“æ„å±•ç°æ”¯æŒæ•°æ®æƒé™\nå²—ä½ç®¡ç†\né…ç½®ç³»ç»Ÿç”¨æˆ·æ‰€å±æ‹…ä»»èŒåŠ¡\nğŸš€\nç§Ÿæˆ·ç®¡ç†\né…ç½®ç³»ç»Ÿç§Ÿæˆ·ï¼Œæ”¯æŒ SaaS åœºæ™¯ä¸‹çš„å¤šç§Ÿæˆ·åŠŸèƒ½\nğŸš€\nç§Ÿæˆ·å¥—é¤\né…ç½®ç§Ÿæˆ·å¥—é¤ï¼Œè‡ªå®šæ¯ä¸ªç§Ÿæˆ·çš„èœå•ã€æ“ä½œã€æŒ‰é’®çš„æƒé™\nå­—å…¸ç®¡ç†\nå¯¹ç³»ç»Ÿä¸­ç»å¸¸ä½¿ç”¨çš„ä¸€äº›è¾ƒä¸ºå›ºå®šçš„æ•°æ®è¿›è¡Œç»´æŠ¤\nğŸš€\nçŸ­ä¿¡ç®¡ç†\nçŸ­ä¿¡æ¸ é“ã€çŸ­æ¯æ¨¡æ¿ã€çŸ­ä¿¡æ—¥å¿—ï¼Œå¯¹æ¥é˜¿é‡Œäº‘ã€è…¾è®¯äº‘ç­‰ä¸»æµçŸ­ä¿¡å¹³å°\nğŸš€\né‚®ä»¶ç®¡ç†\né‚®ç®±è´¦å·ã€é‚®ä»¶æ¨¡ç‰ˆã€é‚®ä»¶å‘é€æ—¥å¿—ï¼Œæ”¯æŒæ‰€æœ‰é‚®ä»¶å¹³å°\nğŸš€\nç«™å†…ä¿¡\nç³»ç»Ÿå†…çš„æ¶ˆæ¯é€šçŸ¥ï¼Œæä¾›ç«™å†…ä¿¡æ¨¡ç‰ˆã€ç«™å†…ä¿¡æ¶ˆæ¯\nğŸš€\næ“ä½œæ—¥å¿—\nç³»ç»Ÿæ­£å¸¸æ“ä½œæ—¥å¿—è®°å½•å’ŒæŸ¥è¯¢ï¼Œé›†æˆ Swagger ç”Ÿæˆæ—¥å¿—å†…å®¹\nâ­ï¸\nç™»å½•æ—¥å¿—\nç³»ç»Ÿç™»å½•æ—¥å¿—è®°å½•æŸ¥è¯¢ï¼ŒåŒ…å«ç™»å½•å¼‚å¸¸\nğŸš€\né”™è¯¯ç ç®¡ç†\nç³»ç»Ÿæ‰€æœ‰é”™è¯¯ç çš„ç®¡ç†ï¼Œå¯åœ¨çº¿ä¿®æ”¹é”™è¯¯æç¤ºï¼Œæ— éœ€é‡å¯æœåŠ¡\né€šçŸ¥å…¬å‘Š\nç³»ç»Ÿé€šçŸ¥å…¬å‘Šä¿¡æ¯å‘å¸ƒç»´æŠ¤\nğŸš€\næ•æ„Ÿè¯\né…ç½®ç³»ç»Ÿæ•æ„Ÿè¯ï¼Œæ”¯æŒæ ‡ç­¾åˆ†ç»„\nğŸš€\nåº”ç”¨ç®¡ç†\nç®¡ç† SSO å•ç‚¹ç™»å½•çš„åº”ç”¨ï¼Œæ”¯æŒå¤šç§ OAuth2 æˆæƒæ–¹å¼\nğŸš€\nåœ°åŒºç®¡ç†\nå±•ç¤ºçœä»½ã€åŸå¸‚ã€åŒºé•‡ç­‰åŸå¸‚ä¿¡æ¯ï¼Œæ”¯æŒ IP å¯¹åº”åŸå¸‚\nå·¥ä½œæµç¨‹\nåŸºäº Flowable æ„å»ºï¼Œå¯æ”¯æŒä¿¡åˆ›ï¼ˆå›½äº§ï¼‰æ•°æ®åº“ï¼Œæ»¡è¶³ä¸­å›½ç‰¹è‰²æµç¨‹æ“ä½œï¼š\nBPMN è®¾è®¡å™¨\né’‰é’‰/é£ä¹¦è®¾è®¡å™¨\nå†ç»å¤´éƒ¨ä¼ä¸šç”Ÿäº§éªŒè¯ï¼Œå·¥ä½œæµå¼•æ“é¡»æ ‡é…ä»¿é’‰é’‰/é£ä¹¦ + BPMN åŒè®¾è®¡å™¨ï¼ï¼ï¼\nå‰è€…æ”¯æŒè½»é‡é…ç½®ç®€å•æµç¨‹ï¼Œåè€…å®ç°å¤æ‚åœºæ™¯æ·±åº¦ç¼–æ’\nåŠŸèƒ½åˆ—è¡¨\nåŠŸèƒ½æè¿°\næ˜¯å¦å®Œæˆ\nSIMPLE è®¾è®¡å™¨\nä»¿é’‰é’‰/é£ä¹¦è®¾è®¡å™¨ï¼Œæ”¯æŒæ‹–æ‹½æ­å»ºè¡¨å•æµç¨‹ï¼Œ10 åˆ†é’Ÿå¿«é€Ÿå®Œæˆå®¡æ‰¹æµç¨‹é…ç½®\nâœ…\nBPMN è®¾è®¡å™¨\nåŸºäº BPMN æ ‡å‡†å¼€å‘ï¼Œé€‚é…å¤æ‚ä¸šåŠ¡åœºæ™¯ï¼Œæ»¡è¶³å¤šå±‚çº§å®¡æ‰¹åŠæµç¨‹è‡ªåŠ¨åŒ–éœ€æ±‚\nâœ…\nä¼šç­¾\nåŒä¸€ä¸ªå®¡æ‰¹èŠ‚ç‚¹è®¾ç½®å¤šä¸ªäººï¼ˆå¦‚ Aã€Bã€C ä¸‰äººï¼Œä¸‰äººä¼šåŒæ—¶æ”¶åˆ°å¾…åŠä»»åŠ¡ï¼‰ï¼Œéœ€å…¨éƒ¨åŒæ„ä¹‹åï¼Œå®¡æ‰¹æ‰å¯åˆ°ä¸‹ä¸€å®¡æ‰¹èŠ‚ç‚¹\nâœ…\næˆ–ç­¾\nåŒä¸€ä¸ªå®¡æ‰¹èŠ‚ç‚¹è®¾ç½®å¤šä¸ªäººï¼Œä»»æ„ä¸€ä¸ªäººå¤„ç†åï¼Œå°±èƒ½è¿›å…¥ä¸‹ä¸€ä¸ªèŠ‚ç‚¹\nâœ…\nä¾æ¬¡å®¡æ‰¹\nï¼ˆé¡ºåºä¼šç­¾ï¼‰åŒä¸€ä¸ªå®¡æ‰¹èŠ‚ç‚¹è®¾ç½®å¤šä¸ªäººï¼ˆå¦‚ Aã€Bã€C ä¸‰äººï¼‰ï¼Œä¸‰äººæŒ‰é¡ºåºä¾æ¬¡æ”¶åˆ°å¾…åŠï¼Œå³ A å…ˆå®¡æ‰¹ï¼ŒA æäº¤å B æ‰èƒ½å®¡æ‰¹ï¼Œéœ€å…¨éƒ¨åŒæ„ä¹‹åï¼Œå®¡æ‰¹æ‰å¯åˆ°ä¸‹ä¸€å®¡æ‰¹èŠ‚ç‚¹\nâœ…\næŠ„é€\nå°†å®¡æ‰¹ç»“æœé€šçŸ¥ç»™æŠ„é€äººï¼ŒåŒä¸€ä¸ªå®¡æ‰¹é»˜è®¤æ’é‡ï¼Œä¸é‡å¤æŠ„é€ç»™åŒä¸€äºº\nâœ…\né©³å›\nï¼ˆé€€å›ï¼‰å°†å®¡æ‰¹é‡ç½®å‘é€ç»™æŸèŠ‚ç‚¹ï¼Œé‡æ–°å®¡æ‰¹ã€‚å¯é©³å›è‡³å‘èµ·äººã€ä¸Šä¸€èŠ‚ç‚¹ã€ä»»æ„èŠ‚ç‚¹\nâœ…\nè½¬åŠ\nA è½¬ç»™å…¶ B å®¡æ‰¹ï¼ŒB å®¡æ‰¹åï¼Œè¿›å…¥ä¸‹ä¸€èŠ‚ç‚¹\nâœ…\nå§”æ´¾\nA è½¬ç»™å…¶ B å®¡æ‰¹ï¼ŒB å®¡æ‰¹åï¼Œè½¬ç»™ Aï¼ŒA ç»§ç»­å®¡æ‰¹åè¿›å…¥ä¸‹ä¸€èŠ‚ç‚¹\nâœ…\nåŠ ç­¾\nå…è®¸å½“å‰å®¡æ‰¹äººæ ¹æ®éœ€è¦ï¼Œè‡ªè¡Œå¢åŠ å½“å‰èŠ‚ç‚¹çš„å®¡æ‰¹äººï¼Œæ”¯æŒå‘å‰ã€å‘ååŠ ç­¾\nâœ…\nå‡ç­¾\nï¼ˆå–æ¶ˆåŠ ç­¾ï¼‰åœ¨å½“å‰å®¡æ‰¹äººæ“ä½œä¹‹å‰ï¼Œå‡å°‘å®¡æ‰¹äºº\nâœ…\næ’¤é”€\nï¼ˆå–æ¶ˆæµç¨‹ï¼‰æµç¨‹å‘èµ·äººï¼Œå¯ä»¥å¯¹æµç¨‹è¿›è¡Œæ’¤é”€å¤„ç†\nâœ…\nç»ˆæ­¢\nç³»ç»Ÿç®¡ç†å‘˜ï¼Œåœ¨ä»»æ„èŠ‚ç‚¹ç»ˆæ­¢æµç¨‹å®ä¾‹\nâœ…\nè¡¨å•æƒé™\næ”¯æŒæ‹–æ‹‰æ‹½é…ç½®è¡¨å•ï¼Œæ¯ä¸ªå®¡æ‰¹èŠ‚ç‚¹å¯é…ç½®åªè¯»ã€ç¼–è¾‘ã€éšè—æƒé™\nâœ…\nè¶…æ—¶å®¡æ‰¹\né…ç½®è¶…æ—¶å®¡æ‰¹æ—¶é—´ï¼Œè¶…æ—¶åè‡ªåŠ¨è§¦å‘å®¡æ‰¹é€šè¿‡ã€ä¸é€šè¿‡ã€é©³å›ç­‰æ“ä½œ\nâœ…\nè‡ªåŠ¨æé†’\né…ç½®æé†’æ—¶é—´ï¼Œåˆ°è¾¾æ—¶é—´åè‡ªåŠ¨è§¦å‘çŸ­ä¿¡ã€é‚®ç®±ã€ç«™å†…ä¿¡ç­‰é€šçŸ¥æé†’ï¼Œæ”¯æŒè‡ªå®šä¹‰é‡å¤æé†’é¢‘æ¬¡\nâœ…\nçˆ¶å­æµç¨‹\nä¸»æµç¨‹è®¾ç½®å­æµç¨‹èŠ‚ç‚¹ï¼Œå­æµç¨‹èŠ‚ç‚¹ä¼šè‡ªåŠ¨è§¦å‘å­æµç¨‹ã€‚å­æµç¨‹ç»“æŸåï¼Œä¸»æµç¨‹æ‰ä¼šæ‰§è¡Œï¼ˆç»§ç»­å¾€ä¸‹ä¸‹æ‰§è¡Œï¼‰ï¼Œæ”¯æŒåŒæ­¥å­æµç¨‹ã€å¼‚æ­¥å­æµç¨‹\nâœ…\næ¡ä»¶åˆ†æ”¯\nï¼ˆæ’å®ƒåˆ†æ”¯ï¼‰ç”¨äºåœ¨æµç¨‹ä¸­å®ç°å†³ç­–ï¼Œå³æ ¹æ®æ¡ä»¶é€‰æ‹©ä¸€ä¸ªåˆ†æ”¯æ‰§è¡Œ\nâœ…\nå¹¶è¡Œåˆ†æ”¯\nå…è®¸å°†æµç¨‹åˆ†æˆå¤šæ¡åˆ†æ”¯ï¼Œä¸è¿›è¡Œæ¡ä»¶åˆ¤æ–­ï¼Œæ‰€æœ‰åˆ†æ”¯éƒ½ä¼šæ‰§è¡Œ\nâœ…\nåŒ…å®¹åˆ†æ”¯\nï¼ˆæ¡ä»¶åˆ†æ”¯ + å¹¶è¡Œåˆ†æ”¯çš„ç»“åˆä½“ï¼‰å…è®¸åŸºäºæ¡ä»¶é€‰æ‹©å¤šæ¡åˆ†æ”¯æ‰§è¡Œï¼Œä½†å¦‚æœæ²¡æœ‰ä»»ä½•ä¸€ä¸ªåˆ†æ”¯æ»¡è¶³æ¡ä»¶ï¼Œåˆ™å¯ä»¥é€‰æ‹©é»˜è®¤åˆ†æ”¯\nâœ…\nè·¯ç”±åˆ†æ”¯\næ ¹æ®æ¡ä»¶é€‰æ‹©ä¸€ä¸ªåˆ†æ”¯æ‰§è¡Œï¼ˆé‡å®šå‘åˆ°æŒ‡å®šé…ç½®èŠ‚ç‚¹ï¼‰ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©é»˜è®¤åˆ†æ”¯æ‰§è¡Œï¼ˆç»§ç»­å¾€ä¸‹æ‰§è¡Œï¼‰\nâœ…\nè§¦å‘èŠ‚ç‚¹\næ‰§è¡Œåˆ°è¯¥èŠ‚ç‚¹ï¼Œè§¦å‘ HTTP è¯·æ±‚ã€HTTP å›è°ƒã€æ›´æ–°æ•°æ®ã€åˆ é™¤æ•°æ®ç­‰\nâœ…\nå»¶è¿ŸèŠ‚ç‚¹\næ‰§è¡Œåˆ°è¯¥èŠ‚ç‚¹ï¼Œå®¡æ‰¹ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ‰§è¡Œï¼Œæ”¯æŒå›ºå®šæ—¶é•¿ã€å›ºå®šæ—¥æœŸç­‰\nâœ…\næ‹“å±•è®¾ç½®\næµç¨‹å‰ç½®/åç½®é€šçŸ¥ï¼ŒèŠ‚ç‚¹ï¼ˆä»»åŠ¡ï¼‰å‰ç½®ã€åç½®é€šçŸ¥ï¼Œæµç¨‹æŠ¥è¡¨ï¼Œè‡ªåŠ¨å®¡æ‰¹å»é‡ï¼Œè‡ªå®šæµç¨‹ç¼–å·ã€æ ‡é¢˜ã€æ‘˜è¦ï¼Œæµç¨‹æŠ¥è¡¨ç­‰\nâœ…\næ”¯ä»˜ç³»ç»Ÿ\nåŠŸèƒ½\næè¿°\nğŸš€\nåº”ç”¨ä¿¡æ¯\né…ç½®å•†æˆ·çš„åº”ç”¨ä¿¡æ¯ï¼Œå¯¹æ¥æ”¯ä»˜å®ã€å¾®ä¿¡ç­‰å¤šä¸ªæ”¯ä»˜æ¸ é“\nğŸš€\næ”¯ä»˜è®¢å•\næŸ¥çœ‹ç”¨æˆ·å‘èµ·çš„æ”¯ä»˜å®ã€å¾®ä¿¡ç­‰çš„ã€æ”¯ä»˜ã€‘è®¢å•\nğŸš€\né€€æ¬¾è®¢å•\næŸ¥çœ‹ç”¨æˆ·å‘èµ·çš„æ”¯ä»˜å®ã€å¾®ä¿¡ç­‰çš„ã€é€€æ¬¾ã€‘è®¢å•\nğŸš€\nå›è°ƒé€šçŸ¥\næŸ¥çœ‹æ”¯ä»˜å›è°ƒä¸šåŠ¡çš„ã€æ”¯ä»˜ã€‘ã€é€€æ¬¾ã€‘çš„é€šçŸ¥ç»“æœ\nğŸš€\næ¥å…¥ç¤ºä¾‹\næä¾›æ¥å…¥æ”¯ä»˜ç³»ç»Ÿçš„ã€æ”¯ä»˜ã€‘ã€é€€æ¬¾ã€‘çš„åŠŸèƒ½å®æˆ˜\nåŸºç¡€è®¾æ–½\nåŠŸèƒ½\næè¿°\nğŸš€\nä»£ç ç”Ÿæˆ\nå‰åç«¯ä»£ç çš„ç”Ÿæˆï¼ˆJavaã€Vueã€SQLã€å•å…ƒæµ‹è¯•ï¼‰ï¼Œæ”¯æŒ CRUD ä¸‹è½½\nğŸš€\nç³»ç»Ÿæ¥å£\nåŸºäº Swagger è‡ªåŠ¨ç”Ÿæˆç›¸å…³çš„ RESTful API æ¥å£æ–‡æ¡£\nğŸš€\næ•°æ®åº“æ–‡æ¡£\nåŸºäº Screw è‡ªåŠ¨ç”Ÿæˆæ•°æ®åº“æ–‡æ¡£ï¼Œæ”¯æŒå¯¼å‡º Wordã€HTMLã€MD æ ¼å¼\nè¡¨å•æ„å»º\næ‹–åŠ¨è¡¨å•å…ƒç´ ç”Ÿæˆç›¸åº”çš„ HTML ä»£ç ï¼Œæ”¯æŒå¯¼å‡º JSONã€Vue æ–‡ä»¶\nğŸš€\né…ç½®ç®¡ç†\nå¯¹ç³»ç»ŸåŠ¨æ€é…ç½®å¸¸ç”¨å‚æ•°ï¼Œæ”¯æŒ SpringBoot åŠ è½½\nâ­ï¸\nå®šæ—¶ä»»åŠ¡\nåœ¨çº¿ï¼ˆæ·»åŠ ã€ä¿®æ”¹ã€åˆ é™¤)ä»»åŠ¡è°ƒåº¦åŒ…å«æ‰§è¡Œç»“æœæ—¥å¿—\nğŸš€\næ–‡ä»¶æœåŠ¡\næ”¯æŒå°†æ–‡ä»¶å­˜å‚¨åˆ° S3ï¼ˆMinIOã€é˜¿é‡Œäº‘ã€è…¾è®¯äº‘ã€ä¸ƒç‰›äº‘ï¼‰ã€æœ¬åœ°ã€FTPã€æ•°æ®åº“ç­‰\nğŸš€\nWebSocket\næä¾› WebSocket æ¥å…¥ç¤ºä¾‹ï¼Œæ”¯æŒä¸€å¯¹ä¸€ã€ä¸€å¯¹å¤šå‘é€æ–¹å¼\nğŸš€\nAPI æ—¥å¿—\nåŒ…æ‹¬ RESTful API è®¿é—®æ—¥å¿—ã€å¼‚å¸¸æ—¥å¿—ä¸¤éƒ¨åˆ†ï¼Œæ–¹ä¾¿æ’æŸ¥ API ç›¸å…³çš„é—®é¢˜\nMySQL ç›‘æ§\nç›‘è§†å½“å‰ç³»ç»Ÿæ•°æ®åº“è¿æ¥æ± çŠ¶æ€ï¼Œå¯è¿›è¡Œåˆ†æSQLæ‰¾å‡ºç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆ\nRedis ç›‘æ§\nç›‘æ§ Redis æ•°æ®åº“çš„ä½¿ç”¨æƒ…å†µï¼Œä½¿ç”¨çš„ Redis Key ç®¡ç†\nğŸš€\næ¶ˆæ¯é˜Ÿåˆ—\nåŸºäº Redis å®ç°æ¶ˆæ¯é˜Ÿåˆ—ï¼ŒStream æä¾›é›†ç¾¤æ¶ˆè´¹ï¼ŒPub/Sub æä¾›å¹¿æ’­æ¶ˆè´¹\nğŸš€\nJava ç›‘æ§\nåŸºäº Spring Boot Admin å®ç° Java åº”ç”¨çš„ç›‘æ§\nğŸš€\né“¾è·¯è¿½è¸ª\næ¥å…¥ SkyWalking ç»„ä»¶ï¼Œå®ç°é“¾è·¯è¿½è¸ª\nğŸš€\næ—¥å¿—ä¸­å¿ƒ\næ¥å…¥ SkyWalking ç»„ä»¶ï¼Œå®ç°æ—¥å¿—ä¸­å¿ƒ\nğŸš€\næœåŠ¡ä¿éšœ\nåŸºäº Redis å®ç°åˆ†å¸ƒå¼é”ã€å¹‚ç­‰ã€é™æµåŠŸèƒ½ï¼Œæ»¡è¶³é«˜å¹¶å‘åœºæ™¯\nğŸš€\næ—¥å¿—æœåŠ¡\nè½»é‡çº§æ—¥å¿—ä¸­å¿ƒï¼ŒæŸ¥çœ‹è¿œç¨‹æœåŠ¡å™¨çš„æ—¥å¿—\nğŸš€\nå•å…ƒæµ‹è¯•\nåŸºäº JUnit + Mockito å®ç°å•å…ƒæµ‹è¯•ï¼Œä¿è¯åŠŸèƒ½çš„æ­£ç¡®æ€§ã€ä»£ç çš„è´¨é‡ç­‰\næ•°æ®æŠ¥è¡¨\nåŠŸèƒ½\næè¿°\nğŸš€\næŠ¥è¡¨è®¾è®¡å™¨\næ”¯æŒæ•°æ®æŠ¥è¡¨ã€å›¾å½¢æŠ¥è¡¨ã€æ‰“å°è®¾è®¡ç­‰\nğŸš€\nå¤§å±è®¾è®¡å™¨\næ‹–æ‹½ç”Ÿæˆæ•°æ®å¤§å±ï¼Œå†…ç½®å‡ åç§å›¾è¡¨ç»„ä»¶\nå¾®ä¿¡å…¬ä¼—å·\nåŠŸèƒ½\næè¿°\nğŸš€\nè´¦å·ç®¡ç†\né…ç½®æ¥å…¥çš„å¾®ä¿¡å…¬ä¼—å·ï¼Œå¯æ”¯æŒå¤šä¸ªå…¬ä¼—å·\nğŸš€\næ•°æ®ç»Ÿè®¡\nç»Ÿè®¡å…¬ä¼—å·çš„ç”¨æˆ·å¢å‡ã€ç´¯è®¡ç”¨æˆ·ã€æ¶ˆæ¯æ¦‚å†µã€æ¥å£åˆ†æç­‰æ•°æ®\nğŸš€\nç²‰ä¸ç®¡ç†\næŸ¥çœ‹å·²å…³æ³¨ã€å–å…³çš„ç²‰ä¸åˆ—è¡¨ï¼Œå¯å¯¹ç²‰ä¸è¿›è¡ŒåŒæ­¥ã€æ‰“æ ‡ç­¾ç­‰æ“ä½œ\nğŸš€\næ¶ˆæ¯ç®¡ç†\næŸ¥çœ‹ç²‰ä¸å‘é€çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œå¯ä¸»åŠ¨å›å¤ç²‰ä¸æ¶ˆæ¯\nğŸš€\nè‡ªåŠ¨å›å¤\nè‡ªåŠ¨å›å¤ç²‰ä¸å‘é€çš„æ¶ˆæ¯ï¼Œæ”¯æŒå…³æ³¨å›å¤ã€æ¶ˆæ¯å›å¤ã€å…³é”®å­—å›å¤\nğŸš€\næ ‡ç­¾ç®¡ç†\nå¯¹å…¬ä¼—å·çš„æ ‡ç­¾è¿›è¡Œåˆ›å»ºã€æŸ¥è¯¢ã€ä¿®æ”¹ã€åˆ é™¤ç­‰æ“ä½œ\nğŸš€\nèœå•ç®¡ç†\nè‡ªå®šä¹‰å…¬ä¼—å·çš„èœå•ï¼Œä¹Ÿå¯ä»¥ä»å…¬ä¼—å·åŒæ­¥èœå•\nğŸš€\nç´ æç®¡ç†\nç®¡ç†å…¬ä¼—å·çš„å›¾ç‰‡ã€è¯­éŸ³ã€è§†é¢‘ç­‰ç´ æï¼Œæ”¯æŒåœ¨çº¿æ’­æ”¾è¯­éŸ³ã€è§†é¢‘\nğŸš€\nå›¾æ–‡è‰ç¨¿ç®±\næ–°å¢å¸¸ç”¨çš„å›¾æ–‡ç´ æåˆ°è‰ç¨¿ç®±ï¼Œå¯å‘å¸ƒåˆ°å…¬ä¼—å·\nğŸš€\nå›¾æ–‡å‘è¡¨è®°å½•\næŸ¥çœ‹å·²å‘å¸ƒæˆåŠŸçš„å›¾æ–‡ç´ æï¼Œæ”¯æŒåˆ é™¤æ“ä½œ\nå•†åŸç³»ç»Ÿ\næ¼”ç¤ºåœ°å€ï¼š\nhttps://doc.iocoder.cn/mall-preview/\nä¼šå‘˜ä¸­å¿ƒ\nåŠŸèƒ½\næè¿°\nğŸš€\nä¼šå‘˜ç®¡ç†\nä¼šå‘˜æ˜¯ C ç«¯çš„æ¶ˆè´¹è€…ï¼Œè¯¥åŠŸèƒ½ç”¨äºä¼šå‘˜çš„æœç´¢ä¸ç®¡ç†\nğŸš€\nä¼šå‘˜æ ‡ç­¾\nå¯¹ä¼šå‘˜çš„æ ‡ç­¾è¿›è¡Œåˆ›å»ºã€æŸ¥è¯¢ã€ä¿®æ”¹ã€åˆ é™¤ç­‰æ“ä½œ\nğŸš€\nä¼šå‘˜ç­‰çº§\nå¯¹ä¼šå‘˜çš„ç­‰çº§ã€æˆé•¿å€¼è¿›è¡Œç®¡ç†ï¼Œå¯ç”¨äºè®¢å•æŠ˜æ‰£ç­‰ä¼šå‘˜æƒç›Š\nğŸš€\nä¼šå‘˜åˆ†ç»„\nå¯¹ä¼šå‘˜è¿›è¡Œåˆ†ç»„ï¼Œç”¨äºç”¨æˆ·ç”»åƒã€å†…å®¹æ¨é€ç­‰è¿è¥æ‰‹æ®µ\nğŸš€\nç§¯åˆ†ç­¾åˆ°\nå›é¦ˆç»™ç­¾åˆ°ã€æ¶ˆè´¹ç­‰è¡Œä¸ºçš„ç§¯åˆ†ï¼Œä¼šå‘˜å¯è®¢å•æŠµç°ã€ç§¯åˆ†å…‘æ¢ç­‰é€”å¾„æ¶ˆè€—\nERP ç³»ç»Ÿ\næ¼”ç¤ºåœ°å€ï¼š\nhttps://doc.iocoder.cn/erp-preview/\nCRM ç³»ç»Ÿ\næ¼”ç¤ºåœ°å€ï¼š\nhttps://doc.iocoder.cn/crm-preview/\nAI å¤§æ¨¡å‹\næ¼”ç¤ºåœ°å€ï¼š\nhttps://doc.iocoder.cn/ai-preview/\nğŸ¨ æŠ€æœ¯æ ˆ\næ¨¡å—\né¡¹ç›®\nè¯´æ˜\nyudao-dependencies\nMaven ä¾èµ–ç‰ˆæœ¬ç®¡ç†\nyudao-framework\nJava æ¡†æ¶æ‹“å±•\nyudao-server\nç®¡ç†åå° + ç”¨æˆ· APP çš„æœåŠ¡ç«¯\nyudao-module-system\nç³»ç»ŸåŠŸèƒ½çš„ Module æ¨¡å—\nyudao-module-member\nä¼šå‘˜ä¸­å¿ƒçš„ Module æ¨¡å—\nyudao-module-infra\nåŸºç¡€è®¾æ–½çš„ Module æ¨¡å—\nyudao-module-bpm\nå·¥ä½œæµç¨‹çš„ Module æ¨¡å—\nyudao-module-pay\næ”¯ä»˜ç³»ç»Ÿçš„ Module æ¨¡å—\nyudao-module-mall\nå•†åŸç³»ç»Ÿçš„ Module æ¨¡å—\nyudao-module-erp\nERP ç³»ç»Ÿçš„ Module æ¨¡å—\nyudao-module-crm\nCRM ç³»ç»Ÿçš„ Module æ¨¡å—\nyudao-module-ai\nAI å¤§æ¨¡å‹çš„ Module æ¨¡å—\nyudao-module-mp\nå¾®ä¿¡å…¬ä¼—å·çš„ Module æ¨¡å—\nyudao-module-report\nå¤§å±æŠ¥è¡¨ Module æ¨¡å—\næ¡†æ¶\næ¡†æ¶\nè¯´æ˜\nç‰ˆæœ¬\nå­¦ä¹ æŒ‡å—\nSpring Boot\nåº”ç”¨å¼€å‘æ¡†æ¶\n2.7.18\næ–‡æ¡£\nMySQL\næ•°æ®åº“æœåŠ¡å™¨\n5.7 / 8.0+\nDruid\nJDBC è¿æ¥æ± ã€ç›‘æ§ç»„ä»¶\n1.2.23\næ–‡æ¡£\nMyBatis Plus\nMyBatis å¢å¼ºå·¥å…·åŒ…\n3.5.7\næ–‡æ¡£\nDynamic Datasource\nåŠ¨æ€æ•°æ®æº\n3.6.1\næ–‡æ¡£\nRedis\nkey-value æ•°æ®åº“\n5.0 / 6.0 /7.0\nRedisson\nRedis å®¢æˆ·ç«¯\n3.32.0\næ–‡æ¡£\nSpring MVC\nMVC æ¡†æ¶\n5.3.24\næ–‡æ¡£\nSpring Security\nSpring å®‰å…¨æ¡†æ¶\n5.7.11\næ–‡æ¡£\nHibernate Validator\nå‚æ•°æ ¡éªŒç»„ä»¶\n6.2.5\næ–‡æ¡£\nFlowable\nå·¥ä½œæµå¼•æ“\n6.8.0\næ–‡æ¡£\nQuartz\nä»»åŠ¡è°ƒåº¦ç»„ä»¶\n2.3.2\næ–‡æ¡£\nSpringdoc\nSwagger æ–‡æ¡£\n1.7.0\næ–‡æ¡£\nSkyWalking\nåˆ†å¸ƒå¼åº”ç”¨è¿½è¸ªç³»ç»Ÿ\n8.12.0\næ–‡æ¡£\nSpring Boot Admin\nSpring Boot ç›‘æ§å¹³å°\n2.7.10\næ–‡æ¡£\nJackson\nJSON å·¥å…·åº“\n2.13.5\nMapStruct\nJava Bean è½¬æ¢\n1.6.3\næ–‡æ¡£\nLombok\næ¶ˆé™¤å†—é•¿çš„ Java ä»£ç \n1.18.34\næ–‡æ¡£\nJUnit\nJava å•å…ƒæµ‹è¯•æ¡†æ¶\n5.8.2\n-\nMockito\nJava Mock æ¡†æ¶\n4.8.0\n-\nğŸ· æ¼”ç¤ºå›¾\nç³»ç»ŸåŠŸèƒ½\næ¨¡å—\nbiu\nbiu\nbiu\nç™»å½• & é¦–é¡µ\nç”¨æˆ· & åº”ç”¨\nç§Ÿæˆ· & å¥—é¤\n-\néƒ¨é—¨ & å²—ä½\n-\nèœå• & è§’è‰²\n-\nå®¡è®¡æ—¥å¿—\n-\nçŸ­ä¿¡\nå­—å…¸ & æ•æ„Ÿè¯\né”™è¯¯ç  & é€šçŸ¥\n-\nå·¥ä½œæµç¨‹\næ¨¡å—\nbiu\nbiu\nbiu\næµç¨‹æ¨¡å‹\nè¡¨å• & åˆ†ç»„\n-\næˆ‘çš„æµç¨‹\nå¾…åŠ & å·²åŠ\nOA è¯·å‡\nåŸºç¡€è®¾æ–½\næ¨¡å—\nbiu\nbiu\nbiu\nä»£ç ç”Ÿæˆ\n-\næ–‡æ¡£\n-\næ–‡ä»¶ & é…ç½®\nå®šæ—¶ä»»åŠ¡\n-\nAPI æ—¥å¿—\n-\nMySQL & Redis\n-\nç›‘æ§å¹³å°\næ”¯ä»˜ç³»ç»Ÿ\næ¨¡å—\nbiu\nbiu\nbiu\nå•†å®¶ & åº”ç”¨\næ”¯ä»˜ & é€€æ¬¾\n---\næ•°æ®æŠ¥è¡¨\næ¨¡å—\nbiu\nbiu\nbiu\næŠ¥è¡¨è®¾è®¡å™¨\nå¤§å±è®¾è®¡å™¨\nç§»åŠ¨ç«¯ï¼ˆç®¡ç†åå°ï¼‰\nbiu\nbiu\nbiu\nç›®å‰å·²ç»å®ç°ç™»å½•ã€æˆ‘çš„ã€å·¥ä½œå°ã€ç¼–è¾‘èµ„æ–™ã€å¤´åƒä¿®æ”¹ã€å¯†ç ä¿®æ”¹ã€å¸¸è§é—®é¢˜ã€å…³äºæˆ‘ä»¬ç­‰åŸºç¡€åŠŸèƒ½ã€‚",
        "ä»Šæ—¥ã®ç²å¾—ã‚¹ã‚¿ãƒ¼æ•°: 30",
        "ç´¯ç©ã‚¹ã‚¿ãƒ¼æ•°: 33,522"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/YunaiV/ruoyi-vue-pro"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    }
  ]
}