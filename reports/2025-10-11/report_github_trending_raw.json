{
  "generated_at": "2025-10-11T02:07:31Z",
  "site": "github-trending",
  "num_articles": 47,
  "articles": [
    {
      "url": "https://github.com/TibixDev/winboat",
      "title": "TibixDev/winboat",
      "date": null,
      "executive_summary": [
        "Run Windows apps on 🐧 Linux with ✨ seamless integration",
        "---",
        "WinBoat\nWindows for Penguins.\nRun Windows apps on 🐧 Linux with ✨ seamless integration\nScreenshots\n⚠️\nWork in Progress\n⚠️\nWinBoat is currently in beta, so expect to occasionally run into hiccups and bugs. You should be comfortable with some level of troubleshooting if you decide to try it, however we encourage you to give it a shot anyway.\nFeatures\n🎨 Elegant Interface\n: Sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience\n📦 Automated Installs\n: Simple installation process through our interface - pick your preferences & specs and let us handle the rest\n🚀 Run Any App\n: If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications as native OS-level windows in your Linux environment\n🖥️ Full Windows Desktop\n: Access the complete Windows desktop experience when you need it, or run individual apps seamlessly integrated into your Linux workflow\n📁 Filesystem Integration\n: Your home directory is mounted in Windows, allowing easy file sharing between the two systems without any hassle\n✨ And many more\n: Smartcard passthrough, resource monitoring, and more features being added regularly\nHow Does It Work?\nWinBoat is an Electron app which allows you to run Windows apps on Linux using a containerized approach. Windows runs as a VM inside a Docker container, we communicate with it using the\nWinBoat Guest Server\nto retrieve data we need from Windows. For compositing applications as native OS-level windows, we use FreeRDP together with Windows's RemoteApp protocol.\nPrerequisites\nBefore running WinBoat, ensure your system meets the following requirements:\nRAM\n: At least 4 GB of RAM\nCPU\n: At least 2 CPU threads\nStorage\n: At least 32 GB free space in\n/var\nVirtualization\n: KVM enabled in BIOS/UEFI\nHow to enable virtualization\nDocker\n: Required for containerization\nInstallation Guide\n⚠️\nNOTE:\nDocker Desktop is\nnot\nsupported, you will run into issues if you use it\nDocker Compose v2\n: Required for compatibility with docker-compose.yml files\nInstallation Guide\nDocker User Group\n: Add your user to the\ndocker\ngroup\nSetup Instructions\nFreeRDP\n: Required for remote desktop connection (Please make sure you have\nVersion 3.x.x\nwith sound support included)\nInstallation Guide\n[OPTIONAL]\nKernel Modules\n: The\niptables\n/\nnftables\nand\niptable_nat\nkernel modules can be loaded for network autodiscovery and better shared filesystem performance, but this is not obligatory in newer versions of WinBoat\nModule loading instructions\nDownloading\nYou can download the latest Linux builds under the\nReleases\ntab. We currently offer four variants:\nAppImage:\nA popular & portable app format which should run fine on most distributions\nUnpacked:\nThe raw unpacked files, simply run the executable (\nlinux-unpacked/winboat\n)\n.deb:\nThe intended format for Debian based distributions\n.rpm:\nThe intended format for Fedora based distributions\nKnown Issues About Container Runtimes\nPodman is\nunsupported\nfor now\nDocker Desktop is\nunsupported\nfor now\nDistros that emulate Docker through a Podman socket are\nunsupported\nAny rootless containerization solution is currently\nunsupported\nBuilding WinBoat\nFor building you need to have NodeJS and Go installed on your system\nClone the repo (\ngit clone https://github.com/TibixDev/WinBoat\n)\nInstall the dependencies (\nnpm i\n)\nBuild the app and the guest server using\nnpm run build:linux-gs\nYou can now find the built app under\ndist\nwith an AppImage and an Unpacked variant\nRunning WinBoat in development mode\nMake sure you meet the\nprerequisites\nAdditionally, for development you need to have NodeJS and Go installed on your system\nClone the repo (\ngit clone https://github.com/TibixDev/WinBoat\n)\nInstall the dependencies (\nnpm i\n)\nBuild the guest server (\nnpm run build-guest-server\n)\nRun the app (\nnpm run dev\n)\nContributing\nContributions are welcome! Whether it's bug fixes, feature improvements, or documentation updates, we appreciate your help making WinBoat better.\nPlease note\n: We maintain a focus on technical contributions only. Pull requests containing political/sexual content, or other sensitive/controversial topics will not be accepted. Let's keep things focused on making great software! 🚀\nFeel free to:\nReport bugs and issues\nSubmit feature requests\nContribute code improvements\nHelp with documentation\nShare feedback and suggestions\nCheck out our issues page to get started, or feel free to open a new issue if you've found something that needs attention.\nLicense\nWinBoat is licensed under the\nMIT\nlicense\nInspiration / Alternatives\nThese past few years some cool projects have surfaced with similar concepts, some of which we've also taken inspirations from.\nThey're awesome and you should check them out:\nWinApps\nCassowary\ndockur/windows\n(🌟 Also used in WinBoat)\nSocials & Contact\nStar History",
        "今日の獲得スター数: 1,263",
        "累積スター数: 9,280"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/TibixDev/winboat"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/Stremio/stremio-web",
      "title": "Stremio/stremio-web",
      "date": null,
      "executive_summary": [
        "Stremio - Freedom to Stream",
        "---",
        "Stremio - Freedom to Stream\nStremio is a modern media center that's a one-stop solution for your video entertainment. You discover, watch and organize video content from easy to install addons.\nBuild\nPrerequisites\nNode.js 12 or higher\npnpm\n10 or higher\nInstall dependencies\npnpm install\nStart development server\npnpm start\nProduction build\npnpm run build\nRun with Docker\ndocker build -t stremio-web\n.\ndocker run -p 8080:8080 stremio-web\nScreenshots\nBoard\nDiscover\nMeta Details\nLicense\nStremio is copyright 2017-2023 Smart code and available under GPLv2 license. See the\nLICENSE\nfile in the project for more information.",
        "今日の獲得スター数: 956",
        "累積スター数: 7,250"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/Stremio/stremio-web"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/TapXWorld/ChinaTextbook",
      "title": "TapXWorld/ChinaTextbook",
      "date": null,
      "executive_summary": [
        "所有小初高、大学PDF教材。",
        "---",
        "项目的由来\n虽然国内教育网站已提供免费资源，但大多数普通人获取信息的途径依然受限。有些人利用这一点，在某站上销售这些带有私人水印的资源。为了应对这种情况，我计划将这些资源集中并开源，以促进义务教育的普及和消除地区间的教育贫困。\n还有一个最重要的原因是，希望海外华人能够让自己的孩子继续了解国内教育。\n学习数学\n希望未来出现更多不是为了考学而读书的人。\n小学数学\n一年级上册\n一年级下册\n二年级上册\n二年级下册\n三年级上册\n三年级下册\n四年级上册\n四年级下册\n五年级上册\n五年级下册\n六年级上册\n六年级下册\n初中数学\n初一上册\n初一下册\n初二上册\n初二下册\n初三上册\n初三下册\n高中数学\n目录\n大学数学\n高等数学\n线性代数\n离散数学\n概率论\n更多数学资料-(大学数学网)\n问题：如何合并被拆分的文件？\n由于 GitHub 对单个文件的上传有最大限制，超过 100MB 的文件会被拒绝上传，超过 50MB 的文件上传时会收到警告。因此，文件大小超过 50MB 的文件会被拆分成每个 35MB 的多个文件。\n示例\n文件被拆分的示例：\n义务教育教科书 · 数学一年级上册.pdf.1\n义务教育教科书 · 数学一年级上册.pdf.2\n解决办法\n要合并这些被拆分的文件，您只需执行以下步骤(其他操作系统同理)：\n将合并程序\nmergePDFs-windows-amd64.exe\n下载到包含 PDF 文件的文件夹中。\n确保\nmergePDFs-windows-amd64.exe\n和被拆分的 PDF 文件在同一目录下。\n双击\nmergePDFs-windows-amd64.exe\n程序即可自动完成文件合并。\n下载方式\n您可以通过以下链接，下载文件合并程序：\n下载文件合并程序\n文件和程序示例\nmergePDFs-windows-amd64.exe\n义务教育教科书 · 数学一年级上册.pdf.1\n义务教育教科书 · 数学一年级上册.pdf.2\n重新下载\n如果您位于内地，并且网络不错，想重新下载，您可以使用\ntchMaterial-parser\n项目（鼓励开源），进行重新下载。\n如果您位于国外，和内地网络通信速度较慢，建议使用本存储库进行签出。\n教材捐献\n如果这个项目帮助您免费获取教育资源，请考虑支持我们推广开放教育的努力！您的捐献将帮助我们维护和扩展这个资源库。\n加入我们的 Telegram 社区，获取最新动态并分享您的想法：\nhttps://t.me/+1V6WjEq8WEM4MDM1\n支持我\n如果您觉得这个项目对您有帮助，您可以扫描以下二维码进行捐赠：",
        "今日の獲得スター数: 441",
        "累積スター数: 52,599"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/TapXWorld/ChinaTextbook"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/timelinize/timelinize",
      "title": "timelinize/timelinize",
      "date": null,
      "executive_summary": [
        "Store your data from all your accounts and devices in a single cohesive timeline on your own computer",
        "---",
        "Organize your photos & videos, chats & messages, location history, social media content, contacts, and more into a single cohesive timeline on your own computer where you can keep them alive forever.\nTimelinize lets you import your data from practically anywhere: your computer, phone, online accounts, GPS-enabled radios, various apps and programs, contact lists, cameras, and more.\nJoin our Discord\nto discuss!\nNote\nI am looking for a better name for this project. If you have an idea for a good name that is short, relevant, unique, and available,\nI'd love to hear it!\nScreenshots\nThese were captured using a dev repository of mine filled with a subset of my real data, so I've run Timelinize in obfuscation mode: images and videos are blurred (except profile pictures---need to fix that); names, identifiers, and locations around sensitive areas are all randomized, and text has been replaced with random words so that the string is about the same length.\n(I hope to make a video tour soon.)\nPlease remember this is an early alpha preview, and the software is very much evolving and improving. And you can help!\nWIP dashboard.\nVery\nWIP. The bubble chart is particularly interesting as it shows you what kinds of data are most common at which times of day throughout the years.\nThe classic timeline view is a combination of all data grouped by types and time segments for reconstructing a day or other custom time period.\nViewing an item shows all the information about it, regardless of type: text, photo, live photo, video, location, etc.\nI had to make a custom file picker since browser APIs are too limiting. This is how you'll import most of your data into your timeline, but this flow is being revised soon.\nThe large map view is capable of 3D exploration, showing your memories right where they happened with a color-coded path that represents time.\nBecause Timelinize is entity-aware and supports multiple data sources, it can show data on a map even if it doesn't have geolocation information. That's what the gray dots or pins represent. In this example, a text message was received while at church, even though it doesn't have any geolocation info associated with it directly.\nTimelinize treats entities (people, pets/animals, organizations, etc.) as first-class data points which you can filter and organize.\nTimelinize will automatically recognize the same entity across data sources with enough information, but if it isn't possible automatically, you can manually merge entities with a click.\nConversations are aggregated across data sources that have messaging capabilities. They become emergent from the database by querying relationships between items and entities.\nIn this conversation view, you can see messages exchanged with this person across both Facebook and SMS/text message are displayed together. Reactions are also supported.\nA gallery displays photos and videos, but not just those in your photo library: it includes pictures and memes sent via messages, photos and videos uploaded to social media, and any other photos/videos in your data. You can always filter to drill down.\nHow it works\nObtain your data.\nThis usually involves exporting your data from apps, online accounts, or devices. For example, requesting an archive from Google Takeout. (Apple iCloud, Facebook, Twitter/X, Strava, Instagram, etc. all offer similar functionality for GDPR compliance.) Do this early/soon, because some services take days to provide your data.\nImport your data using Timelinize. You don't need to extract or decompress .tar or .zip archives; Timelinize will attempt to recognize your data in its original format and folder structure. All the data you import is indexed in a SQLite database and stored on disk organized by date -- no obfuscation or proprietary formats; you can simply browse your files if you wish.\nExplore and organize! Timelinize has a UI that portrays data using various projections and filters. It can recall moments from your past and help you view your life more comprehensively. (It's a great living family history tool.)\nRepeat steps 1-3 as often as desired. Timelinize will skip any existing data that is the same and only import new content. You could do this every few weeks or months for busy accounts that are most important to you.\nCaution\nTimelinize is in active development and is still considered unstable. The schema is still changing, necessitating starting over from a clean slate when updating. Always keep your original source data. Expect to delete and recreate your timelines as you upgrade during this alpha development period.\nDownload and run\nDownload the\nlatest release\nfor your platform.\nSee the website for\ninstallation instructions\n.\nDevelop\nSee our\nproject wiki\nfor instructions on\ncompiling from source\n.\nCommand line interface\nTimelinize has a symmetric HTTP API and CLI. When an HTTP API endpoint is created in the code, it automatically adds to the command line as well.\nRun\ntimelinize help\n(or\ngo run main.go help\nif you're running from source) to view the list of commands, which are also HTTP endpoints. JSON or form inputs are converted to command line args/flags that represent the JSON schema or form fields.\nSetup Development Environment\nDev Container setup is provided for easy development using GitHub Codespaces or Visual Studio Code with the DevContainers extension.\nGetting started with VSCode\nMake sure you have the following installed:\nDocker\nDevContainers for VSCode\nOpen this project in VSCode\nGo to the\nRemote Explorer\non Activity Bar\nClick on\nNew Dev Container (+)\nClick on\nOpen Current Folder in Container\nThis sets up a docker container with all the dependencies required for building this project. You can get started with contributing quickly.\nMotivation and vision\n(For roadmap, see\nissues tagged\nlong-term 🔭\n.)\nThe motivation for this project is two-fold. Both press upon me with a sense of urgency, which is why I dedicated some nights and weekends to work on this.\nConnecting with my family -- both living and deceased -- is important to me and my close relatives. But I wish we had more insights into the lives of those who came before us. What was important to them? Where did they live / travel / spend their time? What lessons did they learn? How did global and local events -- or heck, even the weather -- affect them? What hardships did they endure? What would they have wanted to remember? What would it be like to talk to them? A lot of this could not be known unless they wrote it all down. But these days, we have that data for ourselves. What better time than right now to start collecting personal histories from all available sources and develop a rich timeline of our life for our family, or even just for our own reference and nostalgia.\nOur lives are better-documented than any before us, but the record of our life is more ephemeral than any before us, too. We lose control of our data by relying on centralized, proprietary apps and cloud services which are useful today, and gone tomorrow. I wrote Timelinize because now is the time to liberate my data from corporations who don't own it, yet who have the only copy of it. This reality has made me feel uneasy for years, and it's not going away soon. Timelinize makes it bearable.\nImagine being able to pull up a single screen with your data from any and all of your online accounts and services -- while offline. And there you see so many aspects of your life at a glance: your photos and videos, social media posts, locations on a map and how you got there, emails and letters, documents, health and physical activities, mental and emotional wellness, and maybe even music you listened to, for any given day. You can \"zoom out\" and get the big picture. Machine learning algorithms could suggest major clusters based on your content to summarize your days, months, or years, and from that, even recommend printing physical memorabilia. It's like a highly-detailed, automated journal, fully in your control, which you can add to in the app: augment it with your own thoughts like a regular journal.\nThen cross-reference your own timeline with a global public timeline: see how locations you went to changed over time, or what major news events may have affected you, or what the political/social climate -- or the literal climate -- was like at the time. For example, you may wonder, \"Why did the family stay inside so much of the summer one year?\" You could then see, \"Oh, because it was 110 F (43 C) degrees for two months straight.\"\nOr translate the projection sideways, and instead of looking at time cross-sections, look at cross-sections of your timeline by media type: photos, posts, location, sentiment. Look at plots, charts, graphs, of your physical activity.\nOr view projections by space instead of time: view interrelations between items on a map, even items that don't have location data, because the database is entity-aware. So if a person receives a text message and the same person has location information at about the same time from a photo or GPS device, then the text message can appear on a map too, reminding you where you first got the text with the news about your nephew's birth.\nAnd all of this runs on your own computer: no one else has access to it, no one else owns it, but you.\nAnd if everyone had their own timeline, in theory they could be merged into a global supertimeline to become a thorough record of the human race, all without the need for centralizing our data on cloud services that are controlled by greedy corporations.\nHistory\nI've been working on this project since about 2013, even before I conceptualized\nCaddy\n. My initial vision was to create an automated backup of my Picasa albums that I could store on my own hard drive. This project was called Photobak. Picasa eventually became Google Photos, and about the same time I realized I wanted to backup my photos posted to Facebook, Instagram, and Twitter, too. And while I was at it, why not include my Google Location History to augment the location data from the photos. The vision continued to expand as I realized that my family could use this too, so the schema was upgraded to support multiple people/entities as well. This could allow us to merge databases, or timelines, as family members pass, or as they share parts of their timeline around with each other. Timelinize is the mature evolution of the original project that is now designed to be a comprehensive, highly detailed archive of one's life through digital (or\ndigitized\n) content. An authoritative, unified record that is easy to preserve and organize.\nLicense\nThis project is licensed with AGPL. I chose this license because I do not want others to make proprietary or commercial software using this package. The point of this project is liberation of and control over one's own, personal data, and I want to ensure that this project won't be used in anything that would perpetuate the walled garden dilemma we already face today. Even if the future of this project ever has proprietary source code, I can ensure it will stay aligned with my values and the project's original goals.",
        "今日の獲得スター数: 336",
        "累積スター数: 2,387"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/timelinize/timelinize"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/MODSetter/SurfSense",
      "title": "MODSetter/SurfSense",
      "date": null,
      "executive_summary": [
        "Open Source Alternative to NotebookLM / Perplexity, connected to external sources such as Search Engines, Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more. Join our discord: https://discord.gg/ejRNvftDp9",
        "---",
        "SurfSense\nWhile tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar, Luma and more to come.\nVideo\ntemp_demo_v7.mp4\nPodcast Sample\nelon_vs_trump_podcast.mp4\nKey Features\n💡\nIdea\n:\nHave your own highly customizable private NotebookLM and Perplexity integrated with external sources.\n📁\nMultiple File Format Uploading Support\nSave content from your own personal files\n(Documents, images, videos and supports\n50+ file extensions\n)\nto your own personal knowledge base .\n🔍\nPowerful Search\nQuickly research or find anything in your saved content .\n💬\nChat with your Saved Content\nInteract in Natural Language and get cited answers.\n📄\nCited Answers\nGet Cited answers just like Perplexity.\n🔔\nPrivacy & Local LLM Support\nWorks Flawlessly with Ollama local LLMs.\n🏠\nSelf Hostable\nOpen source and easy to deploy locally.\n🎙️ Podcasts\nBlazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)\nConvert your chat conversations into engaging audio content\nSupport for local TTS providers (Kokoro TTS)\nSupport for multiple TTS providers (OpenAI, Azure, Google Vertex AI)\n📊\nAdvanced RAG Techniques\nSupports 100+ LLM's\nSupports 6000+ Embedding Models.\nSupports all major Rerankers (Pinecode, Cohere, Flashrank etc)\nUses Hierarchical Indices (2 tiered RAG setup).\nUtilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).\nRAG as a Service API Backend.\nℹ️\nExternal Sources\nSearch Engines (Tavily, LinkUp)\nSlack\nLinear\nJira\nClickUp\nConfluence\nNotion\nGmail\nYoutube Videos\nGitHub\nDiscord\nAirtable\nGoogle Calendar\nLuma\nand more to come.....\n📄\nSupported File Extensions\nNote\n: File format support depends on your ETL service configuration. LlamaCloud supports 50+ formats, Unstructured supports 34+ core formats, and Docling (core formats, local processing, privacy-focused, no API key).\nDocuments & Text\nLlamaCloud\n:\n.pdf\n,\n.doc\n,\n.docx\n,\n.docm\n,\n.dot\n,\n.dotm\n,\n.rtf\n,\n.txt\n,\n.xml\n,\n.epub\n,\n.odt\n,\n.wpd\n,\n.pages\n,\n.key\n,\n.numbers\n,\n.602\n,\n.abw\n,\n.cgm\n,\n.cwk\n,\n.hwp\n,\n.lwp\n,\n.mw\n,\n.mcw\n,\n.pbd\n,\n.sda\n,\n.sdd\n,\n.sdp\n,\n.sdw\n,\n.sgl\n,\n.sti\n,\n.sxi\n,\n.sxw\n,\n.stw\n,\n.sxg\n,\n.uof\n,\n.uop\n,\n.uot\n,\n.vor\n,\n.wps\n,\n.zabw\nUnstructured\n:\n.doc\n,\n.docx\n,\n.odt\n,\n.rtf\n,\n.pdf\n,\n.xml\n,\n.txt\n,\n.md\n,\n.markdown\n,\n.rst\n,\n.html\n,\n.org\n,\n.epub\nDocling\n:\n.pdf\n,\n.docx\n,\n.html\n,\n.htm\n,\n.xhtml\n,\n.adoc\n,\n.asciidoc\nPresentations\nLlamaCloud\n:\n.ppt\n,\n.pptx\n,\n.pptm\n,\n.pot\n,\n.potm\n,\n.potx\n,\n.odp\n,\n.key\nUnstructured\n:\n.ppt\n,\n.pptx\nDocling\n:\n.pptx\nSpreadsheets & Data\nLlamaCloud\n:\n.xlsx\n,\n.xls\n,\n.xlsm\n,\n.xlsb\n,\n.xlw\n,\n.csv\n,\n.tsv\n,\n.ods\n,\n.fods\n,\n.numbers\n,\n.dbf\n,\n.123\n,\n.dif\n,\n.sylk\n,\n.slk\n,\n.prn\n,\n.et\n,\n.uos1\n,\n.uos2\n,\n.wk1\n,\n.wk2\n,\n.wk3\n,\n.wk4\n,\n.wks\n,\n.wq1\n,\n.wq2\n,\n.wb1\n,\n.wb2\n,\n.wb3\n,\n.qpw\n,\n.xlr\n,\n.eth\nUnstructured\n:\n.xls\n,\n.xlsx\n,\n.csv\n,\n.tsv\nDocling\n:\n.xlsx\n,\n.csv\nImages\nLlamaCloud\n:\n.jpg\n,\n.jpeg\n,\n.png\n,\n.gif\n,\n.bmp\n,\n.svg\n,\n.tiff\n,\n.webp\n,\n.html\n,\n.htm\n,\n.web\nUnstructured\n:\n.jpg\n,\n.jpeg\n,\n.png\n,\n.bmp\n,\n.tiff\n,\n.heic\nDocling\n:\n.jpg\n,\n.jpeg\n,\n.png\n,\n.bmp\n,\n.tiff\n,\n.tif\n,\n.webp\nAudio & Video\n(Always Supported)\n.mp3\n,\n.mpga\n,\n.m4a\n,\n.wav\n,\n.mp4\n,\n.mpeg\n,\n.webm\nEmail & Communication\nUnstructured\n:\n.eml\n,\n.msg\n,\n.p7s\n🔖 Cross Browser Extension\nThe SurfSense extension can be used to save any webpage you like.\nIts main usecase is to save any webpages protected beyond authentication.\nFEATURE REQUESTS AND FUTURE\nSurfSense is actively being developed.\nWhile it's not yet production-ready, you can help us speed up the process.\nJoin the\nSurfSense Discord\nand help shape the future of SurfSense!\n🚀 Roadmap\nStay up to date with our development progress and upcoming features!\nCheck out our public roadmap and contribute your ideas or feedback:\nView the Roadmap:\nSurfSense Roadmap on GitHub Projects\nHow to get started?\nInstallation Options\nSurfSense provides two installation methods:\nDocker Installation\n- The easiest way to get SurfSense up and running with all dependencies containerized.\nIncludes pgAdmin for database management through a web UI\nSupports environment variable customization via\n.env\nfile\nFlexible deployment options (full stack or core services only)\nNo need to manually edit configuration files between environments\nSee\nDocker Setup Guide\nfor detailed instructions\nFor deployment scenarios and options, see\nDeployment Guide\nManual Installation (Recommended)\n- For users who prefer more control over their setup or need to customize their deployment.\nBoth installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.\nBefore installation, make sure to complete the\nprerequisite setup steps\nincluding:\nPGVector setup\nFile Processing ETL Service\n(choose one):\nUnstructured.io API key (supports 34+ formats)\nLlamaIndex API key (enhanced parsing, supports 50+ formats)\nDocling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)\nOther required API keys\nScreenshots\nResearch Agent\nSearch Spaces\nManage Documents\nPodcast Agent\nAgent Chat\nBrowser Extension\nTech Stack\nBackEnd\nFastAPI\n: Modern, fast web framework for building APIs with Python\nPostgreSQL with pgvector\n: Database with vector search capabilities for similarity searches\nSQLAlchemy\n: SQL toolkit and ORM (Object-Relational Mapping) for database interactions\nAlembic\n: A database migrations tool for SQLAlchemy.\nFastAPI Users\n: Authentication and user management with JWT and OAuth support\nLangGraph\n: Framework for developing AI-agents.\nLangChain\n: Framework for developing AI-powered applications.\nLLM Integration\n: Integration with LLM models through LiteLLM\nRerankers\n: Advanced result ranking for improved search relevance\nHybrid Search\n: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)\nVector Embeddings\n: Document and text embeddings for semantic search\npgvector\n: PostgreSQL extension for efficient vector similarity operations\nChonkie\n: Advanced document chunking and embedding library\nUses\nAutoEmbeddings\nfor flexible embedding model selection\nLateChunker\nfor optimized document chunking based on embedding model's max sequence length\nFrontEnd\nNext.js 15.2.3\n: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.\nReact 19.0.0\n: JavaScript library for building user interfaces.\nTypeScript\n: Static type-checking for JavaScript, enhancing code quality and developer experience.\nVercel AI SDK Kit UI Stream Protocol\n: To create scalable chat UI.\nTailwind CSS 4.x\n: Utility-first CSS framework for building custom UI designs.\nShadcn\n: Headless components library.\nLucide React\n: Icon set implemented as React components.\nFramer Motion\n: Animation library for React.\nSonner\n: Toast notification library.\nGeist\n: Font family from Vercel.\nReact Hook Form\n: Form state management and validation.\nZod\n: TypeScript-first schema validation with static type inference.\n@hookform/resolvers\n: Resolvers for using validation libraries with React Hook Form.\n@tanstack/react-table\n: Headless UI for building powerful tables & datagrids.\nDevOps\nDocker\n: Container platform for consistent deployment across environments\nDocker Compose\n: Tool for defining and running multi-container Docker applications\npgAdmin\n: Web-based PostgreSQL administration tool included in Docker setup\nExtension\nManifest v3 on Plasmo\nFuture Work\nAdd More Connectors.\nPatch minor bugs.\nDocument Podcasts\nContribute\nContributions are very welcome! A contribution can be as small as a ⭐ or even finding and creating issues.\nFine-tuning the Backend is always desired.\nFor detailed contribution guidelines, please see our\nCONTRIBUTING.md\nfile.\nStar History",
        "今日の獲得スター数: 334",
        "累積スター数: 9,147"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/MODSetter/SurfSense"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/browserbase/stagehand",
      "title": "browserbase/stagehand",
      "date": null,
      "executive_summary": [
        "The AI Browser Automation Framework",
        "---",
        "The AI Browser Automation Framework\nRead the Docs\nIf you're looking for the Python implementation, you can find it\nhere\nVibe code\nStagehand with\nDirector\nWhy Stagehand?\nMost existing browser automation tools either require you to write low-level code in a framework like Selenium, Playwright, or Puppeteer, or use high-level agents that can be unpredictable in production. By letting developers choose what to write in code vs. natural language, Stagehand is the natural choice for browser automations in production.\nChoose when to write code vs. natural language\n: use AI when you want to navigate unfamiliar pages, and use code (\nPlaywright\n) when you know exactly what you want to do.\nPreview and cache actions\n: Stagehand lets you preview AI actions before running them, and also helps you easily cache repeatable actions to save time and tokens.\nComputer use models with one line of code\n: Stagehand lets you integrate SOTA computer use models from OpenAI and Anthropic into the browser with one line of code.\nExample\nHere's how to build a sample browser automation with Stagehand:\n// Use Playwright functions on the page object\nconst\npage\n=\nstagehand\n.\npage\n;\nawait\npage\n.\ngoto\n(\n\"https://github.com/browserbase\"\n)\n;\n// Use act() to execute individual actions\nawait\npage\n.\nact\n(\n\"click on the stagehand repo\"\n)\n;\n// Use Computer Use agents for larger actions\nconst\nagent\n=\nstagehand\n.\nagent\n(\n{\nprovider\n:\n\"openai\"\n,\nmodel\n:\n\"computer-use-preview\"\n,\n}\n)\n;\nawait\nagent\n.\nexecute\n(\n\"Get to the latest PR\"\n)\n;\n// Use extract() to read data from the page\nconst\n{\nauthor\n,\ntitle\n}\n=\nawait\npage\n.\nextract\n(\n{\ninstruction\n:\n\"extract the author and title of the PR\"\n,\nschema\n:\nz\n.\nobject\n(\n{\nauthor\n:\nz\n.\nstring\n(\n)\n.\ndescribe\n(\n\"The username of the PR author\"\n)\n,\ntitle\n:\nz\n.\nstring\n(\n)\n.\ndescribe\n(\n\"The title of the PR\"\n)\n,\n}\n)\n,\n}\n)\n;\nDocumentation\nVisit\ndocs.stagehand.dev\nto view the full documentation.\nGetting Started\nStart with Stagehand with one line of code, or check out our\nQuickstart Guide\nfor more information:\nnpx create-browser-app\nWatch Anirudh demo create-browser-app to create a Stagehand project!\nBuild and Run from Source\ngit clone https://github.com/browserbase/stagehand.git\ncd\nstagehand\npnpm install\npnpm playwright install\npnpm run build\npnpm run example\n#\nrun the blank script at ./examples/example.ts\npnpm run example 2048\n#\nrun the 2048 example at ./examples/2048.ts\npnpm run evals -man\n#\nsee evaluation suite options\nStagehand is best when you have an API key for an LLM provider and Browserbase credentials. To add these to your project, run:\ncp .env.example .env\nnano .env\n#\nEdit the .env file to add API keys\nContributing\nNote\nWe highly value contributions to Stagehand! For questions or support, please join our\nSlack community\n.\nAt a high level, we're focused on improving reliability, speed, and cost in that order of priority. If you're interested in contributing, we strongly recommend reaching out to\nMiguel Gonzalez\nor\nPaul Klein\nin our\nSlack community\nbefore starting to ensure that your contribution aligns with our goals.\nFor more information, please see our\nContributing Guide\n.\nAcknowledgements\nThis project heavily relies on\nPlaywright\nas a resilient backbone to automate the web. It also would not be possible without the awesome techniques and discoveries made by\ntarsier\n,\ngemini-zod\n, and\nfuji-web\n.\nWe'd like to thank the following people for their major contributions to Stagehand:\nPaul Klein\nAnirudh Kamath\nSean McGuire\nMiguel Gonzalez\nSameel Arif\nFilip Michalsky\nJeremy Press\nNavid Pour\nLicense\nLicensed under the MIT License.\nCopyright 2025 Browserbase, Inc.",
        "今日の獲得スター数: 248",
        "累積スター数: 18,145"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/browserbase/stagehand"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/rustdesk/rustdesk",
      "title": "rustdesk/rustdesk",
      "date": null,
      "executive_summary": [
        "An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.",
        "---",
        "Build\n•\nDocker\n•\nStructure\n•\nSnapshot\n[\nУкраїнська\n] | [\nčesky\n] | [\n中文\n] | [\nMagyar\n] | [\nEspañol\n] | [\nفارسی\n] | [\nFrançais\n] | [\nDeutsch\n] | [\nPolski\n] | [\nIndonesian\n] | [\nSuomi\n] | [\nമലയാളം\n] | [\n日本語\n] | [\nNederlands\n] | [\nItaliano\n] | [\nРусский\n] | [\nPortuguês (Brasil)\n] | [\nEsperanto\n] | [\n한국어\n] | [\nالعربي\n] | [\nTiếng Việt\n] | [\nDansk\n] | [\nΕλληνικά\n] | [\nTürkçe\n] | [\nNorsk\n]\nWe need your help to translate this README,\nRustDesk UI\nand\nRustDesk Doc\nto your native language\nCaution\nMisuse Disclaimer:\nThe developers of RustDesk do not condone or support any unethical or illegal use of this software. Misuse, such as unauthorized access, control or invasion of privacy, is strictly against our guidelines. The authors are not responsible for any misuse of the application.\nChat with us:\nDiscord\n|\nTwitter\n|\nReddit\n|\nYouTube\nYet another remote desktop solution, written in Rust. Works out of the box with no configuration required. You have full control of your data, with no concerns about security. You can use our rendezvous/relay server,\nset up your own\n, or\nwrite your own rendezvous/relay server\n.\nRustDesk welcomes contribution from everyone. See\nCONTRIBUTING.md\nfor help getting started.\nFAQ\nBINARY DOWNLOAD\nNIGHTLY BUILD\nDependencies\nDesktop versions use Flutter or Sciter (deprecated) for GUI, this tutorial is for Sciter only, since it is easier and more friendly to start. Check out our\nCI\nfor building Flutter version.\nPlease download Sciter dynamic library yourself.\nWindows\n|\nLinux\n|\nmacOS\nRaw Steps to build\nPrepare your Rust development env and C++ build env\nInstall\nvcpkg\n, and set\nVCPKG_ROOT\nenv variable correctly\nWindows: vcpkg install libvpx:x64-windows-static libyuv:x64-windows-static opus:x64-windows-static aom:x64-windows-static\nLinux/macOS: vcpkg install libvpx libyuv opus aom\nrun\ncargo run\nBuild\nHow to Build on Linux\nUbuntu 18 (Debian 10)\nsudo apt install -y zip g++ gcc git curl wget nasm yasm libgtk-3-dev clang libxcb-randr0-dev libxdo-dev \\\n        libxfixes-dev libxcb-shape0-dev libxcb-xfixes0-dev libasound2-dev libpulse-dev cmake make \\\n        libclang-dev ninja-build libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libpam0g-dev\nopenSUSE Tumbleweed\nsudo zypper install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libXfixes-devel cmake alsa-lib-devel gstreamer-devel gstreamer-plugins-base-devel xdotool-devel pam-devel\nFedora 28 (CentOS 8)\nsudo yum -y install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libxdo-devel libXfixes-devel pulseaudio-libs-devel cmake alsa-lib-devel gstreamer1-devel gstreamer1-plugins-base-devel pam-devel\nArch (Manjaro)\nsudo pacman -Syu --needed unzip git cmake gcc curl wget yasm nasm zip make pkg-config clang gtk3 xdotool libxcb libxfixes alsa-lib pipewire\nInstall vcpkg\ngit clone https://github.com/microsoft/vcpkg\ncd\nvcpkg\ngit checkout 2023.04.15\ncd\n..\nvcpkg/bootstrap-vcpkg.sh\nexport\nVCPKG_ROOT=\n$HOME\n/vcpkg\nvcpkg/vcpkg install libvpx libyuv opus aom\nFix libvpx (For Fedora)\ncd\nvcpkg/buildtrees/libvpx/src\ncd\n*\n./configure\nsed -i\n'\ns/CFLAGS+=-I/CFLAGS+=-fPIC -I/g\n'\nMakefile\nsed -i\n'\ns/CXXFLAGS+=-I/CXXFLAGS+=-fPIC -I/g\n'\nMakefile\nmake\ncp libvpx.a\n$HOME\n/vcpkg/installed/x64-linux/lib/\ncd\nBuild\ncurl --proto\n'\n=https\n'\n--tlsv1.2 -sSf https://sh.rustup.rs\n|\nsh\nsource\n$HOME\n/.cargo/env\ngit clone --recurse-submodules https://github.com/rustdesk/rustdesk\ncd\nrustdesk\nmkdir -p target/debug\nwget https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so\nmv libsciter-gtk.so target/debug\nVCPKG_ROOT=\n$HOME\n/vcpkg cargo run\nHow to build with Docker\nBegin by cloning the repository and building the Docker container:\ngit clone https://github.com/rustdesk/rustdesk\ncd\nrustdesk\ngit submodule update --init --recursive\ndocker build -t\n\"\nrustdesk-builder\n\"\n.\nThen, each time you need to build the application, run the following command:\ndocker run --rm -it -v\n$PWD\n:/home/user/rustdesk -v rustdesk-git-cache:/home/user/.cargo/git -v rustdesk-registry-cache:/home/user/.cargo/registry -e PUID=\n\"\n$(\nid -u\n)\n\"\n-e PGID=\n\"\n$(\nid -g\n)\n\"\nrustdesk-builder\nNote that the first build may take longer before dependencies are cached, subsequent builds will be faster. Additionally, if you need to specify different arguments to the build command, you may do so at the end of the command in the\n<OPTIONAL-ARGS>\nposition. For instance, if you wanted to build an optimized release version, you would run the command above followed by\n--release\n. The resulting executable will be available in the target folder on your system, and can be run with:\ntarget/debug/rustdesk\nOr, if you're running a release executable:\ntarget/release/rustdesk\nPlease ensure that you run these commands from the root of the RustDesk repository, or the application may not find the required resources. Also note that other cargo subcommands such as\ninstall\nor\nrun\nare not currently supported via this method as they would install or run the program inside the container instead of the host.\nFile Structure\nlibs/hbb_common\n: video codec, config, tcp/udp wrapper, protobuf, fs functions for file transfer, and some other utility functions\nlibs/scrap\n: screen capture\nlibs/enigo\n: platform specific keyboard/mouse control\nlibs/clipboard\n: file copy and paste implementation for Windows, Linux, macOS.\nsrc/ui\n: obsolete Sciter UI (deprecated)\nsrc/server\n: audio/clipboard/input/video services, and network connections\nsrc/client.rs\n: start a peer connection\nsrc/rendezvous_mediator.rs\n: Communicate with\nrustdesk-server\n, wait for remote direct (TCP hole punching) or relayed connection\nsrc/platform\n: platform specific code\nflutter\n: Flutter code for desktop and mobile\nflutter/web/js\n: JavaScript for Flutter web client\nScreenshots",
        "今日の獲得スター数: 231",
        "累積スター数: 99,705"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/rustdesk/rustdesk"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/shadcn-ui/ui",
      "title": "shadcn-ui/ui",
      "date": null,
      "executive_summary": [
        "A set of beautifully-designed, accessible components and a code distribution platform. Works with your favorite frameworks. Open Source. Open Code.",
        "---",
        "shadcn/ui\nAccessible and customizable components that you can copy and paste into your apps. Free. Open Source.\nUse this to build your own component library\n.\nDocumentation\nVisit\nhttp://ui.shadcn.com/docs\nto view the documentation.\nContributing\nPlease read the\ncontributing guide\n.\nLicense\nLicensed under the\nMIT license\n.",
        "今日の獲得スター数: 231",
        "累積スター数: 97,333"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/shadcn-ui/ui"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/openai/codex",
      "title": "openai/codex",
      "date": null,
      "executive_summary": [
        "Lightweight coding agent that runs in your terminal",
        "---",
        "npm i -g @openai/codex\nor\nbrew install codex\nCodex CLI\nis a coding agent from OpenAI that runs locally on your computer.\nIf you want Codex in your code editor (VS Code, Cursor, Windsurf),\ninstall in your IDE\nIf you are looking for the\ncloud-based agent\nfrom OpenAI,\nCodex Web\n, go to\nchatgpt.com/codex\nQuickstart\nInstalling and running Codex CLI\nInstall globally with your preferred package manager. If you use npm:\nnpm install -g @openai/codex\nAlternatively, if you use Homebrew:\nbrew install codex\nThen simply run\ncodex\nto get started:\ncodex\nYou can also go to the\nlatest GitHub Release\nand download the appropriate binary for your platform.\nEach GitHub Release contains many executables, but in practice, you likely want one of these:\nmacOS\nApple Silicon/arm64:\ncodex-aarch64-apple-darwin.tar.gz\nx86_64 (older Mac hardware):\ncodex-x86_64-apple-darwin.tar.gz\nLinux\nx86_64:\ncodex-x86_64-unknown-linux-musl.tar.gz\narm64:\ncodex-aarch64-unknown-linux-musl.tar.gz\nEach archive contains a single entry with the platform baked into the name (e.g.,\ncodex-x86_64-unknown-linux-musl\n), so you likely want to rename it to\ncodex\nafter extracting it.\nUsing Codex with your ChatGPT plan\nRun\ncodex\nand select\nSign in with ChatGPT\n. We recommend signing into your ChatGPT account to use Codex as part of your Plus, Pro, Team, Edu, or Enterprise plan.\nLearn more about what's included in your ChatGPT plan\n.\nYou can also use Codex with an API key, but this requires\nadditional setup\n. If you previously used an API key for usage-based billing, see the\nmigration steps\n. If you're having trouble with login, please comment on\nthis issue\n.\nModel Context Protocol (MCP)\nCodex can access MCP servers. To configure them, refer to the\nconfig docs\n.\nConfiguration\nCodex CLI supports a rich set of configuration options, with preferences stored in\n~/.codex/config.toml\n. For full configuration options, see\nConfiguration\n.\nDocs & FAQ\nGetting started\nCLI usage\nRunning with a prompt as input\nExample prompts\nMemory with AGENTS.md\nConfiguration\nSandbox & approvals\nAuthentication\nAuth methods\nLogin on a \"Headless\" machine\nAutomating Codex\nGitHub Action\nTypeScript SDK\nNon-interactive mode (\ncodex exec\n)\nAdvanced\nTracing / verbose logging\nModel Context Protocol (MCP)\nZero data retention (ZDR)\nContributing\nInstall & build\nSystem Requirements\nDotSlash\nBuild from source\nFAQ\nOpen source fund\nLicense\nThis repository is licensed under the\nApache-2.0 License\n.",
        "今日の獲得スター数: 188",
        "累積スター数: 46,874"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/openai/codex"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/Zie619/n8n-workflows",
      "title": "Zie619/n8n-workflows",
      "date": null,
      "executive_summary": [
        "all of the workflows of n8n i could find (also from the site itself)",
        "---",
        "⚡ N8N Workflow Collection & Documentation\nA professionally organized collection of\n2,057 n8n workflows\nwith a lightning-fast documentation system that provides instant search, analysis, and browsing capabilities.\n⚠️\nIMPORTANT NOTICE (Aug 14, 2025):\nRepository history has been rewritten due to DMCA compliance. If you have a fork or local clone, please see\nIssue 85\nfor instructions on syncing your copy.\nSupport My Work\nIf you'd like to say thanks, consider buying me a coffee—your support helps me keep improving this project!\n🚀\nNEW: Public Search Interface & High-Performance Documentation\n🌐\nBrowse workflows online\n- No installation required!\nOr run locally for development with 100x performance improvement:\nOption 1: Online Search (Recommended for Users)\n🔗 Visit:\nzie619.github.io/n8n-workflows\n⚡\nInstant access\n- No setup required\n🔍\nSearch 2,057+ workflows\ndirectly in browser\n📱\nMobile-friendly\ninterface\n🏷️\nCategory filtering\nacross 15 categories\n📥\nDirect download\nof workflow JSON files\nOption 2: Local Development System\n#\nInstall dependencies\npip install -r requirements.txt\n#\nStart the fast API server\npython run.py\n#\nOpen in browser\nhttp://localhost:8000\nFeatures:\n⚡\nSub-100ms response times\nwith SQLite FTS5 search\n🔍\nInstant full-text search\nwith advanced filtering\n📱\nResponsive design\n- works perfectly on mobile\n🌙\nDark/light themes\nwith system preference detection\n📊\nLive statistics\n- 365 unique integrations, 29,445 total nodes\n🎯\nSmart categorization\nby trigger type and complexity\n🎯\nUse case categorization\nby service name mapped to categories\n📄\nOn-demand JSON viewing\nand download\n🔗\nMermaid diagram generation\nfor workflow visualization\n🔄\nReal-time workflow naming\nwith intelligent formatting\nPerformance Comparison\nMetric\nOld System\nNew System\nImprovement\nFile Size\n71MB HTML\n<100KB\n700x smaller\nLoad Time\n10+ seconds\n<1 second\n10x faster\nSearch\nClient-side only\nFull-text with FTS5\nInstant\nMemory Usage\n~2GB RAM\n<50MB RAM\n40x less\nMobile Support\nPoor\nExcellent\nFully responsive\n📂 Repository Organization\nWorkflow Collection\n2,057 workflows\nwith meaningful, searchable names\n365 unique integrations\nacross popular platforms\n29,445 total nodes\nwith professional categorization\nQuality assurance\n- All workflows analyzed and categorized\nAdvanced Naming System ✨\nOur intelligent naming system converts technical filenames into readable titles:\nBefore\n:\n2051_Telegram_Webhook_Automation_Webhook.json\nAfter\n:\nTelegram Webhook Automation\n100% meaningful names\nwith smart capitalization\nAutomatic integration detection\nfrom node analysis\nUse Case Category ✨\nThe search interface includes a dropdown filter that lets you browse 2,057+ workflows by category.\nThe system includes an automated categorization feature that organizes workflows by service categories to make them easier to discover and filter.\nHow Categorization Works\nRun the categorization script\npython create_categories.py\nService Name Recognition\nThe script analyzes each workflow JSON filename to identify recognized service names (e.g., \"Twilio\", \"Slack\", \"Gmail\", etc.)\nCategory Mapping\nEach recognized service name is matched to its corresponding category using the definitions in\ncontext/def_categories.json\n. For example:\nTwilio → Communication & Messaging\nGmail → Communication & Messaging\nAirtable → Data Processing & Analysis\nSalesforce → CRM & Sales\nSearch Categories Generation\nThe script produces a\nsearch_categories.json\nfile that contains the categorized workflow data\nFilter Interface\nUsers can then filter workflows by category in the search interface, making it easier to find workflows for specific use cases\nAvailable Categories\nThe categorization system includes the following main categories:\nAI Agent Development\nBusiness Process Automation\nCloud Storage & File Management\nCommunication & Messaging\nCreative Content & Video Automation\nCreative Design Automation\nCRM & Sales\nData Processing & Analysis\nE-commerce & Retail\nFinancial & Accounting\nMarketing & Advertising Automation\nProject Management\nSocial Media Management\nTechnical Infrastructure & DevOps\nWeb Scraping & Data Extraction\nContribute Categories\nYou can help expand the categorization by adding more service-to-category mappings (e.g., Twilio → Communication & Messaging) in context/defs_categories.json.\nMany workflow JSON files are conveniently named with the service name, often separated by underscores (_).\n🛠 Usage Instructions\nOption 1: Modern Fast System (Recommended)\n#\nClone repository\ngit clone\n<\nrepo-url\n>\ncd\nn8n-workflows\n#\nInstall Python dependencies\npip install -r requirements.txt\n#\nStart the documentation server\npython run.py\n#\nBrowse workflows at http://localhost:8000\n#\n- Instant search across 2,057 workflows\n#\n- Professional responsive interface\n#\n- Real-time workflow statistics\nOption 2: Development Mode\n#\nStart with auto-reload for development\npython run.py --dev\n#\nOr specify custom host/port\npython run.py --host 0.0.0.0 --port 3000\n#\nForce database reindexing\npython run.py --reindex\nImport Workflows into n8n\n#\nUse the Python importer (recommended)\npython import_workflows.py\n#\nOr manually import individual workflows:\n#\n1. Open your n8n Editor UI\n#\n2. Click menu (☰) → Import workflow\n#\n3. Choose any .json file from the workflows/ folder\n#\n4. Update credentials/webhook URLs before running\n📊 Workflow Statistics\nCurrent Collection Stats\nTotal Workflows\n: 2,057 automation workflows\nActive Workflows\n: 215 (10.5% active rate)\nTotal Nodes\n: 29,528 (avg 14.4 nodes per workflow)\nUnique Integrations\n: 367 different services and APIs\nDatabase\n: SQLite with FTS5 full-text search\nTrigger Distribution\nComplex\n: 832 workflows (40.4%) - Multi-trigger systems\nWebhook\n: 521 workflows (25.3%) - API-triggered automations\nManual\n: 478 workflows (23.2%) - User-initiated workflows\nScheduled\n: 226 workflows (11.0%) - Time-based executions\nComplexity Analysis\nLow (≤5 nodes)\n: ~35% - Simple automations\nMedium (6-15 nodes)\n: ~45% - Standard workflows\nHigh (16+ nodes)\n: ~20% - Complex enterprise systems\nPopular Integrations\nTop services by usage frequency:\nCommunication\n: Telegram, Discord, Slack, WhatsApp\nCloud Storage\n: Google Drive, Google Sheets, Dropbox\nDatabases\n: PostgreSQL, MySQL, MongoDB, Airtable\nAI/ML\n: OpenAI, Anthropic, Hugging Face\nDevelopment\n: HTTP Request, Webhook, GraphQL\n🔍 Advanced Search Features\nSmart Search Categories\nOur system automatically categorizes workflows into 15 main categories:\nAvailable Categories:\nAI Agent Development\n: OpenAI, Anthropic, Hugging Face, CalcsLive\nBusiness Process Automation\n: Workflow utilities, scheduling, data processing\nCloud Storage & File Management\n: Google Drive, Dropbox, OneDrive, Box\nCommunication & Messaging\n: Telegram, Discord, Slack, WhatsApp, Email\nCreative Content & Video Automation\n: YouTube, Vimeo, content creation\nCreative Design Automation\n: Canva, Figma, image processing\nCRM & Sales\n: Salesforce, HubSpot, Pipedrive, customer management\nData Processing & Analysis\n: Database operations, analytics, data transformation\nE-commerce & Retail\n: Shopify, Stripe, PayPal, online stores\nFinancial & Accounting\n: Financial tools, payment processing, accounting\nMarketing & Advertising Automation\n: Email marketing, campaigns, lead generation\nProject Management\n: Jira, Trello, Asana, task management\nSocial Media Management\n: LinkedIn, Twitter/X, Facebook, Instagram\nTechnical Infrastructure & DevOps\n: GitHub, deployment, monitoring\nWeb Scraping & Data Extraction\n: HTTP requests, webhooks, data collection\nAPI Usage Examples\n#\nSearch workflows by text\ncurl\n\"\nhttp://localhost:8000/api/workflows?q=telegram+automation\n\"\n#\nFilter by trigger type and complexity\ncurl\n\"\nhttp://localhost:8000/api/workflows?trigger=Webhook&complexity=high\n\"\n#\nFind all messaging workflows\ncurl\n\"\nhttp://localhost:8000/api/workflows/category/messaging\n\"\n#\nGet database statistics\ncurl\n\"\nhttp://localhost:8000/api/stats\n\"\n#\nBrowse available categories\ncurl\n\"\nhttp://localhost:8000/api/categories\n\"\n🏗 Technical Architecture\nModern Stack\nSQLite Database\n- FTS5 full-text search with 365 indexed integrations\nFastAPI Backend\n- RESTful API with automatic OpenAPI documentation\nResponsive Frontend\n- Modern HTML5 with embedded CSS/JavaScript\nSmart Analysis\n- Automatic workflow categorization and naming\nKey Features\nChange Detection\n- MD5 hashing for efficient re-indexing\nBackground Processing\n- Non-blocking workflow analysis\nCompressed Responses\n- Gzip middleware for optimal speed\nError Handling\n- Graceful degradation and comprehensive logging\nMobile Optimization\n- Touch-friendly interface design\nDatabase Performance\n--\nOptimized schema for lightning-fast queries\nCREATE\nTABLE\nworkflows\n(\n    id\nINTEGER\nPRIMARY KEY\n,\n    filename\nTEXT\nUNIQUE,\n    name\nTEXT\n,\n    active\nBOOLEAN\n,\n    trigger_type\nTEXT\n,\n    complexity\nTEXT\n,\n    node_count\nINTEGER\n,\n    integrations\nTEXT\n,\n--\nJSON array of 365 unique services\ndescription\nTEXT\n,\n    file_hash\nTEXT\n,\n--\nMD5 for change detection\nanalyzed_at\nTIMESTAMP\n);\n--\nFull-text search with ranking\nCREATE VIRTUAL TABLE workflows_fts USING fts5(\n    filename, name, description, integrations, tags,\n    content\n=\n'\nworkflows\n'\n, content_rowid\n=\n'\nid\n'\n);\n🔧 Setup & Requirements\nSystem Requirements\nPython 3.7+\n- For running the documentation system\nModern Browser\n- Chrome, Firefox, Safari, Edge\n50MB Storage\n- For SQLite database and indexes\nn8n Instance\n- For importing and running workflows\nInstallation\n#\nClone repository\ngit clone\n<\nrepo-url\n>\ncd\nn8n-workflows\n#\nInstall dependencies\npip install -r requirements.txt\n#\nStart documentation server\npython run.py\n#\nAccess at http://localhost:8000\nDevelopment Setup\n#\nCreate virtual environment\npython3 -m venv .venv\nsource\n.venv/bin/activate\n#\nLinux/Mac\n#\nor .venv\\Scripts\\activate  # Windows\n#\nInstall dependencies\npip install -r requirements.txt\n#\nRun with auto-reload for development\npython api_server.py --reload\n#\nForce database reindexing\npython workflow_db.py --index --force\n📋 Naming Convention\nIntelligent Formatting System\nOur system automatically converts technical filenames to user-friendly names:\n#\nAutomatic transformations:\n2051_Telegram_Webhook_Automation_Webhook.json →\n\"\nTelegram Webhook Automation\n\"\n0250_HTTP_Discord_Import_Scheduled.json →\n\"\nHTTP Discord Import Scheduled\n\"\n0966_OpenAI_Data_Processing_Manual.json →\n\"\nOpenAI Data Processing Manual\n\"\nTechnical Format\n[ID]_[Service1]_[Service2]_[Purpose]_[Trigger].json\nSmart Capitalization Rules\nHTTP\n→ HTTP (not Http)\nAPI\n→ API (not Api)\nwebhook\n→ Webhook\nautomation\n→ Automation\nscheduled\n→ Scheduled\n🚀 API Documentation\nCore Endpoints\nGET /\n- Main workflow browser interface\nGET /api/stats\n- Database statistics and metrics\nGET /api/workflows\n- Search with filters and pagination\nGET /api/workflows/{filename}\n- Detailed workflow information\nGET /api/workflows/{filename}/download\n- Download workflow JSON\nGET /api/workflows/{filename}/diagram\n- Generate Mermaid diagram\nAdvanced Search\nGET /api/workflows/category/{category}\n- Search by service category\nGET /api/categories\n- List all available categories\nGET /api/integrations\n- Get integration statistics\nPOST /api/reindex\n- Trigger background reindexing\nResponse Examples\n// GET /api/stats\n{\n\"total\"\n:\n2053\n,\n\"active\"\n:\n215\n,\n\"inactive\"\n:\n1838\n,\n\"triggers\"\n: {\n\"Complex\"\n:\n831\n,\n\"Webhook\"\n:\n519\n,\n\"Manual\"\n:\n477\n,\n\"Scheduled\"\n:\n226\n},\n\"total_nodes\"\n:\n29445\n,\n\"unique_integrations\"\n:\n365\n}\n🤝 Contributing\n🎉 This project solves\nIssue #84\n- providing online access to workflows without requiring local setup!\nAdding New Workflows\nExport workflow\nas JSON from n8n\nName descriptively\nfollowing the established pattern:\n[ID]_[Service]_[Purpose]_[Trigger].json\nAdd to workflows/\ndirectory (create service folder if needed)\nRemove sensitive data\n(credentials, personal URLs)\nAdd tags\nfor better searchability (calculation, automation, etc.)\nGitHub Actions automatically\nupdates the public search interface\nQuality Standards\n✅ Workflow must be functional and tested\n✅ Remove all credentials and sensitive data\n✅ Follow naming convention for consistency\n✅ Verify compatibility with recent n8n versions\n✅ Include meaningful description or comments\n✅ Add relevant tags for search optimization\nCustom Node Workflows\n✅ Include npm package links in descriptions\n✅ Document custom node requirements\n✅ Add installation instructions\n✅ Use descriptive tags (like CalcsLive example)\nReindexing (for local development)\n#\nForce database reindexing after adding workflows\npython run.py --reindex\n#\nOr update search index only\npython scripts/generate_search_index.py\n⚠️\nImportant Notes\nSecurity & Privacy\nReview before use\n- All workflows shared as-is for educational purposes\nUpdate credentials\n- Replace API keys, tokens, and webhooks\nTest safely\n- Verify in development environment first\nCheck permissions\n- Ensure proper access rights for integrations\nCompatibility\nn8n Version\n- Compatible with n8n 1.0+ (most workflows)\nCommunity Nodes\n- Some workflows may require additional node installations\nAPI Changes\n- External services may have updated their APIs since creation\nDependencies\n- Verify required integrations before importing\n📚 Resources & References\nWorkflow Sources\nThis comprehensive collection includes workflows from:\nOfficial n8n.io\n- Documentation and community examples\nGitHub repositories\n- Open source community contributions\nBlog posts & tutorials\n- Real-world automation patterns\nUser submissions\n- Tested and verified workflows\nEnterprise use cases\n- Business process automations\nLearn More\nn8n Documentation\n- Official documentation\nn8n Community\n- Community forum and support\nWorkflow Templates\n- Official template library\nIntegration Docs\n- Service-specific guides\n🏆 Project Achievements\nRepository Transformation\n2,053 workflows\nprofessionally organized and named\n365 unique integrations\nautomatically detected and categorized\n100% meaningful names\n(improved from basic filename patterns)\nZero data loss\nduring intelligent renaming process\nAdvanced search\nwith 15 service categories\nPerformance Revolution\nSub-100ms search\nwith SQLite FTS5 full-text indexing\nInstant filtering\nacross 29,445 workflow nodes\nMobile-optimized\nresponsive design for all devices\nReal-time statistics\nwith live database queries\nProfessional interface\nwith modern UX principles\nSystem Reliability\nRobust error handling\nwith graceful degradation\nChange detection\nfor efficient database updates\nBackground processing\nfor non-blocking operations\nComprehensive logging\nfor debugging and monitoring\nProduction-ready\nwith proper middleware and security\nThis repository represents the most comprehensive and well-organized collection of n8n workflows available, featuring cutting-edge search technology and professional documentation that makes workflow discovery and usage a delightful experience.\n🎯 Perfect for\n: Developers, automation engineers, business analysts, and anyone looking to streamline their workflows with proven n8n automations.\n中文",
        "今日の獲得スター数: 186",
        "累積スター数: 35,868"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/Zie619/n8n-workflows"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/davila7/claude-code-templates",
      "title": "davila7/claude-code-templates",
      "date": null,
      "executive_summary": [
        "CLI tool for configuring and monitoring Claude Code",
        "---",
        "Claude Code Templates (aitmpl.com)\nReady-to-use configurations for Anthropic's Claude Code.\nA comprehensive collection of AI agents, custom commands, settings, hooks, external integrations (MCPs), and project templates to enhance your development workflow.\nBrowse & Install Components and Templates\nBrowse All Templates\n- Interactive web interface to explore and install 100+ agents, commands, settings, hooks, and MCPs.\n🚀 Quick Installation\n#\nInstall a complete development stack\nnpx claude-code-templates@latest --agent frontend-developer --command generate-tests --mcp github-integration\n#\nBrowse and install interactively\nnpx claude-code-templates@latest\n#\nInstall specific components\nnpx claude-code-templates@latest --agent security-auditor\nnpx claude-code-templates@latest --command optimize-bundle\nnpx claude-code-templates@latest --setting mcp-timeouts\nnpx claude-code-templates@latest --hook pre-commit-validation\nnpx claude-code-templates@latest --mcp postgresql-integration\nWhat You Get\nComponent\nDescription\nExamples\n🤖 Agents\nAI specialists for specific domains\nSecurity auditor, React performance optimizer, database architect\n⚡ Commands\nCustom slash commands\n/generate-tests\n,\n/optimize-bundle\n,\n/check-security\n🔌 MCPs\nExternal service integrations\nGitHub, PostgreSQL, Stripe, AWS, OpenAI\n⚙️ Settings\nClaude Code configurations\nTimeouts, memory settings, output styles\n🪝 Hooks\nAutomation triggers\nPre-commit validation, post-completion actions\n📦 Templates\nComplete project configurations with CLAUDE.md, .claude/* files and .mcp.json\nFramework-specific setups, project best practices\n🛠️ Additional Tools\nBeyond the template catalog, Claude Code Templates includes powerful development tools:\n📊 Claude Code Analytics\nMonitor your AI-powered development sessions in real-time with live state detection and performance metrics.\nnpx claude-code-templates@latest --analytics\n💬 Conversation Monitor\nMobile-optimized interface to view Claude responses in real-time with secure remote access.\n#\nLocal access\nnpx claude-code-templates@latest --chats\n#\nSecure remote access via Cloudflare Tunnel\nnpx claude-code-templates@latest --chats --tunnel\n🔍 Health Check\nComprehensive diagnostics to ensure your Claude Code installation is optimized.\nnpx claude-code-templates@latest --health-check\n📖 Documentation\n📚 docs.aitmpl.com\n- Complete guides, examples, and API reference for all components and tools.\nContributing\nWe welcome contributions!\nBrowse existing templates\nto see what's available, then check our\ncontributing guidelines\nto add your own agents, commands, MCPs, settings, or hooks.\nPlease read our\nCode of Conduct\nbefore contributing.\nAttribution\nThis collection includes components from multiple sources:\nAgents Collection:\nwshobson/agents Collection\nby\nwshobson\n- Licensed under MIT License (48 agents)\nCommands Collection:\nawesome-claude-code Commands\nby\nhesreallyhim\n- Licensed under CC0 1.0 Universal (21 commands)\n📄 License\nThis project is licensed under the MIT License - see the\nLICENSE\nfile for details.\n🔗 Links\n🌐 Browse Templates\n:\naitmpl.com\n📚 Documentation\n:\ndocs.aitmpl.com\n💬 Community\n:\nGitHub Discussions\n🐛 Issues\n:\nGitHub Issues\n⭐ Star History\n⭐ Found this useful? Give us a star to support the project!",
        "今日の獲得スター数: 186",
        "累積スター数: 6,984"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/davila7/claude-code-templates"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/anthropics/claude-code",
      "title": "anthropics/claude-code",
      "date": null,
      "executive_summary": [
        "Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.",
        "---",
        "Claude Code\nClaude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows -- all through natural language commands. Use it in your terminal, IDE, or tag @claude on Github.\nLearn more in the\nofficial documentation\n.\nGet started\nInstall Claude Code:\nnpm install -g @anthropic-ai/claude-code\nNavigate to your project directory and run\nclaude\n.\nReporting Bugs\nWe welcome your feedback. Use the\n/bug\ncommand to report issues directly within Claude Code, or file a\nGitHub issue\n.\nConnect on Discord\nJoin the\nClaude Developers Discord\nto connect with other developers using Claude Code. Get help, share feedback, and discuss your projects with the community.\nData collection, usage, and retention\nWhen you use Claude Code, we collect feedback, which includes usage data (such as code acceptance or rejections), associated conversation data, and user feedback submitted via the\n/bug\ncommand.\nHow we use your data\nSee our\ndata usage policies\n.\nPrivacy safeguards\nWe have implemented several safeguards to protect your data, including limited retention periods for sensitive information, restricted access to user session data, and clear policies against using feedback for model training.\nFor full details, please review our\nCommercial Terms of Service\nand\nPrivacy Policy\n.",
        "今日の獲得スター数: 177",
        "累積スター数: 36,195"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/anthropics/claude-code"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/CapSoftware/Cap",
      "title": "CapSoftware/Cap",
      "date": null,
      "executive_summary": [
        "Open source Loom alternative. Beautiful, shareable screen recordings.",
        "---",
        "Cap\nThe open source Loom alternative.\nCap.so »\nDownloads for\nmacOS & Windows\nCap is the open source alternative to Loom. It's a video messaging tool that allows you to record, edit and share videos in seconds.\nSelf Hosting\nCap Web is available to self-host using Docker or Railway, see our\nself-hosting docs\nto learn more.\nYou can also use the button below to deploy Cap Web to Railway:\nCap Desktop can connect to your self-hosted Cap Web instance regardless of if you build it yourself or\ndownload from our website\n.\nMonorepo App Architecture\nWe use a combination of Rust, React (Next.js), TypeScript, Tauri, Drizzle (ORM), MySQL, TailwindCSS throughout this Turborepo powered monorepo.\nA note about database: The codebase is currently designed to work with MySQL only. MariaDB or other compatible databases might partially work but are not officially supported.\nApps:\ndesktop\n: A\nTauri\n(Rust) app, using\nSolidStart\non the frontend.\nweb\n: A\nNext.js\nweb app.\nPackages:\nui\n: A\nReact\nShared component library.\nutils\n: A\nReact\nShared utility library.\ntsconfig\n: Shared\ntsconfig\nconfigurations used throughout the monorepo.\ndatabase\n: A\nReact\nand\nDrizzle ORM\nShared database library.\nconfig\n:\neslint\nconfigurations (includes\neslint-config-next\n,\neslint-config-prettier\nother configs used throughout the monorepo).\nLicense:\nPortions of this software are licensed as follows:\nAll code residing in the\ncap-camera*\nand\nscap-*\nfamilies of crates is licensed under the MIT License (see\nlicenses/LICENSE-MIT\n).\nAll third party components are licensed under the original license provided by the owner of the applicable component\nAll other content not mentioned above is available under the AGPLv3 license as defined in\nLICENSE\nContributing\nSee\nCONTRIBUTING.md\nfor more information. This guide is a work in progress, and is updated regularly as the app matures.",
        "今日の獲得スター数: 136",
        "累積スター数: 12,407"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/CapSoftware/Cap"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/open-webui/open-webui",
      "title": "open-webui/open-webui",
      "date": null,
      "executive_summary": [
        "User-friendly AI Interface (Supports Ollama, OpenAI API, ...)",
        "---",
        "Open WebUI 👋\nOpen WebUI is an\nextensible\n, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.\nIt supports various LLM runners like\nOllama\nand\nOpenAI-compatible APIs\n, with\nbuilt-in inference engine\nfor RAG, making it a\npowerful AI deployment solution\n.\nPassionate about open-source AI?\nJoin our team →\nTip\nLooking for an\nEnterprise Plan\n?\n–\nSpeak with Our Sales Team Today!\nGet\nenhanced capabilities\n, including\ncustom theming and branding\n,\nService Level Agreement (SLA) support\n,\nLong-Term Support (LTS) versions\n, and\nmore!\nFor more information, be sure to check out our\nOpen WebUI Documentation\n.\nKey Features of Open WebUI ⭐\n🚀\nEffortless Setup\n: Install seamlessly using Docker or Kubernetes (kubectl, kustomize or helm) for a hassle-free experience with support for both\n:ollama\nand\n:cuda\ntagged images.\n🤝\nOllama/OpenAI API Integration\n: Effortlessly integrate OpenAI-compatible APIs for versatile conversations alongside Ollama models. Customize the OpenAI API URL to link with\nLMStudio, GroqCloud, Mistral, OpenRouter, and more\n.\n🛡️\nGranular Permissions and User Groups\n: By allowing administrators to create detailed user roles and permissions, we ensure a secure user environment. This granularity not only enhances security but also allows for customized user experiences, fostering a sense of ownership and responsibility amongst users.\n🔄\nSCIM 2.0 Support\n: Enterprise-grade user and group provisioning through SCIM 2.0 protocol, enabling seamless integration with identity providers like Okta, Azure AD, and Google Workspace for automated user lifecycle management.\n📱\nResponsive Design\n: Enjoy a seamless experience across Desktop PC, Laptop, and Mobile devices.\n📱\nProgressive Web App (PWA) for Mobile\n: Enjoy a native app-like experience on your mobile device with our PWA, providing offline access on localhost and a seamless user interface.\n✒️🔢\nFull Markdown and LaTeX Support\n: Elevate your LLM experience with comprehensive Markdown and LaTeX capabilities for enriched interaction.\n🎤📹\nHands-Free Voice/Video Call\n: Experience seamless communication with integrated hands-free voice and video call features, allowing for a more dynamic and interactive chat environment.\n🛠️\nModel Builder\n: Easily create Ollama models via the Web UI. Create and add custom characters/agents, customize chat elements, and import models effortlessly through\nOpen WebUI Community\nintegration.\n🐍\nNative Python Function Calling Tool\n: Enhance your LLMs with built-in code editor support in the tools workspace. Bring Your Own Function (BYOF) by simply adding your pure Python functions, enabling seamless integration with LLMs.\n📚\nLocal RAG Integration\n: Dive into the future of chat interactions with groundbreaking Retrieval Augmented Generation (RAG) support. This feature seamlessly integrates document interactions into your chat experience. You can load documents directly into the chat or add files to your document library, effortlessly accessing them using the\n#\ncommand before a query.\n🔍\nWeb Search for RAG\n: Perform web searches using providers like\nSearXNG\n,\nGoogle PSE\n,\nBrave Search\n,\nserpstack\n,\nserper\n,\nSerply\n,\nDuckDuckGo\n,\nTavilySearch\n,\nSearchApi\nand\nBing\nand inject the results directly into your chat experience.\n🌐\nWeb Browsing Capability\n: Seamlessly integrate websites into your chat experience using the\n#\ncommand followed by a URL. This feature allows you to incorporate web content directly into your conversations, enhancing the richness and depth of your interactions.\n🎨\nImage Generation Integration\n: Seamlessly incorporate image generation capabilities using options such as AUTOMATIC1111 API or ComfyUI (local), and OpenAI's DALL-E (external), enriching your chat experience with dynamic visual content.\n⚙️\nMany Models Conversations\n: Effortlessly engage with various models simultaneously, harnessing their unique strengths for optimal responses. Enhance your experience by leveraging a diverse set of models in parallel.\n🔐\nRole-Based Access Control (RBAC)\n: Ensure secure access with restricted permissions; only authorized individuals can access your Ollama, and exclusive model creation/pulling rights are reserved for administrators.\n🌐🌍\nMultilingual Support\n: Experience Open WebUI in your preferred language with our internationalization (i18n) support. Join us in expanding our supported languages! We're actively seeking contributors!\n🧩\nPipelines, Open WebUI Plugin Support\n: Seamlessly integrate custom logic and Python libraries into Open WebUI using\nPipelines Plugin Framework\n. Launch your Pipelines instance, set the OpenAI URL to the Pipelines URL, and explore endless possibilities.\nExamples\ninclude\nFunction Calling\n, User\nRate Limiting\nto control access,\nUsage Monitoring\nwith tools like Langfuse,\nLive Translation with LibreTranslate\nfor multilingual support,\nToxic Message Filtering\nand much more.\n🌟\nContinuous Updates\n: We are committed to improving Open WebUI with regular updates, fixes, and new features.\nWant to learn more about Open WebUI's features? Check out our\nOpen WebUI documentation\nfor a comprehensive overview!\nSponsors 🙌\nEmerald\nTailscale\n• Connect self-hosted AI to any device with Tailscale\nWarp\n• The intelligent terminal for developers\nWe are incredibly grateful for the generous support of our sponsors. Their contributions help us to maintain and improve our project, ensuring we can continue to deliver quality work to our community. Thank you!\nHow to Install 🚀\nInstallation via Python pip 🐍\nOpen WebUI can be installed using pip, the Python package installer. Before proceeding, ensure you're using\nPython 3.11\nto avoid compatibility issues.\nInstall Open WebUI\n:\nOpen your terminal and run the following command to install Open WebUI:\npip install open-webui\nRunning Open WebUI\n:\nAfter installation, you can start Open WebUI by executing:\nopen-webui serve\nThis will start the Open WebUI server, which you can access at\nhttp://localhost:8080\nQuick Start with Docker 🐳\nNote\nPlease note that for certain Docker environments, additional configurations might be needed. If you encounter any connection issues, our detailed guide on\nOpen WebUI Documentation\nis ready to assist you.\nWarning\nWhen using Docker to install Open WebUI, make sure to include the\n-v open-webui:/app/backend/data\nin your Docker command. This step is crucial as it ensures your database is properly mounted and prevents any loss of data.\nTip\nIf you wish to utilize Open WebUI with Ollama included or CUDA acceleration, we recommend utilizing our official images tagged with either\n:cuda\nor\n:ollama\n. To enable CUDA, you must install the\nNvidia CUDA container toolkit\non your Linux/WSL system.\nInstallation with Default Configuration\nIf Ollama is on your computer\n, use this command:\ndocker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\nIf Ollama is on a Different Server\n, use this command:\nTo connect to Ollama on another server, change the\nOLLAMA_BASE_URL\nto the server's URL:\ndocker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\nTo run Open WebUI with Nvidia GPU support\n, use this command:\ndocker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda\nInstallation for OpenAI API Usage Only\nIf you're only using OpenAI API\n, use this command:\ndocker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\nInstalling Open WebUI with Bundled Ollama Support\nThis installation method uses a single container image that bundles Open WebUI with Ollama, allowing for a streamlined setup via a single command. Choose the appropriate command based on your hardware setup:\nWith GPU Support\n:\nUtilize GPU resources by running the following command:\ndocker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\nFor CPU Only\n:\nIf you're not using a GPU, use this command instead:\ndocker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\nBoth commands facilitate a built-in, hassle-free installation of both Open WebUI and Ollama, ensuring that you can get everything up and running swiftly.\nAfter installation, you can access Open WebUI at\nhttp://localhost:3000\n. Enjoy! 😄\nOther Installation Methods\nWe offer various installation alternatives, including non-Docker native installation methods, Docker Compose, Kustomize, and Helm. Visit our\nOpen WebUI Documentation\nor join our\nDiscord community\nfor comprehensive guidance.\nLook at the\nLocal Development Guide\nfor instructions on setting up a local development environment.\nTroubleshooting\nEncountering connection issues? Our\nOpen WebUI Documentation\nhas got you covered. For further assistance and to join our vibrant community, visit the\nOpen WebUI Discord\n.\nOpen WebUI: Server Connection Error\nIf you're experiencing connection issues, it’s often due to the WebUI docker container not being able to reach the Ollama server at 127.0.0.1:11434 (host.docker.internal:11434) inside the container . Use the\n--network=host\nflag in your docker command to resolve this. Note that the port changes from 3000 to 8080, resulting in the link:\nhttp://localhost:8080\n.\nExample Docker Command\n:\ndocker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main\nKeeping Your Docker Installation Up-to-Date\nIn case you want to update your local Docker installation to the latest version, you can do it with\nWatchtower\n:\ndocker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui\nIn the last part of the command, replace\nopen-webui\nwith your container name if it is different.\nCheck our Updating Guide available in our\nOpen WebUI Documentation\n.\nUsing the Dev Branch 🌙\nWarning\nThe\n:dev\nbranch contains the latest unstable features and changes. Use it at your own risk as it may have bugs or incomplete features.\nIf you want to try out the latest bleeding-edge features and are okay with occasional instability, you can use the\n:dev\ntag like this:\ndocker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev\nOffline Mode\nIf you are running Open WebUI in an offline environment, you can set the\nHF_HUB_OFFLINE\nenvironment variable to\n1\nto prevent attempts to download models from the internet.\nexport\nHF_HUB_OFFLINE=1\nWhat's Next? 🌟\nDiscover upcoming features on our roadmap in the\nOpen WebUI Documentation\n.\nLicense 📜\nThis project contains code under multiple licenses. The current codebase includes components licensed under the Open WebUI License with an additional requirement to preserve the \"Open WebUI\" branding, as well as prior contributions under their respective original licenses. For a detailed record of license changes and the applicable terms for each section of the code, please refer to\nLICENSE_HISTORY\n. For complete and updated licensing details, please see the\nLICENSE\nand\nLICENSE_HISTORY\nfiles.\nSupport 💬\nIf you have any questions, suggestions, or need assistance, please open an issue or join our\nOpen WebUI Discord community\nto connect with us! 🤝\nStar History\nCreated by\nTimothy Jaeryang Baek\n- Let's make Open WebUI even more amazing together! 💪",
        "今日の獲得スター数: 110",
        "累積スター数: 112,077"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/open-webui/open-webui"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/78/xiaozhi-esp32",
      "title": "78/xiaozhi-esp32",
      "date": null,
      "executive_summary": [
        "An MCP-based chatbot | 一个基于MCP的聊天机器人",
        "---",
        "An MCP-based Chatbot\n（中文 |\nEnglish\n|\n日本語\n）\n介绍\n👉\n人类：给 AI 装摄像头 vs AI：当场发现主人三天没洗头【bilibili】\n👉\n手工打造你的 AI 女友，新手入门教程【bilibili】\n小智 AI 聊天机器人作为一个语音交互入口，利用 Qwen / DeepSeek 等大模型的 AI 能力，通过 MCP 协议实现多端控制。\n版本说明\n当前 v2 版本与 v1 版本分区表不兼容，所以无法从 v1 版本通过 OTA 升级到 v2 版本。分区表说明参见\npartitions/v2/README.md\n。\n使用 v1 版本的所有硬件，可以通过手动烧录固件来升级到 v2 版本。\nv1 的稳定版本为 1.9.2，可以通过\ngit checkout v1\n来切换到 v1 版本，该分支会持续维护到 2026 年 2 月。\n已实现功能\nWi-Fi / ML307 Cat.1 4G\n离线语音唤醒\nESP-SR\n支持两种通信协议（\nWebsocket\n或 MQTT+UDP）\n采用 OPUS 音频编解码\n基于流式 ASR + LLM + TTS 架构的语音交互\n声纹识别，识别当前说话人的身份\n3D Speaker\nOLED / LCD 显示屏，支持表情显示\n电量显示与电源管理\n支持多语言（中文、英文、日文）\n支持 ESP32-C3、ESP32-S3、ESP32-P4 芯片平台\n通过设备端 MCP 实现设备控制（音量、灯光、电机、GPIO 等）\n通过云端 MCP 扩展大模型能力（智能家居控制、PC桌面操作、知识搜索、邮件收发等）\n自定义唤醒词、字体、表情与聊天背景，支持网页端在线修改 (\n自定义Assets生成器\n)\n硬件\n面包板手工制作实践\n详见飞书文档教程：\n👉\n《小智 AI 聊天机器人百科全书》\n面包板效果图如下：\n支持 70 多个开源硬件（仅展示部分）\n立创·实战派 ESP32-S3 开发板\n乐鑫 ESP32-S3-BOX3\nM5Stack CoreS3\nM5Stack AtomS3R + Echo Base\n神奇按钮 2.4\n微雪电子 ESP32-S3-Touch-AMOLED-1.8\nLILYGO T-Circle-S3\n虾哥 Mini C3\n璀璨·AI 吊坠\n无名科技 Nologo-星智-1.54TFT\nSenseCAP Watcher\nESP-HI 超低成本机器狗\n软件\n固件烧录\n新手第一次操作建议先不要搭建开发环境，直接使用免开发环境烧录的固件。\n固件默认接入\nxiaozhi.me\n官方服务器，个人用户注册账号可以免费使用 Qwen 实时模型。\n👉\n新手烧录固件教程\n开发环境\nCursor 或 VSCode\n安装 ESP-IDF 插件，选择 SDK 版本 5.4 或以上\nLinux 比 Windows 更好，编译速度快，也免去驱动问题的困扰\n本项目使用 Google C++ 代码风格，提交代码时请确保符合规范\n开发者文档\n自定义开发板指南\n- 学习如何为小智 AI 创建自定义开发板\nMCP 协议物联网控制用法说明\n- 了解如何通过 MCP 协议控制物联网设备\nMCP 协议交互流程\n- 设备端 MCP 协议的实现方式\nMQTT + UDP 混合通信协议文档\n一份详细的 WebSocket 通信协议文档\n大模型配置\n如果你已经拥有一个小智 AI 聊天机器人设备，并且已接入官方服务器，可以登录\nxiaozhi.me\n控制台进行配置。\n👉\n后台操作视频教程（旧版界面）\n相关开源项目\n在个人电脑上部署服务器，可以参考以下第三方开源的项目：\nxinnan-tech/xiaozhi-esp32-server\nPython 服务器\njoey-zhou/xiaozhi-esp32-server-java\nJava 服务器\nAnimeAIChat/xiaozhi-server-go\nGolang 服务器\n使用小智通信协议的第三方客户端项目：\nhuangjunsen0406/py-xiaozhi\nPython 客户端\nTOM88812/xiaozhi-android-client\nAndroid 客户端\n100askTeam/xiaozhi-linux\n百问科技提供的 Linux 客户端\n78/xiaozhi-sf32\n思澈科技的蓝牙芯片固件\nQuecPython/solution-xiaozhiAI\n移远提供的 QuecPython 固件\n关于项目\n这是一个由虾哥开源的 ESP32 项目，以 MIT 许可证发布，允许任何人免费使用，修改或用于商业用途。\n我们希望通过这个项目，能够帮助大家了解 AI 硬件开发，将当下飞速发展的大语言模型应用到实际的硬件设备中。\n如果你有任何想法或建议，请随时提出 Issues 或加入 QQ 群：1011329060\nStar History",
        "今日の獲得スター数: 107",
        "累積スター数: 19,437"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/78/xiaozhi-esp32"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/evershopcommerce/evershop",
      "title": "evershopcommerce/evershop",
      "date": null,
      "executive_summary": [
        "🛍️ Typescript E-commerce Platform",
        "---",
        "EverShop\nDocumentation\n|\nDemo\nIntroduction\nEverShop is a modern, TypeScript-first eCommerce platform built with GraphQL and React. Designed for developers, it offers essential commerce features in a modular, fully customizable architecture—perfect for building tailored shopping experiences with confidence and speed.\nInstallation Using Docker\nYou can get started with EverShop in minutes by using the Docker image. The Docker image is a great way to get started with EverShop without having to worry about installing dependencies or configuring your environment.\ncurl -sSL https://raw.githubusercontent.com/evershopcommerce/evershop/main/docker-compose.yml\n>\ndocker-compose.yml\ndocker-compose up -d\nFor the full installation guide, please refer to our\nInstallation guide\n.\nDocumentation\nInstallation guide\n.\nExtension development\n.\nTheme development\n.\nDemo\nExplore our demo store.\nDemo user:\nEmail:\ndemo@evershop.io\nPassword: 123456\nSupport\nIf you like my work, feel free to:\n⭐ this repository. It helps.\nabout EverShop. Thank you!\nContributing\nEverShop is an open-source project. We are committed to a fully transparent development process and appreciate highly any contributions. Whether you are helping us fix bugs, proposing new features, improving our documentation or spreading the word - we would love to have you as part of the EverShop community.\nAsk a question about EverShop\nYou can ask questions, and participate in discussions about EverShop-related topics in the EverShop Discord channel.\nCreate a bug report\nIf you see an error message or run into an issue, please\ncreate bug report\n. This effort is valued and it will help all EverShop users.\nSubmit a feature request\nIf you have an idea, or you're missing a capability that would make development easier and more robust, please\nSubmit feature request\n.\nIf a similar feature request already exists, don't forget to leave a \"+1\".\nIf you add some more information such as your thoughts and vision about the feature, your comments will be embraced warmly :)\nPlease refer to our\nContribution Guidelines\nand\nCode of Conduct\n.\nLicense\nGPL-3.0 License",
        "今日の獲得スター数: 99",
        "累積スター数: 6,383"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/evershopcommerce/evershop"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/rizinorg/cutter",
      "title": "rizinorg/cutter",
      "date": null,
      "executive_summary": [
        "Free and Open Source Reverse Engineering Platform powered by rizin",
        "---",
        "Cutter\nCutter is a free and open-source reverse engineering platform powered by\nrizin\n. It aims at being an advanced and customizable reverse engineering platform while keeping the user experience in mind. Cutter is created by reverse engineers for reverse engineers.\nLearn more at\ncutter.re\n.\nGetting Cutter\nDownload\nCutter release binaries for all major platforms (Linux, macOS, Windows) can be downloaded from\nGitHub Releases\n.\nLinux\n: If your distribution provides it, check for\ncutter\npackage in your package manager (or\ncutter-re\n/\nrz-cutter\n). If not available there, we have setup repositories in\nOBS\nfor some common distributions. Look at\nhttps://software.opensuse.org/package/cutter-re\nand follow the instructions there. Otherwise download the\n.AppImage\nfile from our release, make it executable and run as below or use\nAppImageLauncher\n.\nchmod +x Cutter*.AppImage; ./Cutter*.AppImage\nmacOS\n: Download the\n.dmg\nfile or use\nHomebrew Cask\n:\nbrew install --cask cutter\nWindows\n: Download the\n.zip\narchive, or use either\nChocolatey\nor\nScoop\n:\nchoco install cutter\nscoop bucket add extras\nfollowed by\nscoop install cutter\nBuild from sources\nTo build Cutter from sources, please check the\nBuilding Docs\n.\nDocker image\nTo deploy\ncutter\nusing a pre-built\nDockerfile\n, it's possible to use the\nprovided configuration\n. The corresponding\nREADME.md\nfile also contains instructions on how to get started using the docker image with minimal effort.\nDocumentation\nUser Guide\nContribution Guidelines\nDevelopers Docs\nPlugins\nCutter supports both Python and Native C++ plugins.\nOur community has built many plugins and useful scripts for Cutter such as the native integration of\nGhidra decompiler\nor the plugin to visualize DynamoRIO code coverage. You can find a list of cutter plugins linked below. Feel free to extend it with your own plugins and scripts for Cutter.\nOfficial & Community Plugins\nPlugins Development Guide\nGetting Help\nPlease use the following channels to ask for help from Cutter developers and community:\nTelegram:\nhttps://t.me/cutter_re\nMattermost:\nhttps://im.rizin.re\nIRC:\n#cutter on\nhttps://web.libera.chat/\nTwitter:\n@cutter_re",
        "今日の獲得スター数: 97",
        "累積スター数: 17,754"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/rizinorg/cutter"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/xyflow/xyflow",
      "title": "xyflow/xyflow",
      "date": null,
      "executive_summary": [
        "React Flow | Svelte Flow - Powerful open source libraries for building node-based UIs with React (https://reactflow.dev) or Svelte (https://svelteflow.dev). Ready out-of-the-box and infinitely customizable.",
        "---",
        "Powerful open source libraries for building node-based UIs with React or Svelte. Ready out-of-the-box and infinitely customizable.\nReact Flow\n·\nSvelte Flow\n·\nReact Flow Pro\n·\nDiscord\nThe xyflow mono repo\nThe xyflow repository is the home of four packages:\nReact Flow 12\n@xyflow/react\npackages/react\nReact Flow 11\nreactflow\nv11 branch\nSvelte Flow\n@xyflow/svelte\npackages/svelte\nShared helper library\n@xyflow/system\npackages/system\nCommercial usage\nAre you using React Flow or Svelte Flow for a personal project?\nGreat! No sponsorship needed, you can support us by reporting any bugs you find, sending us screenshots of your projects, and starring us on Github 🌟\nAre you using React Flow or Svelte Flow at your organization and making money from it?\nAwesome! We rely on your support to keep our libraries developed and maintained under an MIT License, just how we like it. For React Flow you can do that on the\nReact Flow Pro website\nand for both of our libraries you can do it through\nGithub Sponsors\n.\nGetting started\nThe best way to get started is to check out the\nReact Flow\nor\nSvelte Flow\nlearn section. However if you want to get a sneak peek of how to install and use the libraries you can see it here:\nReact Flow\nbasic usage\nInstallation\nnpm install @xyflow/react\nBasic usage\nimport\n{\nuseCallback\n}\nfrom\n'react'\n;\nimport\n{\nReactFlow\n,\nMiniMap\n,\nControls\n,\nBackground\n,\nuseNodesState\n,\nuseEdgesState\n,\naddEdge\n,\n}\nfrom\n'@xyflow/react'\n;\nimport\n'@xyflow/react/dist/style.css'\n;\nconst\ninitialNodes\n=\n[\n{\nid\n:\n'1'\n,\nposition\n:\n{\nx\n:\n0\n,\ny\n:\n0\n}\n,\ndata\n:\n{\nlabel\n:\n'1'\n}\n}\n,\n{\nid\n:\n'2'\n,\nposition\n:\n{\nx\n:\n0\n,\ny\n:\n100\n}\n,\ndata\n:\n{\nlabel\n:\n'2'\n}\n}\n,\n]\n;\nconst\ninitialEdges\n=\n[\n{\nid\n:\n'e1-2'\n,\nsource\n:\n'1'\n,\ntarget\n:\n'2'\n}\n]\n;\nfunction\nFlow\n(\n)\n{\nconst\n[\nnodes\n,\nsetNodes\n,\nonNodesChange\n]\n=\nuseNodesState\n(\ninitialNodes\n)\n;\nconst\n[\nedges\n,\nsetEdges\n,\nonEdgesChange\n]\n=\nuseEdgesState\n(\ninitialEdges\n)\n;\nconst\nonConnect\n=\nuseCallback\n(\n(\nparams\n)\n=>\nsetEdges\n(\n(\neds\n)\n=>\naddEdge\n(\nparams\n,\neds\n)\n)\n,\n[\nsetEdges\n]\n)\n;\nreturn\n(\n<\nReactFlow\nnodes\n=\n{\nnodes\n}\nedges\n=\n{\nedges\n}\nonNodesChange\n=\n{\nonNodesChange\n}\nonEdgesChange\n=\n{\nonEdgesChange\n}\nonConnect\n=\n{\nonConnect\n}\n>\n<\nMiniMap\n/>\n<\nControls\n/>\n<\nBackground\n/>\n</\nReactFlow\n>\n)\n;\n}\nexport\ndefault\nFlow\n;\nSvelte Flow\nbasic usage\nInstallation\nnpm install @xyflow/svelte\nBasic usage\n<\nscript\nlang\n=\n\"\nts\n\"\n>\nimport\n{\nwritable\n}\nfrom\n'\nsvelte/store\n'\n;\nimport\n{\nSvelteFlow\n,\nControls\n,\nBackground\n,\nBackgroundVariant\n,\nMiniMap\n,\n}\nfrom\n'\n@xyflow/svelte\n'\n;\nimport\n'\n@xyflow/svelte/dist/style.css\n'\nconst\nnodes\n=\nwritable\n([\n{\nid:\n'\n1\n'\n,\ntype:\n'\ninput\n'\n,\ndata: { label:\n'\nInput Node\n'\n},\nposition: { x:\n0\n, y:\n0\n}\n},\n{\nid:\n'\n2\n'\n,\ntype:\n'\ncustom\n'\n,\ndata: { label:\n'\nNode\n'\n},\nposition: { x:\n0\n, y:\n150\n}\n}\n]);\nconst\nedges\n=\nwritable\n([\n{\nid:\n'\n1-2\n'\n,\ntype:\n'\ndefault\n'\n,\nsource:\n'\n1\n'\n,\ntarget:\n'\n2\n'\n,\nlabel:\n'\nEdge Text\n'\n}\n]);\n</\nscript\n>\n\n<\nSvelteFlow\n{\nnodes\n}\n{\nedges\n}\nfitView\non:nodeclick\n={(\nevent\n)\n=>\nconsole\n.\nlog\n(\n'\non node click\n'\n,\nevent\n)}\n>\n<\nControls\n/>\n<\nBackground\nvariant\n={\nBackgroundVariant\n.\nDots\n} />\n<\nMiniMap\n/>\n</\nSvelteFlow\n>\nReleases\nFor releasing packages we are using\nchangesets\nin combination with the\nchangeset Github action\n. The rough idea is:\ncreate PRs for new features, updates and fixes (with a changeset if relevant for changelog)\nmerge into main\nchangset creates a PR that bumps all packages based on the changesets\nmerge changeset PR if you want to release to Github and npm\nBuilt by\nxyflow\nReact Flow and Svelte Flow are maintained by the\nxyflow team\n. If you need help or want to talk to us about a collaboration, reach out through our\ncontact form\nor by joining our\nDiscord Server\n.\nLicense\nReact Flow and Svelte Flow are\nMIT licensed\n.",
        "今日の獲得スター数: 92",
        "累積スター数: 32,452"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/xyflow/xyflow"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/rust-lang/rustfmt",
      "title": "rust-lang/rustfmt",
      "date": null,
      "executive_summary": [
        "Format Rust code",
        "---",
        "rustfmt\nA tool for formatting Rust code according to style guidelines.\nIf you'd like to help out (and you should, it's a fun project!), see\nContributing.md\nand our\nCode of\nConduct\n.\nYou can use rustfmt in Travis CI builds. We provide a minimal Travis CI\nconfiguration (see\nhere\n).\nQuick start\nYou can run\nrustfmt\nwith Rust 1.24 and above.\nOn the Stable toolchain\nTo install:\nrustup component add rustfmt\nTo run on a cargo project in the current working directory:\ncargo fmt\nOn the Nightly toolchain\nFor the latest and greatest\nrustfmt\n, nightly is required.\nTo install:\nrustup component add rustfmt --toolchain nightly\nTo run on a cargo project in the current working directory:\ncargo +nightly fmt\nLimitations\nRustfmt tries to work on as much Rust code as possible. Sometimes, the code\ndoesn't even need to compile! In general, we are looking to limit areas of\ninstability; in particular, post-1.0, the formatting of most code should not\nchange as Rustfmt improves. However, there are some things that Rustfmt can't\ndo or can't do well (and thus where formatting might change significantly,\neven post-1.0). We would like to reduce the list of limitations over time.\nThe following list enumerates areas where Rustfmt does not work or where the\nstability guarantees do not apply (we don't make a distinction between the two\nbecause in the future Rustfmt might work on code where it currently does not):\na program where any part of the program does not parse (parsing is an early\nstage of compilation and in Rust includes macro expansion).\nMacro declarations and uses (current status: some macro declarations and uses\nare formatted).\nComments, including any AST node with a comment 'inside' (Rustfmt does not\ncurrently attempt to format comments, it does format code with comments inside, but that formatting may change in the future).\nRust code in code blocks in comments.\nAny fragment of a program (i.e., stability guarantees only apply to whole\nprograms, even where fragments of a program can be formatted today).\nCode containing non-ascii unicode characters (we believe Rustfmt mostly works\nhere, but do not have the test coverage or experience to be 100% sure).\nBugs in Rustfmt (like any software, Rustfmt has bugs, we do not consider bug\nfixes to break our stability guarantees).\nRunning\nYou can run Rustfmt by just typing\nrustfmt filename\nif you used\ncargo install\n. This runs rustfmt on the given file, if the file includes out of line\nmodules, then we reformat those too. So to run on a whole module or crate, you\njust need to run on the root file (usually mod.rs or lib.rs). Rustfmt can also\nread data from stdin. Alternatively, you can use\ncargo fmt\nto format all\nbinary and library targets of your crate.\nYou can run\nrustfmt --help\nfor information about available arguments.\nThe easiest way to run rustfmt against a project is with\ncargo fmt\n.\ncargo fmt\nworks on both\nsingle-crate projects and\ncargo workspaces\n.\nPlease see\ncargo fmt --help\nfor usage information.\nYou can specify the path to your own\nrustfmt\nbinary for cargo to use by setting the\nRUSTFMT\nenvironment variable. This was added in v1.4.22, so you must have this version or newer to leverage this feature (\ncargo fmt --version\n)\nRunning\nrustfmt\ndirectly\nTo format individual files or arbitrary codes from stdin, the\nrustfmt\nbinary should be used. Some\nexamples follow:\nrustfmt lib.rs main.rs\nwill format \"lib.rs\" and \"main.rs\" in place\nrustfmt\nwill read a code from stdin and write formatting to stdout\necho \"fn     main() {}\" | rustfmt\nwould emit \"fn main() {}\".\nFor more information, including arguments and emit options, see\nrustfmt --help\n.\nVerifying code is formatted\nWhen running with\n--check\n, Rustfmt will exit with\n0\nif Rustfmt would not\nmake any formatting changes to the input, and\n1\nif Rustfmt would make changes.\nIn other modes, Rustfmt will exit with\n1\nif there was some error during\nformatting (for example a parsing or internal error) and\n0\nif formatting\ncompleted without error (whether or not changes were made).\nRunning Rustfmt from your editor\nVim\nEmacs\nSublime Text 3\nAtom\nVisual Studio Code\nIntelliJ or CLion\nChecking style on a CI server\nTo keep your code base consistently formatted, it can be helpful to fail the CI build\nwhen a pull request contains unformatted code. Using\n--check\ninstructs\nrustfmt to exit with an error code if the input is not formatted correctly.\nIt will also print any found differences. (Older versions of Rustfmt don't\nsupport\n--check\n, use\n--write-mode diff\n).\nA minimal Travis setup could look like this (requires Rust 1.31.0 or greater):\nlanguage\n:\nrust\nbefore_script\n:\n-\nrustup component add rustfmt\nscript\n:\n-\ncargo build\n-\ncargo test\n-\ncargo fmt --all -- --check\nSee\nthis blog post\nfor more info.\nHow to build and test\ncargo build\nto build.\ncargo test\nto run all tests.\nTo run rustfmt after this, use\ncargo run --bin rustfmt -- filename\n. See the\nnotes above on running rustfmt.\nConfiguring Rustfmt\nRustfmt is designed to be very configurable. You can create a TOML file called\nrustfmt.toml\nor\n.rustfmt.toml\n, place it in the project or any other parent\ndirectory and it will apply the options in that file. See\nrustfmt --help=config\nfor the options which are available, or if you prefer to see\nvisual style previews,\nGitHub page\n.\nBy default, Rustfmt uses a style which conforms to the\nRust style guide\nthat has been formalized through the\nstyle RFC\nprocess\n.\nConfiguration options are either stable or unstable. Stable options can always\nbe used, while unstable ones are only available on a nightly toolchain, and opt-in.\nSee\nGitHub page\nfor details.\nRust's Editions\nThe\nedition\noption determines the Rust language edition used for parsing the code. This is important for syntax compatibility but does not directly control formatting behavior (see\nStyle Editions\n).\nWhen running\ncargo fmt\n, the\nedition\nis automatically read from the\nCargo.toml\nfile. However, when running\nrustfmt\ndirectly, the\nedition\ndefaults to 2015. For consistent parsing between rustfmt and\ncargo fmt\n, you should configure the\nedition\nin your\nrustfmt.toml\nfile:\nedition\n=\n\"\n2018\n\"\nStyle Editions\nThis option is inferred from the\nedition\nif not specified.\nSee\nRust Style Editions\nfor details on formatting differences between style editions.\nrustfmt has a default style edition of\n2015\nwhile\ncargo fmt\ninfers the style edition from the\nedition\nset in\nCargo.toml\n. This can lead to inconsistencies between\nrustfmt\nand\ncargo fmt\nif the style edition is not explicitly configured.\nTo ensure consistent formatting, it is recommended to specify the\nstyle_edition\nin a\nrustfmt.toml\nconfiguration file. For example:\nstyle_edition\n=\n\"\n2024\n\"\nTips\nTo ensure consistent parsing between\ncargo fmt\nand\nrustfmt\n, you should configure the\nedition\nin your\nrustfmt.toml\nfile.\nTo ensure consistent formatting between\ncargo fmt\nand\nrustfmt\n, you should configure the\nstyle_edition\nin your\nrustfmt.toml\nfile.\nFor things you do not want rustfmt to mangle, use\n#[rustfmt::skip]\nTo prevent rustfmt from formatting a macro or an attribute,\nuse\n#[rustfmt::skip::macros(target_macro_name)]\nor\n#[rustfmt::skip::attributes(target_attribute_name)]\nExample:\n#!\n[\nrustfmt\n::\nskip\n::\nattributes\n(\ncustom_attribute\n)\n]\n#\n[\ncustom_attribute\n(\nformatting\n,\nhere\n,\nshould\n,\nbe\n,\nSkipped\n)\n]\n#\n[\nrustfmt\n::\nskip\n::\nmacros\n(\nhtml\n)\n]\nfn\nmain\n(\n)\n{\nlet\nmacro_result1 =\nhtml\n!\n{\n<div>\nHello\n</div>\n}\n.\nto_string\n(\n)\n;\nWhen you run rustfmt, place a file named\nrustfmt.toml\nor\n.rustfmt.toml\nin\ntarget file directory or its parents to override the default settings of\nrustfmt. You can generate a file containing the default configuration with\nrustfmt --print-config default rustfmt.toml\nand customize as needed.\nAfter successful compilation, a\nrustfmt\nexecutable can be found in the\ntarget directory.\nIf you're having issues compiling Rustfmt (or compile errors when trying to\ninstall), make sure you have the most recent version of Rust installed.\nYou can change the way rustfmt emits the changes with the --emit flag:\nExample:\ncargo fmt -- --emit files\nOptions:\nFlag\nDescription\nNightly Only\nfiles\noverwrites output to files\nNo\nstdout\nwrites output to stdout\nNo\ncoverage\ndisplays how much of the input file was processed\nYes\ncheckstyle\nemits in a checkstyle format\nYes\njson\nemits diffs in a json format\nYes\nLicense\nRustfmt is distributed under the terms of both the MIT license and the\nApache License (Version 2.0).\nSee\nLICENSE-APACHE\nand\nLICENSE-MIT\nfor details.",
        "今日の獲得スター数: 81",
        "累積スター数: 6,583"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/rust-lang/rustfmt"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/tile-ai/tilelang",
      "title": "tile-ai/tilelang",
      "date": null,
      "executive_summary": [
        "Domain-specific language designed to streamline the development of high-performance GPU/CPU/Accelerators kernels",
        "---",
        "Tile Language\nTile Language (\ntile-lang\n) is a concise domain-specific language designed to streamline the development of high-performance GPU/CPU kernels (e.g., GEMM, Dequant GEMM, FlashAttention, LinearAttention). By employing a Pythonic syntax with an underlying compiler infrastructure on top of\nTVM\n, tile-lang allows developers to focus on productivity without sacrificing the low-level optimizations necessary for state-of-the-art performance.\nLatest News\n10/07/2025 🍎: Added Apple Metal Device support, check out\nPull Request #799\nfor details.\n09/29/2025  🎉: Thrilled to announce that ​​AscendC​​ and ​Ascend​NPU IR​​ backends targeting Huawei Ascend chips are now supported!\nCheck out the preview here:\n🔗\nlink\n.\nThis includes implementations across two branches:\nascendc_pto\nand\nnpuir\n.\nFeel free to explore and share your feedback!\n07/04/2025 🚀: Introduced\nT.gemm_sp\nfor 2:4 sparse tensor core support, check out\nPull Request #526\nfor details.\n06/05/2025 ✨: Added\nNVRTC Backend\nto significantly reduce compilation time for cute templates!\n04/14/2025 🚀: Added high-performance FlashMLA implementation for AMD MI300X, achieving performance parity with hand-optimized assembly kernels of Aiter! See\nexample_mla_amd\nfor details.\n03/03/2025 🚀: Added high-performance MLA Decoding support using only 80 lines of Python code, achieving performance on par with FlashMLA on H100 (see\nexample_mla_decode.py\n)! We also provide\ndocumentation\nexplaining how TileLang achieves this.\n02/15/2025 ✨: Added WebGPU Codegen support, see\nPull Request #86\n!\n02/12/2025 ✨: Excited to announce the release of\nv0.1.0\n!\n02/10/2025 🚀: Added debug tools for TileLang—\nT.print\nfor printing variables/buffers (\ndocs\n) and a memory layout plotter (\nexamples/plot_layout\n).\n01/20/2025 ✨: We are excited to announce that tile-lang, a dsl for high performance AI workloads, is now open source and available to the public!\nTested Devices\nAlthough tile-lang aims to be portable across a range of Devices, it has been specifically tested and validated on the following devices: for NVIDIA GPUs, this includes the H100 (with Auto TMA/WGMMA support), A100, V100, RTX 4090, RTX 3090, and RTX A6000; for AMD GPUs, it includes the MI250 (with Auto MatrixCore support) and the MI300X (with Async Copy support).\nOP Implementation Examples\ntile-lang\nprovides the building blocks to implement a wide variety of operators. Some examples include:\nMatrix Multiplication\nDequantization GEMM\nFlash Attention\nFlash Linear Attention\nFlash MLA Decoding\nNative Sparse Attention\nWithin the\nexamples\ndirectory, you will also find additional complex kernels—such as convolutions, forward/backward passes for FlashAttention, more operators will continuously be added.\nBenchmark Summary\nTileLang achieves exceptional performance across a variety of computational patterns. Comprehensive benchmark scripts and settings are available at\ntilelang-benchmark\n. Below are selected results showcasing its capabilities:\nMLA Decoding Performance on H100\nFlash Attention Performance on H100\nMatmul Performance on GPUs (RTX 4090, A100, H100, MI300X)\nDequantize Matmul Performance on A100\nInstallation\nMethod 1: Install with Pip\nThe quickest way to get started is to install the latest release from PyPI:\npip install tilelang\nAlternatively, you can install directly from the GitHub repository:\npip install git+https://github.com/tile-ai/tilelang\nOr install locally:\n#\ninstall required system dependencies\nsudo apt-get update\nsudo apt-get install -y python3-setuptools gcc libtinfo-dev zlib1g-dev build-essential cmake libedit-dev libxml2-dev\n\npip install -e\n.\n-v\n#\nremove -e option if you don't want to install in editable mode, -v for verbose output\nMethod 2: Build from Source\nWe currently provide three ways to install\ntile-lang\nfrom source:\nInstall from Source (using your own TVM installation)\nInstall from Source (using the bundled TVM submodule)\nInstall Using the Provided Script\nMethod 3: Install with Nightly Version\nFor users who want access to the latest features and improvements before official releases, we provide nightly builds of\ntile-lang\n.\npip install tilelang -f https://tile-ai.github.io/whl/nightly/cu121/\n#\nor pip install tilelang --find-links https://tile-ai.github.io/whl/nightly/cu121/\nNote:\nNightly builds contain the most recent code changes but may be less stable than official releases. They're ideal for testing new features or if you need a specific bugfix that hasn't been released yet.\nQuick Start\nIn this section, you'll learn how to write and execute a straightforward GEMM (matrix multiplication) kernel using tile-lang, followed by techniques for layout optimizations, pipelining, and L2-cache–friendly swizzling.\nGEMM Example with Annotations (Layout, L2 Cache Swizzling, and Pipelining, etc.)\nBelow is an example that demonstrates more advanced features: layout annotation, parallelized copy, and swizzle for improved L2 cache locality. This snippet shows how to adapt your kernel to maximize performance on complex hardware.\nimport\ntilelang\nimport\ntilelang\n.\nlanguage\nas\nT\n# @tilelang.jit(target=\"cuda\")\n# target currently can be \"cuda\" or \"hip\" or \"cpu\".\n# if not specified, it will be inferred from the input tensors during compile time\n@\ntilelang\n.\njit\ndef\nmatmul\n(\nM\n,\nN\n,\nK\n,\nblock_M\n,\nblock_N\n,\nblock_K\n,\ndtype\n=\n\"float16\"\n,\naccum_dtype\n=\n\"float\"\n):\n@\nT\n.\nprim_func\ndef\nmatmul_relu_kernel\n(\nA\n:\nT\n.\nTensor\n((\nM\n,\nK\n),\ndtype\n),\nB\n:\nT\n.\nTensor\n((\nK\n,\nN\n),\ndtype\n),\nC\n:\nT\n.\nTensor\n((\nM\n,\nN\n),\ndtype\n),\n    ):\n# Initialize Kernel Context\nwith\nT\n.\nKernel\n(\nT\n.\nceildiv\n(\nN\n,\nblock_N\n),\nT\n.\nceildiv\n(\nM\n,\nblock_M\n),\nthreads\n=\n128\n)\nas\n(\nbx\n,\nby\n):\nA_shared\n=\nT\n.\nalloc_shared\n((\nblock_M\n,\nblock_K\n),\ndtype\n)\nB_shared\n=\nT\n.\nalloc_shared\n((\nblock_K\n,\nblock_N\n),\ndtype\n)\nC_local\n=\nT\n.\nalloc_fragment\n((\nblock_M\n,\nblock_N\n),\naccum_dtype\n)\n# Enable rasterization for better L2 cache locality (Optional)\n# T.use_swizzle(panel_size=10, enable=True)\n# Clear local accumulation\nT\n.\nclear\n(\nC_local\n)\nfor\nko\nin\nT\n.\nPipelined\n(\nT\n.\nceildiv\n(\nK\n,\nblock_K\n),\nnum_stages\n=\n3\n):\n# Copy tile of A\n# This is a sugar syntax for parallelized copy\nT\n.\ncopy\n(\nA\n[\nby\n*\nblock_M\n,\nko\n*\nblock_K\n],\nA_shared\n)\n# Copy tile of B\nT\n.\ncopy\n(\nB\n[\nko\n*\nblock_K\n,\nbx\n*\nblock_N\n],\nB_shared\n)\n# Perform a tile-level GEMM on the shared buffers\n# Currently we dispatch to the cute/hip on Nvidia/AMD GPUs\nT\n.\ngemm\n(\nA_shared\n,\nB_shared\n,\nC_local\n)\n# relu\nfor\ni\n,\nj\nin\nT\n.\nParallel\n(\nblock_M\n,\nblock_N\n):\nC_local\n[\ni\n,\nj\n]\n=\nT\n.\nmax\n(\nC_local\n[\ni\n,\nj\n],\n0\n)\n# Copy result back to global memory\nT\n.\ncopy\n(\nC_local\n,\nC\n[\nby\n*\nblock_M\n,\nbx\n*\nblock_N\n])\nreturn\nmatmul_relu_kernel\nM\n=\n1024\n# M = T.symbolic(\"m\") if you want to use dynamic shape\nN\n=\n1024\nK\n=\n1024\nblock_M\n=\n128\nblock_N\n=\n128\nblock_K\n=\n32\n# 1. Define the kernel (matmul) and compile/lower it into an executable module\nmatmul_relu_kernel\n=\nmatmul\n(\nM\n,\nN\n,\nK\n,\nblock_M\n,\nblock_N\n,\nblock_K\n)\n# 3. Test the kernel in Python with PyTorch data\nimport\ntorch\n# Create random input tensors on the GPU\na\n=\ntorch\n.\nrandn\n(\nM\n,\nK\n,\ndevice\n=\n\"cuda\"\n,\ndtype\n=\ntorch\n.\nfloat16\n)\nb\n=\ntorch\n.\nrandn\n(\nK\n,\nN\n,\ndevice\n=\n\"cuda\"\n,\ndtype\n=\ntorch\n.\nfloat16\n)\nc\n=\ntorch\n.\nempty\n(\nM\n,\nN\n,\ndevice\n=\n\"cuda\"\n,\ndtype\n=\ntorch\n.\nfloat16\n)\n# Run the kernel through the Profiler\nmatmul_relu_kernel\n(\na\n,\nb\n,\nc\n)\nprint\n(\nc\n)\n# Reference multiplication using PyTorch\nref_c\n=\ntorch\n.\nrelu\n(\na\n@\nb\n)\n# Validate correctness\ntorch\n.\ntesting\n.\nassert_close\n(\nc\n,\nref_c\n,\nrtol\n=\n1e-2\n,\natol\n=\n1e-2\n)\nprint\n(\n\"Kernel output matches PyTorch reference.\"\n)\n# 4. Retrieve and inspect the generated CUDA source (optional)\n# cuda_source = jit_kernel.get_kernel_source()\n# print(\"Generated CUDA kernel:\\n\", cuda_source)\n# 5.Profile latency with kernel\nprofiler\n=\nmatmul_relu_kernel\n.\nget_profiler\n(\ntensor_supply_type\n=\ntilelang\n.\nTensorSupplyType\n.\nNormal\n)\nlatency\n=\nprofiler\n.\ndo_bench\n()\nprint\n(\nf\"Latency:\n{\nlatency\n}\nms\"\n)\nDive Deep into TileLang Beyond GEMM\nIn addition to GEMM, we provide a variety of examples to showcase the versatility and power of TileLang, including:\nDequantize GEMM\n: Achieve high-performance dequantization by\nfine-grained control over per-thread operations\n, with many features now adopted as default behaviors in\nBitBLAS\n, which utilizing magic layout transformation and intrins to accelerate dequantize gemm.\nFlashAttention\n: Enable cross-operator fusion with simple and intuitive syntax, and we also provide an example of auto tuning.\nLinearAttention\n: Examples include RetNet and Mamba implementations.\nConvolution\n: Implementations of Convolution with IM2Col.\nUpcoming Features\nCheck our\ntilelang v0.2.0 release plan\nfor upcoming features.\nTileLang has now been used in project\nBitBLAS\nand\nAttentionEngine\n.\nJoin the Discussion\nWelcome to join our Discord community for discussions, support, and collaboration!\nAcknowledgements\nWe would like to express our gratitude to the\nTVM\ncommunity for their invaluable contributions. The initial version of this project was mainly developed by\nLeiWang1999\n,\nchengyupku\nand\nnox-410\nwith supervision from Prof.\nZhi Yang\nat Peking University. Part of this work was carried out during an internship at Microsoft Research, where Dr. Lingxiao Ma, Dr. Yuqing Xia, Dr. Jilong Xue, and Dr. Fan Yang offered valuable advice and support. We deeply appreciate their mentorship and contributions.",
        "今日の獲得スター数: 73",
        "累積スター数: 3,425"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/tile-ai/tilelang"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/vllm-project/vllm",
      "title": "vllm-project/vllm",
      "date": null,
      "executive_summary": [
        "A high-throughput and memory-efficient inference and serving engine for LLMs",
        "---",
        "Easy, fast, and cheap LLM serving for everyone\n|\nDocumentation\n|\nBlog\n|\nPaper\n|\nTwitter/X\n|\nUser Forum\n|\nDeveloper Slack\n|\nJoin us at the\nPyTorch Conference, October 22-23\nand\nRay Summit, November 3-5\nin San Francisco for our latest updates on vLLM and to meet the vLLM team! Register now for the largest vLLM community events of the year!\nLatest News\n🔥\n[2025/09] We hosted\nvLLM Toronto Meetup\nfocused on tackling inference at scale and speculative decoding with speakers from NVIDIA and Red Hat! Please find the meetup slides\nhere\n.\n[2025/08] We hosted\nvLLM Shenzhen Meetup\nfocusing on the ecosystem around vLLM! Please find the meetup slides\nhere\n.\n[2025/08] We hosted\nvLLM Singapore Meetup\n. We shared V1 updates, disaggregated serving and MLLM speedups with speakers from Embedded LLM, AMD, WekaIO, and A*STAR. Please find the meetup slides\nhere\n.\n[2025/08] We hosted\nvLLM Shanghai Meetup\nfocusing on building, developing, and integrating with vLLM! Please find the meetup slides\nhere\n.\n[2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement\nhere\n.\n[2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post\nhere\n.\nPrevious News\n[2025/08] We hosted\nvLLM Korea Meetup\nwith Red Hat and Rebellions! We shared the latest advancements in vLLM along with project spotlights from the vLLM Korea community. Please find the meetup slides\nhere\n.\n[2025/08] We hosted\nvLLM Beijing Meetup\nfocusing on large-scale LLM deployment! Please find the meetup slides\nhere\nand the recording\nhere\n.\n[2025/05] We hosted\nNYC vLLM Meetup\n! Please find the meetup slides\nhere\n.\n[2025/04] We hosted\nAsia Developer Day\n! Please find the meetup slides from the vLLM team\nhere\n.\n[2025/03] We hosted\nvLLM x Ollama Inference Night\n! Please find the meetup slides from the vLLM team\nhere\n.\n[2025/03] We hosted\nthe first vLLM China Meetup\n! Please find the meetup slides from vLLM team\nhere\n.\n[2025/03] We hosted\nthe East Coast vLLM Meetup\n! Please find the meetup slides\nhere\n.\n[2025/02] We hosted\nthe ninth vLLM meetup\nwith Meta! Please find the meetup slides from vLLM team\nhere\nand AMD\nhere\n. The slides from Meta will not be posted.\n[2025/01] We hosted\nthe eighth vLLM meetup\nwith Google Cloud! Please find the meetup slides from vLLM team\nhere\n, and Google Cloud team\nhere\n.\n[2024/12] vLLM joins\npytorch ecosystem\n! Easy, Fast, and Cheap LLM Serving for Everyone!\n[2024/11] We hosted\nthe seventh vLLM meetup\nwith Snowflake! Please find the meetup slides from vLLM team\nhere\n, and Snowflake team\nhere\n.\n[2024/10] We have just created a developer slack (\nslack.vllm.ai\n) focusing on coordinating contributions and discussing features. Please feel free to join us there!\n[2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team\nhere\n. Learn more from the\ntalks\nfrom other vLLM contributors and users!\n[2024/09] We hosted\nthe sixth vLLM meetup\nwith NVIDIA! Please find the meetup slides\nhere\n.\n[2024/07] We hosted\nthe fifth vLLM meetup\nwith AWS! Please find the meetup slides\nhere\n.\n[2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post\nhere\n.\n[2024/06] We hosted\nthe fourth vLLM meetup\nwith Cloudflare and BentoML! Please find the meetup slides\nhere\n.\n[2024/04] We hosted\nthe third vLLM meetup\nwith Roblox! Please find the meetup slides\nhere\n.\n[2024/01] We hosted\nthe second vLLM meetup\nwith IBM! Please find the meetup slides\nhere\n.\n[2023/10] We hosted\nthe first vLLM meetup\nwith a16z! Please find the meetup slides\nhere\n.\n[2023/08] We would like to express our sincere gratitude to\nAndreessen Horowitz\n(a16z) for providing a generous grant to support the open-source development and research of vLLM.\n[2023/06] We officially released vLLM! FastChat-vLLM integration has powered\nLMSYS Vicuna and Chatbot Arena\nsince mid-April. Check out our\nblog post\n.\nAbout\nvLLM is a fast and easy-to-use library for LLM inference and serving.\nOriginally developed in the\nSky Computing Lab\nat UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.\nvLLM is fast with:\nState-of-the-art serving throughput\nEfficient management of attention key and value memory with\nPagedAttention\nContinuous batching of incoming requests\nFast model execution with CUDA/HIP graph\nQuantizations:\nGPTQ\n,\nAWQ\n,\nAutoRound\n, INT4, INT8, and FP8\nOptimized CUDA kernels, including integration with FlashAttention and FlashInfer\nSpeculative decoding\nChunked prefill\nvLLM is flexible and easy to use with:\nSeamless integration with popular Hugging Face models\nHigh-throughput serving with various decoding algorithms, including\nparallel sampling\n,\nbeam search\n, and more\nTensor, pipeline, data and expert parallelism support for distributed inference\nStreaming outputs\nOpenAI-compatible API server\nSupport for NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, and TPU. Additionally, support for diverse hardware plugins such as Intel Gaudi, IBM Spyre and Huawei Ascend.\nPrefix caching support\nMulti-LoRA support\nvLLM seamlessly supports most popular open-source models on HuggingFace, including:\nTransformer-like LLMs (e.g., Llama)\nMixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)\nEmbedding Models (e.g., E5-Mistral)\nMulti-modal LLMs (e.g., LLaVA)\nFind the full list of supported models\nhere\n.\nGetting Started\nInstall vLLM with\npip\nor\nfrom source\n:\npip install vllm\nVisit our\ndocumentation\nto learn more.\nInstallation\nQuickstart\nList of Supported Models\nContributing\nWe welcome and value any contributions and collaborations.\nPlease check out\nContributing to vLLM\nfor how to get involved.\nSponsors\nvLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!\nCash Donations:\na16z\nDropbox\nSequoia Capital\nSkywork AI\nZhenFund\nCompute Resources:\nAlibaba Cloud\nAMD\nAnyscale\nAWS\nCrusoe Cloud\nDatabricks\nDeepInfra\nGoogle Cloud\nIntel\nLambda Lab\nNebius\nNovita AI\nNVIDIA\nReplicate\nRoblox\nRunPod\nTrainy\nUC Berkeley\nUC San Diego\nVolcengine\nSlack Sponsor: Anyscale\nWe also have an official fundraising venue through\nOpenCollective\n. We plan to use the fund to support the development, maintenance, and adoption of vLLM.\nCitation\nIf you use vLLM for your research, please cite our\npaper\n:\n@inproceedings\n{\nkwon2023efficient\n,\ntitle\n=\n{\nEfficient Memory Management for Large Language Model Serving with PagedAttention\n}\n,\nauthor\n=\n{\nWoosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica\n}\n,\nbooktitle\n=\n{\nProceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles\n}\n,\nyear\n=\n{\n2023\n}\n}\nContact Us\nFor technical questions and feature requests, please use GitHub\nIssues\nFor discussing with fellow users, please use the\nvLLM Forum\nFor coordinating contributions and development, please use\nSlack\nFor security disclosures, please use GitHub's\nSecurity Advisories\nfeature\nFor collaborations and partnerships, please contact us at\nvllm-questions@lists.berkeley.edu\nMedia Kit\nIf you wish to use vLLM's logo, please refer to\nour media kit repo",
        "今日の獲得スター数: 70",
        "累積スター数: 59,787"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/vllm-project/vllm"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/aaPanel/BillionMail",
      "title": "aaPanel/BillionMail",
      "date": null,
      "executive_summary": [
        "BillionMail gives you open-source MailServer, NewsLetter, Email Marketing — fully self-hosted, dev-friendly, and free from monthly fees. Join the discord: https://discord.gg/asfXzBUhZr",
        "---",
        "BillionMail 📧\nAn Open-Source MailServer, NewsLetter, Email Marketing Solution for Smarter Campaigns\nEnglish |\n简体中文\n|\n日本語\n|\nTürkçe\nWhat is BillionMail?\nBillionMail is a\nfuture open-source Mail server, Email marketing platform\ndesigned to help businesses and individuals manage their email campaigns with ease. Whether you're sending newsletters, promotional emails, or transactional messages, this tool will provide\nfull control\nover your email marketing efforts. With features like\nadvanced analytics\n, and\ncustomer management\n, you'll be able to create, send, and track emails like a pro.\nJust 3 steps to send a billion emails!\nBillion emails. Any business. Guaranteed.\nStep 1️⃣ Install BillionMail:\n✅ It takes\nonly 8️⃣ minutes\nfrom installation to\n✅ successful email sending\ncd\n/opt\n&&\ngit clone https://github.com/aaPanel/BillionMail\n&&\ncd\nBillionMail\n&&\nbash install.sh\nStep 2️⃣: Connect Your Domain\nAdd the sending domain\nVerify DNS records\nAuto-enable free SSL\nStep 3️⃣: Build Your Campaign\nWrite or paste your email\nChoose list & tags\nSet send time or send now\nWatch on Youtube\nOther installation methods\nOne-click installation on aaPanel\n👉\nhttps://www.aapanel.com/new/download.html\n(Log in to ✅aaPanel --> 🐳Docker --> 1️⃣OneClick install)\nDocker\ncd\n/opt\n&&\ngit clone https://github.com/aaPanel/BillionMail\n&&\ncd\nBillionMail\n&&\ncp env_init .env\n&&\ndocker compose up -d\n||\ndocker-compose up -d\nManagement script\nManagement help\nbm help\nView Login default info\nbm default\nShow domain DNS record\nbm show-record\nUpdate BillionMail\nbm update\nLive Demo\nBillionMail Demo:\nhttps://demo.billionmail.com/billionmail\nUsername:\nbillionmail\nPassword:\nbillionmail\nWebMail\nBillionMail has integrated\nRoundCube\n, you can access WebMail via\n/roundcube/\n.\nWhy BillionMail?\nMost email marketing platforms are either\nexpensive\n,\nclosed-source\n, or\nlack essential features\n. BillionMail aims to be different:\n✅\nFully Open-Source\n– No hidden costs, no vendor lock-in.\n📊\nAdvanced Analytics\n– Track email delivery, open rates, click-through rates, and more.\n📧\nUnlimited Sending\n– No restrictions on the number of emails you can send.\n🎨\nCustomizable Templates\n– Custom professional marketing templates for reuse.\n🔒\nPrivacy-First\n– Your data stays with you, no third-party tracking.\n🚀\nSelf-Hosted\n– Run it on your own server for complete control.\nHow You Can Help 🌟\nBillionMail is a\ncommunity-driven project\n, and we need your support to get started! Here's how you can help:\nStar This Repository\n: Show your interest by starring this repo.\nSpread the Word\n: Share BillionMail with your network—developers, marketers, and open-source enthusiasts.\nShare Feedback\n: Let us know what features you'd like to see in BillionMail by opening an issue or joining the discussion.\nContribute\n: Once development begins, we'll welcome contributions from the community. Stay tuned for updates!\n📧\nBillionMail – The Future of Open-Source Email Marketing.\nIssues\nIf you encounter any issues or have feature requests, please\nopen an issue\n. Be sure to include:\nA clear description of the problem or request.\nSteps to reproduce the issue (if applicable).\nScreenshots or error logs (if applicable).\nInstall Now:\n✅It takes\nonly 8 minutes\nfrom installation to\nsuccessful email sending\ncd\n/opt\n&&\ngit clone https://github.com/aaPanel/BillionMail\n&&\ncd\nBillionMail\n&&\nbash install.sh\nInstall with Docker:\n(Please install Docker and docker-compose-plugin manually, and modify .env file)\ncd\n/opt\n&&\ngit clone https://github.com/aaPanel/BillionMail\n&&\ncd\nBillionMail\n&&\ncp env_init .env\n&&\ndocker compose up -d\n||\ndocker-compose up -d\nStar History\nLicense\nBillionMail is licensed under the\nAGPLv3 License\n. This means you can:\n✅ Use the software for free.\n✅ Modify and distribute the code.\n✅ Use it privately without restrictions.\nSee the\nLICENSE\nfile for more details.",
        "今日の獲得スター数: 65",
        "累積スター数: 11,612"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/aaPanel/BillionMail"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/supermemoryai/supermemory",
      "title": "supermemoryai/supermemory",
      "date": null,
      "executive_summary": [
        "Memory engine and app that is extremely fast, scalable. The Memory API for the AI era.",
        "---",
        "Features\nCore Functionality\nAdd Memories from Any Content\n: Easily add memories from URLs, PDFs, and plain text—just paste, upload, or link.\nChat with Your Memories\n: Converse with your stored content using natural language chat.\nSupermemory MCP Integration\n: Seamlessly connect with all major AI tools (Claude, Cursor, etc.) via Supermemory MCP.\nHow do i use this?\nGo to\napp.supermemory.ai\nand sign into with your account\nStart Adding Memory with your choice of format (Note, Link, File)\nYou can also Connect to your favourite services (Notion, Google Drive, OneDrive)\nOnce Memories are added, you can chat with Supermemory by clicking on \"Open Chat\" and retrieve info from your saved memories\nAdd MCP to your AI Tools (by clicking on \"Connect to your AI\" and select the AI tool you are trying to integrate)\nSupport\nHave questions or feedback? We're here to help:\nEmail:\ndhravya@supermemory.com\nDocumentation:\ndocs.supermemory.ai\nContributing\nWe welcome contributions from developers of all skill levels! Whether you're fixing bugs, adding features, or improving documentation, your help makes supermemory better for everyone.\nQuick Start for Contributors\nFork and clone\nthe repository\nInstall dependencies\nwith\nbun install\nSet up your environment\nby copying\n.env.example\nto\n.env.local\nStart developing\nwith\nbun run dev\nFor detailed guidelines, development setup, coding standards, and the complete contribution workflow, please see our\nContributing Guide\n.\nWays to Contribute\n🐛\nBug fixes\n- Help us squash those pesky issues\n✨\nNew features\n- Add functionality that users will love\n🎨\nUI/UX improvements\n- Make the interface more intuitive\n⚡\nPerformance optimizations\n- Help us make supermemory faster\nCheck out our\nIssues\npage for\ngood first issue\nand\nhelp wanted\nlabels to get started!\nUpdates & Roadmap\nStay up to date with the latest improvements:\nChangelog\nX\n.",
        "今日の獲得スター数: 62",
        "累積スター数: 11,473"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/supermemoryai/supermemory"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/chen08209/FlClash",
      "title": "chen08209/FlClash",
      "date": null,
      "executive_summary": [
        "A multi-platform proxy client based on ClashMeta,simple and easy to use, open-source and ad-free.",
        "---",
        "简体中文\nFlClash\nA multi-platform proxy client based on ClashMeta, simple and easy to use, open-source and ad-free.\non Desktop:\non Mobile:\nFeatures\n✈️\nMulti-platform: Android, Windows, macOS and Linux\n💻 Adaptive multiple screen sizes, Multiple color themes available\n💡 Based on Material You Design,\nSurfboard\n-like UI\n☁️ Supports data sync via WebDAV\n✨ Support subscription link, Dark mode\nUse\nLinux\n⚠️\nMake sure to install the following dependencies before using them\nsudo apt-get install libayatana-appindicator3-dev\n sudo apt-get install libkeybinder-3.0-dev\nAndroid\nSupport the following actions\ncom.follow.clash.action.START\n \n com.follow.clash.action.STOP\n \n com.follow.clash.action.TOGGLE\nDownload\nBuild\nUpdate submodules\ngit submodule update --init --recursive\nInstall\nFlutter\nand\nGolang\nenvironment\nBuild Application\nandroid\nInstall\nAndroid SDK\n,\nAndroid NDK\nSet\nANDROID_NDK\nenvironment variables\nRun Build script\ndart .\n\\s\netup.dart android\nwindows\nYou need a windows client\nInstall\nGcc\n，\nInno Setup\nRun build script\ndart .\n\\s\netup.dart windows --arch\n<\narm64\n|\namd\n64>\nlinux\nYou need a linux client\nRun build script\ndart .\n\\s\netup.dart linux --arch\n<\narm64\n|\namd\n64>\nmacOS\nYou need a macOS client\nRun build script\ndart .\n\\s\netup.dart macos --arch\n<\narm64\n|\namd\n64>\nStar\nThe easiest way to support developers is to click on the star (⭐) at the top of the page.",
        "今日の獲得スター数: 61",
        "累積スター数: 23,044"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/chen08209/FlClash"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/jiji262/douyin-downloader",
      "title": "jiji262/douyin-downloader",
      "date": null,
      "executive_summary": [
        "抖音批量下载工具，去水印，支持视频、图集、合集、音乐(原声)。免费！免费！免费！",
        "---",
        "抖音下载器 - 无水印批量下载工具\n一个功能强大的抖音内容批量下载工具，支持视频、图集、音乐、直播等多种内容类型的下载。提供两个版本：V1.0（稳定版）和 V2.0（增强版）。\n📋 目录\n快速开始\n版本说明\nV1.0 使用指南\nV2.0 使用指南\nCookie 配置工具\n支持的链接类型\n常见问题\n更新日志\n⚡ 快速开始\n环境要求\nPython 3.9+\n操作系统\n：Windows、macOS、Linux\n安装步骤\n克隆项目\ngit clone https://github.com/jiji262/douyin-downloader.git\ncd\ndouyin-downloader\n安装依赖\npip install -r requirements.txt\n配置 Cookie\n（首次使用需要）\n#\n方式1：自动获取（推荐）\npython cookie_extractor.py\n#\n方式2：手动获取\npython get_cookies_manual.py\n📦 版本说明\nV1.0 (DouYinCommand.py) - 稳定版\n✅\n经过验证\n：稳定可靠，经过大量测试\n✅\n简单易用\n：配置文件驱动，使用简单\n✅\n功能完整\n：支持所有内容类型下载\n✅\n单个视频下载\n：完全正常工作\n⚠️\n需要手动配置\n：需要手动获取和配置 Cookie\nV2.0 (downloader.py) - 增强版\n🚀\n自动 Cookie 管理\n：支持自动获取和刷新 Cookie\n🚀\n统一入口\n：整合所有功能到单一脚本\n🚀\n异步架构\n：性能更优，支持并发下载\n🚀\n智能重试\n：自动重试和错误恢复\n🚀\n增量下载\n：支持增量更新，避免重复下载\n⚠️\n单个视频下载\n：目前 API 返回空响应（已知问题）\n✅\n用户主页下载\n：完全正常工作\n🎯 V1.0 使用指南\n配置文件设置\n编辑配置文件\ncp config.example.yml config.yml\n#\n编辑 config.yml 文件\n配置示例\n#\n下载链接\nlink\n:\n  -\nhttps://v.douyin.com/xxxxx/\n#\n单个视频\n-\nhttps://www.douyin.com/user/xxxxx\n#\n用户主页\n-\nhttps://www.douyin.com/collection/xxxxx\n#\n合集\n#\n保存路径\npath\n:\n./Downloaded/\n#\nCookie配置（必填）\ncookies\n:\nmsToken\n:\nYOUR_MS_TOKEN_HERE\nttwid\n:\nYOUR_TTWID_HERE\nodin_tt\n:\nYOUR_ODIN_TT_HERE\npassport_csrf_token\n:\nYOUR_PASSPORT_CSRF_TOKEN_HERE\nsid_guard\n:\nYOUR_SID_GUARD_HERE\n#\n下载选项\nmusic\n:\nTrue\n#\n下载音乐\ncover\n:\nTrue\n#\n下载封面\navatar\n:\nTrue\n#\n下载头像\njson\n:\nTrue\n#\n保存JSON数据\n#\n下载模式\nmode\n:\n  -\npost\n#\n下载发布的作品\n#\n- like     # 下载喜欢的作品\n#\n- mix      # 下载合集\n#\n下载数量（0表示全部）\nnumber\n:\npost\n:\n0\n#\n发布作品数量\nlike\n:\n0\n#\n喜欢作品数量\nallmix\n:\n0\n#\n合集数量\nmix\n:\n0\n#\n单个合集内作品数量\n#\n其他设置\nthread\n:\n5\n#\n下载线程数\ndatabase\n:\nTrue\n#\n使用数据库记录\n运行程序\n#\n使用配置文件运行\npython DouYinCommand.py\n#\n或者使用命令行参数\npython DouYinCommand.py --cmd False\n使用示例\n#\n下载单个视频\n#\n在 config.yml 中设置 link 为单个视频链接\npython DouYinCommand.py\n#\n下载用户主页\n#\n在 config.yml 中设置 link 为用户主页链接\npython DouYinCommand.py\n#\n下载合集\n#\n在 config.yml 中设置 link 为合集链接\npython DouYinCommand.py\n🚀 V2.0 使用指南\n命令行使用\n#\n下载单个视频（需要先配置 Cookie）\npython downloader.py -u\n\"\nhttps://v.douyin.com/xxxxx/\n\"\n#\n下载用户主页（推荐）\npython downloader.py -u\n\"\nhttps://www.douyin.com/user/xxxxx\n\"\n#\n自动获取 Cookie 并下载\npython downloader.py --auto-cookie -u\n\"\nhttps://www.douyin.com/user/xxxxx\n\"\n#\n指定保存路径\npython downloader.py -u\n\"\n链接\n\"\n--path\n\"\n./my_videos/\n\"\n#\n使用配置文件\npython downloader.py --config\n配置文件使用\n创建配置文件\ncp config.example.yml config_simple.yml\n配置示例\n#\n下载链接\nlink\n:\n  -\nhttps://www.douyin.com/user/xxxxx\n#\n保存路径\npath\n:\n./Downloaded/\n#\n自动 Cookie 管理\nauto_cookie\n:\ntrue\n#\n下载选项\nmusic\n:\ntrue\ncover\n:\ntrue\navatar\n:\ntrue\njson\n:\ntrue\n#\n下载模式\nmode\n:\n  -\npost\n#\n下载数量\nnumber\n:\npost\n:\n10\n#\n增量下载\nincrease\n:\npost\n:\nfalse\n#\n数据库\ndatabase\n:\ntrue\n运行程序\npython downloader.py --config\n命令行参数\npython downloader.py [选项] [链接...]\n\n选项：\n  -u, --url URL          下载链接\n  -p, --path PATH        保存路径\n  -c, --config           使用配置文件\n  --auto-cookie          自动获取 Cookie\n  --cookies COOKIES      手动指定 Cookie\n  -h, --help            显示帮助信息\n🍪 Cookie 配置工具\n1. cookie_extractor.py - 自动获取工具\n功能\n：使用 Playwright 自动打开浏览器，自动获取 Cookie\n使用方式\n：\n#\n安装 Playwright\npip install playwright\nplaywright install chromium\n#\n运行自动获取\npython cookie_extractor.py\n特点\n：\n✅ 自动打开浏览器\n✅ 支持扫码登录\n✅ 自动检测登录状态\n✅ 自动保存到配置文件\n✅ 支持多种登录方式\n使用步骤\n：\n运行\npython cookie_extractor.py\n选择提取方式（推荐选择1）\n在打开的浏览器中完成登录\n程序自动提取并保存 Cookie\n2. get_cookies_manual.py - 手动获取工具\n功能\n：通过浏览器开发者工具手动获取 Cookie\n使用方式\n：\npython get_cookies_manual.py\n特点\n：\n✅ 无需安装 Playwright\n✅ 详细的操作教程\n✅ 支持 Cookie 验证\n✅ 自动保存到配置文件\n✅ 支持备份和恢复\n使用步骤\n：\n运行\npython get_cookies_manual.py\n选择\"获取新的Cookie\"\n按照教程在浏览器中获取 Cookie\n粘贴 Cookie 内容\n程序自动解析并保存\nCookie 获取教程\n方法一：浏览器开发者工具\n打开浏览器，访问\n抖音网页版\n登录你的抖音账号\n按\nF12\n打开开发者工具\n切换到\nNetwork\n标签页\n刷新页面，找到任意请求\n在请求头中找到\nCookie\n字段\n复制以下关键 cookie 值：\nmsToken\nttwid\nodin_tt\npassport_csrf_token\nsid_guard\n方法二：使用自动工具\n#\n推荐使用自动工具\npython cookie_extractor.py\n📋 支持的链接类型\n🎬 视频内容\n单个视频分享链接\n：\nhttps://v.douyin.com/xxxxx/\n单个视频直链\n：\nhttps://www.douyin.com/video/xxxxx\n图集作品\n：\nhttps://www.douyin.com/note/xxxxx\n👤 用户内容\n用户主页\n：\nhttps://www.douyin.com/user/xxxxx\n支持下载用户发布的所有作品\n支持下载用户喜欢的作品（需要权限）\n📚 合集内容\n用户合集\n：\nhttps://www.douyin.com/collection/xxxxx\n音乐合集\n：\nhttps://www.douyin.com/music/xxxxx\n🔴 直播内容\n直播间\n：\nhttps://live.douyin.com/xxxxx\n🔧 常见问题\nQ: 为什么单个视频下载失败？\nA\n:\nV1.0：请检查 Cookie 是否有效，确保包含必要的字段\nV2.0：目前已知问题，API 返回空响应，建议使用用户主页下载\nQ: Cookie 过期怎么办？\nA\n:\n使用\npython cookie_extractor.py\n重新获取\n或使用\npython get_cookies_manual.py\n手动获取\nQ: 下载速度慢怎么办？\nA\n:\n调整\nthread\n参数增加并发数\n检查网络连接\n避免同时下载过多内容\nQ: 如何批量下载？\nA\n:\nV1.0：在\nconfig.yml\n中添加多个链接\nV2.0：使用命令行传入多个链接或使用配置文件\nQ: 支持哪些格式？\nA\n:\n视频：MP4 格式（无水印）\n图片：JPG 格式\n音频：MP3 格式\n数据：JSON 格式\n📝 更新日志\nV2.0 (2025-08)\n✅\n统一入口\n：整合所有功能到\ndownloader.py\n✅\n自动 Cookie 管理\n：支持自动获取和刷新\n✅\n异步架构\n：性能优化，支持并发下载\n✅\n智能重试\n：自动重试和错误恢复\n✅\n增量下载\n：支持增量更新\n✅\n用户主页下载\n：完全正常工作\n⚠️\n单个视频下载\n：API 返回空响应（已知问题）\nV1.0 (2024-12)\n✅\n稳定可靠\n：经过大量测试验证\n✅\n功能完整\n：支持所有内容类型\n✅\n单个视频下载\n：完全正常工作\n✅\n配置文件驱动\n：简单易用\n✅\n数据库支持\n：记录下载历史\n⚖️ 法律声明\n本项目仅供\n学习交流\n使用\n请遵守相关法律法规和平台服务条款\n不得用于商业用途或侵犯他人权益\n下载内容请尊重原作者版权\n🤝 贡献指南\n欢迎提交 Issue 和 Pull Request！\n报告问题\n使用\nIssues\n报告 bug\n请提供详细的错误信息和复现步骤\n功能建议\n在 Issues 中提出新功能建议\n详细描述功能需求和使用场景\n📄 许可证\n本项目采用\nMIT License\n开源许可证。\n如果这个项目对你有帮助，请给个 ⭐ Star 支持一下！\n🐛 报告问题\n•\n💡 功能建议\n•\n📖 查看文档\nMade with ❤️ by\njiji262",
        "今日の獲得スター数: 60",
        "累積スター数: 5,265"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/jiji262/douyin-downloader"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/wmjordan/PDFPatcher",
      "title": "wmjordan/PDFPatcher",
      "date": null,
      "executive_summary": [
        "PDF补丁丁——PDF工具箱，可以编辑书签、剪裁旋转页面、解除限制、提取或合并文档，探查文档结构，提取图片、转成图片等等",
        "---",
        "PDF 补丁丁（PDFPatcher）\n感谢您关注 PDF 补丁丁，请在使用软件或源代码前阅读本说明和授权协议。本软件及源代码采用 AGPL＋“\n良心授权\n”协议——\n用户每次使用本软件后如有所获益，应行一善事；如使用源代码开发了新的软件并获得收益，应将收益中不低于千分之一的金额捐赠给社会的弱势群体\n。\n功能简介\nPDF 补丁丁是一个 PDF 处理工具。它具有以下功能：\n修改 PDF 文档：修改文档属性、页码编号、页面链接；统一页面尺寸；删除自动打开网页等动作；去除复制及打印限制；设置阅读器初始模式；清理文档隐藏垃圾数据；重新压缩黑白图片；旋转页面。\n贴心 PDF 书签编辑器：带有阅读界面（具有便于阅读竖排文档的从右到左阅读方式），可批量修改 PDF 书签属性（颜色、样式、目标页码、缩放比例等），书签可精确定位到页面中间；在书签中执行查找替换（支持正则表达式及 XPath 匹配、可快速选择篇、章、节书签），\n自动快速生成文档书签\n。\n制作 PDF 文件：合并已有 PDF 文件或图片，生成新的 PDF 文件；合并后的 PDF 文档带有原文档的书签，还可挂上新书签（或根据文件名生成），新书签文本和样式可自定义；合并的 PDF 文档可指定统一的页面尺寸，以便打印和阅读。\n拆分或合并 PDF 文件，并保留原文件的书签或挂上新的书签。\n高速无损导出 PDF 文档的图片。\n将 PDF 页面转换为图片。\n提取或删除 PDF 文档中指定的页面，调整 PDF 文档的页面顺序。\n根据 PDF 文档元数据重命名 PDF 文件名。\n调用微软 Office 的图像识别引擎分析 PDF 文档图片中的文字；将图片 PDF 的目录页转换为 PDF 书签。识别结果可写入 PDF 文件。\n替换字体：替换文档中使用的字体；嵌入字库到 PDF 文档，消除复制文本时的乱码，使之可在没有字库的设备（如 Kindle 等电子书阅读器）上阅读。\n分析文档结构：以树视图显示 PDF 文档结构，可编辑修改 PDF 文档节点，或将 PDF 文档导出成 XML 文件，供 PDF 爱好者分析、调试之用。\n永久免费，绝不过期，无广告，无弹出废话对话框，不窥探隐私。\n授权协议\n《PDF 补丁丁》软件（以下简称本软件）受著作权法及国际条约条款和其它知识产权法及条约的保护。\n本软件对于最终用户免费。由于本软件使用了带有 AGPL 条款的第三方开源组件，因此，本软件及其源代码的使用协议也基于 AGPL。另外还带有如下附加条件。在遵守本软件的前提条件下，你可以在遵循本协议的基础上自由的使用和传播它，你一旦安装、复制或使用本软件，则表示您已经同意本协议条款。如果你不同意本协议，请不要安装使用本软件，也不应利用其源代码。\n附加条件：\n每一个使用本软件的用户，如果本软件帮助了您，每使用本软件后，您应当做 1 件善事。善事无分大小，有心则行。例如：\n如果您的父母在身边，你可以为您的父母做一顿美味的饭菜，或者为他们按摩、洗脚；如果他们身处远方，你可以向他们发起通话，问候他们的健康和生活。\n在大雨滂沱的时候，如果您有雨伞，可与同路的人共享；在烈日当空的时节，如果您看到环卫工人太阳下工作，您可以为他们买一瓶水送给他们；在拥挤的公共交通工具上，或在公共场合排队等候之际，如果您有座位，可以让给老人、孕妇或提着重物的人就坐。\n您可以用您擅长的技能，为身边的人排难解困；您可以将您的知识，分享给其他人，让他们有所获益；您可以向比您困难的人捐资赠物。\n如果您觉得这个软件真的好用，请将它的使用方法介绍给别人，让别人也通过使用本软件而得到好处；或者将其它您觉得好用的软件介绍给别人。\n如果您无法做到使用本软件后做 1 件善事，请记在心中。在有机会的时候，多行善积德。本用户协议之遵循与否，全在于您的良心。是为“\n良心授权\n”。\n相关定义：\n软件：软件是指《PDF 补丁丁》软件以及它的更新、产品手册，以及在线文档等相关载体。\n限制：你可以使用本软件的源代码开发应用程序（自由、共享或商用），也可以任意方式分发数量不限的本软件的完整拷贝，但前提是：\n① 你分发软件时必须提供本软件的完整版本，未经许可不得对软件乃至它的安装程序做任何修改；\n② 你分发软件时不能更改本授权协议；\n③ 你如果在商业性宣传活动、产品中附加本软件，应当获得著作权人的书面许可；\n④ 你如果利用本软件的源代码编写了其它软件，并且产生了销售收入，应当将该软件销售收入不低于千分之一的金额捐献给社会上的弱势群体。\n支持：软件会由于用户的需求而不断更新，著作权人将提供包括用户手册、电子邮件等各种相关信息支持，但软件不确保支持内容和功能不发生变更。\n终止：当你不同意或者违背本协议的时候，协议将自动终止，你必须立即删除本软件产品。\n版权：本软件及源代码受著作权法及国际条约条款和其它知识产权法及条约的保护。\n免责：对于本软件安装、复制、使用中导致的任何损失，本软件及著作权人不负责任。\n常用的 PDF 开源组件简介\nPDF 文档的规范（ISO 32000-1:2008 《Document management — Portable document format — Part 1:PDF 1.7》）可从网上找到，一般来说，它是 PDF 处理程序开发者的必读文献。\nPDF 文档格式中涉及印刷领域的多项技术，并有其独特的文档结构，还使用了多种数据压缩算法。要从零开始编写 PDF 文档的处理程序，对于一般人而言，通常是困难而不太现实的。PDF 补丁丁使用 .NET Framework 开发，主要采用 iText 和 MuPDF 这两个开放源代码的组件库来处理 PDF 文档。\n前者是 .NET 组件，与 PDF 主程序具有较好的互操作性，并且在解析、生成和修改 PDF 文档，以及嵌入 TTF 字体子集这些功能上，优胜于后者。\n后者采用 C 语言开发并编译，与前者相比，其最大的优点是具有渲染 PDF 文档为位图的功能。MuPDF 编译出来的动态组件库可在作者另一个开放源代码库\nSharpMuPDF\n下载。PDF 补丁丁通过 P/Invoke 技术调用该组件库的功能。\n除了 PDF 开源组件之外，程序还使用了其它优秀开源组件。例如 ObjectListView 这个强大的列表控件、FreeImage 来读取和解码各种类型的点阵图像文件、Cyotek 的 ImageBox 用于显示渲染好的 PDF 文档页面、TabControlExtra 用于构建选项卡式文档界面、HTMLRenderer 用于显示 HTML 网页界面等等。\n源代码的结构\nApp 目录：PDF 补丁丁主程序\nCommon：一些常用的工具类\nFunctions：用于呈现软件各类功能的窗体和控件\nLib：程序使用的第三方组件\nModel：编辑文档时所用的高级模型（基础数据模型由 iText 和 MuPDF 的类实现）\nOptions：程序的选项\nProcessor：处理 PDF 文档的算法（其中 Mupdf 目录里放置了 P/Invoke 调用 MuPDF 的类）\ndoc 目录：放置程序的使用文档\nJBig2 目录：放置 JBIG2 图像的编码和解码库代码\n运行环境\nWindows 7 以上版本的操作系统。\n.NET Framework 4.0 到 4.8 版本。\n使用文字识别功能需要安装 Microsoft Office 2003（或 2007）的 Document Imaging 组件（MODI）。\n编译程序源代码，建议使用 Visual Studio 2022 或更新版本，并安装“.NET 桌面开发”（用于编译 PDF 补丁丁源代码）和“C++ 桌面开发”（用于编译 JBIG2 编码组件）两个工作负载。可能会遇到项目“面向不再受支持的 .NET Framework”、需要“将目标更新为 .NET Framework 4.8”的问题。简单方法是将目标更新为 .NET Framework 4.8，如不更新目标，请参考\n这篇文章介绍的方法\n。\n联系作者\n除第三方组件外，本软件的源代码完全开放：\nhttps://github.com/wmjordan/PDFPatcher\nhttps://gitee.com/wmjordan/pdfpatcher\n建议通过开放源代码网站通过提交 issue 的方式提交您的建议或需求。因日常工作繁忙，暂不提供加 QQ 或微信咨询的服务，敬请谅解。\n在邮件或消息中，请注明你的版本号，附上截图和附件，详细说明你遇到的问题。\n如遇到需要提供附件的情况，请把它搞小一点。一般情况下，最好不要发送超过 10M 的附件。\n对于 PDF 文件，可用“提取页面”功能提取有代表性的页面。\n对于图片文件，请压缩源文件，或提供有代表性的一两页图片。",
        "今日の獲得スター数: 56",
        "累積スター数: 10,561"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/wmjordan/PDFPatcher"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/Mentra-Community/MentraOS",
      "title": "Mentra-Community/MentraOS",
      "date": null,
      "executive_summary": [
        "The open-source OS for smart glasses with dozens of apps. Get captions, AI assistant, notifications, translation, and more. Devs now write 1 app that runs on any pair of smart glases.",
        "---",
        "MentraOS\nThe open source operating system for smart glasses\nWebsite\n•\nDocumentation\n•\nDeveloper Console\n•\nMentra Store\nSupported Smart Glasses\nWorks with Even Realities G1, Mentra Mach 1, Mentra Live. See\nsmart glasses compatibility list here\n.\nApps on Mentra Store\nThe Mentra Store already has a ton of useful apps that real users are running everyday. Here are some apps already published by developers on the Mentra Store:\nLive Captions\nLink\nMerge\nNotes\nCalendar\nDash\nTranslation\n→ Browse All Apps\nWrite Once, Run on Any Smart Glasses\nMentraOS is how developers build smart glasses apps.\nWe handle the pairing, connection, data streaming, and cross-compatibility, so you can focus on creating amazing apps. Every component is 100% open source (MIT license).\nWhy Build with MentraOS?\nCross Compatibility\n: Your app runs on any pair of smart glasses\nSpeed\n: TypeScript SDK means you're making apps in minutes, not months\nControl\n: Access smart glasses I/O - displays, microphones, cameras, speakers\nDistribution\n: Get your app in front of everyone using smart glasses\nMentraOS Community\nThe MentraOS Community is a group of developers, companies, and users dedicated to ensuring the next personal computer is open, cross-compatible, and user-controlled. That's why we're building MentraOS.\nTo get involved, join the\nMentraOS Community Discord server\n.\nContact\nHave questions or ideas? We'd love to hear from you!\nEmail\n:\nteam@mentra.glass\nDiscord\n:\nJoin our community\nTwitter\n:\nFollow @mentralabs\nContributing\nMentraOS is made by a community and we welcome PRs. Here's the Contributors Guide:\ndocs.mentra.glass/contributing\nLicense\nMIT License Copyright 2025 MentraOS Community\n© 2025 Mentra Labs",
        "今日の獲得スター数: 55",
        "累積スター数: 1,316"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/Mentra-Community/MentraOS"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/rustfs/rustfs",
      "title": "rustfs/rustfs",
      "date": null,
      "executive_summary": [
        "🚀 High-performance distributed object storage for MinIO alternative.",
        "---",
        "RustFS is a high-performance distributed object storage software built using Rust\nGetting Started\n·\nDocs\n·\nBug reports\n·\nDiscussions\nEnglish |\n简体中文\n|\nDeutsch\n|\nEspañol\n|\nfrançais\n|\n日本語\n|\n한국어\n|\nPortuguês\n|\nРусский\nRustFS is a high-performance distributed object storage software built using Rust, one of the most popular languages worldwide. Along with MinIO, it shares a range of advantages such as simplicity, S3 compatibility, open-source nature, support for data lakes, AI, and big data. Furthermore, it has a better and more user-friendly open-source license in comparison to other storage systems, being constructed under the Apache license. As Rust serves as its foundation, RustFS provides faster speed and safer distributed features for high-performance object storage.\n⚠️\nRustFS is under rapid development. Do NOT use in production environments!\nFeatures\nHigh Performance\n: Built with Rust, ensuring speed and efficiency.\nDistributed Architecture\n: Scalable and fault-tolerant design for large-scale deployments.\nS3 Compatibility\n: Seamless integration with existing S3-compatible applications.\nData Lake Support\n: Optimized for big data and AI workloads.\nOpen Source\n: Licensed under Apache 2.0, encouraging community contributions and transparency.\nUser-Friendly\n: Designed with simplicity in mind, making it easy to deploy and manage.\nRustFS vs MinIO\nStress test server parameters\nType\nparameter\nRemark\nCPU\n2 Core\nIntel Xeon(Sapphire Rapids) Platinum 8475B , 2.7/3.2 GHz\nMemory\n4GB\nNetwork\n15Gbp\nDriver\n40GB x 4\nIOPS 3800 / Driver\nrustfs.mp4\nRustFS vs Other object storage\nRustFS\nOther object storage\nPowerful Console\nSimple and useless Console\nDeveloped based on Rust language, memory is safer\nDeveloped in Go or C, with potential issues like memory GC/leaks\nDoes not report logs to third-party countries\nReporting logs to other third countries may violate national security laws\nLicensed under Apache, more business-friendly\nAGPL V3 License and other License, polluted open source and License traps, infringement of intellectual property rights\nComprehensive S3 support, works with domestic and international cloud providers\nFull support for S3, but no local cloud vendor support\nRust-based development, strong support for secure and innovative devices\nPoor support for edge gateways and secure innovative devices\nStable commercial prices, free community support\nHigh pricing, with costs up to $250,000 for 1PiB\nNo risk\nIntellectual property risks and risks of prohibited uses\nQuickstart\nTo get started with RustFS, follow these steps:\nOne-click installation script (Option 1)​​\ncurl -O  https://rustfs.com/install_rustfs.sh\n&&\nbash install_rustfs.sh\nDocker Quick Start (Option 2)​​\n#\ncreate data and logs directories\nmkdir -p data logs\n#\nusing latest alpha version\ndocker run -d -p 9000:9000 -v\n$(\npwd\n)\n/data:/data -v\n$(\npwd\n)\n/logs:/logs rustfs/rustfs:alpha\n#\nSpecific version\ndocker run -d -p 9000:9000 -v\n$(\npwd\n)\n/data:/data -v\n$(\npwd\n)\n/logs:/logs rustfs/rustfs:1.0.0.alpha.45\nFor docker installation, you can also run the container with docker compose. With the\ndocker-compose.yml\nfile under root directory, running the command:\ndocker compose --profile observability up -d\nNOTE\n: You should be better to have a look for\ndocker-compose.yaml\nfile. Because, several services contains in the file. Grafan,prometheus,jaeger containers will be launched using docker compose file, which is helpful for rustfs observability. If you want to start redis as well as nginx container, you can specify the corresponding profiles.\nBuild from Source (Option 3) - Advanced Users\nFor developers who want to build RustFS Docker images from source with multi-architecture support:\n#\nBuild multi-architecture images locally\n./docker-buildx.sh --build-arg RELEASE=latest\n#\nBuild and push to registry\n./docker-buildx.sh --push\n#\nBuild specific version\n./docker-buildx.sh --release v1.0.0 --push\n#\nBuild for custom registry\n./docker-buildx.sh --registry your-registry.com --namespace yourname --push\nThe\ndocker-buildx.sh\nscript supports:\nMulti-architecture builds\n:\nlinux/amd64\n,\nlinux/arm64\nAutomatic version detection\n: Uses git tags or commit hashes\nRegistry flexibility\n: Supports Docker Hub, GitHub Container Registry, etc.\nBuild optimization\n: Includes caching and parallel builds\nYou can also use Make targets for convenience:\nmake docker-buildx\n#\nBuild locally\nmake docker-buildx-push\n#\nBuild and push\nmake docker-buildx-version VERSION=v1.0.0\n#\nBuild specific version\nmake help-docker\n#\nShow all Docker-related commands\nAccess the Console\n: Open your web browser and navigate to\nhttp://localhost:9000\nto access the RustFS console, default username and password is\nrustfsadmin\n.\nCreate a Bucket\n: Use the console to create a new bucket for your objects.\nUpload Objects\n: You can upload files directly through the console or use S3-compatible APIs to interact with your RustFS instance.\nNOTE\n: If you want to access RustFS instance with\nhttps\n, you can refer to\nTLS configuration docs\n.\nDocumentation\nFor detailed documentation, including configuration options, API references, and advanced usage, please visit our\nDocumentation\n.\nGetting Help\nIf you have any questions or need assistance, you can:\nCheck the\nFAQ\nfor common issues and solutions.\nJoin our\nGitHub Discussions\nto ask questions and share your experiences.\nOpen an issue on our\nGitHub Issues\npage for bug reports or feature requests.\nLinks\nDocumentation\n- The manual you should read\nChangelog\n- What we broke and fixed\nGitHub Discussions\n- Where the community lives\nContact\nBugs\n:\nGitHub Issues\nBusiness\n:\nhello@rustfs.com\nJobs\n:\njobs@rustfs.com\nGeneral Discussion\n:\nGitHub Discussions\nContributing\n:\nCONTRIBUTING.md\nContributors\nRustFS is a community-driven project, and we appreciate all contributions. Check out the\nContributors\npage to see the amazing people who have helped make RustFS better.\nLicense\nApache 2.0\nRustFS\nis a trademark of RustFS, Inc. All other trademarks are the property of their respective owners.",
        "今日の獲得スター数: 52",
        "累積スター数: 8,877"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/rustfs/rustfs"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/krahets/hello-algo",
      "title": "krahets/hello-algo",
      "date": null,
      "executive_summary": [
        "《Hello 算法》：动画图解、一键运行的数据结构与算法教程。支持 Python, Java, C++, C, C#, JS, Go, Swift, Rust, Ruby, Kotlin, TS, Dart 代码。简体版和繁体版同步更新，English version in translation",
        "---",
        "动画图解、一键运行的数据结构与算法教程\n简体中文\n  ｜\n繁體中文\n｜\nEnglish\n关于本书\n本项目旨在打造一本开源免费、新手友好的数据结构与算法入门教程。\n全书采用动画图解，内容清晰易懂、学习曲线平滑，引导初学者探索数据结构与算法的知识地图。\n源代码可一键运行，帮助读者在练习中提升编程技能，了解算法工作原理和数据结构底层实现。\n提倡读者互助学习，欢迎大家在评论区提出问题与分享见解，在交流讨论中共同进步。\n若本书对您有所帮助，请在页面右上角点个 Star ⭐ 支持一下，谢谢！\n推荐语\n“一本通俗易懂的数据结构与算法入门书，引导读者手脑并用地学习，强烈推荐算法初学者阅读。”\n—— 邓俊辉，清华大学计算机系教授\n“如果我当年学数据结构与算法的时候有《Hello 算法》，学起来应该会简单 10 倍！”\n—— 李沐，亚马逊资深首席科学家\n鸣谢\nWarp is built for coding with multiple AI agents.\n强烈推荐 Warp 终端，高颜值 + 好用的 AI，体验非常棒！\n贡献\n本开源书仍在持续更新之中，欢迎您参与本项目，一同为读者提供更优质的学习内容。\n内容修正\n：请您协助修正或在评论区指出语法错误、内容缺失、文字歧义、无效链接或代码 bug 等问题。\n代码转译\n：期待您贡献各种语言代码，已支持 Python、Java、C++、Go、JavaScript 等 12 门编程语言。\n中译英\n：诚邀您加入我们的翻译小组，成员主要来自计算机相关专业、英语专业和英文母语者。\n欢迎您提出宝贵意见和建议，如有任何问题请提交 Issues 或微信联系\nkrahets-jyd\n。\n感谢本开源书的每一位撰稿人，是他们的无私奉献让这本书变得更好，他们是：\nLicense\nThe texts, code, images, photos, and videos in this repository are licensed under\nCC BY-NC-SA 4.0\n.",
        "今日の獲得スター数: 51",
        "累積スター数: 117,629"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/krahets/hello-algo"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/coollabsio/coolify",
      "title": "coollabsio/coolify",
      "date": null,
      "executive_summary": [
        "An open-source & self-hostable Heroku / Netlify / Vercel alternative.",
        "---",
        "About the Project\nCoolify is an open-source & self-hostable alternative to Heroku / Netlify / Vercel / etc.\nIt helps you manage your servers, applications, and databases on your own hardware; you only need an SSH connection. You can manage VPS, Bare Metal, Raspberry PIs, and anything else.\nImagine having the ease of a cloud but with your own servers. That is\nCoolify\n.\nNo vendor lock-in, which means that all the configurations for your applications/databases/etc are saved to your server. So, if you decide to stop using Coolify (oh nooo), you could still manage your running resources. You lose the automations and all the magic. 🪄️\nFor more information, take a look at our landing page at\ncoolify.io\n.\nInstallation\ncurl -fsSL https://cdn.coollabs.io/coolify/install.sh\n|\nbash\nYou can find the installation script source\nhere\n.\nNote\nPlease refer to the\ndocs\nfor more information about the installation.\nSupport\nContact us at\ncoolify.io/docs/contact\n.\nCloud\nIf you do not want to self-host Coolify, there is a paid cloud version available:\napp.coolify.io\nFor more information & pricing, take a look at our landing page\ncoolify.io\n.\nWhy should I use the Cloud version?\nThe recommended way to use Coolify is to have one server for Coolify and one (or more) for the resources you are deploying. A server is around 4-5$/month.\nBy subscribing to the cloud version, you get the Coolify server for the same price, but with:\nHigh-availability\nFree email notifications\nBetter support\nLess maintenance for you\nDonations\nTo stay completely free and open-source, with no feature behind the paywall and evolve the project, we need your help. If you like Coolify, please consider donating to help us fund the project's future development.\ncoolify.io/sponsorships\nThank you so much!\nBig Sponsors\nCubePath\n- Dedicated Servers & Instant Deploy\nGlueOps\n- DevOps automation and infrastructure management\nAlgora\n- Open source contribution platform\nUbicloud\n- Open source cloud infrastructure platform\nLiquidWeb\n- Premium managed hosting solutions\nConvex\n- Open-source reactive database for web app developers\nArcjet\n- Advanced web security and performance solutions\nSaasyKit\n- Complete SaaS starter kit for developers\nSupaGuide\n- Your comprehensive guide to Supabase\nLogto\n- The better identity infrastructure for developers\nTrieve\n- AI-powered search and analytics\nSupadata AI\n- Scrape YouTube, web, and files. Get AI-ready, clean data\nDarweb\n- Design. Develop. Deliver. Specialized in 3D CPQ Solutions\nHetzner\n- Server, cloud, hosting, and data center solutions\nCOMIT\n- New York Times award–winning contractor\nBlacksmith\n- Infrastructure automation platform\nWZ-IT\n- German agency for customised cloud solutions\nBC Direct\n- Your trusted technology consulting partner\nTigris\n- Modern developer data platform\nHostinger\n- Web hosting and VPS solutions\nQuantCDN\n- Enterprise-grade content delivery network\nPFGLabs\n- Build Real Projects with Golang\nJobsCollider\n- 30,000+ remote jobs for developers\nJuxtdigital\n- Digital PR & AI Authority Building Agency\nCloudify.ro\n- Cloud hosting solutions\nCodeRabbit\n- Cut Code Review Time & Bugs in Half\nAmerican Cloud\n- US-based cloud infrastructure services\nMassiveGrid\n- Enterprise cloud hosting solutions\nSyntax.fm\n- Podcast for web developers\nTolgee\n- The open source localization platform\nCompAI\n- Open source compliance automation platform\nGoldenVM\n- Premium virtual machine hosting solutions\nGozunga\n- Seriously Simple Cloud Infrastructure\nMacarne\n- Best IP Transit & Carrier Ethernet Solutions for Simplified Network Connectivity\nSmall Sponsors\n...and many more at\nGitHub Sponsors\nRecognitions\nCore Maintainers\nAndras Bacsai\n🏔️ Peak\nRepo Activity\nStar History",
        "今日の獲得スター数: 49",
        "累積スター数: 46,176"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/coollabsio/coolify"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/hyprwm/Hyprland",
      "title": "hyprwm/Hyprland",
      "date": null,
      "executive_summary": [
        "Hyprland is an independent, highly customizable, dynamic tiling Wayland compositor that doesn't sacrifice on its looks.",
        "---",
        "Hyprland is a 100% independent, dynamic tiling Wayland compositor that doesn't sacrifice on its looks.\nIt provides the latest Wayland features, is highly customizable, has all the eyecandy, the most powerful plugins,\neasy IPC, much more QoL stuff than other compositors and more...\nInstall\nQuick Start\nConfigure\nContribute\nFeatures\nAll of the eyecandy: gradient borders, blur, animations, shadows and much more\nA lot of customization\n100% independent, no wlroots, no libweston, no kwin, no mutter.\nCustom bezier curves for the best animations\nPowerful plugin support\nBuilt-in plugin manager\nTearing support for better gaming performance\nEasily expandable and readable codebase\nFast and active development\nNot afraid to provide bleeding-edge features\nConfig reloaded instantly upon saving\nFully dynamic workspaces\nTwo built-in layouts and more available as plugins\nGlobal keybinds passed to your apps of choice\nTiling/pseudotiling/floating/fullscreen windows\nSpecial workspaces (scratchpads)\nWindow groups (tabbed mode)\nPowerful window/monitor/layer rules\nSocket-based IPC\nNative IME and Input Panels Support\nand much more...\nGallery\nSpecial Thanks\nwlroots\n-\nFor powering Hyprland in the past\ntinywl\n-\nFor showing how 2 do stuff\nSway\n-\nFor showing how 2 do stuff the overkill way\nVivarium\n-\nFor showing how 2 do stuff the simple way\ndwl\n-\nFor showing how 2 do stuff the hacky way\nWayfire\n-\nFor showing how 2 do some graphics stuff",
        "今日の獲得スター数: 49",
        "累積スター数: 31,051"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/hyprwm/Hyprland"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/tangyoha/telegram_media_downloader",
      "title": "tangyoha/telegram_media_downloader",
      "date": null,
      "executive_summary": [
        "基于Dineshkarthik的项目， 电报视频下载，电报资源下载，跨平台，支持web查看下载进度 ，支持bot下发指令下载，支持下载已经加入的私有群但是限制下载的资源， telegram media download,Download media files from a telegram conversation/chat/channel up to 2GiB per file",
        "---",
        "Telegram Media Downloader\n中文\n·\nFeature request\n·\nReport a bug\n·\nSupport:\nDiscussions\n&\nTelegram Community\nOverview\nSupport two default running\nThe robot is running, and the command\ndownload\nor\nforward\nis issued from the robot\nDownload as a one-time download tool\nUI\nWeb page\nAfter running, open a browser and visit\nlocalhost:5000\nIf it is a remote machine, you need to configure web_host: 0.0.0.0\nRobot\nNeed to configure bot_token, please refer to\nDocumentation\nSupport\nCategory\nSupport\nLanguage\nPython 3.7\nand above\nDownload media types\naudio, document, photo, video, video_note, voice\nVersion release plan\nv2.2.0\nInstallation\nFor *nix os distributions with\nmake\navailability\ngit clone https://github.com/tangyoha/telegram_media_downloader.git\ncd\ntelegram_media_downloader\nmake install\nFor Windows which doesn't have\nmake\ninbuilt\ngit clone https://github.com/tangyoha/telegram_media_downloader.git\ncd\ntelegram_media_downloader\npip3 install -r requirements.txt\nDocker\nFor more detailed installation tutorial, please check the wiki\nMake sure you have\ndocker\nand\ndocker-compose\ninstalled\ndocker pull tangyoha/telegram_media_downloader:latest\nmkdir -p\n~\n/app\n&&\nmkdir -p\n~\n/app/log/\n&&\ncd\n~\n/app\nwget https://raw.githubusercontent.com/tangyoha/telegram_media_downloader/master/docker-compose.yaml -O docker-compose.yaml\nwget https://raw.githubusercontent.com/tangyoha/telegram_media_downloader/master/config.yaml -O config.yaml\nwget https://raw.githubusercontent.com/tangyoha/telegram_media_downloader/master/data.yaml -O data.yaml\n#\nvi config.yaml and docker-compose.yaml\nvi config.yaml\n#\nThe first time you need to start the foreground\n#\nenter your phone number and code, then exit(ctrl + c)\ndocker-compose run --rm telegram_media_downloader\n#\nAfter performing the above operations, all subsequent startups will start in the background\ndocker-compose up -d\n#\nUpgrade\ndocker pull tangyoha/telegram_media_downloader:latest\ncd\n~\n/app\ndocker-compose down\ndocker-compose up -d\nUpgrade installation\ncd\ntelegram_media_downloader\npip3 install -r requirements.txt\nConfiguration\nAll the configurations are  passed to the Telegram Media Downloader via\nconfig.yaml\nfile.\nGetting your API Keys:\nThe very first step requires you to obtain a valid Telegram API key (API id/hash pair):\nVisit\nhttps://my.telegram.org/apps\nand log in with your Telegram Account.\nFill out the form to register a new Telegram application.\nDone! The API key consists of two parts:\napi_id\nand\napi_hash\n.\nGetting chat id:\n1. Using web telegram:\nOpen\nhttps://web.telegram.org/?legacy=1#/im\nNow go to the chat/channel and you will see the URL as something like\nhttps://web.telegram.org/?legacy=1#/im?p=u853521067_2449618633394\nhere\n853521067\nis the chat id.\nhttps://web.telegram.org/?legacy=1#/im?p=@somename\nhere\nsomename\nis the chat id.\nhttps://web.telegram.org/?legacy=1#/im?p=s1301254321_6925449697188775560\nhere take\n1301254321\nand add\n-100\nto the start of the id =>\n-1001301254321\n.\nhttps://web.telegram.org/?legacy=1#/im?p=c1301254321_6925449697188775560\nhere take\n1301254321\nand add\n-100\nto the start of the id =>\n-1001301254321\n.\n2. Using bot:\nUse\n@username_to_id_bot\nto get the chat_id of\nalmost any telegram user: send username to the bot or just forward their message to the bot\nany chat: send chat username or copy and send its joinchat link to the bot\npublic or private channel: same as chats, just copy and send to the bot\nid of any telegram bot\nconfig.yaml\napi_hash\n:\nyour_api_hash\napi_id\n:\nyour_api_id\nchat\n:\n-\nchat_id\n:\ntelegram_chat_id\nlast_read_message_id\n:\n0\ndownload_filter\n:\nmessage_date >= 2022-12-01 00:00:00 and message_date <= 2023-01-17 00:00:00\n-\nchat_id\n:\ntelegram_chat_id_2\nlast_read_message_id\n:\n0\n#\nnote we remove ids_to_retry to data.yaml\nids_to_retry\n:\n[]\nmedia_types\n:\n-\naudio\n-\ndocument\n-\nphoto\n-\nvideo\n-\nvoice\n-\nanimation\n#\ngif\nfile_formats\n:\naudio\n:\n  -\nall\ndocument\n:\n  -\npdf\n-\nepub\nvideo\n:\n  -\nmp4\nsave_path\n:\nD:\\telegram_media_downloader\nfile_path_prefix\n:\n-\nchat_title\n-\nmedia_datetime\nupload_drive\n:\n#\nrequired\nenable_upload_file\n:\ntrue\n#\nrequired\nremote_dir\n:\ndrive:/telegram\n#\nrequired\nupload_adapter\n:\nrclone\n#\noption,when config upload_adapter rclone then this config are required\nrclone_path\n:\nD:\\rclone\\rclone.exe\n#\noption\nbefore_upload_file_zip\n:\nTrue\n#\noption\nafter_upload_file_delete\n:\nTrue\nhide_file_name\n:\ntrue\nfile_name_prefix\n:\n-\nmessage_id\n-\nfile_name\nfile_name_prefix_split\n:\n'\n-\n'\nmax_download_task\n:\n5\nweb_host\n:\n127.0.0.1\nweb_port\n:\n5000\nlanguage\n:\nEN\nweb_login_secret\n:\n123\nallowed_user_ids\n:\n-\n'\nme\n'\ndate_format\n:\n'\n%Y_%m\n'\nenable_download_txt\n:\nfalse\napi_hash\n- The api_hash you got from telegram apps\napi_id\n- The api_id you got from telegram apps\nbot_token\n- Your bot token\nchat\n- Chat list\nchat_id\n-  The id of the chat/channel you want to download media. Which you get from the above-mentioned steps.\ndownload_filter\n- Download filter, see\nHow to use Filter\nlast_read_message_id\n- If it is the first time you are going to read the channel let it be\n0\nor if you have already used this script to download media it will have some numbers which are auto-updated after the scripts successful execution. Don't change it.\nids_to_retry\n-\nLeave it as it is.\nThis is used by the downloader script to keep track of all skipped downloads so that it can be downloaded during the next execution of the script.\nmedia_types\n- Type of media to download, you can update which type of media you want to download it can be one or any of the available types.\nfile_formats\n- File types to download for supported media types which are\naudio\n,\ndocument\nand\nvideo\n. Default format is\nall\n, downloads all files.\nsave_path\n- The root directory where you want to store downloaded files.\nfile_path_prefix\n- Store file subfolders, the order of the list is not fixed, can be randomly combined.\nchat_title\n- Channel or group title, it will be chat id if not exist title.\nmedia_datetime\n- Media date.\nmedia_type\n- Media type, also see\nmedia_types\n.\nupload_drive\n- You can upload file to cloud drive.\nenable_upload_file\n- Enable upload file, default\nfalse\n.\nremote_dir\n- Where you upload, like\ndrive_id/drive_name\n.\nupload_adapter\n- Upload file adapter, which can be\nrclone\n,\naligo\n. If it is\nrclone\n, it supports all\nrclone\nservers that support uploading. If it is\naligo\n, it supports uploading\nAli cloud disk\n.\nrclone_path\n- RClone exe path, see\nHow to use rclone\nbefore_upload_file_zip\n- Zip file before upload, default\nfalse\n.\nafter_upload_file_delete\n- Delete file after upload success, default\nfalse\n.\nfile_name_prefix\n- Custom file name, use the same as\nfile_path_prefix\nmessage_id\n- Message id\nfile_name\n- File name (may be empty)\ncaption\n- The title of the message (may be empty)\nfile_name_prefix_split\n- Custom file name prefix symbol, the default is\n-\nmax_download_task\n- The maximum number of task download tasks, the default is 5.\nhide_file_name\n- Whether to hide the web interface file name, default\nfalse\nweb_host\n- Web host\nweb_port\n- Web port\nlanguage\n- Application language, the default is English (\nEN\n), optional\nZH\n(Chinese),\nRU\n,\nUA\nweb_login_secret\n- Web page login password, if not configured, no login is required to access the web page\nlog_level\n- see\nlogging._nameToLevel\n.\nforward_limit\n- Limit the number of forwards per minute, the default is 33, please do not modify this parameter by default.\nallowed_user_ids\n- Who is allowed to use the robot? The default login account can be used. Please add single quotes to the name with @.\ndate_format\nSupport custom configuration of media_datetime format in file_path_prefix.see\npython-datetime\nenable_download_txt\nEnable download txt file, default\nfalse\nExecution\npython3 media_downloader.py\nAll downloaded media will be stored at the root of\nsave_path\n.\nThe specific location reference is as follows:\nThe complete directory of video download is:\nsave_path\n/\nchat_title\n/\nmedia_datetime\n/\nmedia_type\n.\nThe order of the list is not fixed and can be randomly combined.\nIf the configuration is empty, all files are saved under\nsave_path\n.\nProxy\nsocks4, socks5, http\nproxies are supported in this project currently. To use it, add the following to the bottom of your\nconfig.yaml\nfile\nproxy\n:\nscheme\n:\nsocks5\nhostname\n:\n127.0.0.1\nport\n:\n1234\nusername\n:\nyour_username(delete the line if none)\npassword\n:\nyour_password(delete the line if none)\nIf your proxy doesn’t require authorization you can omit username and password. Then the proxy will automatically be enabled.\nContributing\nContributing Guidelines\nRead through our\ncontributing guidelines\nto learn about our submission process, coding rules and more.\nWant to Help?\nWant to file a bug, contribute some code, or improve documentation? Excellent! Read up on our guidelines for\ncontributing\n.\nCode of Conduct\nHelp us keep Telegram Media Downloader open and inclusive. Please read and follow our\nCode of Conduct\n.\nSponsor\nPayPal",
        "今日の獲得スター数: 48",
        "累積スター数: 4,112"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/tangyoha/telegram_media_downloader"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/DioxusLabs/dioxus",
      "title": "DioxusLabs/dioxus",
      "date": null,
      "executive_summary": [
        "Fullstack app framework for web, desktop, and mobile.",
        "---",
        "Website\n|\nExamples\n|\nGuide\n|\n中文\n|\nPT-BR\n|\n日本語\n|\nTürkçe\n|\n한국어\n✨ Dioxus 0.7 is in alpha - test it out! ✨\nBuild for web, desktop, and mobile, and more with a single codebase. Zero-config setup, integrated hot-reloading, and signals-based state management. Add backend functionality with Server Functions and bundle with our CLI.\nfn\napp\n(\n)\n->\nElement\n{\nlet\nmut\ncount =\nuse_signal\n(\n||\n0\n)\n;\nrsx\n!\n{\nh1\n{\n\"High-Five counter: {count}\"\n}\nbutton\n{\nonclick\n:\nmove |_| count +=\n1\n,\n\"Up high!\"\n}\nbutton\n{\nonclick\n:\nmove |_| count -=\n1\n,\n\"Down low!\"\n}\n}\n}\n⭐️ Unique features:\nCross-platform apps in three lines of code (web, desktop, mobile, server, and more)\nErgonomic state management\ncombines the best of React, Solid, and Svelte\nBuilt-in featureful, type-safe, fullstack web framework\nIntegrated bundler for deploying to the web, macOS, Linux, and Windows\nSubsecond Rust hot-patching and asset hot-reloading\nAnd more!\nTake a tour of Dioxus\n.\nInstant hot-reloading\nWith one command,\ndx serve\nand your app is running. Edit your markup, styles, and see changes in milliseconds. Use our experimental\ndx serve --hotpatch\nto update Rust code in real time.\nBuild Beautiful Apps\nDioxus apps are styled with HTML and CSS. Use the built-in TailwindCSS support or load your favorite CSS library. Easily call into native code (objective-c, JNI, Web-Sys) for a perfect native touch.\nTruly fullstack applications\nDioxus deeply integrates with\naxum\nto provide powerful fullstack capabilities for both clients and servers. Pick from a wide array of built-in batteries like WebSockets, SSE, Streaming, File Upload/Download, Server-Side-Rendering, Forms, Middleware, and Hot-Reload, or go fully custom and integrate your existing axum backend.\nExperimental Native Renderer\nRender using web-sys, webview, server-side-rendering, liveview, or even with our experimental WGPU-based renderer. Embed Dioxus in Bevy, WGPU, or even run on embedded Linux!\nFirst-party primitive components\nGet started quickly with a complete set of primitives modeled after shadcn/ui and Radix-Primitives.\nFirst-class Android and iOS support\nDioxus is the fastest way to build native mobile apps with Rust. Simply run\ndx serve --platform android\nand your app is running in an emulator or on device in seconds. Call directly into JNI and Native APIs.\nBundle for web, desktop, and mobile\nSimply run\ndx bundle\nand your app will be built and bundled with maximization optimizations. On the web, take advantage of\n.avif\ngeneration,\n.wasm\ncompression, minification\n, and more. Build WebApps weighing\nless than 50kb\nand desktop/mobile apps less than 5mb.\nFantastic documentation\nWe've put a ton of effort into building clean, readable, and comprehensive documentation. All html elements and listeners are documented with MDN docs, and our Docs runs continuous integration with Dioxus itself to ensure that the docs are always up to date. Check out the\nDioxus website\nfor guides, references, recipes, and more. Fun fact: we use the Dioxus website as a testbed for new Dioxus features -\ncheck it out!\nModular and Customizable\nBuild your own renderer, or use a community renderer like\nFreya\n. Use our modular components like RSX, VirtualDom, Blitz, Taffy, and Subsecond.\nCommunity\nDioxus is a community-driven project, with a very active\nDiscord\nand\nGitHub\ncommunity. We're always looking for help, and we're happy to answer questions and help you get started.\nOur SDK\nis community-run and we even have a\nGitHub organization\nfor the best Dioxus crates that receive free upgrades and support.\nFull-time core team\nDioxus has grown from a side project to a small team of fulltime engineers. Thanks to the generous support of FutureWei, Satellite.im, the GitHub Accelerator program, we're able to work on Dioxus full-time. Our long term goal is for Dioxus to become self-sustaining by providing paid high-quality enterprise tools. If your company is interested in adopting Dioxus and would like to work with us, please reach out!\nSupported Platforms\nWeb\nRender directly to the DOM using WebAssembly\nPre-render with SSR and rehydrate on the client\nSimple \"hello world\" at about 50kb, comparable to React\nBuilt-in dev server and hot reloading for quick iteration\nDesktop\nRender using Webview or - experimentally - with WGPU or\nFreya\n(Skia)\nZero-config setup. Simply `cargo run` or `dx serve` to build your app\nFull support for native system access without IPC\nSupports macOS, Linux, and Windows. Portable <3mb binaries\nMobile\nRender using Webview or - experimentally - with WGPU or Skia\nBuild .ipa and .apk files for iOS and Android\nCall directly into Java and Objective-C with minimal overhead\nFrom \"hello world\" to running on device in seconds\nServer-side Rendering\nSuspense, hydration, and server-side rendering\nQuickly drop in backend functionality with server functions\nExtractors, middleware, and routing integrations\nStatic-site generation and incremental regeneration\nRunning the examples\nThe examples in the main branch of this repository target the git version of dioxus and the CLI. If you are looking for examples that work with the latest stable release of dioxus, check out the\n0.6 branch\n.\nThe examples in the top level of this repository can be run with:\ncargo run --example\n<\nexample\n>\nHowever, we encourage you to download the dioxus-cli to test out features like hot-reloading. To install the most recent binary CLI, you can use cargo binstall.\ncargo binstall dioxus-cli@0.7.0-rc.1 --force\nIf this CLI is out-of-date, you can install it directly from git\ncargo install --git https://github.com/DioxusLabs/dioxus dioxus-cli --locked\nWith the CLI, you can also run examples with the web platform. You will need to disable the default desktop feature and enable the web feature with this command:\ndx serve --example\n<\nexample\n>\n--platform web -- --no-default-features\nContributing\nCheck out the website\nsection on contributing\n.\nReport issues on our\nissue tracker\n.\nJoin\nthe discord and ask questions!\nLicense\nThis project is licensed under either the\nMIT license\nor the\nApache-2 License\n.\nUnless you explicitly state otherwise, any contribution intentionally submitted\nfor inclusion in Dioxus by you, shall be licensed as MIT or Apache-2, without any additional\nterms or conditions.",
        "今日の獲得スター数: 47",
        "累積スター数: 31,009"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/DioxusLabs/dioxus"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/WECENG/ticket-purchase",
      "title": "WECENG/ticket-purchase",
      "date": null,
      "executive_summary": [
        "大麦自动抢票，支持人员、城市、日期场次、价格选择",
        "---",
        "大麦抢票脚本 V1.0\n特征\n自动无延时抢票\n支持人员、城市、日期场次、价格选择\n功能介绍\n通过selenium打开页面进行登录，模拟用户购票流程自动购票\n其流程图如下:\n准备工作\n1. 配置环境\n1.1安装python3环境\nWindows\n访问Python官方网站：\nhttps://www.python.org/downloads/windows/\n下载最新的Python 3.9+版本的安装程序。\n运行安装程序。\n在安装程序中，确保勾选 \"Add Python X.X to PATH\" 选项，这将自动将Python添加到系统环境变量中，方便在命令行中使用Python。\n完成安装后，你可以在命令提示符或PowerShell中输入\npython3\n来启动Python解释器。\nmacOS\n你可以使用Homebrew来安装Python 3。\n安装Homebrew（如果未安装）：打开终端并运行以下命令：\n/bin/bash -c\n\"\n$(\ncurl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh\n)\n\"\n安装Python 3：运行以下命令来安装Python 3：\nbrew install python@3\n1.2 安装所需要的环境\n在命令窗口输入如下指令\npip3 install selenium\n1.3 下载google chrome浏览器\n下载地址:\nhttps://www.google.cn/intl/zh-CN/chrome/?brand=YTUH&gclid=Cj0KCQjwj5mpBhDJARIsAOVjBdoV_1sBwdqKGHV3rUU1vJmNKZdy5QNzbRT8F5O0-_jq1WHXurE8a7MaAkWrEALw_wcB&gclsrc=aw.ds\n2. 修改配置文件\n在运行程序之前，需要先修改\nconfig.json\n文件。该文件用于指定用户需要抢票的相关信息，包括演唱会的场次、观演的人员、城市、日期、价格等。文件结果如下图所示：\n2.1 文件内容说明\nindex_url\n为大麦网的地址，\n无需修改\nlogin_url\n为大麦网的登录地址，\n无需修改\ntarget_url\n为用户需要抢的演唱会票的目标地址，\n待修改\nusers\n为观演人的姓名，\n观演人需要用户在手机大麦APP中先填写好，然后再填入该配置文件中\n，\n待修改\ncity\n为城市，\n如果用户需要抢的演唱会票需要选择城市，请把城市填入此处。如无需选择，则不填\ndate\n为场次日期，\n待修改，可多选\nprice\n为票档的价格，\n待修改，可多选\nif_commit_order\n为是否要自动提交订单，\n改成 true\nif_listen为是否回流监听，\n改成true\n2.2 示例说明\n进入大麦网\nhttps://www.damai.cn/，选择你需要抢票的演唱会。假设如下图所示：\n接下来按照下图的标注对配置文件进行修改：\n最终\nconfig.json\n的文件内容如下：\n{\n\"index_url\"\n:\n\"\nhttps://www.damai.cn/\n\"\n,\n\"login_url\"\n:\n\"\nhttps://passport.damai.cn/login?ru=https%3A%2F%2Fwww.damai.cn%2F\n\"\n,\n\"target_url\"\n:\n\"\nhttps://detail.damai.cn/item.htm?spm=a2oeg.home.card_0.ditem_1.591b23e1JQGWHg&id=740680932762\n\"\n,\n\"users\"\n: [\n\"\n名字1\n\"\n,\n\"\n名字2\n\"\n],\n\"city\"\n:\n\"\n广州\n\"\n,\n\"date\"\n:\n\"\n2023-10-28\n\"\n,\n\"price\"\n:\n\"\n1039\n\"\n,\n\"if_listen\"\n:\ntrue\n,\n\"if_commit_order\"\n:\ntrue\n}\n3.运行程序\n运行程序开始抢票，进入命令窗口，执行如下命令：\ncd\ndamai\npython3 damai.py\n大麦app抢票\n大麦app抢票脚本需要依赖appium，因此需要现在安装appium server&client环境，步骤如下：\nappium server\n下载\n先安装好node环境（具备npm）node版本号18.0.0\n先下载并安装好android sdk，并配置环境变量（appium server运行需依赖android sdk)\n下载appium\nnpm install -g appium\n查看appium是否安装成功\nappium -v\n下载UiAutomator2驱动\nnpm install appium-uiautomator2-driver\n​\t\t可能会遇到如下错误：\n➜  xcode git:(master) ✗ npm install appium-uiautomator2-driver\n\nnpm ERR! code 1\nnpm ERR! path /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/appium-chromedriver\nnpm ERR! command failed\nnpm ERR! command sh -c node install-npm.js\nnpm ERR! [11:57:54] Error installing Chromedriver: Request failed with status code 404\nnpm ERR! [11:57:54] AxiosError: Request failed with status code 404\nnpm ERR!     at settle (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/core/settle.js:19:12)\nnpm ERR!     at IncomingMessage.handleStreamEnd (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/adapters/http.js:572:11)\nnpm ERR!     at IncomingMessage.emit (node:events:539:35)\nnpm ERR!     at endReadableNT (node:internal/streams/readable:1344:12)\nnpm ERR!     at processTicksAndRejections (node:internal/process/task_queues:82:21)\nnpm ERR! [11:57:54] Downloading Chromedriver can be skipped by setting the'APPIUM_SKIP_CHROMEDRIVER_INSTALL' environment variable.\n\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     /Users/chenweicheng/.npm/_logs/2023-10-26T03_57_35_950Z-debug-0.log\n​\t\t解决办法（添加环境变量，错误原因是没有找到chrome浏览器驱动，忽略即可）\nexport\nAPPIUM_SKIP_CHROMEDRIVER_INSTALL=true\n启动\n启动appium server并使用uiautomator2驱动\nappium --use-plugins uiautomator2\n启动成功将出现如下信息：\n[Appium] Welcome to Appium v2.2.1 (REV 2176894a5be5da17a362bf3f20678641a78f4b69)\n[Appium] Non-default server args:\n[Appium] {\n[Appium]   usePlugins: [\n[Appium]     'uiautomator2'\n[Appium]   ]\n[Appium] }\n[Appium] Attempting to load driver uiautomator2...\n[Appium] Requiring driver at /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver\n[Appium] Appium REST http interface listener started on http://0.0.0.0:4723\n[Appium] You can provide the following URLs in your client code to connect to this server:\n[Appium] \thttp://127.0.0.1:4723/ (only accessible from the same host)\n[Appium] \thttp://172.31.102.45:4723/\n[Appium] \thttp://198.18.0.1:4723/\n[Appium] Available drivers:\n[Appium]   - uiautomator2@2.32.3 (automationName 'UiAutomator2')\n[Appium] No plugins have been installed. Use the \"appium plugin\" command to install the one(s) you want to use.\n其中\n[Appium] \thttp://127.0.0.1:4723/ (only accessible from the same host) [Appium] \thttp://172.31.102.45:4723/ [Appium] \thttp://198.18.0.1:4723/\n为appium server连接地址\nappium client\n先下载并安装好python3和pip3\n安装\npip3 install appium-python-client\n在代码中引入并使用appium\nfrom\nappium\nimport\nwebdriver\nfrom\nappium\n.\noptions\n.\ncommon\n.\nbase\nimport\nAppiumOptions\ndevice_app_info\n=\nAppiumOptions\n()\ndevice_app_info\n.\nset_capability\n(\n'platformName'\n,\n'Android'\n)\ndevice_app_info\n.\nset_capability\n(\n'platformVersion'\n,\n'10'\n)\ndevice_app_info\n.\nset_capability\n(\n'deviceName'\n,\n'YourDeviceName'\n)\ndevice_app_info\n.\nset_capability\n(\n'appPackage'\n,\n'cn.damai'\n)\ndevice_app_info\n.\nset_capability\n(\n'appActivity'\n,\n'.launcher.splash.SplashMainActivity'\n)\ndevice_app_info\n.\nset_capability\n(\n'unicodeKeyboard'\n,\nTrue\n)\ndevice_app_info\n.\nset_capability\n(\n'resetKeyboard'\n,\nTrue\n)\ndevice_app_info\n.\nset_capability\n(\n'noReset'\n,\nTrue\n)\ndevice_app_info\n.\nset_capability\n(\n'newCommandTimeout'\n,\n6000\n)\ndevice_app_info\n.\nset_capability\n(\n'automationName'\n,\n'UiAutomator2'\n)\n# 连接appium server，server地址查看appium启动信息\ndriver\n=\nwebdriver\n.\nRemote\n(\n'http://127.0.0.1:4723'\n,\noptions\n=\ndevice_app_info\n)\n启动脚本程序\ncd\ndamai_appium\npython3 damai_appium.py",
        "今日の獲得スター数: 46",
        "累積スター数: 4,482"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/WECENG/ticket-purchase"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/coze-dev/coze-studio",
      "title": "coze-dev/coze-studio",
      "date": null,
      "executive_summary": [
        "An AI agent development platform with all-in-one visual tools, simplifying agent creation, debugging, and deployment like never before. Coze your way to AI Agent creation.",
        "---",
        "Coze Studio\n•\nFeature list\n•\nQuickstart\n•\nDeveloper Guide\nEnglish |\n中文\nWhat is Coze Studio?\nCoze Studio\nis an all-in-one AI agent development tool. Providing the latest large models and tools, various development modes and frameworks, Coze Studio offers the most convenient AI agent development environment, from development to deployment.\nProvides all core technologies needed for AI agent development\n: prompt, RAG, plugin, workflow, enabling developers to focus on creating the core value of AI.\nReady to use for professional AI agent development at the lowest cost\n: Coze Studio provides developers with complete app templates and build frameworks, allowing you to quickly construct various AI agents and turn creative ideas into reality.\nCoze Studio, derived from the \"Coze Development Platform\" which has served tens of thousands of enterprises and millions of developers, we have made its core engine completely open. It is a one-stop visual development tool for AI Agents that makes creating, debugging, and deploying AI Agents unprecedentedly simple. Through Coze Studio's visual design and build tools, developers can quickly create and debug agents, apps, and workflows using no-code or low-code approaches, enabling powerful AI app development and more customized business logic. It's an ideal choice for building low-code AI products tailored . Coze Studio aims to lower the threshold for AI agent development and application, encouraging community co-construction and sharing for deeper exploration and practice in the AI field.\nThe backend of Coze Studio is developed using Golang, the frontend uses React + TypeScript, and the overall architecture is based on microservices and built following domain-driven design (DDD) principles. Provide developers with a high-performance, highly scalable, and easy-to-customize underlying framework to help them address complex business needs.\nFeature list\nModule\nFeature\nModel service\nManage the model list, integrate services such as OpenAI and Volcengine\nBuild agent\n* Build, publish, and manage agent\n* Support configuring workflows, knowledge bases, and other resources\nBuild apps\n* Create and publish apps\n* Build business logic through workflows\nBuild a workflow\nCreate, modify, publish, and delete workflows\nDevelop resources\nSupport creating and managing the following resources:\n* Plugins\n* Knowledge bases\n* Databases\n* Prompts\nAPI and SDK\n* Create conversations, initiate chats, and other OpenAPI\n* Integrate agents or apps into your own app through Chat SDK\nQuickstart\nLearn how to obtain and deploy the open-source version of Coze Studio, quickly build projects, and experience Coze Studio's open-source version.\nEnvironment requirements:\nBefore installing Coze Studio, please ensure that your machine meets the following minimum system requirements: 2 Core、4 GB\nPre-install Docker and Docker Compose, and start the Docker service.\nDeployment steps:\nRetrieve the source code.\n#\nClone code\ngit clone https://github.com/coze-dev/coze-studio.git\nConfigure the model.\nCopy the template files of the doubao-seed-1.6 model from the template directory and paste them into the configuration file directory.\ncd\ncoze-studio\n#\nCopy model configuration template\ncp backend/conf/model/template/model_template_ark_doubao-seed-1.6.yaml backend/conf/model/ark_doubao-seed-1.6.yaml\nModify the template file in the configuration file directory.\nEnter the directory\nbackend/conf/model\n. Open the file\nark_doubao-seed-1.6.yaml\n.\nSet the fields\nid\n,\nmeta.conn_config.api_key\n,\nmeta.conn_config.model\n, and save the file.\nid\n: The model ID in Coze Studio, defined by the developer, must be a non-zero integer and globally unique. Agents or workflows call models based on model IDs. For models that have already been launched, do not modify their IDs; otherwise, it may result in model call failures.\nmeta.conn_config.api_key\n: The API Key for the model service. In this example, it is the API Key for Ark API Key. For more information, see\nGet Volcengine Ark API Key\nor\nGet BytePlus ModelArk API Key\n.\nmeta.conn_config.model\n: The Model name for the model service. In this example, it refers to the Model ID or Endpoint ID of Ark. For more information, see\nGet Volcengine Ark Model ID\n/\nGet Volcengine Ark Endpoint ID\nor\nGet BytePlus ModelArk Model ID\n/\nGet BytePlus ModelArk Endpoint ID\n.\nFor users in China, you may use Volcengine Ark; for users outside China, you may use BytePlus ModelArk instead.\nDeploy and start the service.\nWhen deploying and starting Coze Studio for the first time, it may take a while to retrieve images and build local images. Please be patient. During deployment, you will see the following log information. If you see the message \"Container coze-server Started,\" it means the Coze Studio service has started successfully.\n#\nStart the service\ncd\ndocker\ncp .env.example .env\ndocker compose up -d\nFor common startup failure issues,\nplease refer to the\nFAQ\n.\nAfter starting the service, you can open Coze Studio by accessing\nhttp://localhost:8888/\nthrough your browser.\nWarning\nIf you want to deploy Coze Studio in a public network environment, it is recommended to assess security risks before you begin, and take corresponding protection measures. Possible security risks include account registration functions, Python execution environments in workflow code nodes, Coze Server listening address configurations, SSRF (Server - Side Request Forgery), and some horizontal privilege escalations in APIs.  For more details, refer to\nQuickstart\n.\nDeveloper Guide\nProject Configuration\n:\nModel Configuration\n: Before deploying the open-source version of Coze Studio, you must configure the model service. Otherwise, you cannot select models when building agents, workflows, and apps.\nPlugin Configuration\n: To use official plugins from the plugin store, you must first configure the plugins and add the authentication keys for third-party services.\nBasic Component Configuration\n: Learn how to configure components such as image uploaders to use functions like image uploading in Coze Studio .\nAPI Reference\n: The Coze Studio Community Edition API and Chat SDK are authenticated using Personal Access Token, providing APIs for conversations and workflows.\nDevelopment Guidelines\n:\nProject Architecture\n: Learn about the technical architecture and core components of the open-source version of Coze Studio.\nCode Development and Testing\n: Learn how to perform secondary development and testing based on the open-source version of Coze Studio.\nTroubleshooting\n: Learn how to view container states and system logs.\nUsing the open-source version of Coze Studio\nRegarding how to use Coze Studio, refer to the\nCoze Development Platform Official Documentation Center\nfor more information. Please note that certain features, such as tone customization, are limited to the commercial version. Differences between the open-source and commercial versions can be found in the\nFeature List\n.\nQuick Start\n: Quickly build an AI assistant agent with Coze Studio.\nDeveloping Agents\n: Learn how to create, build, publish, and manage agents. You can use functions such as knowledge, plugins, etc., to resolve model hallucination and lack of expertise in professional fields. In addition, Coze Studio provides rich memory features that enable agents to generate more accurate responses based on a personal user's historical conversations during interactions.\nDevelop workflows\n: A workflow is a set of executable instructions used to implement business logic or complete specific tasks. It structures data flow and task processing for apps or agents. Coze Studio provides a visual canvas where you can quickly build workflows by dragging and dropping nodes.\nResources such as plugins\n: In Coze Studio, workflows, plugins, databases, knowledge bases, and variables are collectively referred to as resources.\nAPI & SDK\n: Coze Studio supports\nAPI related to chat and workflows\n, and you can also integrate agents or apps with local business systems through\nChat SDK\n.\nTutorials for practice\n: Learn how to use Coze Studio to implement various AI scenarios, such as building web-based online customer service using Chat SDK.\nLicense\nThis project uses the Apache 2.0 license. For details, please refer to the\nLICENSE\nfile.\nCommunity contributions\nWe welcome community contributions. For contribution guidelines, please refer to\nCONTRIBUTING\nand\nCode of conduct\n. We look forward to your contributions!\nSecurity and privacy\nIf you discover potential security issues in the project, or believe you may have found a security issue, please notify the ByteDance security team through our\nsecurity center\nor\nvulnerability reporting email\n.\nPlease\ndo not\ncreate public GitHub Issues.\nJoin Community\nWe are committed to building an open and friendly developer community. All developers interested in AI Agent development are welcome to join us!\n🐛 Issue Reports & Feature Requests\nTo efficiently track and resolve issues while ensuring transparency and collaboration, we recommend participating through:\nGitHub Issues\n:\nSubmit bug reports or feature requests\nPull Requests\n:\nContribute code or documentation improvements\n💬 Technical Discussion & Communication\nJoin our technical discussion groups to share experiences with other developers and stay updated with the latest project developments:\nFeishu Group Chat\nScan the QR code below with Feishu mobile app to join:\nDiscord Server\nClick to join:\nCoze Community\nTelegram Group\nClick to join: Telegram Group\nCoze\nAcknowledgments\nThank you to all the developers and community members who have contributed to the Coze Studio project. Special thanks:\nThe\nEino\nframework team - providing powerful support for Coze Studio's agent and workflow runtime engines, model abstractions and implementations, and knowledge base indexing and retrieval\nThe\nFlowGram\nteam - providing a high-quality workflow building engine for Coze Studio's frontend workflow canvas editor\nThe\nHertz\nteam - Go HTTP framework with high-performance and strong-extensibility for building micro-services\nAll users who participated in testing and feedback",
        "今日の獲得スター数: 44",
        "累積スター数: 17,538"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/coze-dev/coze-studio"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/jgraph/drawio-desktop",
      "title": "jgraph/drawio-desktop",
      "date": null,
      "executive_summary": [
        "Official electron build of draw.io",
        "---",
        "About\ndrawio-desktop\nis a diagramming desktop app based on\nElectron\nthat wraps the\ncore draw.io editor\n.\nDownload built binaries from the\nreleases section\n.\nCan I use this app for free?\nYes, under the apache 2.0 license. If you don't change the code and accept it is provided \"as-is\", you can use it for any purpose.\nSecurity\ndraw.io Desktop is designed to be completely isolated from the Internet, apart from the update process. This checks github.com at startup for a newer version and downloads it from an AWS S3 bucket owned by Github. All JavaScript files are self-contained, the Content Security Policy forbids running remotely loaded JavaScript.\nNo diagram data is ever sent externally, nor do we send any analytics about app usage externally. There is a Content Security Policy in place on the web part of the interface to ensure external transmission cannot happen, even by accident.\nSecurity and isolating the app are the primarily objectives of draw.io desktop. If you ask for anything that involves external connections enabled in the app by default, the answer will be no.\nSupport\nSupport is provided on a reasonable business constraints basis, but without anything contractually binding. All support is provided via this repo. There is no private ticketing support for non-paying users.\nPurchasing draw.io for Confluence or Jira does not entitle you to commercial support for draw.io desktop.\nDeveloping\ndraw.io\nis a git submodule of\ndrawio-desktop\n. To get both you need to clone recursively:\ngit clone --recursive https://github.com/jgraph/drawio-desktop.git\nTo run this:\nnpm install\n(in the root directory of this repo)\n[internal use only] export DRAWIO_ENV=dev if you want to develop/debug in dev mode.\nnpm start\nin the root directory of this repo\nruns the app. For debugging, use\nnpm start --enable-logging\n.\nNote: If a symlink is used to refer to drawio repo (instead of the submodule), then symlink the\nnode_modules\ndirectory inside\ndrawio/src/main/webapp\nalso.\nTo release:\nUpdate the draw.io sub-module and push the change. Add version tag before pushing to origin.\nWait for the builds to complete (\nhttps://travis-ci.org/jgraph/drawio-desktop\nand\nhttps://ci.appveyor.com/project/davidjgraph/drawio-desktop\n)\nGo to\nhttps://github.com/jgraph/drawio-desktop/releases\n, edit the preview release.\nDownload the windows exe and windows portable, sign them using\nsigntool sign /a /tr http://rfc3161timestamp.globalsign.com/advanced /td SHA256 c:/path/to/your/file.exe\nRe-upload signed file as\ndraw.io-windows-installer-x.y.z.exe\nand\ndraw.io-windows-no-installer-x.y.z.exe\nAdd release notes\nPublish release\nNote\n: In Windows release, when using both x64 and is32 as arch, the result is one big file with both archs. This is why we split them.\nLocal Storage and Session Storage is stored in the AppData folder:\nmacOS:\n~/Library/Application Support/draw.io\nWindows:\nC:\\Users\\<USER-NAME>\\AppData\\Roaming\\draw.io\\\nNot open-contribution\ndraw.io is closed to contributions (unless a maintainer permits it, which is extremely rare).\nThe level of complexity of this project means that even simple changes\ncan break a\nlot\nof other moving parts. The amount of testing required\nis far more than it first seems. If we were to receive a PR, we'd have\nto basically throw it away and write it how we want it to be implemented.\nWe are grateful for community involvement, bug reports, & feature requests. We do\nnot wish to come off as anything but welcoming, however, we've\nmade the decision to keep this project closed to contributions for\nthe long term viability of the project.",
        "今日の獲得スター数: 42",
        "累積スター数: 57,217"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/jgraph/drawio-desktop"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/microsoft/RD-Agent",
      "title": "microsoft/RD-Agent",
      "date": null,
      "executive_summary": [
        "Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through R&D-Agent, which lets AI drive data-driven AI. 🔗https://aka.ms/RD-Agent-Tech-Report",
        "---",
        "🖥️ Live Demo\n|\n🎥 Demo Video\n▶️\nYouTube\n|\n📖 Documentation\n|\n📄 Tech Report\n|\n📃 Papers\n📰 News\n🗞️ News\n📝 Description\nNeurIPS 2025 Acceptance\nWe are thrilled to announce that our paper\nR&D-Agent-Quant\nhas been accepted to NeurIPS 2025\nTechnical Report Release\nOverall framework description and results on MLE-bench\nR&D-Agent-Quant Release\nApply R&D-Agent to quant trading\nMLE-Bench Results Released\nR&D-Agent currently leads as the\ntop-performing machine learning engineering agent\non MLE-bench\nSupport LiteLLM Backend\nWe now fully support\nLiteLLM\nas our default backend for integration with multiple LLM providers.\nGeneral Data Science Agent\nData Science Agent\nKaggle Scenario release\nWe release\nKaggle Agent\n, try the new features!\nOfficial WeChat group release\nWe created a WeChat group, welcome to join! (🗪\nQR Code\n)\nOfficial Discord release\nWe launch our first chatting channel in Discord (🗪\n)\nFirst release\nR&D-Agent\nis released on GitHub\n🏆 The Best Machine Learning Engineering Agent!\nMLE-bench\nis a comprehensive benchmark evaluating the performance of AI agents on machine learning engineering tasks. Utilizing datasets from 75 Kaggle competitions, MLE-bench provides robust assessments of AI systems' capabilities in real-world ML engineering scenarios.\nR&D-Agent currently leads as the top-performing machine learning engineering agent on MLE-bench:\nAgent\nLow == Lite (%)\nMedium (%)\nHigh (%)\nAll (%)\nR&D-Agent o3(R)+GPT-4.1(D)\n51.52 ± 6.9\n19.3 ± 5.5\n26.67 ± 0\n30.22 ± 1.5\nR&D-Agent o1-preview\n48.18 ± 2.49\n8.95 ± 2.36\n18.67 ± 2.98\n22.4 ± 1.1\nAIDE o1-preview\n34.3 ± 2.4\n8.8 ± 1.1\n10.0 ± 1.9\n16.9 ± 1.1\nNotes:\nO3(R)+GPT-4.1(D)\n: This version is designed to both reduce average time per loop and leverage a cost-effective combination of backend LLMs by seamlessly integrating Research Agent (o3) with Development Agent (GPT-4.1).\nAIDE o1-preview\n: Represents the previously best public result on MLE-bench as reported in the original MLE-bench paper.\nAverage and standard deviation results for R&D-Agent o1-preview is based on a independent of 5 seeds and for R&D-Agent o3(R)+GPT-4.1(D) is based on 6 seeds.\nAccording to MLE-Bench, the 75 competitions are categorized into three levels of complexity:\nLow==Lite\nif we estimate that an experienced ML engineer can produce a sensible solution in under 2 hours, excluding the time taken to train any models;\nMedium\nif it takes between 2 and 10 hours; and\nHigh\nif it takes more than 10 hours.\nYou can inspect the detailed runs of the above results online.\nR&D-Agent o1-preview detailed runs\nR&D-Agent o3(R)+GPT-4.1(D) detailed runs\nFor running R&D-Agent on MLE-bench, refer to\nMLE-bench Guide: Running ML Engineering via MLE-bench\n🥇 The First Data-Centric Quant Multi-Agent Framework!\nR&D-Agent for Quantitative Finance, in short\nRD-Agent(Q)\n, is the first data-centric, multi-agent framework designed to automate the full-stack research and development of quantitative strategies via coordinated factor-model co-optimization.\nExtensive experiments in real stock markets show that, at a cost under $10, RD-Agent(Q) achieves approximately 2× higher ARR than benchmark factor libraries while using over 70% fewer factors. It also surpasses state-of-the-art deep time-series models under smaller resource budgets. Its alternating factor–model optimization further delivers excellent trade-off between predictive accuracy and strategy robustness.\nYou can learn more details about\nRD-Agent(Q)\nthrough the\npaper\nand reproduce it through the\ndocumentation\n.\nData Science Agent Preview\nCheck out our demo video showcasing the current progress of our Data Science Agent under development:\nDS.Agent.Preview.mp4\n🌟 Introduction\nR&D-Agent aims to automate the most critical and valuable aspects of the industrial R&D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data.\nMethodologically, we have identified a framework with two key components: 'R' for proposing new ideas and 'D' for implementing them.\nWe believe that the automatic evolution of R&D will lead to solutions of significant industrial value.\nR&D is a very general scenario. The advent of R&D-Agent can be your\n💰\nAutomatic Quant Factory\n(\n🎥Demo Video\n|\n▶️\nYouTube\n)\n🤖\nData Mining Agent:\nIteratively proposing data & models (\n🎥Demo Video 1\n|\n▶️\nYouTube\n) (\n🎥Demo Video 2\n|\n▶️\nYouTube\n)  and implementing them by gaining knowledge from data.\n🦾\nResearch Copilot:\nAuto read research papers (\n🎥Demo Video\n|\n▶️\nYouTube\n) / financial reports (\n🎥Demo Video\n|\n▶️\nYouTube\n) and implement model structures or building datasets.\n🤖\nKaggle Agent:\nAuto Model Tuning and Feature Engineering(\n🎥Demo Video Coming Soon...\n) and implementing them to achieve more in competitions.\n...\nYou can click the links above to view the demo. We're continuously adding more methods and scenarios to the project to enhance your R&D processes and boost productivity.\nAdditionally, you can take a closer look at the examples in our\n🖥️ Live Demo\n.\n⚡ Quick start\nRD-Agent currently only supports Linux.\nYou can try above demos by running the following command:\n🐳 Docker installation.\nUsers must ensure Docker is installed before attempting most scenarios. Please refer to the\nofficial 🐳Docker page\nfor installation instructions.\nEnsure the current user can run Docker commands\nwithout using sudo\n. You can verify this by executing\ndocker run hello-world\n.\n🐍 Create a Conda Environment\nCreate a new conda environment with Python (3.10 and 3.11 are well-tested in our CI):\nconda create -n rdagent python=3.10\nActivate the environment:\nconda activate rdagent\n🛠️ Install the R&D-Agent\nFor Users\nYou can directly install the R&D-Agent package from PyPI:\npip install rdagent\nFor Developers\nIf you want to try the latest version or contribute to RD-Agent, you can install it from the source and follow the development setup:\ngit clone https://github.com/microsoft/RD-Agent\ncd\nRD-Agent\nmake dev\nMore details can be found in the\ndevelopment setup\n.\n💊 Health check\nrdagent provides a health check that currently checks two things.\nwhether the docker installation was successful.\nwhether the default port used by the\nrdagent ui\nis occupied.\nrdagent health_check --no-check-env\n⚙️ Configuration\nThe demos requires following ability:\nChatCompletion\njson_mode\nembedding query\nYou can set your Chat Model and Embedding Model in the following ways:\n🔥 Attention\n: We now provide experimental support for\nDeepSeek\nmodels! You can use DeepSeek's official API for cost-effective and high-performance inference. See the configuration example below for DeepSeek setup.\nUsing LiteLLM (Default)\n: We now support LiteLLM as a backend for integration with multiple LLM providers. You can configure in multiple ways:\nOption 1: Unified API base for both models\nConfiguration Example:\nOpenAI\nSetup :\ncat\n<<\nEOF\n> .env\n# Set to any model supported by LiteLLM.\nCHAT_MODEL=gpt-4o\nEMBEDDING_MODEL=text-embedding-3-small\n# Configure unified API base\nOPENAI_API_BASE=<your_unified_api_base>\nOPENAI_API_KEY=<replace_with_your_openai_api_key>\nConfiguration Example:\nAzure OpenAI\nSetup :\nBefore using this configuration, please confirm in advance that your\nAzure OpenAI API key\nsupports\nembedded models\n.\ncat\n<<\nEOF\n> .env\nEMBEDDING_MODEL=azure/<Model deployment supporting embedding>\nCHAT_MODEL=azure/<your deployment name>\nAZURE_API_KEY=<replace_with_your_openai_api_key>\nAZURE_API_BASE=<your_unified_api_base>\nAZURE_API_VERSION=<azure api version>\nOption 2: Separate API bases for Chat and Embedding models\ncat\n<<\nEOF\n> .env\n# Set to any model supported by LiteLLM.\n# Configure separate API bases for chat and embedding\n# CHAT MODEL:\nCHAT_MODEL=gpt-4o\nOPENAI_API_BASE=<your_chat_api_base>\nOPENAI_API_KEY=<replace_with_your_openai_api_key>\n# EMBEDDING MODEL:\n# TAKE siliconflow as an example, you can use other providers.\n# Note: embedding requires litellm_proxy prefix\nEMBEDDING_MODEL=litellm_proxy/BAAI/bge-large-en-v1.5\nLITELLM_PROXY_API_KEY=<replace_with_your_siliconflow_api_key>\nLITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1\nConfiguration Example:\nDeepSeek\nSetup :\nSince many users encounter configuration errors when setting up DeepSeek. Here's a complete working example for DeepSeek Setup:\ncat\n<<\nEOF\n> .env\n# CHAT MODEL: Using DeepSeek Official API\nCHAT_MODEL=deepseek/deepseek-chat\nDEEPSEEK_API_KEY=<replace_with_your_deepseek_api_key>\n# EMBEDDING MODEL: Using SiliconFlow for embedding since deepseek has no embedding model.\n# Note: embedding requires litellm_proxy prefix\nEMBEDDING_MODEL=litellm_proxy/BAAI/bge-m3\nLITELLM_PROXY_API_KEY=<replace_with_your_siliconflow_api_key>\nLITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1\nNotice: If you are using reasoning models that include thought processes in their responses (such as <think> tags), you need to set the following environment variable:\nREASONING_THINK_RM=True\nYou can also use a deprecated backend if you only use\nOpenAI API\nor\nAzure OpenAI\ndirectly. For this deprecated setting and more configuration information, please refer to the\ndocumentation\n.\nIf your environment configuration is complete, please execute the following commands to check if your configuration is valid. This step is necessary.\nrdagent health_check\n🚀 Run the Application\nThe\n🖥️ Live Demo\nis implemented by the following commands(each item represents one demo, you can select the one you prefer):\nRun the\nAutomated Quantitative Trading & Iterative Factors Model Joint Evolution\n:\nQlib\nself-loop factor & model proposal and implementation application\nrdagent fin_quant\nRun the\nAutomated Quantitative Trading & Iterative Factors Evolution\n:\nQlib\nself-loop factor proposal and implementation application\nrdagent fin_factor\nRun the\nAutomated Quantitative Trading & Iterative Model Evolution\n:\nQlib\nself-loop model proposal and implementation application\nrdagent fin_model\nRun the\nAutomated Quantitative Trading & Factors Extraction from Financial Reports\n:  Run the\nQlib\nfactor extraction and implementation application based on financial reports\n#\n1. Generally, you can run this scenario using the following command:\nrdagent fin_factor_report --report-folder=\n<\nYour financial reports folder path\n>\n#\n2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:\nwget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip\nunzip all_reports.zip -d git_ignore_folder/reports\nrdagent fin_factor_report --report-folder=git_ignore_folder/reports\nRun the\nAutomated Model Research & Development Copilot\n: model extraction and implementation application\n#\n1. Generally, you can run your own papers/reports with the following command:\nrdagent general_model\n<\nYour paper URL\n>\n#\n2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:\nrdagent general_model\n\"\nhttps://arxiv.org/pdf/2210.09789\n\"\nRun the\nAutomated Medical Prediction Model Evolution\n: Medical self-loop model proposal and implementation application\n#\nGenerally, you can run the data science program with the following command:\nrdagent data_science --competition\n<\nyour competition name\n>\n#\nSpecifically, you need to create a folder for storing competition files (e.g., competition description file, competition datasets, etc.), and configure the path to the folder in your environment. In addition, you need to use chromedriver when you download the competition descriptors, which you can follow for this specific example:\n#\n1. Download the dataset, extract it to the target folder.\nwget https://github.com/SunsetWolf/rdagent_resource/releases/download/ds_data/arf-12-hours-prediction-task.zip\nunzip arf-12-hours-prediction-task.zip -d ./git_ignore_folder/ds_data/\n#\n2. Configure environment variables in the `.env` file\ndotenv\nset\nDS_LOCAL_DATA_PATH\n\"\n$(\npwd\n)\n/git_ignore_folder/ds_data\n\"\ndotenv\nset\nDS_CODER_ON_WHOLE_PIPELINE True\ndotenv\nset\nDS_IF_USING_MLE_DATA False\ndotenv\nset\nDS_SAMPLE_DATA_BY_LLM False\ndotenv\nset\nDS_SCEN rdagent.scenarios.data_science.scen.DataScienceScen\n#\n3. run the application\nrdagent data_science --competition arf-12-hours-prediction-task\nNOTE:\nFor more information about the dataset, please refer to the\ndocumentation\n.\nRun the\nAutomated Kaggle Model Tuning & Feature Engineering\n:  self-loop model proposal and feature engineering implementation application\nUsing\ntabular-playground-series-dec-2021\nas an example.\nRegister and login on the\nKaggle\nwebsite.\nConfiguring the Kaggle API.\n(1) Click on the avatar (usually in the top right corner of the page) ->\nSettings\n->\nCreate New Token\n, A file called\nkaggle.json\nwill be downloaded.\n(2) Move\nkaggle.json\nto\n~/.config/kaggle/\n(3) Modify the permissions of the kaggle.json file. Reference command:\nchmod 600 ~/.config/kaggle/kaggle.json\nJoin the competition: Click\nJoin the competition\n->\nI Understand and Accept\nat the bottom of the\ncompetition details page\n.\n#\nGenerally, you can run the Kaggle competition program with the following command:\nrdagent data_science --competition\n<\nyour competition name\n>\n#\n1. Configure environment variables in the `.env` file\nmkdir -p ./git_ignore_folder/ds_data\ndotenv\nset\nDS_LOCAL_DATA_PATH\n\"\n$(\npwd\n)\n/git_ignore_folder/ds_data\n\"\ndotenv\nset\nDS_CODER_ON_WHOLE_PIPELINE True\ndotenv\nset\nDS_IF_USING_MLE_DATA True\ndotenv\nset\nDS_SAMPLE_DATA_BY_LLM True\ndotenv\nset\nDS_SCEN rdagent.scenarios.data_science.scen.KaggleScen\n#\n2. run the application\nrdagent data_science --competition tabular-playground-series-dec-2021\n🖥️ Monitor the Application Results\nYou can run the following command for our demo program to see the run logs.\nrdagent ui --port 19899 --log-dir\n<\nyour log folder like\n\"\nlog/\n\"\n>\n--data-science\nAbout the\ndata_science\nparameter: If you want to see the logs of the data science scenario, set the\ndata_science\nparameter to\nTrue\n; otherwise set it to\nFalse\n.\nAlthough port 19899 is not commonly used, but before you run this demo, you need to check if port 19899 is occupied. If it is, please change it to another port that is not occupied.\nYou can check if a port is occupied by running the following command.\nrdagent health_check --no-check-env --no-check-docker\n🏭 Scenarios\nWe have applied R&D-Agent to multiple valuable data-driven industrial scenarios.\n🎯 Goal: Agent for Data-driven R&D\nIn this project, we are aiming to build an Agent to automate Data-Driven R&D that can\n📄 Read real-world material (reports, papers, etc.) and\nextract\nkey formulas, descriptions of interested\nfeatures\nand\nmodels\n, which are the key components of data-driven R&D .\n🛠️\nImplement\nthe extracted formulas (e.g., features, factors, and models) in runnable codes.\nDue to the limited ability of LLM in implementing at once, build an evolving process for the agent to improve performance by learning from feedback and knowledge.\n💡 Propose\nnew ideas\nbased on current knowledge and observations.\n📈 Scenarios/Demos\nIn the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: 🦾Copilot and 🤖Agent.\nThe 🦾Copilot follows human instructions to automate repetitive tasks.\nThe 🤖Agent, being more autonomous, actively proposes ideas for better results in the future.\nThe supported scenarios are listed below:\nScenario/Target\nModel Implementation\nData Building\n💹 Finance\n🤖\nIteratively Proposing Ideas & Evolving\n▶️\nYouTube\n🤖\nIteratively Proposing Ideas & Evolving\n▶️\nYouTube\n🦾\nAuto reports reading & implementation\n▶️\nYouTube\n🩺 Medical\n🤖\nIteratively Proposing Ideas & Evolving\n▶️\nYouTube\n-\n🏭 General\n🦾\nAuto paper reading & implementation\n▶️\nYouTube\n🤖 Auto Kaggle Model Tuning\n🤖Auto Kaggle feature Engineering\nRoadMap\n: Currently, we are working hard to add new features to the Kaggle scenario.\nDifferent scenarios vary in entrance and configuration. Please check the detailed setup tutorial in the scenarios documents.\nHere is a gallery of\nsuccessful explorations\n(5 traces showed in\n🖥️ Live Demo\n). You can download and view the execution trace using\nthis command\nfrom the documentation.\nPlease refer to\n📖readthedocs_scen\nfor more details of the scenarios.\n⚙️ Framework\nAutomating the R&D process in data science is a highly valuable yet underexplored area in industry. We propose a framework to push the boundaries of this important research field.\nThe research questions within this framework can be divided into three main categories:\nResearch Area\nPaper/Work List\nBenchmark the R&D abilities\nBenchmark\nIdea proposal:\nExplore new ideas or refine existing ones\nResearch\nAbility to realize ideas:\nImplement and execute ideas\nDevelopment\nWe believe that the key to delivering high-quality solutions lies in the ability to evolve R&D capabilities. Agents should learn like human experts, continuously improving their R&D skills.\nMore documents can be found in the\n📖 readthedocs\n.\n📃 Paper/Work list\nOverall Technical Report\nR&D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution\n@misc\n{\nyang2024rdagent\n,\ntitle\n=\n{\nR\\&D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution\n}\n,\nauthor\n=\n{\nXu Yang and Xiao Yang and Shikai Fang and Bowen Xian and Yuante Li and Jian Wang and Minrui Xu and Haoran Pan and Xinpeng Hong and Weiqing Liu and Yelong Shen and Weizhu Chen and Jiang Bian\n}\n,\nyear\n=\n{\n2025\n}\n,\neprint\n=\n{\n2505.14738\n}\n,\narchivePrefix\n=\n{\narXiv\n}\n,\nprimaryClass\n=\n{\ncs.AI\n}\n,\nurl\n=\n{\nhttps://arxiv.org/abs/2505.14738\n}\n}\n📊 Benchmark\nTowards Data-Centric Automatic R&D\n@misc\n{\nchen2024datacentric\n,\ntitle\n=\n{\nTowards Data-Centric Automatic R&D\n}\n,\nauthor\n=\n{\nHaotian Chen and Xinjie Shen and Zeqi Ye and Wenjun Feng and Haoxue Wang and Xiao Yang and Xu Yang and Weiqing Liu and Jiang Bian\n}\n,\nyear\n=\n{\n2024\n}\n,\neprint\n=\n{\n2404.11276\n}\n,\narchivePrefix\n=\n{\narXiv\n}\n,\nprimaryClass\n=\n{\ncs.AI\n}\n}\n🔍 Research\nIn a data mining expert's daily research and development process, they propose a hypothesis (e.g., a model structure like RNN can capture patterns in time-series data), design experiments (e.g., finance data contains time-series and we can verify the hypothesis in this scenario), implement the experiment as code (e.g., Pytorch model structure), and then execute the code to get feedback (e.g., metrics, loss curve, etc.). The experts learn from the feedback and improve in the next iteration.\nBased on the principles above, we have established a basic method framework that continuously proposes hypotheses, verifies them, and gets feedback from the real-world practice. This is the first scientific research automation framework that supports linking with real-world verification.\nFor more detail, please refer to our\n🖥️ Live Demo page\n.\n🛠️ Development\nCollaborative Evolving Strategy for Automatic Data-Centric Development\n@misc\n{\nyang2024collaborative\n,\ntitle\n=\n{\nCollaborative Evolving Strategy for Automatic Data-Centric Development\n}\n,\nauthor\n=\n{\nXu Yang and Haotian Chen and Wenjun Feng and Haoxue Wang and Zeqi Ye and Xinjie Shen and Xiao Yang and Shizhao Sun and Weiqing Liu and Jiang Bian\n}\n,\nyear\n=\n{\n2024\n}\n,\neprint\n=\n{\n2407.18690\n}\n,\narchivePrefix\n=\n{\narXiv\n}\n,\nprimaryClass\n=\n{\ncs.AI\n}\n}\nDeep Application in Diverse Scenarios\nR&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization\n@misc\n{\nli2025rdagentquant\n,\ntitle\n=\n{\nR\\&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization\n}\n,\nauthor\n=\n{\nYuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian\n}\n,\nyear\n=\n{\n2025\n}\n,\neprint\n=\n{\n2505.15155\n}\n,\narchivePrefix\n=\n{\narXiv\n}\n,\nprimaryClass\n=\n{\ncs.AI\n}\n}\n🤝 Contributing\nWe welcome contributions and suggestions to improve R&D-Agent. Please refer to the\nContributing Guide\nfor more details on how to contribute.\nBefore submitting a pull request, ensure that your code passes the automatic CI checks.\n📝 Guidelines\nThis project welcomes contributions and suggestions.\nContributing to this project is straightforward and rewarding. Whether it's solving an issue, addressing a bug, enhancing documentation, or even correcting a typo, every contribution is valuable and helps improve R&D-Agent.\nTo get started, you can explore the issues list, or search for\nTODO:\ncomments in the codebase by running the command\ngrep -r \"TODO:\"\n.\nBefore we released R&D-Agent as an open-source project on GitHub, it was an internal project within our group. Unfortunately, the internal commit history was not preserved when we removed some confidential code. As a result, some contributions from our group members, including Haotian Chen, Wenjun Feng, Haoxue Wang, Zeqi Ye, Xinjie Shen, and Jinhui Li, were not included in the public commits.\n⚖️ Legal disclaimer\nThe RD-agent is provided “as is”, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. The RD-agent is aimed to facilitate research and development process in the financial industry and not ready-to-use for any financial investment or advice. Users shall independently assess and test the risks of the RD-agent in a specific use scenario, ensure the responsible use of AI technology, including but not limited to developing and integrating risk mitigation measures, and comply with all applicable laws and regulations in all applicable jurisdictions. The RD-agent does not provide financial opinions or reflect the opinions of Microsoft, nor is it designed to replace the role of qualified financial professionals in formulating, assessing, and approving finance products. The inputs and outputs of the RD-agent belong to the users and users shall assume all liability under any theory of liability, whether in contract, torts, regulatory, negligence, products liability, or otherwise, associated with use of the RD-agent and any inputs and outputs thereof.",
        "今日の獲得スター数: 42",
        "累積スター数: 8,357"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/microsoft/RD-Agent"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/JannisX11/blockbench",
      "title": "JannisX11/blockbench",
      "date": null,
      "executive_summary": [
        "Blockbench - A low poly 3D model editor",
        "---",
        "Blockbench\nBlockbench is a free and open source model editor for low-poly models with pixel art textures.\nModels can be exported into standardized formats, to be shared, rendered, 3D-printed, or used in game engines. There are also multiple dedicated formats for Minecraft Java and Bedrock Edition with format-specific features.\nBlockbench features a modern and beginner friendly interface, but also offers lots of customization and advanced features for experienced 3D artists. Plugins can extend the functionality of the program even further.\nWebsite and download:\nblockbench.net\nContribution\nCheck out the\nContribution Guidelines\n.\nLaunching Blockbench\nTo launch Blockbench from source, you can clone the repository, navigate to the correct branch and launch the program in development mode using the instructions below. If you just want to use the latest version, please download the app from the website.\nInstall\nNodeJS\n.\nThen install all dependencies via\nnpm install\nBundle the code via\nnpm run bundle\nFinally, launch Blockbench using\nnpm run dev\nPlugins\nBlockbench supports Javascript-based plugins. Learn more about creating plugins on\nhttps://www.blockbench.net/wiki/docs/plugin\n.\nLicense\nThe Blockbench source-code is licensed under the GPL license version 3. See\nLICENSE.MD\n.\nModifications to the source code can be made under the terms of that license.\nBlockbench plugins (external scripts) and themes (theme files to customize the design) that interact with the Blockbench API are an exception. Plugins and themes can be created and/or published as open source, proprietary or paid software.\nAll assets created with Blockbench (models, textures, animations, screenshots etc.) are your own!",
        "今日の獲得スター数: 39",
        "累積スター数: 4,437"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/JannisX11/blockbench"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/fatedier/frp",
      "title": "fatedier/frp",
      "date": null,
      "executive_summary": [
        "A fast reverse proxy to help you expose a local server behind a NAT or firewall to the internet.",
        "---",
        "frp\nREADME\n|\n中文文档\nSponsors\nfrp is an open source project with its ongoing development made possible entirely by the support of our awesome sponsors. If you'd like to join them, please consider\nsponsoring frp's development\n.\nGold Sponsors\nRecall.ai - API for meeting recordings\nIf you're looking for a meeting recording API, consider checking out\nRecall.ai\n,\nan API that records Zoom, Google Meet, Microsoft Teams, in-person meetings, and more.\nWarp, built for collaborating with AI Agents\nAvailable for macOS, Linux and Windows\nThe complete IDE crafted for professional Go developers\nSecure and Elastic Infrastructure for Running Your AI-Generated Code\nThe sovereign cloud that puts you in control\nAn open source, self-hosted alternative to public clouds, built for data ownership and privacy\nWhat is frp?\nfrp is a fast reverse proxy that allows you to expose a local server located behind a NAT or firewall to the Internet. It currently supports\nTCP\nand\nUDP\n, as well as\nHTTP\nand\nHTTPS\nprotocols, enabling requests to be forwarded to internal services via domain name.\nfrp also offers a P2P connect mode.\nTable of Contents\nDevelopment Status\nAbout V2\nArchitecture\nExample Usage\nAccess your computer in a LAN network via SSH\nMultiple SSH services sharing the same port\nAccessing Internal Web Services with Custom Domains in LAN\nForward DNS query requests\nForward Unix Domain Socket\nExpose a simple HTTP file server\nEnable HTTPS for a local HTTP(S) service\nExpose your service privately\nP2P Mode\nFeatures\nConfiguration Files\nUsing Environment Variables\nSplit Configures Into Different Files\nServer Dashboard\nClient Admin UI\nMonitor\nPrometheus\nAuthenticating the Client\nToken Authentication\nOIDC Authentication\nEncryption and Compression\nTLS\nHot-Reloading frpc configuration\nGet proxy status from client\nOnly allowing certain ports on the server\nPort Reuse\nBandwidth Limit\nFor Each Proxy\nTCP Stream Multiplexing\nSupport KCP Protocol\nSupport QUIC Protocol\nConnection Pooling\nLoad balancing\nService Health Check\nRewriting the HTTP Host Header\nSetting other HTTP Headers\nGet Real IP\nHTTP X-Forwarded-For\nProxy Protocol\nRequire HTTP Basic Auth (Password) for Web Services\nCustom Subdomain Names\nURL Routing\nTCP Port Multiplexing\nConnecting to frps via PROXY\nPort range mapping\nClient Plugins\nServer Manage Plugins\nSSH Tunnel Gateway\nVirtual Network (VirtualNet)\nFeature Gates\nAvailable Feature Gates\nEnabling Feature Gates\nFeature Lifecycle\nRelated Projects\nContributing\nDonation\nGitHub Sponsors\nPayPal\nDevelopment Status\nfrp is currently under development. You can try the latest release version in the\nmaster\nbranch, or use the\ndev\nbranch to access the version currently in development.\nWe are currently working on version 2 and attempting to perform some code refactoring and improvements. However, please note that it will not be compatible with version 1.\nWe will transition from version 0 to version 1 at the appropriate time and will only accept bug fixes and improvements, rather than big feature requests.\nAbout V2\nThe complexity and difficulty of the v2 version are much higher than anticipated. I can only work on its development during fragmented time periods, and the constant interruptions disrupt productivity significantly. Given this situation, we will continue to optimize and iterate on the current version until we have more free time to proceed with the major version overhaul.\nThe concept behind v2 is based on my years of experience and reflection in the cloud-native domain, particularly in K8s and ServiceMesh. Its core is a modernized four-layer and seven-layer proxy, similar to envoy. This proxy itself is highly scalable, not only capable of implementing the functionality of intranet penetration but also applicable to various other domains. Building upon this highly scalable core, we aim to implement all the capabilities of frp v1 while also addressing the functionalities that were previously unachievable or difficult to implement in an elegant manner. Furthermore, we will maintain efficient development and iteration capabilities.\nIn addition, I envision frp itself becoming a highly extensible system and platform, similar to how we can provide a range of extension capabilities based on K8s. In K8s, we can customize development according to enterprise needs, utilizing features such as CRD, controller mode, webhook, CSI, and CNI. In frp v1, we introduced the concept of server plugins, which implemented some basic extensibility. However, it relies on a simple HTTP protocol and requires users to start independent processes and manage them on their own. This approach is far from flexible and convenient, and real-world demands vary greatly. It is unrealistic to expect a non-profit open-source project maintained by a few individuals to meet everyone's needs.\nFinally, we acknowledge that the current design of modules such as configuration management, permission verification, certificate management, and API management is not modern enough. While we may carry out some optimizations in the v1 version, ensuring compatibility remains a challenging issue that requires a considerable amount of effort to address.\nWe sincerely appreciate your support for frp.\nArchitecture\nExample Usage\nTo begin, download the latest program for your operating system and architecture from the\nRelease\npage.\nNext, place the\nfrps\nbinary and server configuration file on Server A, which has a public IP address.\nFinally, place the\nfrpc\nbinary and client configuration file on Server B, which is located on a LAN that cannot be directly accessed from the public internet.\nSome antiviruses improperly mark frpc as malware and delete it. This is due to frp being a networking tool capable of creating reverse proxies. Antiviruses sometimes flag reverse proxies due to their ability to bypass firewall port restrictions. If you are using antivirus, then you may need to whitelist/exclude frpc in your antivirus settings to avoid accidental quarantine/deletion. See\nissue 3637\nfor more details.\nAccess your computer in a LAN network via SSH\nModify\nfrps.toml\non server A by setting the\nbindPort\nfor frp clients to connect to:\n#\nfrps.toml\nbindPort\n=\n7000\nStart\nfrps\non server A:\n./frps -c ./frps.toml\nModify\nfrpc.toml\non server B and set the\nserverAddr\nfield to the public IP address of your frps server:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nssh\n\"\ntype\n=\n\"\ntcp\n\"\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n22\nremotePort\n=\n6000\nNote that the\nlocalPort\n(listened on the client) and\nremotePort\n(exposed on the server) are used for traffic going in and out of the frp system, while the\nserverPort\nis used for communication between frps and frpc.\nStart\nfrpc\non server B:\n./frpc -c ./frpc.toml\nTo access server B from another machine through server A via SSH (assuming the username is\ntest\n), use the following command:\nssh -oPort=6000 test@x.x.x.x\nMultiple SSH services sharing the same port\nThis example implements multiple SSH services exposed through the same port using a proxy of type tcpmux. Similarly, as long as the client supports the HTTP Connect proxy connection method, port reuse can be achieved in this way.\nDeploy frps on a machine with a public IP and modify the frps.toml file. Here is a simplified configuration:\nbindPort\n=\n7000\ntcpmuxHTTPConnectPort\n=\n5002\nDeploy frpc on the internal machine A with the following configuration:\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nssh1\n\"\ntype\n=\n\"\ntcpmux\n\"\nmultiplexer\n=\n\"\nhttpconnect\n\"\ncustomDomains\n= [\n\"\nmachine-a.example.com\n\"\n]\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n22\nDeploy another frpc on the internal machine B with the following configuration:\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nssh2\n\"\ntype\n=\n\"\ntcpmux\n\"\nmultiplexer\n=\n\"\nhttpconnect\n\"\ncustomDomains\n= [\n\"\nmachine-b.example.com\n\"\n]\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n22\nTo access internal machine A using SSH ProxyCommand, assuming the username is \"test\":\nssh -o 'proxycommand socat - PROXY:x.x.x.x:%h:%p,proxyport=5002' test@machine-a.example.com\nTo access internal machine B, the only difference is the domain name, assuming the username is \"test\":\nssh -o 'proxycommand socat - PROXY:x.x.x.x:%h:%p,proxyport=5002' test@machine-b.example.com\nAccessing Internal Web Services with Custom Domains in LAN\nSometimes we need to expose a local web service behind a NAT network to others for testing purposes with our own domain name.\nUnfortunately, we cannot resolve a domain name to a local IP. However, we can use frp to expose an HTTP(S) service.\nModify\nfrps.toml\nand set the HTTP port for vhost to 8080:\n#\nfrps.toml\nbindPort\n=\n7000\nvhostHTTPPort\n=\n8080\nIf you want to configure an https proxy, you need to set up the\nvhostHTTPSPort\n.\nStart\nfrps\n:\n./frps -c ./frps.toml\nModify\nfrpc.toml\nand set\nserverAddr\nto the IP address of the remote frps server. Specify the\nlocalPort\nof your web service:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nweb\n\"\ntype\n=\n\"\nhttp\n\"\nlocalPort\n=\n80\ncustomDomains\n= [\n\"\nwww.example.com\n\"\n]\nStart\nfrpc\n:\n./frpc -c ./frpc.toml\nMap the A record of\nwww.example.com\nto either the public IP of the remote frps server or a CNAME record pointing to your original domain.\nVisit your local web service using url\nhttp://www.example.com:8080\n.\nForward DNS query requests\nModify\nfrps.toml\n:\n#\nfrps.toml\nbindPort\n=\n7000\nStart\nfrps\n:\n./frps -c ./frps.toml\nModify\nfrpc.toml\nand set\nserverAddr\nto the IP address of the remote frps server. Forward DNS query requests to the Google Public DNS server\n8.8.8.8:53\n:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\ndns\n\"\ntype\n=\n\"\nudp\n\"\nlocalIP\n=\n\"\n8.8.8.8\n\"\nlocalPort\n=\n53\nremotePort\n=\n6000\nStart frpc:\n./frpc -c ./frpc.toml\nTest DNS resolution using the\ndig\ncommand:\ndig @x.x.x.x -p 6000 www.google.com\nForward Unix Domain Socket\nExpose a Unix domain socket (e.g. the Docker daemon socket) as TCP.\nConfigure\nfrps\nas above.\nStart\nfrpc\nwith the following configuration:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nunix_domain_socket\n\"\ntype\n=\n\"\ntcp\n\"\nremotePort\n=\n6000\n[\nproxies\n.\nplugin\n]\ntype\n=\n\"\nunix_domain_socket\n\"\nunixPath\n=\n\"\n/var/run/docker.sock\n\"\nTest the configuration by getting the docker version using\ncurl\n:\ncurl http://x.x.x.x:6000/version\nExpose a simple HTTP file server\nExpose a simple HTTP file server to access files stored in the LAN from the public Internet.\nConfigure\nfrps\nas described above, then:\nStart\nfrpc\nwith the following configuration:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\ntest_static_file\n\"\ntype\n=\n\"\ntcp\n\"\nremotePort\n=\n6000\n[\nproxies\n.\nplugin\n]\ntype\n=\n\"\nstatic_file\n\"\nlocalPath\n=\n\"\n/tmp/files\n\"\nstripPrefix\n=\n\"\nstatic\n\"\nhttpUser\n=\n\"\nabc\n\"\nhttpPassword\n=\n\"\nabc\n\"\nVisit\nhttp://x.x.x.x:6000/static/\nfrom your browser and specify correct username and password to view files in\n/tmp/files\non the\nfrpc\nmachine.\nEnable HTTPS for a local HTTP(S) service\nYou may substitute\nhttps2https\nfor the plugin, and point the\nlocalAddr\nto a HTTPS endpoint.\nStart\nfrpc\nwith the following configuration:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\ntest_https2http\n\"\ntype\n=\n\"\nhttps\n\"\ncustomDomains\n= [\n\"\ntest.example.com\n\"\n]\n\n[\nproxies\n.\nplugin\n]\ntype\n=\n\"\nhttps2http\n\"\nlocalAddr\n=\n\"\n127.0.0.1:80\n\"\ncrtPath\n=\n\"\n./server.crt\n\"\nkeyPath\n=\n\"\n./server.key\n\"\nhostHeaderRewrite\n=\n\"\n127.0.0.1\n\"\nrequestHeaders.set.x-from-where\n=\n\"\nfrp\n\"\nVisit\nhttps://test.example.com\n.\nExpose your service privately\nTo mitigate risks associated with exposing certain services directly to the public network, STCP (Secret TCP) mode requires a preshared key to be used for access to the service from other clients.\nConfigure\nfrps\nsame as above.\nStart\nfrpc\non machine B with the following config. This example is for exposing the SSH service (port 22), and note the\nsecretKey\nfield for the preshared key, and that the\nremotePort\nfield is removed here:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nsecret_ssh\n\"\ntype\n=\n\"\nstcp\n\"\nsecretKey\n=\n\"\nabcdefg\n\"\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n22\nStart another\nfrpc\n(typically on another machine C) with the following config to access the SSH service with a security key (\nsecretKey\nfield):\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nvisitors\n]]\nname\n=\n\"\nsecret_ssh_visitor\n\"\ntype\n=\n\"\nstcp\n\"\nserverName\n=\n\"\nsecret_ssh\n\"\nsecretKey\n=\n\"\nabcdefg\n\"\nbindAddr\n=\n\"\n127.0.0.1\n\"\nbindPort\n=\n6000\nOn machine C, connect to SSH on machine B, using this command:\nssh -oPort=6000 127.0.0.1\nP2P Mode\nxtcp\nis designed to transmit large amounts of data directly between clients. A frps server is still needed, as P2P here only refers to the actual data transmission.\nNote that it may not work with all types of NAT devices. You might want to fallback to stcp if xtcp doesn't work.\nStart\nfrpc\non machine B, and expose the SSH port. Note that the\nremotePort\nfield is removed:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n#\nset up a new stun server if the default one is not available.\n#\nnatHoleStunServer = \"xxx\"\n[[\nproxies\n]]\nname\n=\n\"\np2p_ssh\n\"\ntype\n=\n\"\nxtcp\n\"\nsecretKey\n=\n\"\nabcdefg\n\"\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n22\nStart another\nfrpc\n(typically on another machine C) with the configuration to connect to SSH using P2P mode:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n#\nset up a new stun server if the default one is not available.\n#\nnatHoleStunServer = \"xxx\"\n[[\nvisitors\n]]\nname\n=\n\"\np2p_ssh_visitor\n\"\ntype\n=\n\"\nxtcp\n\"\nserverName\n=\n\"\np2p_ssh\n\"\nsecretKey\n=\n\"\nabcdefg\n\"\nbindAddr\n=\n\"\n127.0.0.1\n\"\nbindPort\n=\n6000\n#\nwhen automatic tunnel persistence is required, set it to true\nkeepTunnelOpen\n=\nfalse\nOn machine C, connect to SSH on machine B, using this command:\nssh -oPort=6000 127.0.0.1\nFeatures\nConfiguration Files\nSince v0.52.0, we support TOML, YAML, and JSON for configuration. Please note that INI is deprecated and will be removed in future releases. New features will only be available in TOML, YAML, or JSON. Users wanting these new features should switch their configuration format accordingly.\nRead the full example configuration files to find out even more features not described here.\nExamples use TOML format, but you can still use YAML or JSON.\nThese configuration files is for reference only. Please do not use this configuration directly to run the program as it may have various issues.\nFull configuration file for frps (Server)\nFull configuration file for frpc (Client)\nUsing Environment Variables\nEnvironment variables can be referenced in the configuration file, using Go's standard format:\n#\nfrpc.toml\nserverAddr\n=\n\"\n{{ .Envs.FRP_SERVER_ADDR }}\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nssh\n\"\ntype\n=\n\"\ntcp\n\"\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n22\nremotePort\n= {{ .Envs.FRP_SSH_REMOTE_PORT }\n}\nWith the config above, variables can be passed into\nfrpc\nprogram like this:\nexport FRP_SERVER_ADDR=x.x.x.x\nexport FRP_SSH_REMOTE_PORT=6000\n./frpc -c ./frpc.toml\nfrpc\nwill render configuration file template using OS environment variables. Remember to prefix your reference with\n.Envs\n.\nSplit Configures Into Different Files\nYou can split multiple proxy configs into different files and include them in the main file.\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\nincludes\n= [\n\"\n./confd/*.toml\n\"\n]\n#\n./confd/test.toml\n[[\nproxies\n]]\nname\n=\n\"\nssh\n\"\ntype\n=\n\"\ntcp\n\"\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n22\nremotePort\n=\n6000\nServer Dashboard\nCheck frp's status and proxies' statistics information by Dashboard.\nConfigure a port for dashboard to enable this feature:\n#\nThe default value is 127.0.0.1. Change it to 0.0.0.0 when you want to access it from a public network.\nwebServer.addr\n=\n\"\n0.0.0.0\n\"\nwebServer.port\n=\n7500\n#\ndashboard's username and password are both optional\nwebServer.user\n=\n\"\nadmin\n\"\nwebServer.password\n=\n\"\nadmin\n\"\nThen visit\nhttp://[serverAddr]:7500\nto see the dashboard, with username and password both being\nadmin\n.\nAdditionally, you can use HTTPS port by using your domains wildcard or normal SSL certificate:\nwebServer.port\n=\n7500\n#\ndashboard's username and password are both optional\nwebServer.user\n=\n\"\nadmin\n\"\nwebServer.password\n=\n\"\nadmin\n\"\nwebServer.tls.certFile\n=\n\"\nserver.crt\n\"\nwebServer.tls.keyFile\n=\n\"\nserver.key\n\"\nThen visit\nhttps://[serverAddr]:7500\nto see the dashboard in secure HTTPS connection, with username and password both being\nadmin\n.\nClient Admin UI\nThe Client Admin UI helps you check and manage frpc's configuration.\nConfigure an address for admin UI to enable this feature:\nwebServer.addr\n=\n\"\n127.0.0.1\n\"\nwebServer.port\n=\n7400\nwebServer.user\n=\n\"\nadmin\n\"\nwebServer.password\n=\n\"\nadmin\n\"\nThen visit\nhttp://127.0.0.1:7400\nto see admin UI, with username and password both being\nadmin\n.\nMonitor\nWhen web server is enabled, frps will save monitor data in cache for 7 days. It will be cleared after process restart.\nPrometheus is also supported.\nPrometheus\nEnable dashboard first, then configure\nenablePrometheus = true\nin\nfrps.toml\n.\nhttp://{dashboard_addr}/metrics\nwill provide prometheus monitor data.\nAuthenticating the Client\nThere are 2 authentication methods to authenticate frpc with frps.\nYou can decide which one to use by configuring\nauth.method\nin\nfrpc.toml\nand\nfrps.toml\n, the default one is token.\nConfiguring\nauth.additionalScopes = [\"HeartBeats\"]\nwill use the configured authentication method to add and validate authentication on every heartbeat between frpc and frps.\nConfiguring\nauth.additionalScopes = [\"NewWorkConns\"]\nwill do the same for every new work connection between frpc and frps.\nToken Authentication\nWhen specifying\nauth.method = \"token\"\nin\nfrpc.toml\nand\nfrps.toml\n- token based authentication will be used.\nMake sure to specify the same\nauth.token\nin\nfrps.toml\nand\nfrpc.toml\nfor frpc to pass frps validation\nToken Source\nfrp supports reading authentication tokens from external sources using the\ntokenSource\nconfiguration. Currently, file-based token source is supported.\nFile-based token source:\n#\nfrpc.toml\nauth.method\n=\n\"\ntoken\n\"\nauth.tokenSource.type\n=\n\"\nfile\n\"\nauth.tokenSource.file.path\n=\n\"\n/path/to/token/file\n\"\nThe token will be read from the specified file at startup. This is useful for scenarios where tokens are managed by external systems or need to be kept separate from configuration files for security reasons.\nOIDC Authentication\nWhen specifying\nauth.method = \"oidc\"\nin\nfrpc.toml\nand\nfrps.toml\n- OIDC based authentication will be used.\nOIDC stands for OpenID Connect, and the flow used is called\nClient Credentials Grant\n.\nTo use this authentication type - configure\nfrpc.toml\nand\nfrps.toml\nas follows:\n#\nfrps.toml\nauth.method\n=\n\"\noidc\n\"\nauth.oidc.issuer\n=\n\"\nhttps://example-oidc-issuer.com/\n\"\nauth.oidc.audience\n=\n\"\nhttps://oidc-audience.com/.default\n\"\n#\nfrpc.toml\nauth.method\n=\n\"\noidc\n\"\nauth.oidc.clientID\n=\n\"\n98692467-37de-409a-9fac-bb2585826f18\n\"\n#\nReplace with OIDC client ID\nauth.oidc.clientSecret\n=\n\"\noidc_secret\n\"\nauth.oidc.audience\n=\n\"\nhttps://oidc-audience.com/.default\n\"\nauth.oidc.tokenEndpointURL\n=\n\"\nhttps://example-oidc-endpoint.com/oauth2/v2.0/token\n\"\nEncryption and Compression\nThe features are off by default. You can turn on encryption and/or compression:\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nssh\n\"\ntype\n=\n\"\ntcp\n\"\nlocalPort\n=\n22\nremotePort\n=\n6000\ntransport.useEncryption\n=\ntrue\ntransport.useCompression\n=\ntrue\nTLS\nSince v0.50.0, the default value of\ntransport.tls.enable\nand\ntransport.tls.disableCustomTLSFirstByte\nhas been changed to true, and tls is enabled by default.\nFor port multiplexing, frp sends a first byte\n0x17\nto dial a TLS connection. This only takes effect when you set\ntransport.tls.disableCustomTLSFirstByte\nto false.\nTo\nenforce\nfrps\nto only accept TLS connections - configure\ntransport.tls.force = true\nin\nfrps.toml\n.\nThis is optional.\nfrpc\nTLS settings:\ntransport.tls.enable\n=\ntrue\ntransport.tls.certFile\n=\n\"\ncertificate.crt\n\"\ntransport.tls.keyFile\n=\n\"\ncertificate.key\n\"\ntransport.tls.trustedCaFile\n=\n\"\nca.crt\n\"\nfrps\nTLS settings:\ntransport.tls.force\n=\ntrue\ntransport.tls.certFile\n=\n\"\ncertificate.crt\n\"\ntransport.tls.keyFile\n=\n\"\ncertificate.key\n\"\ntransport.tls.trustedCaFile\n=\n\"\nca.crt\n\"\nYou will need\na root CA cert\nand\nat least one SSL/TLS certificate\n. It\ncan\nbe self-signed or regular (such as Let's Encrypt or another SSL/TLS certificate provider).\nIf you using\nfrp\nvia IP address and not hostname, make sure to set the appropriate IP address in the Subject Alternative Name (SAN) area when generating SSL/TLS Certificates.\nGiven an example:\nPrepare openssl config file. It exists at\n/etc/pki/tls/openssl.cnf\nin Linux System and\n/System/Library/OpenSSL/openssl.cnf\nin MacOS, and you can copy it to current path, like\ncp /etc/pki/tls/openssl.cnf ./my-openssl.cnf\n. If not, you can build it by yourself, like:\ncat > my-openssl.cnf << EOF\n[ ca ]\ndefault_ca = CA_default\n[ CA_default ]\nx509_extensions = usr_cert\n[ req ]\ndefault_bits        = 2048\ndefault_md          = sha256\ndefault_keyfile     = privkey.pem\ndistinguished_name  = req_distinguished_name\nattributes          = req_attributes\nx509_extensions     = v3_ca\nstring_mask         = utf8only\n[ req_distinguished_name ]\n[ req_attributes ]\n[ usr_cert ]\nbasicConstraints       = CA:FALSE\nnsComment              = \"OpenSSL Generated Certificate\"\nsubjectKeyIdentifier   = hash\nauthorityKeyIdentifier = keyid,issuer\n[ v3_ca ]\nsubjectKeyIdentifier   = hash\nauthorityKeyIdentifier = keyid:always,issuer\nbasicConstraints       = CA:true\nEOF\nbuild ca certificates:\nopenssl genrsa -out ca.key 2048\nopenssl req -x509 -new -nodes -key ca.key -subj \"/CN=example.ca.com\" -days 5000 -out ca.crt\nbuild frps certificates:\nopenssl genrsa -out server.key 2048\n\nopenssl req -new -sha256 -key server.key \\\n    -subj \"/C=XX/ST=DEFAULT/L=DEFAULT/O=DEFAULT/CN=server.com\" \\\n    -reqexts SAN \\\n    -config <(cat my-openssl.cnf <(printf \"\\n[SAN]\\nsubjectAltName=DNS:localhost,IP:127.0.0.1,DNS:example.server.com\")) \\\n    -out server.csr\n\nopenssl x509 -req -days 365 -sha256 \\\n\t-in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\\n\t-extfile <(printf \"subjectAltName=DNS:localhost,IP:127.0.0.1,DNS:example.server.com\") \\\n\t-out server.crt\nbuild frpc certificates：\nopenssl genrsa -out client.key 2048\nopenssl req -new -sha256 -key client.key \\\n    -subj \"/C=XX/ST=DEFAULT/L=DEFAULT/O=DEFAULT/CN=client.com\" \\\n    -reqexts SAN \\\n    -config <(cat my-openssl.cnf <(printf \"\\n[SAN]\\nsubjectAltName=DNS:client.com,DNS:example.client.com\")) \\\n    -out client.csr\n\nopenssl x509 -req -days 365 -sha256 \\\n    -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\\n\t-extfile <(printf \"subjectAltName=DNS:client.com,DNS:example.client.com\") \\\n\t-out client.crt\nHot-Reloading frpc configuration\nThe\nwebServer\nfields are required for enabling HTTP API:\n#\nfrpc.toml\nwebServer.addr\n=\n\"\n127.0.0.1\n\"\nwebServer.port\n=\n7400\nThen run command\nfrpc reload -c ./frpc.toml\nand wait for about 10 seconds to let\nfrpc\ncreate or update or remove proxies.\nNote that global client parameters won't be modified except 'start'.\nYou can run command\nfrpc verify -c ./frpc.toml\nbefore reloading to check if there are config errors.\nGet proxy status from client\nUse\nfrpc status -c ./frpc.toml\nto get status of all proxies. The\nwebServer\nfields are required for enabling HTTP API.\nOnly allowing certain ports on the server\nallowPorts\nin\nfrps.toml\nis used to avoid abuse of ports:\n#\nfrps.toml\nallowPorts\n= [\n  {\nstart\n=\n2000\n,\nend\n=\n3000\n},\n  {\nsingle\n=\n3001\n},\n  {\nsingle\n=\n3003\n},\n  {\nstart\n=\n4000\n,\nend\n=\n50000\n}\n]\nPort Reuse\nvhostHTTPPort\nand\nvhostHTTPSPort\nin frps can use same port with\nbindPort\n. frps will detect the connection's protocol and handle it correspondingly.\nWhat you need to pay attention to is that if you want to configure\nvhostHTTPSPort\nand\nbindPort\nto the same port, you need to first set\ntransport.tls.disableCustomTLSFirstByte\nto false.\nWe would like to try to allow multiple proxies bind a same remote port with different protocols in the future.\nBandwidth Limit\nFor Each Proxy\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nssh\n\"\ntype\n=\n\"\ntcp\n\"\nlocalPort\n=\n22\nremotePort\n=\n6000\ntransport.bandwidthLimit\n=\n\"\n1MB\n\"\nSet\ntransport.bandwidthLimit\nin each proxy's configure to enable this feature. Supported units are\nMB\nand\nKB\n.\nSet\ntransport.bandwidthLimitMode\nto\nclient\nor\nserver\nto limit bandwidth on the client or server side. Default is\nclient\n.\nTCP Stream Multiplexing\nfrp supports tcp stream multiplexing since v0.10.0 like HTTP2 Multiplexing, in which case all logic connections to the same frpc are multiplexed into the same TCP connection.\nYou can disable this feature by modify\nfrps.toml\nand\nfrpc.toml\n:\n#\nfrps.toml and frpc.toml, must be same\ntransport.tcpMux\n=\nfalse\nSupport KCP Protocol\nKCP is a fast and reliable protocol that can achieve the transmission effect of a reduction of the average latency by 30% to 40% and reduction of the maximum delay by a factor of three, at the cost of 10% to 20% more bandwidth wasted than TCP.\nKCP mode uses UDP as the underlying transport. Using KCP in frp:\nEnable KCP in frps:\n#\nfrps.toml\nbindPort\n=\n7000\n#\nSpecify a UDP port for KCP.\nkcpBindPort\n=\n7000\nThe\nkcpBindPort\nnumber can be the same number as\nbindPort\n, since\nbindPort\nfield specifies a TCP port.\nConfigure\nfrpc.toml\nto use KCP to connect to frps:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\n#\nSame as the 'kcpBindPort' in frps.toml\nserverPort\n=\n7000\ntransport.protocol\n=\n\"\nkcp\n\"\nSupport QUIC Protocol\nQUIC is a new multiplexed transport built on top of UDP.\nUsing QUIC in frp:\nEnable QUIC in frps:\n#\nfrps.toml\nbindPort\n=\n7000\n#\nSpecify a UDP port for QUIC.\nquicBindPort\n=\n7000\nThe\nquicBindPort\nnumber can be the same number as\nbindPort\n, since\nbindPort\nfield specifies a TCP port.\nConfigure\nfrpc.toml\nto use QUIC to connect to frps:\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\n#\nSame as the 'quicBindPort' in frps.toml\nserverPort\n=\n7000\ntransport.protocol\n=\n\"\nquic\n\"\nConnection Pooling\nBy default, frps creates a new frpc connection to the backend service upon a user request. With connection pooling, frps keeps a certain number of pre-established connections, reducing the time needed to establish a connection.\nThis feature is suitable for a large number of short connections.\nConfigure the limit of pool count each proxy can use in\nfrps.toml\n:\n#\nfrps.toml\ntransport.maxPoolCount\n=\n5\nEnable and specify the number of connection pool:\n#\nfrpc.toml\ntransport.poolCount\n=\n1\nLoad balancing\nLoad balancing is supported by\ngroup\n.\nThis feature is only available for types\ntcp\n,\nhttp\n,\ntcpmux\nnow.\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\ntest1\n\"\ntype\n=\n\"\ntcp\n\"\nlocalPort\n=\n8080\nremotePort\n=\n80\nloadBalancer.group\n=\n\"\nweb\n\"\nloadBalancer.groupKey\n=\n\"\n123\n\"\n[[\nproxies\n]]\nname\n=\n\"\ntest2\n\"\ntype\n=\n\"\ntcp\n\"\nlocalPort\n=\n8081\nremotePort\n=\n80\nloadBalancer.group\n=\n\"\nweb\n\"\nloadBalancer.groupKey\n=\n\"\n123\n\"\nloadBalancer.groupKey\nis used for authentication.\nConnections to port 80 will be dispatched to proxies in the same group randomly.\nFor type\ntcp\n,\nremotePort\nin the same group should be the same.\nFor type\nhttp\n,\ncustomDomains\n,\nsubdomain\n,\nlocations\nshould be the same.\nService Health Check\nHealth check feature can help you achieve high availability with load balancing.\nAdd\nhealthCheck.type = \"tcp\"\nor\nhealthCheck.type = \"http\"\nto enable health check.\nWith health check type\ntcp\n, the service port will be pinged (TCPing):\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\ntest1\n\"\ntype\n=\n\"\ntcp\n\"\nlocalPort\n=\n22\nremotePort\n=\n6000\n#\nEnable TCP health check\nhealthCheck.type\n=\n\"\ntcp\n\"\n#\nTCPing timeout seconds\nhealthCheck.timeoutSeconds\n=\n3\n#\nIf health check failed 3 times in a row, the proxy will be removed from frps\nhealthCheck.maxFailed\n=\n3\n#\nA health check every 10 seconds\nhealthCheck.intervalSeconds\n=\n10\nWith health check type\nhttp\n, an HTTP request will be sent to the service and an HTTP 2xx OK response is expected:\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nweb\n\"\ntype\n=\n\"\nhttp\n\"\nlocalIP\n=\n\"\n127.0.0.1\n\"\nlocalPort\n=\n80\ncustomDomains\n= [\n\"\ntest.example.com\n\"\n]\n#\nEnable HTTP health check\nhealthCheck.type\n=\n\"\nhttp\n\"\n#\nfrpc will send a GET request to '/status'\n#\nand expect an HTTP 2xx OK response\nhealthCheck.path\n=\n\"\n/status\n\"\nhealthCheck.timeoutSeconds\n=\n3\nhealthCheck.maxFailed\n=\n3\nhealthCheck.intervalSeconds\n=\n10\nRewriting the HTTP Host Header\nBy default frp does not modify the tunneled HTTP requests at all as it's a byte-for-byte copy.\nHowever, speaking of web servers and HTTP requests, your web server might rely on the\nHost\nHTTP header to determine the website to be accessed. frp can rewrite the\nHost\nheader when forwarding the HTTP requests, with the\nhostHeaderRewrite\nfield:\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nweb\n\"\ntype\n=\n\"\nhttp\n\"\nlocalPort\n=\n80\ncustomDomains\n= [\n\"\ntest.example.com\n\"\n]\nhostHeaderRewrite\n=\n\"\ndev.example.com\n\"\nThe HTTP request will have the\nHost\nheader rewritten to\nHost: dev.example.com\nwhen it reaches the actual web server, although the request from the browser probably has\nHost: test.example.com\n.\nSetting other HTTP Headers\nSimilar to\nHost\n, You can override other HTTP request and response headers with proxy type\nhttp\n.\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nweb\n\"\ntype\n=\n\"\nhttp\n\"\nlocalPort\n=\n80\ncustomDomains\n= [\n\"\ntest.example.com\n\"\n]\nhostHeaderRewrite\n=\n\"\ndev.example.com\n\"\nrequestHeaders.set.x-from-where\n=\n\"\nfrp\n\"\nresponseHeaders.set.foo\n=\n\"\nbar\n\"\nIn this example, it will set header\nx-from-where: frp\nin the HTTP request and\nfoo: bar\nin the HTTP response.\nGet Real IP\nHTTP X-Forwarded-For\nThis feature is for\nhttp\nproxies or proxies with the\nhttps2http\nand\nhttps2https\nplugins enabled.\nYou can get user's real IP from HTTP request headers\nX-Forwarded-For\n.\nProxy Protocol\nfrp supports Proxy Protocol to send user's real IP to local services.\nHere is an example for https service:\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nweb\n\"\ntype\n=\n\"\nhttps\n\"\nlocalPort\n=\n443\ncustomDomains\n= [\n\"\ntest.example.com\n\"\n]\n#\nnow v1 and v2 are supported\ntransport.proxyProtocolVersion\n=\n\"\nv2\n\"\nYou can enable Proxy Protocol support in nginx to expose user's real IP in HTTP header\nX-Real-IP\n, and then read\nX-Real-IP\nheader in your web service for the real IP.\nRequire HTTP Basic Auth (Password) for Web Services\nAnyone who can guess your tunnel URL can access your local web server unless you protect it with a password.\nThis enforces HTTP Basic Auth on all requests with the username and password specified in frpc's configure file.\nIt can only be enabled when proxy type is http.\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nweb\n\"\ntype\n=\n\"\nhttp\n\"\nlocalPort\n=\n80\ncustomDomains\n= [\n\"\ntest.example.com\n\"\n]\nhttpUser\n=\n\"\nabc\n\"\nhttpPassword\n=\n\"\nabc\n\"\nVisit\nhttp://test.example.com\nin the browser and now you are prompted to enter the username and password.\nCustom Subdomain Names\nIt is convenient to use\nsubdomain\nconfigure for http and https types when many people share one frps server.\n#\nfrps.toml\nsubDomainHost\n=\n\"\nfrps.com\n\"\nResolve\n*.frps.com\nto the frps server's IP. This is usually called a Wildcard DNS record.\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nweb\n\"\ntype\n=\n\"\nhttp\n\"\nlocalPort\n=\n80\nsubdomain\n=\n\"\ntest\n\"\nNow you can visit your web service on\ntest.frps.com\n.\nNote that if\nsubdomainHost\nis not empty,\ncustomDomains\nshould not be the subdomain of\nsubdomainHost\n.\nURL Routing\nfrp supports forwarding HTTP requests to different backend web services by url routing.\nlocations\nspecifies the prefix of URL used for routing. frps first searches for the most specific prefix location given by literal strings regardless of the listed order.\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nweb01\n\"\ntype\n=\n\"\nhttp\n\"\nlocalPort\n=\n80\ncustomDomains\n= [\n\"\nweb.example.com\n\"\n]\nlocations\n= [\n\"\n/\n\"\n]\n\n[[\nproxies\n]]\nname\n=\n\"\nweb02\n\"\ntype\n=\n\"\nhttp\n\"\nlocalPort\n=\n81\ncustomDomains\n= [\n\"\nweb.example.com\n\"\n]\nlocations\n= [\n\"\n/news\n\"\n,\n\"\n/about\n\"\n]\nHTTP requests with URL prefix\n/news\nor\n/about\nwill be forwarded to\nweb02\nand other requests to\nweb01\n.\nTCP Port Multiplexing\nfrp supports receiving TCP sockets directed to different proxies on a single port on frps, similar to\nvhostHTTPPort\nand\nvhostHTTPSPort\n.\nThe only supported TCP port multiplexing method available at the moment is\nhttpconnect\n- HTTP CONNECT tunnel.\nWhen setting\ntcpmuxHTTPConnectPort\nto anything other than 0 in frps, frps will listen on this port for HTTP CONNECT requests.\nThe host of the HTTP CONNECT request will be used to match the proxy in frps. Proxy hosts can be configured in frpc by configuring\ncustomDomains\nand / or\nsubdomain\nunder\ntcpmux\nproxies, when\nmultiplexer = \"httpconnect\"\n.\nFor example:\n#\nfrps.toml\nbindPort\n=\n7000\ntcpmuxHTTPConnectPort\n=\n1337\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\n[[\nproxies\n]]\nname\n=\n\"\nproxy1\n\"\ntype\n=\n\"\ntcpmux\n\"\nmultiplexer\n=\n\"\nhttpconnect\n\"\ncustomDomains\n= [\n\"\ntest1\n\"\n]\nlocalPort\n=\n80\n[[\nproxies\n]]\nname\n=\n\"\nproxy2\n\"\ntype\n=\n\"\ntcpmux\n\"\nmultiplexer\n=\n\"\nhttpconnect\n\"\ncustomDomains\n= [\n\"\ntest2\n\"\n]\nlocalPort\n=\n8080\nIn the above configuration - frps can be contacted on port 1337 with a HTTP CONNECT header such as:\nCONNECT test1 HTTP/1.1\\r\\n\\r\\n\nand the connection will be routed to\nproxy1\n.\nConnecting to frps via PROXY\nfrpc can connect to frps through proxy if you set OS environment variable\nHTTP_PROXY\n, or if\ntransport.proxyURL\nis set in frpc.toml file.\nIt only works when protocol is tcp.\n#\nfrpc.toml\nserverAddr\n=\n\"\nx.x.x.x\n\"\nserverPort\n=\n7000\ntransport.proxyURL\n=\n\"\nhttp://user:pwd@192.168.1.128:8080\n\"\nPort range mapping\nAdded in v0.56.0\nWe can use the range syntax of Go template combined with the built-in\nparseNumberRangePair\nfunction to achieve port range mapping.\nThe following example, when run, will create 8 proxies named\ntest-6000, test-6001 ... test-6007\n, each mapping the remote port to the local port.\n{{- range $_, $v := parseNumberRangePair \"6000-6006,6007\" \"6000-6006,6007\" }}\n[[proxies]]\nname = \"tcp-{{ $v.First }}\"\ntype = \"tcp\"\nlocalPort = {{ $v.First }}\nremotePort = {{ $v.Second }}\n{{- end }}\nClient Plugins\nfrpc only forwards requests to local TCP or UDP ports by default.\nPlugins are used for providing rich features. There are built-in plugins such as\nunix_domain_socket\n,\nhttp_proxy\n,\nsocks5\n,\nstatic_file\n,\nhttp2https\n,\nhttps2http\n,\nhttps2https\nand you can see\nexample usage\n.\nUsing plugin\nhttp_proxy\n:\n#\nfrpc.toml\n[[\nproxies\n]]\nname\n=\n\"\nhttp_proxy\n\"\ntype\n=\n\"\ntcp\n\"\nremotePort\n=\n6000\n[\nproxies\n.\nplugin\n]\ntype\n=\n\"\nhttp_proxy\n\"\nhttpUser\n=\n\"\nabc\n\"\nhttpPassword\n=\n\"\nabc\n\"\nhttpUser\nand\nhttpPassword\nare configuration parameters used in\nhttp_proxy\nplugin.\nServer Manage Plugins\nRead the\ndocument\n.\nFind more plugins in\ngofrp/plugin\n.\nSSH Tunnel Gateway\nadded in v0.53.0\nfrp supports listening to an SSH port on the frps side and achieves TCP protocol proxying through the SSH -R protocol, without relying on frpc.\n#\nfrps.toml\nsshTunnelGateway.bindPort\n=\n2200\nWhen running\n./frps -c frps.toml\n, a private key file named\n.autogen_ssh_key\nwill be automatically created in the current working directory. This generated private key file will be used by the SSH server in frps.\nExecuting the command\nssh -R :80:127.0.0.1:8080 v0@{frp address} -p 2200 tcp --proxy_name\n\"\ntest-tcp\n\"\n--remote_port 9090\nsets up a proxy on frps that forwards the local 8080 service to the port 9090.\nfrp (via SSH) (Ctrl+C to quit)\n\nUser:\nProxyName: test-tcp\nType: tcp\nRemoteAddress: :9090\nThis is equivalent to:\nfrpc tcp --proxy_name\n\"\ntest-tcp\n\"\n--local_ip 127.0.0.1 --local_port 8080 --remote_port 9090\nPlease refer to this\ndocument\nfor more information.\nVirtual Network (VirtualNet)\nAlpha feature added in v0.62.0\nThe VirtualNet feature enables frp to create and manage virtual network connections between clients and visitors through a TUN interface. This allows for IP-level routing between machines, extending frp beyond simple port forwarding to support full network connectivity.\nFor detailed information about configuration and usage, please refer to the\nVirtualNet documentation\n.\nFeature Gates\nfrp supports feature gates to enable or disable experimental features. This allows users to try out new features before they're considered stable.\nAvailable Feature Gates\nName\nStage\nDefault\nDescription\nVirtualNet\nALPHA\nfalse\nVirtual network capabilities for frp\nEnabling Feature Gates\nTo enable an experimental feature, add the feature gate to your configuration:\nfeatureGates\n= {\nVirtualNet\n=\ntrue\n}\nFeature Lifecycle\nFeatures typically go through three stages:\nALPHA\n: Disabled by default, may be unstable\nBETA\n: May be enabled by default, more stable but still evolving\nGA (Generally Available)\n: Enabled by default, ready for production use\nRelated Projects\ngofrp/plugin\n- A repository for frp plugins that contains a variety of plugins implemented based on the frp extension mechanism, meeting the customization needs of different scenarios.\ngofrp/tiny-frpc\n- A lightweight version of the frp client (around 3.5MB at minimum) implemented using the ssh protocol, supporting some of the most commonly used features, suitable for devices with limited resources.\nContributing\nInterested in getting involved? We would like to help you!\nTake a look at our\nissues list\nand consider sending a Pull Request to\ndev branch\n.\nIf you want to add a new feature, please create an issue first to describe the new feature, as well as the implementation approach. Once a proposal is accepted, create an implementation of the new features and submit it as a pull request.\nSorry for my poor English. Improvements for this document are welcome, even some typo fixes.\nIf you have great ideas, send an email to\nfatedier@gmail.com\n.\nNote: We prefer you to give your advise in\nissues\n, so others with a same question can search it quickly and we don't need to answer them repeatedly.\nDonation\nIf frp helps you a lot, you can support us by:\nGitHub Sponsors\nSupport us by\nGithub Sponsors\n.\nYou can have your company's logo placed on README file of this project.\nPayPal\nDonate money by\nPayPal\nto my account\nfatedier@gmail.com\n.",
        "今日の獲得スター数: 36",
        "累積スター数: 99,464"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/fatedier/frp"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/NaiboWang/EasySpider",
      "title": "NaiboWang/EasySpider",
      "date": null,
      "executive_summary": [
        "A visual no-code/code-free web crawler/spider易采集：一个可视化浏览器自动化测试/数据采集/爬虫软件，可以无代码图形化的设计和执行爬虫任务。别名：ServiceWrapper面向Web应用的智能化服务封装系统。",
        "---",
        "易采集/EasySpider: Visual Code-Free Web Crawler\n一个\n完全免费\n（\n包括商业使用和二次开发\n）的可视化浏览器自动化测试/数据采集/爬虫软件，可以使用图形化界面，无代码可视化的设计和执行任务。只需要在网页上选择自己想要操作的内容并根据提示框操作即可完成任务的设计和执行。同时软件还可以单独以命令行的方式进行执行，从而可以很方便的嵌入到其他系统中。\nA\ncompletely free (including for commercial use and secondary development)\nvisual browser automation test/data collection/crawler software, which can be used to design and execute tasks in a code-free visual way. You only need to select the content you want to operate on the web page and follow the prompts to complete the design and execution of the task. At the same time, the software can also be executed separately in the command line, so that it can be easily embedded into other systems.\n下载易采集/Download EasySpider\n进入\nReleases Page\n下载最新版本。如果下载速度慢，可以考虑中国境内下载地址：\n中国境内下载地址\n。\nRefer to the\nReleases Page\nto download the latest version of EasySpider.\n赞助者/Sponsors\n亮数据BrightData\n是代理市场领导者，覆盖全球的7200万IP，提供真人住宅IP，即时批量采集网络公开数据，成功率亲测有保证。需要性价比高代理IP的可\n点击上方图片注册\n后联系中文客服，开通后免费试用，\n现在有首充多少就送多少的活动\n。BrightData可配合EasySpider进行数据采集。\nBestProxy\n全球独享专属资源池，优选海外195+国家/地区高质量住宅IP，本地ISP原生IP，不限量住宅代理、长效ISP代理、静态数据中心代理、网页爬虫API，城市级精准定位，支持HTTP(S)和SOCKS5协议，低检测风险，全方位代理服务解决方案，助力各种场景业务IP代理需求。$0.66/G起按需付费和长期套餐，适合不同预算需求，24/7多语言支持，联系客服免费试用500M。可与EasySpider工具配合使用，高效采集网络数据。\nIPdodo\n专注为跨境用户，提供独享/纯净/家宽/原生/双ISP的全球代理IP，不限流量。全球8000万真实住宅IP，覆盖200+国家/地区，99.9%匿名保护，且支持Http/Https/Socks5协议，满足爬虫、数据采集、跨境电商、tk/fb流媒体等业务场景。现在前往IPdodo注册，支持免费试用。\n官方网站/Official Website\n访问易采集官网：\nwww.easyspider.cn\nVisit the official website of EasySpider:\nwww.easyspider.net\n软件使用示例/Software Usage Example\n示例1/Example 1\n（右键）选中一个大商品块 -> 软件自动检测到同类型商品块 -> 点击“选中全部”选项 -> 点击“选中子元素”选项 -> 点击“采集数据”选项，即可采集到所有商品的所有信息，并分成不同字段保存。\n(Right click) Select a large product block -> The software will automatically detect similar blocks -> Click the 'Select All' option -> Click the 'Select Child Elements' option -> Click the 'Collect Data' option, you can collect the information of all products, and will be saved by sub-field.\n示例2/Example 2\n（右键）选中一个商品标题，同类型标题会被自动匹配，点击“选中全部”选项 -> 点击“采集数据”选项，即可采集到所有商品的标题信息。\n同时，选中全部后如果选择“循环点击每个元素”选项，即可自动打开每个商品的详情页，然后可以再继续设置采集详情页的信息。\n(Right Click) Select a product title, the same type of title will be automatically matched, click the 'Select All' option -> Click the 'Collect Data' option, you can collect the title information of all products.\nAt the same time, if you select the 'Loop-click every element' option after selecting all, you can automatically open the details page of each product, and then can set to collect the information of the details page.\n更多特性/More Features\n更多特性请翻到页面底部查看。\nMore features please scroll to the bottom of this page to view.\n支持作者/Support Author\n易采集EasySpider是一款完全免费且使用中无广告的开源软件，软件开发和维护全靠作者用爱发电，因此您可以选择支持作者让作者有更多的热情和精力维护此软件，或者您使用了此软件进行了盈利，欢迎您通过下面的方式支持作者：\nGithub Sponsor：直接点击右侧\nSponsor\n按钮赞助。\n支付宝账号：\nnaibowang@foxmail.com\n，也可以扫描下方二维码。\n微信收款：扫描下方二维码。\nPayPal账号：naibowang，也可以扫描下方二维码。\nYou can support the author by clicking the\nSponsor\nbutton at right side or pay via paypal: naibowang.\n文档/Documentation\n请点此进入\n教程文档\n，如有英文可暂时翻译一下，或看作者的\n硕士毕业论文\n（主要看第三章和第五章）。\nEbay样例博客：\nhttps://blog.csdn.net/ihero/article/details/130805504\n。\nDocumentation can be found from\nGitHub Wiki\n.\n视频教程/Video Tutorials\nBilibili/B站视频教程:\nEasySpider介绍 - 中国地震台网采集案例\n设置页面向下滚动\n如何无代码可视化的爬取需要登录才能爬的网站 - 知乎网站案例\n循环点击列表中每个链接进入详情页采集详情页内容+设计时动态调试+动态JS\n实战采集汽车网文章内容并下载文章内图片\n定时执行任务+选中子元素多种模式+将提取值作为变量输入\n【重要】自定义条件判断之使用循环项内的JS命令返回值 - 第二弹\n流程图执行逻辑解析 - 58同城房源描述采集案例\nMacOS系统设计和执行eBay网站爬虫任务教程\n如何执行自己写的JS代码和系统代码 （自定义操作）\n如何自定义循环和判断条件 - 第一弹\n如何对元素和网页截图及命令行执行指南\nOCR识别元素内容功能（常用于文字验证码）\n如何爬需要输入验证码的网站\n如何切换IP池和使用隧道IP - 打开详情页采集案例\n如何同时执行多个任务（并行多开）\nPython代码运算后的结果作为文本框的输入\n实例 - 反人类网站文章采集和代码调试\n写入MySQL数据库教程\n从源代码编译程序并设计运行和调试任务指南（基于Ubuntu24.04）\nRefer to\nYoutube Playlist\nto see the video tutorials of EasySpider.\n样例任务/Sample Tasks\n从本项目的\nExamples\n文件夹中下载样例任务，更名为大于0的数字，导入到EasySpider中的\ntasks\n文件夹中，然后在EasySpider中打开即可。\nDownload sample tasks from the\nExamples\nfolder of this project, rename them to numbers greater than 0, import them into the\ntasks\nfolder in EasySpider, and then open them in EasySpider.\n声明/Declaration\n本软件仅供学习交流使用，\n严禁使用软件进行任何违法违规的操作，如爬取不允许爬取的政府/军事机关网站等\n。使用本软件所造成的\n一切后果由使用者自负\n，与作者本人无关，\n作者不会承担任何责任\n。\nThis software is for learning and communication only.\nIt is strictly forbidden to use the software for any illegal operations, such as crawling government/military websites that are not allowed to be crawled.\nAll consequences caused by the use of this software are\nat the user's own risk, and the author is not responsible for any consequences\n.\n对于政府和军事机关等网站的爬虫操作，\n作者将不会进行任何答疑\n，以免违反国家相关法律法规和政策。\nFor the crawler operations of government and military websites,\nthe author will not answer any questions\nin order to avoid violating relevant national laws, regulations and policies.\nEasySpider遵循AGPL-3.0协议，\n任何个人和企业都可以免费使用软件本身或使用源代码进行二次开发，无需联系作者进行商业（专利）授权\n，但需要注意AGPL-3.0协议的相关规则：\nEasySpider complies with the AGPL-3.0 agreement.\nAny individual or enterprise can use the software for free and use the software source code for secondary development without contacting the author for commercial (patent) authorization.\nHowever, it is necessary to pay attention to the related rules of the AGPL-3.0 agreement:\n1. Copyleft（传染性） / Copyleft (Viral Clause)\n衍生作品 / Derivative Works\n任何基于 AGPL 代码的修改或衍生作品，必须\n以相同许可证（AGPL-3.0）发布\n。\nAny modifications or derivative works based on AGPL code must be\nlicensed under AGPL-3.0\n.\n联动范围 / Scope of Copyleft\n若 AGPL 代码与其他代码结合（如静态链接、紧密集成），整个作品需遵守 AGPL。\nIf AGPL code is combined with other code (e.g., static linking), the entire work must comply with AGPL.\n2. 网络使用条款 / Network Use Clause\nSaaS 触发开源义务 / SaaS Trigger\n若软件以服务形式提供（如网站、API），必须向所有用户公开\n完整对应源代码\n（包括修改后的代码）。\nIf the software is provided as a service (e.g., website, API), the\nfull corresponding source code\n(including modifications) must be made available to all users.\n用户权利 / User Rights\n服务的接收者可通过下载或书面请求获取源码。\nService recipients may obtain the source code via download or written request.\n3. 源码提供要求 / Source Code Provision\n二进制分发 / Binary Distribution\n必须附带源码或提供获取渠道（如下载链接）。\nSource code must be included or a download link provided.\n网络服务场景 / Network Service Scenario\n需通过服务界面\n显式提供源码链接\n，或向用户书面承诺提供源码。\nThe service interface must\nexplicitly provide a source code link\nor offer a written offer for source code.\n4. 专利授权 / Patent Grant\n贡献者自动授予用户与软件相关的专利许可，禁止专利诉讼。\nContributors automatically grant users patent rights related to the software, and prohibit patent litigation.\n5. 免责声明 / Disclaimer\n软件按“原样”提供，作者\n不承担任何责任\n（无担保条款）。\nThe software is provided \"as is\" with\nno warranties or liabilities\n.\n答疑QQ群\n群号：\n682921940\n，建议通过Github提Issue的方式答疑，如果实在有需要才请加QQ群，因为群人数有上限，\nQQ群不提供软件下载功能\n。\n出版物/Publications\nThis software has been accepted by The Web Conference (WWW) 2023 (中国计算机学会顶级会议，CCF A):\nEasySpider: A No-Code Visual System for Crawling the Web\n, April 2023.\n中国国家知识产权局发明专利，\n一种自定义提取流程的服务封装系统\n， 2022年5月。\n浙江大学硕士论文\n，\n面向WEB应用的智能化服务封装系统设计与实现\n，2020年6月。\n编译说明/Compilation Instructions\n查看\n编译说明\n。\nRefer to\nCompilation Instructions\n.\n支持特性/Supported Features\n中文界面截图\n软件界面示例\n块和子块及表单定义\n已选中和待选择示例\n京东商品块选择示例：\n京东商品标题自动匹配选择示例\n分块选择所有子元素示例\n同类型元素自动和手动匹配示例\n四种选择方式示例\n输入文字示例\n循环点击58同城房屋标题以进入详情页采集示例\n采集元素文本示例\n流程图界面介绍\n循环选项示例\n循环点击下一页示例\n条件分支示例\n完整采集流程图示例\n完整采集流程图转换为常规流程图示例\n服务信息示例\n服务调用示例\n58 同城房源信息采集服务部分采集结果展示",
        "今日の獲得スター数: 36",
        "累積スター数: 42,881"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/NaiboWang/EasySpider"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/katanemo/archgw",
      "title": "katanemo/archgw",
      "date": null,
      "executive_summary": [
        "The smart edge and AI gateway for agents. Arch is a high-performance proxy server that handles the low-level work in building agents: like applying guardrails, routing prompts to the right agent, and unifying access to LLMs, etc. Natively designed to handle and process prompts, Arch helps you build agents faster.",
        "---",
        "Arch is a modular ai-native edge and AI gateway for agents.\nArch handles the\npesky low-level work\nin building agentic apps — like applying guardrails, clarifying vague user input, routing prompts to the right agent, and unifying access to any LLM. It’s a language and framework friendly infrastructure layer designed to help you build and ship agentic apps faster.\nQuickstart\n•\nDemos\n•\nRoute LLMs\n•\nBuild agentic apps with Arch\n•\nDocumentation\n•\nContact\nAbout The Latest Release:\n[0.3.15]\nPreference-aware multi LLM routing for Claude Code 2.0\nOverview\nAI demos are easy to hack. But once you move past a prototype, you’re stuck building and maintaining low-level plumbing code that slows down real innovation. For example:\nRouting & orchestration.\nPut routing in code and you’ve got two choices: maintain it yourself or live with a framework’s baked-in logic. Either way, keeping routing consistent means pushing code changes across all your agents, slowing iteration and turning every policy tweak into a refactor instead of a config flip.\nModel integration churn.\nFrameworks wire LLM integrations directly into code abstractions, making it hard to add or swap models without touching application code — meaning you’ll have to do codewide search/replace every time you want to experiment with a new model or version.\nObservability & governance.\nLogging, tracing, and guardrails are baked in as tightly coupled features, so bringing in best-of-breed solutions is painful and often requires digging through the guts of a framework.\nPrompt engineering overhead\n. Input validation, clarifying vague user input, and coercing outputs into the right schema all pile up, turning what should be design work into low-level plumbing work.\nBrittle upgrades\n. Every change (new model, new guardrail, new trace format) means patching and redeploying application servers. Contrast that with bouncing a central proxy—one upgrade, instantly consistent everywhere.\nWith Arch, you can move faster by focusing on higher-level objectives in a language and framework agnostic way.\nArch\nwas built by the contributors of\nEnvoy Proxy\nwith the belief that:\nPrompts are nuanced and opaque user requests, which require the same capabilities as traditional HTTP requests including secure handling, intelligent routing, robust observability, and integration with backend (API) systems to improve speed and accuracy for common agentic scenarios  – all outside core application logic.*\nCore Features\n:\n🚦 Route to Agents\n: Engineered with purpose-built\nLLMs\nfor fast (<100ms) agent routing and hand-off\n🔗 Route to LLMs\n: Unify access to LLMs with support for\nthree routing strategies\n.\n⛨ Guardrails\n: Centrally configure and prevent harmful outcomes and ensure safe user interactions\n⚡ Tools Use\n: For common agentic scenarios let Arch instantly clarify and convert prompts to tools/API calls\n🕵 Observability\n: W3C compatible request tracing and LLM metrics that instantly plugin with popular tools\n🧱 Built on Envoy\n: Arch runs alongside app servers as a containerized process, and builds on top of\nEnvoy's\nproven HTTP management and scalability features to handle ingress and egress traffic related to prompts and LLMs.\nHigh-Level Sequence Diagram\n:\nJump to our\ndocs\nto learn how you can use Arch to improve the speed, security and personalization of your GenAI apps.\nImportant\nToday, the function calling LLM (Arch-Function) designed for the agentic and RAG scenarios is hosted free of charge in the US-central region. To offer consistent latencies and throughput, and to manage our expenses, we will enable access to the hosted version via developers keys soon, and give you the option to run that LLM locally. For more details see this issue\n#258\nContact\nTo get in touch with us, please join our\ndiscord server\n. We will be monitoring that actively and offering support there.\nDemos\nSample App: Weather Forecast Agent\n- A sample agentic weather forecasting app that highlights core function calling capabilities of Arch.\nSample App: Network Operator Agent\n- A simple network device switch operator agent that can retrive device statistics and reboot them.\nUser Case: Connecting to SaaS APIs\n- Connect 3rd party SaaS APIs to your agentic chat experience.\nQuickstart\nFollow this quickstart guide to use Arch as a router for local or hosted LLMs, including dynamic routing. Later in the section we will see how you can Arch to build highly capable agentic applications, and to provide e2e observability.\nPrerequisites\nBefore you begin, ensure you have the following:\nDocker System\n(v24)\nDocker compose\n(v2.29)\nPython\n(v3.13)\nArch's CLI allows you to manage and interact with the Arch gateway efficiently. To install the CLI, simply run the following command:\nTip\nWe recommend that developers create a new Python virtual environment to isolate dependencies before installing Arch. This ensures that archgw and its dependencies do not interfere with other packages on your system.\n$\npython3.12 -m venv venv\n$\nsource\nvenv/bin/activate\n#\nOn Windows, use: venv\\Scripts\\activate\n$\npip install archgw==0.3.15\nUse Arch as a LLM Router\nArch supports three powerful routing strategies for LLMs: model-based routing, alias-based routing, and preference-based routing. Each strategy offers different levels of abstraction and control for managing your LLM infrastructure.\nModel-based Routing\nModel-based routing allows you to configure specific models with static routing. This is ideal when you need direct control over which models handle specific requests. Arch supports 11+ LLM providers including OpenAI, Anthropic, DeepSeek, Mistral, Groq, and more.\nversion\n:\nv0.1.0\nlisteners\n:\negress_traffic\n:\naddress\n:\n0.0.0.0\nport\n:\n12000\nmessage_format\n:\nopenai\ntimeout\n:\n30s\nllm_providers\n:\n  -\nmodel\n:\nopenai/gpt-4o\naccess_key\n:\n$OPENAI_API_KEY\ndefault\n:\ntrue\n-\nmodel\n:\nanthropic/claude-3-5-sonnet-20241022\naccess_key\n:\n$ANTHROPIC_API_KEY\nYou can then route to specific models using any OpenAI-compatible client:\nfrom\nopenai\nimport\nOpenAI\nclient\n=\nOpenAI\n(\nbase_url\n=\n\"http://127.0.0.1:12000/v1\"\n,\napi_key\n=\n\"test\"\n)\n# Route to specific model\nresponse\n=\nclient\n.\nchat\n.\ncompletions\n.\ncreate\n(\nmodel\n=\n\"anthropic/claude-3-5-sonnet-20241022\"\n,\nmessages\n=\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Explain quantum computing\"\n}]\n)\nAlias-based Routing\nAlias-based routing lets you create semantic model names that map to underlying providers. This approach decouples your application code from specific model names, making it easy to experiment with different models or handle provider changes.\nversion\n:\nv0.1.0\nlisteners\n:\negress_traffic\n:\naddress\n:\n0.0.0.0\nport\n:\n12000\nmessage_format\n:\nopenai\ntimeout\n:\n30s\nllm_providers\n:\n  -\nmodel\n:\nopenai/gpt-4o\naccess_key\n:\n$OPENAI_API_KEY\n-\nmodel\n:\nanthropic/claude-3-5-sonnet-20241022\naccess_key\n:\n$ANTHROPIC_API_KEY\nmodel_aliases\n:\n#\nModel aliases - friendly names that map to actual model names\nfast-model\n:\ntarget\n:\ngpt-4o-mini\nreasoning-model\n:\ntarget\n:\ngpt-4o\ncreative-model\n:\ntarget\n:\nclaude-3-5-sonnet-20241022\nUse semantic aliases in your application code:\n# Your code uses semantic names instead of provider-specific ones\nresponse\n=\nclient\n.\nchat\n.\ncompletions\n.\ncreate\n(\nmodel\n=\n\"reasoning-model\"\n,\n# Routes to best available reasoning model\nmessages\n=\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Solve this complex problem...\"\n}]\n)\nPreference-aligned Routing\nPreference-aligned routing provides intelligent, dynamic model selection based on natural language descriptions of tasks and preferences. Instead of hardcoded routing logic, you describe what each model is good at using plain English.\nversion\n:\nv0.1.0\nlisteners\n:\negress_traffic\n:\naddress\n:\n0.0.0.0\nport\n:\n12000\nmessage_format\n:\nopenai\ntimeout\n:\n30s\nllm_providers\n:\n  -\nmodel\n:\nopenai/gpt-4o\naccess_key\n:\n$OPENAI_API_KEY\nrouting_preferences\n:\n      -\nname\n:\ncomplex_reasoning\ndescription\n:\ndeep analysis, mathematical problem solving, and logical reasoning\n-\nname\n:\ncreative_writing\ndescription\n:\nstorytelling, creative content, and artistic writing\n-\nmodel\n:\ndeepseek/deepseek-coder\naccess_key\n:\n$DEEPSEEK_API_KEY\nrouting_preferences\n:\n      -\nname\n:\ncode_generation\ndescription\n:\ngenerating new code, writing functions, and creating scripts\n-\nname\n:\ncode_review\ndescription\n:\nanalyzing existing code for bugs, improvements, and optimization\nArch uses a lightweight 1.5B autoregressive model to intelligently map user prompts to these preferences, automatically selecting the best model for each request. This approach adapts to intent drift, supports multi-turn conversations, and avoids brittle embedding-based classifiers or manual if/else chains. No retraining required when adding models or updating policies — routing is governed entirely by human-readable rules.\nLearn More\n: Check our\ndocumentation\nfor comprehensive provider setup guides and routing strategies. You can learn more about the design, benchmarks, and methodology behind preference-based routing in our paper:\nBuild Agentic Apps with Arch\nIn following quickstart we will show you how easy it is to build AI agent with Arch gateway. We will build a currency exchange agent using following simple steps. For this demo we will use\nhttps://api.frankfurter.dev/\nto fetch latest price for currencies and assume USD as base currency.\nStep 1. Create arch config file\nCreate\narch_config.yaml\nfile with following content,\nversion\n:\nv0.1.0\nlisteners\n:\ningress_traffic\n:\naddress\n:\n0.0.0.0\nport\n:\n10000\nmessage_format\n:\nopenai\ntimeout\n:\n30s\nllm_providers\n:\n  -\naccess_key\n:\n$OPENAI_API_KEY\nmodel\n:\nopenai/gpt-4o\nsystem_prompt\n:\n|\nYou are a helpful assistant.\nprompt_guards\n:\ninput_guards\n:\njailbreak\n:\non_exception\n:\nmessage\n:\nLooks like you're curious about my abilities, but I can only provide assistance for currency exchange.\nprompt_targets\n:\n  -\nname\n:\ncurrency_exchange\ndescription\n:\nGet currency exchange rate from USD to other currencies\nparameters\n:\n      -\nname\n:\ncurrency_symbol\ndescription\n:\nthe currency that needs conversion\nrequired\n:\ntrue\ntype\n:\nstr\nin_path\n:\ntrue\nendpoint\n:\nname\n:\nfrankfurther_api\npath\n:\n/v1/latest?base=USD&symbols={currency_symbol}\nsystem_prompt\n:\n|\nYou are a helpful assistant. Show me the currency symbol you want to convert from USD.\n-\nname\n:\nget_supported_currencies\ndescription\n:\nGet list of supported currencies for conversion\nendpoint\n:\nname\n:\nfrankfurther_api\npath\n:\n/v1/currencies\nendpoints\n:\nfrankfurther_api\n:\nendpoint\n:\napi.frankfurter.dev:443\nprotocol\n:\nhttps\nStep 2. Start arch gateway with currency conversion config\n$ archgw up arch_config.yaml\n2024-12-05 16:56:27,979 - cli.main - INFO - Starting archgw cli version: 0.3.15\n2024-12-05 16:56:28,485 - cli.utils - INFO - Schema validation successful\n!\n2024-12-05 16:56:28,485 - cli.main - INFO - Starting arch model server and arch gateway\n2024-12-05 16:56:51,647 - cli.core - INFO - Container is healthy\n!\nOnce the gateway is up you can start interacting with at port 10000 using openai chat completion API.\nSome of the sample queries you can ask could be\nwhat is currency rate for gbp?\nor\nshow me list of currencies for conversion\n.\nStep 3. Interacting with gateway using curl command\nHere is a sample curl command you can use to interact,\n$ curl --header\n'\nContent-Type: application/json\n'\n\\\n  --data\n'\n{\"messages\": [{\"role\": \"user\",\"content\": \"what is exchange rate for gbp\"}], \"model\": \"none\"}\n'\n\\\n  http://localhost:10000/v1/chat/completions\n|\njq\n\"\n.choices[0].message.content\n\"\n\"\nAs of the date provided in your context, December 5, 2024, the exchange rate for GBP (British Pound) from USD (United States Dollar) is 0.78558. This means that 1 USD is equivalent to 0.78558 GBP.\n\"\nAnd to get list of supported currencies,\n$ curl --header\n'\nContent-Type: application/json\n'\n\\\n  --data\n'\n{\"messages\": [{\"role\": \"user\",\"content\": \"show me list of currencies that are supported for conversion\"}], \"model\": \"none\"}\n'\n\\\n  http://localhost:10000/v1/chat/completions\n|\njq\n\"\n.choices[0].message.content\n\"\n\"\nHere is a list of the currencies that are supported for conversion from USD, along with their symbols:\\n\\n1. AUD - Australian Dollar\\n2. BGN - Bulgarian Lev\\n3. BRL - Brazilian Real\\n4. CAD - Canadian Dollar\\n5. CHF - Swiss Franc\\n6. CNY - Chinese Renminbi Yuan\\n7. CZK - Czech Koruna\\n8. DKK - Danish Krone\\n9. EUR - Euro\\n10. GBP - British Pound\\n11. HKD - Hong Kong Dollar\\n12. HUF - Hungarian Forint\\n13. IDR - Indonesian Rupiah\\n14. ILS - Israeli New Sheqel\\n15. INR - Indian Rupee\\n16. ISK - Icelandic Króna\\n17. JPY - Japanese Yen\\n18. KRW - South Korean Won\\n19. MXN - Mexican Peso\\n20. MYR - Malaysian Ringgit\\n21. NOK - Norwegian Krone\\n22. NZD - New Zealand Dollar\\n23. PHP - Philippine Peso\\n24. PLN - Polish Złoty\\n25. RON - Romanian Leu\\n26. SEK - Swedish Krona\\n27. SGD - Singapore Dollar\\n28. THB - Thai Baht\\n29. TRY - Turkish Lira\\n30. USD - United States Dollar\\n31. ZAR - South African Rand\\n\\nIf you want to convert USD to any of these currencies, you can select the one you are interested in.\n\"\nObservability\nArch is designed to support best-in class observability by supporting open standards. Please read our\ndocs\non observability for more details on tracing, metrics, and logs. The screenshot below is from our integration with Signoz (among others)\nDebugging\nWhen debugging issues / errors application logs and access logs provide key information to give you more context on whats going on with the system. Arch gateway runs in info log level and following is a typical output you could see in a typical interaction between developer and arch gateway,\n$ archgw up --service archgw --foreground\n...\n[2025-03-26 18:32:01.350][26][info] prompt_gateway: on_http_request_body: sending request to model server\n[2025-03-26 18:32:01.851][26][info] prompt_gateway: on_http_call_response: model server response received\n[2025-03-26 18:32:01.852][26][info] prompt_gateway: on_http_call_response: dispatching api call to developer endpoint: weather_forecast_service, path: /weather, method: POST\n[2025-03-26 18:32:01.882][26][info] prompt_gateway: on_http_call_response: developer api call response received: status code: 200\n[2025-03-26 18:32:01.882][26][info] prompt_gateway: on_http_call_response: sending request to upstream llm\n[2025-03-26 18:32:01.883][26][info] llm_gateway: on_http_request_body: provider: gpt-4o-mini, model requested: None, model selected: gpt-4o-mini\n[2025-03-26 18:32:02.818][26][info] llm_gateway: on_http_response_body: time to first token: 1468ms\n[2025-03-26 18:32:04.532][26][info] llm_gateway: on_http_response_body: request latency: 3183ms\n...\nLog level can be changed to debug to get more details. To enable debug logs edit (supervisord.conf)[arch/supervisord.conf], change the log level\n--component-log-level wasm:info\nto\n--component-log-level wasm:debug\n. And after that you need to rebuild docker image and restart the arch gateway using following set of commands,\n# make sure you are at the root of the repo\n$ archgw build\n# go to your service that has arch_config.yaml file and issue following command,\n$ archgw up --service archgw --foreground\nContribution\nWe would love feedback on our\nRoadmap\nand we welcome contributions to\nArch\n!\nWhether you're fixing bugs, adding new features, improving documentation, or creating tutorials, your help is much appreciated.\nPlease visit our\nContribution Guide\nfor more details",
        "今日の獲得スター数: 36",
        "累積スター数: 3,928"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/katanemo/archgw"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/gkd-kit/gkd",
      "title": "gkd-kit/gkd",
      "date": null,
      "executive_summary": [
        "基于无障碍，高级选择器，订阅规则的自定义屏幕点击 Android 应用 | An Android APP with custom screen tapping based on Accessibility, Advanced Selectors, and Subscription Rules",
        "---",
        "gkd\n基于\n高级选择器\n+\n订阅规则\n+\n快照审查\n的自定义屏幕点击 Android 应用\n通过自定义规则，在指定界面，满足指定条件(如屏幕上存在特定文字)时，点击特定的节点或位置或执行其他操作\n快捷操作\n帮助你简化一些重复的流程, 如某些软件自动确认电脑登录\n跳过流程\n某些软件可能在启动时存在一些烦人的流程, 这个软件可以帮助你点击跳过这个流程\n免责声明\n本项目遵循\nGPL-3.0\n开源，项目仅供学习交流，禁止用于商业或非法用途\n安装\n如遇问题请先查看\n疑难解答\n截图\n订阅\nGKD\n默认不提供规则\n，需自行添加本地规则，或者通过订阅链接的方式获取远程规则\n也可通过\nsubscription-template\n快速构建自己的远程订阅\n第三方订阅列表可在\nhttps://github.com/topics/gkd-subscription\n查看\n要加入此列表, 需点击仓库主页右上角设置图标后在 Topics 中添加\ngkd-subscription\n示例图片 - 添加至 Topics (点击展开)\n选择器\n一个类似 CSS 选择器的选择器, 能联系节点上下文信息, 更容易也更精确找到目标节点\nhttps://gkd.li/guide/selector\n@[vid=\"menu\"] < [vid=\"menu_container\"] - [vid=\"dot_text_layout\"] > [text^=\"广告\"]\n示例图片 - 选择器路径视图 (点击展开)\n捐赠\n如果 GKD 对你有用, 可以通过以下链接支持该项目\nhttps://github.com/lisonge/sponsor\n或前往\nGoogle Play\n给个好评\nStar History",
        "今日の獲得スター数: 35",
        "累積スター数: 31,388"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/gkd-kit/gkd"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/keycloak/keycloak",
      "title": "keycloak/keycloak",
      "date": null,
      "executive_summary": [
        "Open Source Identity and Access Management For Modern Applications and Services",
        "---",
        "Open Source Identity and Access Management\nAdd authentication to applications and secure services with minimum effort. No need to deal with storing users or authenticating users.\nKeycloak provides user federation, strong authentication, user management, fine-grained authorization, and more.\nHelp and Documentation\nDocumentation\nUser Mailing List\n- Mailing list for help and general questions about Keycloak\nJoin\n#keycloak\nfor general questions, or\n#keycloak-dev\non Slack for design and development discussions, by creating an account at\nhttps://slack.cncf.io/\n.\nReporting Security Vulnerabilities\nIf you have found a security vulnerability, please look at the\ninstructions on how to properly report it\n.\nReporting an issue\nIf you believe you have discovered a defect in Keycloak, please open\nan issue\n.\nPlease remember to provide a good summary, description as well as steps to reproduce the issue.\nGetting started\nTo run Keycloak, download the distribution from our\nwebsite\n. Unzip and run:\nbin/kc.[sh|bat] start-dev\nAlternatively, you can use the Docker image by running:\ndocker run quay.io/keycloak/keycloak start-dev\nFor more details refer to the\nKeycloak Documentation\n.\nBuilding from Source\nTo build from source, refer to the\nbuilding and working with the code base\nguide.\nTesting\nTo run tests, refer to the\nrunning tests\nguide.\nWriting Tests\nTo write tests, refer to the\nwriting tests\nguide.\nContributing\nBefore contributing to Keycloak, please read our\ncontributing guidelines\n. Participation in the Keycloak project is governed by the\nCNCF Code of Conduct\n.\nJoining a\ncommunity meeting\nis a great way to get involved and help shape the future of Keycloak.\nOther Keycloak Projects\nKeycloak\n- Keycloak Server and Java adapters\nKeycloak QuickStarts\n- QuickStarts for getting started with Keycloak\nKeycloak Node.js Connect\n- Node.js adapter for Keycloak\nLicense\nApache License, Version 2.0",
        "今日の獲得スター数: 35",
        "累積スター数: 30,074"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/keycloak/keycloak"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/dataease/SQLBot",
      "title": "dataease/SQLBot",
      "date": null,
      "executive_summary": [
        "🔥 基于大模型和 RAG 的智能问数系统。Text-to-SQL Generation via LLMs using RAG.",
        "---",
        "基于大模型和 RAG 的智能问数系统\nSQLBot 是一款基于大模型和 RAG 的智能问数系统。SQLBot 的优势包括：\n开箱即用\n: 只需配置大模型和数据源即可开启问数之旅，通过大模型和 RAG 的结合来实现高质量的 text2sql；\n易于集成\n: 支持快速嵌入到第三方业务系统，也支持被 n8n、MaxKB、Dify、Coze 等 AI 应用开发平台集成调用，让各类应用快速拥有智能问数能力；\n安全可控\n: 提供基于工作空间的资源隔离机制，能够实现细粒度的数据权限控制。\n工作原理\n快速开始\n安装部署\n准备一台 Linux 服务器，安装好\nDocker\n，执行以下一键安装脚本：\ndocker run -d \\\n  --name sqlbot \\\n  --restart unless-stopped \\\n  -p 8000:8000 \\\n  -p 8001:8001 \\\n  -v ./data/sqlbot/excel:/opt/sqlbot/data/excel \\\n  -v ./data/sqlbot/file:/opt/sqlbot/data/file \\\n  -v ./data/sqlbot/images:/opt/sqlbot/images \\\n  -v ./data/sqlbot/logs:/opt/sqlbot/app/logs \\\n  -v ./data/postgresql:/var/lib/postgresql/data \\\n  --privileged=true \\\n  dataease/sqlbot\n你也可以通过\n1Panel 应用商店\n快速部署 SQLBot。\n如果是内网环境，你可以通过\n离线安装包方式\n部署 SQLBot。\n访问方式\n在浏览器中打开: http://<你的服务器IP>:8000/\n用户名: admin\n密码: SQLBot@123456\n联系我们\n如你有更多问题，可以加入我们的技术交流群与我们交流。\nUI 展示\nStar History\n飞致云旗下的其他明星项目\nDataEase\n- 人人可用的开源 BI 工具\n1Panel\n- 现代化、开源的 Linux 服务器运维管理面板\nMaxKB\n- 强大易用的企业级智能体平台\nJumpServer\n- 广受欢迎的开源堡垒机\nCordys CRM\n- 新一代的开源 AI CRM 系统\nHalo\n- 强大易用的开源建站工具\nMeterSphere\n- 新一代的开源持续测试工具\nLicense\n本仓库遵循\nFIT2CLOUD Open Source License\n开源协议，该许可证本质上是 GPLv3，但有一些额外的限制。\n你可以基于 SQLBot 的源代码进行二次开发，但是需要遵守以下规定：\n不能替换和修改 SQLBot 的 Logo 和版权信息；\n二次开发后的衍生作品必须遵守 GPL V3 的开源义务。\n如需商业授权，请联系\nsupport@fit2cloud.com\n。",
        "今日の獲得スター数: 33",
        "累積スター数: 3,711"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/dataease/SQLBot"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/intruder-io/autoswagger",
      "title": "intruder-io/autoswagger",
      "date": null,
      "executive_summary": [
        "Autoswagger by Intruder - detect API auth weaknesses",
        "---",
        "Autoswagger\nby\nIntruder\nAutoswagger\nis a command-line tool designed to discover, parse, and test for unauthenticated endpoints using\nSwagger/OpenAPI\ndocumentation. It helps identify potential security issues in unprotected endpoints of APIs, such as PII leaks and common secret exposures.\nPlease note that this initial release of Autoswagger is by no means complete, and there are some types of specification which the tool does not currently handle. Please feel free to use it as you wish, and extend its detection capabilities or add detection regexes to cover your specific use-case!\nTable of Contents\nIntroduction\nKey Features\nInstallation & Usage\nDiscovery Phases\nEndpoint Testing\nPII Detection\nOutput Examples\nStats & Reporting\nAcknowledgments\nIntroduction\nAutoswagger automates the process of finding\nOpenAPI/Swagger\nspecifications, extracting API endpoints, and systematically testing them for\nPII\nexposure,\nsecrets\n, and large or interesting responses. It leverages\nPresidio\nfor PII recognition and\nregex\nfor sensitive key/token detection.\nKey Features\nMultiple Discovery Phases\nDiscovers OpenAPI specs in three ways:\nDirect Spec\n: If a full URL with a path ending in\n.json\n,\n.yaml\n, or\n.yml\nis provided, parse that file directly.\nSwagger UI\n: Parse known paths of Swagger UI (e.g.\n/swagger-ui.html\n), and extract spec from HTML or JavaScript.\nDirect Spec by Bruteforce\n: Attempt discovery using common OpenAPI schema locations (\n/swagger.json\n,\n/openapi.json\n, etc.). Only attempt this if 1. and 2. did not yield a result.\nParallel Endpoint Testing\nMulti-threaded concurrent testing of many endpoints, respecting a configurable rate limit (\n-rate\n).\nBrute-Force of Parameter Values\nIf\n-b\nor\n--brute\nis used, try using various data types with a few example values in an attempt to bypass parameter-specific validations.\nPresidio PII Detection\nCheck output for phone numbers, emails, addresses, and names (with context validation to reduce false positives). Also parse CSV rows and naive “key: value” lines.\nSecrets Detection\nLeverages a set of regex patterns to detect tokens, keys, and debugging artifacts (like environment variables).\nCommand Line or JSON Output\nIn default mode, displays results in a table. With\n-json\n, output a JSON structure.\n-product\nmode filters output to only show those that contain PII, secrets, or large responses.\nInstallation & Usage\nClone\nor\ndownload\nthe repository containing Autoswagger.\ngit clone git@github.com:intruder-io/autoswagger.git\nInstall dependencies\n(e.g., using Python 3.7+):\npip install -r requirements.txt\n(It's recommended to use a virtual environment for this:\npython3 -m venv venv;source venv/bin/activate\n)\nCheck installation, show help:\npython3 autoswagger.py -h\nFlags\nFlag\nDescription\nurls\nList of base URLs or direct spec URLs.\n-v, --verbose\nEnables verbose logging. Creates a log file under\n~/.autoswagger/logs\n.\n-risk\nIncludes non-GET methods (POST, PUT, PATCH, DELETE) in testing.\n-all\nIncludes 200 and 404 endpoints in output (excludes 401/403).\n-product\nOutputs only endpoints with PII or large responses, in JSON format.\n-stats\nDisplays scan statistics (e.g. requests, RPS, hosts with PII).\n-rate <N>\nThrottles requests to N requests per second. Default is 30. Use 0 to disable rate limiting.\n-b, --brute\nEnables brute-forcing of parameter values (multiple test combos).\n-json\nOutputs results in JSON format instead of a Rich table in default mode.\nHelp\n/   | __  __/ /_____  ______      ______ _____ _____ ____  _____\n     / /| |/ / / / __/ __ \\/ ___/ | /| / / __ `/ __ `/ __ `/ _ \\/ ___/\n    / ___ / /_/ / /_/ /_/ (__  )| |/ |/ / /_/ / /_/ / /_/ /  __/ /\n    /_/  |_\\__,_/\\__/\\____/____/ |__/|__/_\\__,_/\\__, /\\__, /\\___/_/\n                                              /____//____/\n                              https://intruder.io\n                          Find unauthenticated endpoints\n\nusage: autoswagger.py [-h] [-v] [-risk] [-all] [-product] [-stats] [-rate RATE] [-b] [-json] [urls ...]\n\nAutoswagger: Detect unauthenticated access control issues via Swagger/OpenAPI documentation.\n\npositional arguments:\n  urls           Base URL(s) or spec URL(s) of the target API(s)\n\noptions:\n  -h, --help     show this help message and exit\n  -v, --verbose  Enable verbose output\n  -risk          Include non-GET requests in testing\n  -all           Include all HTTP status codes in the results, excluding 401 and 403\n  -product       Output all endpoints in JSON, flagging those that contain PII or have large responses.\n  -stats         Display scan statistics. Included in JSON if -product or -json is used.\n  -rate RATE     Set the rate limit in requests per second (default: 30). Use 0 to disable rate limiting.\n  -b, --brute    Enable exhaustive testing of parameter values.\n  -json          Output results in JSON format in default mode.\n\nExample usage:\n  python autoswagger.py https://api.example.com -v\nDiscovery Phases\nDirect Spec\nIf a provided URL ends with\n.json/.yaml/.yml\n, Autoswagger\ndirectly\nattempts to parse the OpenAPI schema.\nSwagger-UI Detection\nTries known UI paths (e.g.,\n/swagger-ui.html\n).\nIf found, parses the HTML or local JavaScript files for a\nswagger.json\nor\nopenapi.json\n.\nCan detect embedded configs like\nwindow.swashbuckleConfig\n.\nDirect Spec by Bruteforce\nIf no spec is found so far, Autoswagger attempts a list of default endpoints like\n/swagger.json\n,\n/openapi.json\n, etc.\nStops when a valid spec is discovered or none are found.\nEndpoint Testing\nCollect Endpoints\nAfter loading a spec, Autoswagger extracts each path and method under the\npaths\nkey.\nHTTP Methods\nBy default, tests\nGET\nonly.\nUse\n-risk\nto include other methods (\nPOST\n,\nPUT\n,\nPATCH\n,\nDELETE\n).\nParameter Values\nFill path/query parameters with defaults or values to enumerate.\nOptionally builds request bodies from the spec’s\nrequestBody\n(OpenAPI 3) or body parameters (Swagger 2).\nRate Limiting & Concurrency\nSupports threading with a cap on requests per second (\n-rate\n).\nEach endpoint is tested in a dedicated job.\nResponse Analysis\nDecodes responses, checks for PII, secrets, and large content.\nLogs relevant findings.\nPII Detection\nPresidio-Based Analysis\nSearches for phone numbers, emails, addresses, names.\nContext-based scanning (e.g., CSV headers, key-value lines).\nSecrets & Debug Info\nTruffleHog-like regex checks for API keys, tokens, environment variables.\nMerges any matches into the PII data structure for final reporting.\nLarge Response Check\nFlags responses with 100+ JSON elements or large XML structures as “interesting.”\nAlso checks raw size threshold (e.g., >100k bytes).\nOutput\nBy default, output is shown in a table.\n-json\nproduces JSON objects, grouping results by endpoint.\n-product\nfilters down to only “interesting” endpoints (PII, large responses and responses with secrets).\nInterpreting Results\nFor most use cases, interpreting results involves looking at the output (endpoints resulting in Status Code 200s), and paying particular attention to endpoints which are marked as 'PII or Secret Detected'. These endpoints are the ones that contain impactful exposures, but they should be manually checked to confirm. You may also wish to look at other 200s that do not contain PII, and determine whether it's intended for these endpoints to be public or not.\nSimple GET endpoints can be triaged using command line tools like curl, but we would recommend using your usual API testing suite (tools such as Postman or Burp Suite) to replay requests and read responses to confirm whether an exposure is present.\nStats & Reporting\n-stats\nappends or prints overall statistics, such as:\nHosts with valid specs\nHosts with PII\nTotal requests sent, average RPS\nPercentage of endpoints responding with 2xx or 4xx\nShown in either a Rich table in default mode or embedded in JSON if\n-json\nor\n-product\nis used.\nAcknowledgments\nAutoswagger is maintained and owned by\nIntruder\n. It was primarily developed by Cale Anderson",
        "今日の獲得スター数: 32",
        "累積スター数: 1,473"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/intruder-io/autoswagger"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/microsoft/edit",
      "title": "microsoft/edit",
      "date": null,
      "executive_summary": [
        "We all edit.",
        "---",
        "Edit\nA simple editor for simple needs.\nThis editor pays homage to the classic\nMS-DOS Editor\n, but with a modern interface and input controls similar to VS Code. The goal is to provide an accessible editor that even users largely unfamiliar with terminals can easily use.\nInstallation\nYou can also download binaries from\nour Releases page\n.\nWindows\nYou can install the latest version with WinGet:\nwinget install Microsoft.Edit\nBuild Instructions\nInstall Rust\nInstall the nightly toolchain:\nrustup install nightly\nAlternatively, set the environment variable\nRUSTC_BOOTSTRAP=1\nClone the repository\nFor a release build, run:\ncargo build --config .cargo/release.toml --release\nBuild Configuration\nDuring compilation you can set various environment variables to configure the build. The following table lists the available configuration options:\nEnvironment variable\nDescription\nEDIT_CFG_ICU*\nSee\nICU library name (SONAME)\nfor details.\nEDIT_CFG_LANGUAGES\nA comma-separated list of languages to include in the build. See\ni18n/edit.toml\nfor available languages.\nNotes to Package Maintainers\nPackage Naming\nThe canonical executable name is \"edit\" and the alternative name is \"msedit\".\nWe're aware of the potential conflict of \"edit\" with existing commands and recommend alternatively naming packages and executables \"msedit\".\nNames such as \"ms-edit\" should be avoided.\nAssigning an \"edit\" alias is recommended, if possible.\nICU library name (SONAME)\nThis project\noptionally\ndepends on the ICU library for its Search and Replace functionality.\nBy default, the project will look for a SONAME without version suffix:\nWindows:\nicuuc.dll\nmacOS:\nlibicuuc.dylib\nUNIX, and other OS:\nlibicuuc.so\nIf your installation uses a different SONAME, please set the following environment variable at build time:\nEDIT_CFG_ICUUC_SONAME\n:\nFor instance,\nlibicuuc.so.76\n.\nEDIT_CFG_ICUI18N_SONAME\n:\nFor instance,\nlibicui18n.so.76\n.\nAdditionally, this project assumes that the ICU exports are exported without\n_\nprefix and without version suffix, such as\nu_errorName\n.\nIf your installation uses versioned exports, please set:\nEDIT_CFG_ICU_CPP_EXPORTS\n:\nIf set to\ntrue\n, it'll look for C++ symbols such as\n_u_errorName\n.\nEnabled by default on macOS.\nEDIT_CFG_ICU_RENAMING_VERSION\n:\nIf set to a version number, such as\n76\n, it'll look for symbols such as\nu_errorName_76\n.\nFinally, you can set the following environment variables:\nEDIT_CFG_ICU_RENAMING_AUTO_DETECT\n:\nIf set to\ntrue\n, the executable will try to detect the\nEDIT_CFG_ICU_RENAMING_VERSION\nvalue at runtime.\nThe way it does this is not officially supported by ICU and as such is not recommended to be relied upon.\nEnabled by default on UNIX (excluding macOS) if no other options are set.\nTo test your settings, run\ncargo test\nagain but with the\n--ignored\nflag. For instance:\ncargo\ntest\n-- --ignored",
        "今日の獲得スター数: 31",
        "累積スター数: 12,384"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/microsoft/edit"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    },
    {
      "url": "https://github.com/YunaiV/ruoyi-vue-pro",
      "title": "YunaiV/ruoyi-vue-pro",
      "date": null,
      "executive_summary": [
        "🔥 官方推荐 🔥 RuoYi-Vue 全新 Pro 版本，优化重构所有功能。基于 Spring Boot + MyBatis Plus + Vue & Element 实现的后台管理系统 + 微信小程序，支持 RBAC 动态权限、数据权限、SaaS 多租户、Flowable 工作流、三方登录、支付、短信、商城、CRM、ERP、AI 大模型等功能。你的 ⭐️ Star ⭐️，是作者生发的动力！",
        "---",
        "严肃声明：现在、未来都不会有商业版本，所有代码全部开源!！\n「我喜欢写代码，乐此不疲」\n「我喜欢做开源，以此为乐」\n我 🐶 在上海艰苦奋斗，早中晚在 top3 大厂认真搬砖，夜里为开源做贡献。\n如果这个项目让你有所收获，记得 Star 关注哦，这对我是非常不错的鼓励与支持。\n🐶 新手必读\n演示地址【Vue3 + element-plus】：\nhttp://dashboard-vue3.yudao.iocoder.cn\n演示地址【Vue3 + vben(ant-design-vue)】：\nhttp://dashboard-vben.yudao.iocoder.cn\n演示地址【Vue2 + element-ui】：\nhttp://dashboard.yudao.iocoder.cn\n启动文档：\nhttps://doc.iocoder.cn/quick-start/\n视频教程：\nhttps://doc.iocoder.cn/video/\n🐰 版本说明\n版本\nJDK 8 + Spring Boot 2.7\nJDK 17/21 + Spring Boot 3.2\n【完整版】\nruoyi-vue-pro\nmaster\n分支\nmaster-jdk17\n分支\n【精简版】\nyudao-boot-mini\nmaster\n分支\nmaster-jdk17\n分支\n【完整版】：包括系统功能、基础设施、会员中心、数据报表、工作流程、商城系统、微信公众号、CRM、ERP 等功能\n【精简版】：只包括系统功能、基础设施功能，不包括会员中心、数据报表、工作流程、商城系统、微信公众号、CRM、ERP 等功能\n可参考\n《迁移文档》\n，只需要 5-10 分钟，即可将【完整版】按需迁移到【精简版】\n🐯 平台简介\n芋道\n，以开发者为中心，打造中国第一流的快速开发平台，全部开源，个人与企业可 100% 免费使用。\n有任何问题，或者想要的功能，可以在\nIssues\n中提给艿艿。\n😜 给项目点点 Star 吧，这对我们真的很重要！\nJava 后端：\nmaster\n分支为 JDK 8 + Spring Boot 2.7，\nmaster-jdk17\n分支为 JDK 17/21 + Spring Boot 3.2\n管理后台的电脑端：Vue3 提供\nelement-plus\n、\nvben(ant-design-vue)\n两个版本，Vue2 提供\nelement-ui\n版本\n管理后台的移动端：采用\nuni-app\n方案，一份代码多终端适配，同时支持 APP、小程序、H5！\n后端采用 Spring Boot 多模块架构、MySQL + MyBatis Plus、Redis + Redisson\n数据库可使用 MySQL、Oracle、PostgreSQL、SQL Server、MariaDB、国产达梦 DM、TiDB 等\n消息队列可使用 Event、Redis、RabbitMQ、Kafka、RocketMQ 等\n权限认证使用 Spring Security & Token & Redis，支持多终端、多种用户的认证系统，支持 SSO 单点登录\n支持加载动态权限菜单，按钮级别权限控制，Redis 缓存提升性能\n支持 SaaS 多租户，可自定义每个租户的权限，提供透明化的多租户底层封装\n工作流使用 Flowable，支持动态表单、在线设计流程、会签 / 或签、多种任务分配方式\n高效率开发，使用代码生成器可以一键生成 Java、Vue 前后端代码、SQL 脚本、接口文档，支持单表、树表、主子表\n实时通信，采用 Spring WebSocket 实现，内置 Token 身份校验，支持 WebSocket 集群\n集成微信小程序、微信公众号、企业微信、钉钉等三方登陆，集成支付宝、微信等支付与退款\n集成阿里云、腾讯云等短信渠道，集成 MinIO、阿里云、腾讯云、七牛云等云存储服务\n集成报表设计器、大屏设计器，通过拖拽即可生成酷炫的报表与大屏\n🐳 项目关系\n三个项目的功能对比，可见社区共同整理的\n国产开源项目对比\n表格。\n后端项目\n项目\nStar\n简介\nruoyi-vue-pro\n基于 Spring Boot 多模块架构\nyudao-cloud\n基于 Spring Cloud 微服务架构\nSpring-Boot-Labs\n系统学习 Spring Boot & Cloud 专栏\n前端项目\n项目\nStar\n简介\nyudao-ui-admin-vue3\n基于 Vue3 + element-plus 实现的管理后台\nyudao-ui-admin-vben\n基于 Vue3 + vben(ant-design-vue) 实现的管理后台\nyudao-mall-uniapp\n基于 uni-app 实现的商城小程序\nyudao-ui-admin-vue2\n基于 Vue2 + element-ui 实现的管理后台\nyudao-ui-admin-uniapp\n基于 Vue2 + element-ui 实现的管理后台\nyudao-ui-go-view\n基于 Vue3 + naive-ui 实现的大屏报表\n😎 开源协议\n为什么推荐使用本项目？\n① 本项目采用比 Apache 2.0 更宽松的\nMIT License\n开源协议，个人与企业可 100% 免费使用，不用保留类作者、Copyright 信息。\n② 代码全部开源，不会像其他项目一样，只开源部分代码，让你无法了解整个项目的架构设计。\n国产开源项目对比\n③ 代码整洁、架构整洁，遵循《阿里巴巴 Java 开发手册》规范，代码注释详细，113770 行 Java 代码，42462 行代码注释。\n🤝 项目外包\n我们也是接外包滴，如果你有项目想要外包，可以微信联系【\nAix9975\n】。\n团队包含专业的项目经理、架构师、前端工程师、后端工程师、测试工程师、运维工程师，可以提供全流程的外包服务。\n项目可以是商城、SCRM 系统、OA 系统、物流系统、ERP 系统、CMS 系统、HIS 系统、支付系统、IM 聊天、微信公众号、微信小程序等等。\n🐼 内置功能\n系统内置多种多种业务功能，可以用于快速你的业务系统：\n通用模块（必选）：系统功能、基础设施\n通用模块（可选）：工作流程、支付系统、数据报表、会员中心\n业务系统（按需）：ERP 系统、CRM 系统、商城系统、微信公众号、AI 大模型\n友情提示：本项目基于 RuoYi-Vue 修改，\n重构优化\n后端的代码，\n美化\n前端的界面。\n额外新增的功能，我们使用 🚀 标记。\n重新实现的功能，我们使用 ⭐️ 标记。\n🙂 所有功能，都通过\n单元测试\n保证高质量。\n系统功能\n功能\n描述\n用户管理\n用户是系统操作者，该功能主要完成系统用户配置\n⭐️\n在线用户\n当前系统中活跃用户状态监控，支持手动踢下线\n角色管理\n角色菜单权限分配、设置角色按机构进行数据范围权限划分\n菜单管理\n配置系统菜单、操作权限、按钮权限标识等，本地缓存提供性能\n部门管理\n配置系统组织机构（公司、部门、小组），树结构展现支持数据权限\n岗位管理\n配置系统用户所属担任职务\n🚀\n租户管理\n配置系统租户，支持 SaaS 场景下的多租户功能\n🚀\n租户套餐\n配置租户套餐，自定每个租户的菜单、操作、按钮的权限\n字典管理\n对系统中经常使用的一些较为固定的数据进行维护\n🚀\n短信管理\n短信渠道、短息模板、短信日志，对接阿里云、腾讯云等主流短信平台\n🚀\n邮件管理\n邮箱账号、邮件模版、邮件发送日志，支持所有邮件平台\n🚀\n站内信\n系统内的消息通知，提供站内信模版、站内信消息\n🚀\n操作日志\n系统正常操作日志记录和查询，集成 Swagger 生成日志内容\n⭐️\n登录日志\n系统登录日志记录查询，包含登录异常\n🚀\n错误码管理\n系统所有错误码的管理，可在线修改错误提示，无需重启服务\n通知公告\n系统通知公告信息发布维护\n🚀\n敏感词\n配置系统敏感词，支持标签分组\n🚀\n应用管理\n管理 SSO 单点登录的应用，支持多种 OAuth2 授权方式\n🚀\n地区管理\n展示省份、城市、区镇等城市信息，支持 IP 对应城市\n工作流程\n基于 Flowable 构建，可支持信创（国产）数据库，满足中国特色流程操作：\nBPMN 设计器\n钉钉/飞书设计器\n历经头部企业生产验证，工作流引擎须标配仿钉钉/飞书 + BPMN 双设计器！！！\n前者支持轻量配置简单流程，后者实现复杂场景深度编排\n功能列表\n功能描述\n是否完成\nSIMPLE 设计器\n仿钉钉/飞书设计器，支持拖拽搭建表单流程，10 分钟快速完成审批流程配置\n✅\nBPMN 设计器\n基于 BPMN 标准开发，适配复杂业务场景，满足多层级审批及流程自动化需求\n✅\n会签\n同一个审批节点设置多个人（如 A、B、C 三人，三人会同时收到待办任务），需全部同意之后，审批才可到下一审批节点\n✅\n或签\n同一个审批节点设置多个人，任意一个人处理后，就能进入下一个节点\n✅\n依次审批\n（顺序会签）同一个审批节点设置多个人（如 A、B、C 三人），三人按顺序依次收到待办，即 A 先审批，A 提交后 B 才能审批，需全部同意之后，审批才可到下一审批节点\n✅\n抄送\n将审批结果通知给抄送人，同一个审批默认排重，不重复抄送给同一人\n✅\n驳回\n（退回）将审批重置发送给某节点，重新审批。可驳回至发起人、上一节点、任意节点\n✅\n转办\nA 转给其 B 审批，B 审批后，进入下一节点\n✅\n委派\nA 转给其 B 审批，B 审批后，转给 A，A 继续审批后进入下一节点\n✅\n加签\n允许当前审批人根据需要，自行增加当前节点的审批人，支持向前、向后加签\n✅\n减签\n（取消加签）在当前审批人操作之前，减少审批人\n✅\n撤销\n（取消流程）流程发起人，可以对流程进行撤销处理\n✅\n终止\n系统管理员，在任意节点终止流程实例\n✅\n表单权限\n支持拖拉拽配置表单，每个审批节点可配置只读、编辑、隐藏权限\n✅\n超时审批\n配置超时审批时间，超时后自动触发审批通过、不通过、驳回等操作\n✅\n自动提醒\n配置提醒时间，到达时间后自动触发短信、邮箱、站内信等通知提醒，支持自定义重复提醒频次\n✅\n父子流程\n主流程设置子流程节点，子流程节点会自动触发子流程。子流程结束后，主流程才会执行（继续往下下执行），支持同步子流程、异步子流程\n✅\n条件分支\n（排它分支）用于在流程中实现决策，即根据条件选择一个分支执行\n✅\n并行分支\n允许将流程分成多条分支，不进行条件判断，所有分支都会执行\n✅\n包容分支\n（条件分支 + 并行分支的结合体）允许基于条件选择多条分支执行，但如果没有任何一个分支满足条件，则可以选择默认分支\n✅\n路由分支\n根据条件选择一个分支执行（重定向到指定配置节点），也可以选择默认分支执行（继续往下执行）\n✅\n触发节点\n执行到该节点，触发 HTTP 请求、HTTP 回调、更新数据、删除数据等\n✅\n延迟节点\n执行到该节点，审批等待一段时间再执行，支持固定时长、固定日期等\n✅\n拓展设置\n流程前置/后置通知，节点（任务）前置、后置通知，流程报表，自动审批去重，自定流程编号、标题、摘要，流程报表等\n✅\n支付系统\n功能\n描述\n🚀\n应用信息\n配置商户的应用信息，对接支付宝、微信等多个支付渠道\n🚀\n支付订单\n查看用户发起的支付宝、微信等的【支付】订单\n🚀\n退款订单\n查看用户发起的支付宝、微信等的【退款】订单\n🚀\n回调通知\n查看支付回调业务的【支付】【退款】的通知结果\n🚀\n接入示例\n提供接入支付系统的【支付】【退款】的功能实战\n基础设施\n功能\n描述\n🚀\n代码生成\n前后端代码的生成（Java、Vue、SQL、单元测试），支持 CRUD 下载\n🚀\n系统接口\n基于 Swagger 自动生成相关的 RESTful API 接口文档\n🚀\n数据库文档\n基于 Screw 自动生成数据库文档，支持导出 Word、HTML、MD 格式\n表单构建\n拖动表单元素生成相应的 HTML 代码，支持导出 JSON、Vue 文件\n🚀\n配置管理\n对系统动态配置常用参数，支持 SpringBoot 加载\n⭐️\n定时任务\n在线（添加、修改、删除)任务调度包含执行结果日志\n🚀\n文件服务\n支持将文件存储到 S3（MinIO、阿里云、腾讯云、七牛云）、本地、FTP、数据库等\n🚀\nWebSocket\n提供 WebSocket 接入示例，支持一对一、一对多发送方式\n🚀\nAPI 日志\n包括 RESTful API 访问日志、异常日志两部分，方便排查 API 相关的问题\nMySQL 监控\n监视当前系统数据库连接池状态，可进行分析SQL找出系统性能瓶颈\nRedis 监控\n监控 Redis 数据库的使用情况，使用的 Redis Key 管理\n🚀\n消息队列\n基于 Redis 实现消息队列，Stream 提供集群消费，Pub/Sub 提供广播消费\n🚀\nJava 监控\n基于 Spring Boot Admin 实现 Java 应用的监控\n🚀\n链路追踪\n接入 SkyWalking 组件，实现链路追踪\n🚀\n日志中心\n接入 SkyWalking 组件，实现日志中心\n🚀\n服务保障\n基于 Redis 实现分布式锁、幂等、限流功能，满足高并发场景\n🚀\n日志服务\n轻量级日志中心，查看远程服务器的日志\n🚀\n单元测试\n基于 JUnit + Mockito 实现单元测试，保证功能的正确性、代码的质量等\n数据报表\n功能\n描述\n🚀\n报表设计器\n支持数据报表、图形报表、打印设计等\n🚀\n大屏设计器\n拖拽生成数据大屏，内置几十种图表组件\n微信公众号\n功能\n描述\n🚀\n账号管理\n配置接入的微信公众号，可支持多个公众号\n🚀\n数据统计\n统计公众号的用户增减、累计用户、消息概况、接口分析等数据\n🚀\n粉丝管理\n查看已关注、取关的粉丝列表，可对粉丝进行同步、打标签等操作\n🚀\n消息管理\n查看粉丝发送的消息列表，可主动回复粉丝消息\n🚀\n自动回复\n自动回复粉丝发送的消息，支持关注回复、消息回复、关键字回复\n🚀\n标签管理\n对公众号的标签进行创建、查询、修改、删除等操作\n🚀\n菜单管理\n自定义公众号的菜单，也可以从公众号同步菜单\n🚀\n素材管理\n管理公众号的图片、语音、视频等素材，支持在线播放语音、视频\n🚀\n图文草稿箱\n新增常用的图文素材到草稿箱，可发布到公众号\n🚀\n图文发表记录\n查看已发布成功的图文素材，支持删除操作\n商城系统\n演示地址：\nhttps://doc.iocoder.cn/mall-preview/\n会员中心\n功能\n描述\n🚀\n会员管理\n会员是 C 端的消费者，该功能用于会员的搜索与管理\n🚀\n会员标签\n对会员的标签进行创建、查询、修改、删除等操作\n🚀\n会员等级\n对会员的等级、成长值进行管理，可用于订单折扣等会员权益\n🚀\n会员分组\n对会员进行分组，用于用户画像、内容推送等运营手段\n🚀\n积分签到\n回馈给签到、消费等行为的积分，会员可订单抵现、积分兑换等途径消耗\nERP 系统\n演示地址：\nhttps://doc.iocoder.cn/erp-preview/\nCRM 系统\n演示地址：\nhttps://doc.iocoder.cn/crm-preview/\nAI 大模型\n演示地址：\nhttps://doc.iocoder.cn/ai-preview/\n🐨 技术栈\n模块\n项目\n说明\nyudao-dependencies\nMaven 依赖版本管理\nyudao-framework\nJava 框架拓展\nyudao-server\n管理后台 + 用户 APP 的服务端\nyudao-module-system\n系统功能的 Module 模块\nyudao-module-member\n会员中心的 Module 模块\nyudao-module-infra\n基础设施的 Module 模块\nyudao-module-bpm\n工作流程的 Module 模块\nyudao-module-pay\n支付系统的 Module 模块\nyudao-module-mall\n商城系统的 Module 模块\nyudao-module-erp\nERP 系统的 Module 模块\nyudao-module-crm\nCRM 系统的 Module 模块\nyudao-module-ai\nAI 大模型的 Module 模块\nyudao-module-mp\n微信公众号的 Module 模块\nyudao-module-report\n大屏报表 Module 模块\n框架\n框架\n说明\n版本\n学习指南\nSpring Boot\n应用开发框架\n2.7.18\n文档\nMySQL\n数据库服务器\n5.7 / 8.0+\nDruid\nJDBC 连接池、监控组件\n1.2.23\n文档\nMyBatis Plus\nMyBatis 增强工具包\n3.5.7\n文档\nDynamic Datasource\n动态数据源\n3.6.1\n文档\nRedis\nkey-value 数据库\n5.0 / 6.0 /7.0\nRedisson\nRedis 客户端\n3.32.0\n文档\nSpring MVC\nMVC 框架\n5.3.24\n文档\nSpring Security\nSpring 安全框架\n5.7.11\n文档\nHibernate Validator\n参数校验组件\n6.2.5\n文档\nFlowable\n工作流引擎\n6.8.0\n文档\nQuartz\n任务调度组件\n2.3.2\n文档\nSpringdoc\nSwagger 文档\n1.7.0\n文档\nSkyWalking\n分布式应用追踪系统\n8.12.0\n文档\nSpring Boot Admin\nSpring Boot 监控平台\n2.7.10\n文档\nJackson\nJSON 工具库\n2.13.5\nMapStruct\nJava Bean 转换\n1.6.3\n文档\nLombok\n消除冗长的 Java 代码\n1.18.34\n文档\nJUnit\nJava 单元测试框架\n5.8.2\n-\nMockito\nJava Mock 框架\n4.8.0\n-\n🐷 演示图\n系统功能\n模块\nbiu\nbiu\nbiu\n登录 & 首页\n用户 & 应用\n租户 & 套餐\n-\n部门 & 岗位\n-\n菜单 & 角色\n-\n审计日志\n-\n短信\n字典 & 敏感词\n错误码 & 通知\n-\n工作流程\n模块\nbiu\nbiu\nbiu\n流程模型\n表单 & 分组\n-\n我的流程\n待办 & 已办\nOA 请假\n基础设施\n模块\nbiu\nbiu\nbiu\n代码生成\n-\n文档\n-\n文件 & 配置\n定时任务\n-\nAPI 日志\n-\nMySQL & Redis\n-\n监控平台\n支付系统\n模块\nbiu\nbiu\nbiu\n商家 & 应用\n支付 & 退款\n---\n数据报表\n模块\nbiu\nbiu\nbiu\n报表设计器\n大屏设计器\n移动端（管理后台）\nbiu\nbiu\nbiu\n目前已经实现登录、我的、工作台、编辑资料、头像修改、密码修改、常见问题、关于我们等基础功能。",
        "今日の獲得スター数: 30",
        "累積スター数: 33,522"
      ],
      "key_findings": null,
      "references": [
        "https://github.com/YunaiV/ruoyi-vue-pro"
      ],
      "retrieved_at": "2025-10-11T02:07:31Z"
    }
  ]
}